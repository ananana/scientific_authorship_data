<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Road</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<addrLine>1101 Kitchawan Road</addrLine>
									<postCode>10598</postCode>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Multilingual Named Entity Recognition with Wikipedia Entity Type Mapping</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1275" to="1284"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The state-of-the-art named entity recognition (NER) systems are statistical machine learning models that have strong generalization capability (i.e., can recognize unseen entities that do not appear in training data) based on lexical and contextual information. However , such a model could still make mistakes if its features favor a wrong entity type. In this paper, we utilize Wikipedia as an open knowledge base to improve multilingual NER systems. Central to our approach is the construction of high-accuracy, high-coverage multilingual Wikipedia entity type mappings. These mappings are built from weakly annotated data and can be extended to new languages with no human annotation or language-dependent knowledge involved. Based on these mappings, we develop several approaches to improve an NER system. We evaluate the performance of the approaches via experiments on NER systems trained for 6 languages. Experimental results show that the proposed approaches are effective in improving the accuracy of such systems on unseen entities, especially when a system is applied to a new domain or it is trained with little training data (up to 18.3 F 1 score improvement).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER) is an important NLP task that automatically detects entities in text and classifies them into pre-defined entity types such as persons, organizations, geopolitical entities, lo- cations, events, etc. NER is a fundamental compo- nent of many information extraction and knowledge discovery applications, including relation extraction, entity linking, question answering and data mining.</p><p>The state-of-the-art NER systems are usually sta- tistical machine learning models that are trained with human-annotated data. Popular models in- clude maximum entropy Markov models (MEMM) ( <ref type="bibr" target="#b8">McCallum et al., 2000</ref>), conditional random fields (CRF) ( <ref type="bibr" target="#b6">Lafferty et al., 2001</ref>) and neural networks <ref type="bibr" target="#b0">(Collobert et al., 2011;</ref><ref type="bibr" target="#b7">Lample et al., 2016</ref>). Such models have strong generalization capability to rec- ognize unseen entities 1 based on lexical and contex- tual information (features). However, a model could still make mistakes if its features favor a wrong en- tity type, which happens more frequently for unseen entities as we have observed in our experiments.</p><p>Wikipedia is an open-access, free-content Inter- net encyclopedia, which has become the de facto on-line source for general reference. A Wikipedia page about an entity normally includes both struc- tured information and unstructured text information, and such information can be used to help determine the entity type of the referred entity.</p><p>So far there are two classes of approaches that exploit Wikipedia to improve NER. The first class of approaches use Wikipedia to generate features for NER systems, e.g., <ref type="bibr" target="#b5">(Kazama and Torisawa, 2007;</ref><ref type="bibr" target="#b11">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b10">Radford et al., 2015)</ref>. <ref type="bibr" target="#b5">Kazama and Torisawa (2007)</ref> try to find the Wikipedia entity for each candidate word sequence and then extract a category label from the first sen- tence of the Wikipedia entity page. A part-of-speech (POS) tagger is used to extract the category label features in the training and decoding phase. <ref type="bibr" target="#b11">Ratinov and Roth (2009)</ref> aggregate several Wikipedia cate- gories into higher-level concept and build a gazetteer on top of it. The two approaches were shown to be able to improve an English NER system. Both approaches, however, are language-dependent be- cause ( <ref type="bibr" target="#b5">Kazama and Torisawa, 2007</ref>) requires a POS tagger and ( <ref type="bibr" target="#b11">Ratinov and Roth, 2009</ref>) requires man- ual category aggregation by inspection of the anno- tation guidelines and the training set. <ref type="bibr" target="#b10">Radford et al. (2015)</ref> assume that document-specific knowledge base (e.g., Wikipedia) tags for each document are provided, and they use those tags to build gazetteer type features for improving an English NER system.</p><p>The second class of approaches use Wikipedia to generate weakly annotated data for training multi- lingual NER systems, e.g., <ref type="bibr" target="#b12">(Richman and Schone, 2008;</ref><ref type="bibr" target="#b10">Nothman et al., 2013</ref>). The motivation is that annotating multilingual NER data by human is both expensive and time-consuming. <ref type="bibr" target="#b12">Richman and Schone (2008)</ref> utilize the category informa- tion of Wikipedia to determine the entity type of an entity based on manually constructed rules (e.g., category phrase "Living People" is mapped to en- tity type PERSON). Such a rule-based entity type mapping is limited both in accuracy and cover- age, e.g., <ref type="bibr" target="#b16">(Toral and Muoz, 2006</ref>). <ref type="bibr" target="#b10">Nothman et al. (2013)</ref> train a Wikipedia entity type classifier using human-annotated Wikipedia pages. Such a supervised-learning based approach has better ac- curacy and coverage, e.g., <ref type="bibr" target="#b1">(Dakka and Cucerzan, 2008)</ref>. A number of heuristic rules are developed in both works to label the Wikipedia text to create weakly annotated NER training data. The NER sys- tems trained with the weakly annotated data may achieve similar accuracy compared with systems trained with little human-annotated data (e.g., up to 40K tokens as in ( <ref type="bibr" target="#b12">Richman and Schone, 2008)</ref>), but they are still significantly worse than well-trained systems (e.g., a drop of 23.9 F 1 score on the CoNLL data and a drop of 19.6 F 1 score on the BBN data as in <ref type="bibr" target="#b10">(Nothman et al., 2013)</ref>).</p><p>In this paper, we propose a new class of ap- proaches that utilize Wikipedia to improve multilin- gual NER systems. Central to our approaches is the construction of high-accuracy, high-coverage mul- tilingual Wikipedia entity type mappings. We use weakly annotated data to train an English Wikipedia entity type classifier, as opposed to using human- annotated data as in <ref type="bibr" target="#b1">(Dakka and Cucerzan, 2008;</ref><ref type="bibr" target="#b10">Nothman et al., 2013)</ref>. The accuracy of the classi- fier is further improved via self-training. We apply the classifier on all the English Wikipedia pages and construct an English Wikipedia entity type mapping that includes entities with high classification confi- dence scores. To build multilingual Wikipedia en- tity type mappings, we generate weakly annotated classifier training data for another language via pro- jection using the inter-language links of Wikipedia. This approach requires no human annotation or language-dependent knowledge, and thus can be easily applied to new languages.</p><p>Our goal is to utilize the Wikipedia entity type mappings to improve NER systems. A natural ap- proach is to use a mapping to create dictionary type features for training an NER system. In addition, we develop several other approaches. The first ap- proach applies an entity type mapping as a decod- ing constraint for an NER system. The second ap- proach uses a mapping to post-process the output of an NER system. We also design a robust joint approach that combines the decoding constraint ap- proach and the post-processing approach in a smart way. We evaluate the performance of the Wikipedia- based approaches on NER systems trained for 6 lan- guages. We find that when a system is well trained (e.g., with 200K to 300K tokens of human-annotated data), the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with little human-annotated training data (e.g., 20K to 30K tokens), a more ag- gressive decoding constraint approach achieves the best improvement. In both scenarios, the Wikipedia- based approaches are effective in improving the ac- curacy on unseen entities, especially when a system is applied to a new domain (3.6 F 1 score improve- ment on political party articles/English NER) or it is trained with little training data (18.3 F 1 score im- provement on Japanese NER).</p><p>We organize the paper as follows. We describe how to build English Wikipedia entity type mapping in Section 2 and extend it to multilingual mappings in Section 3. We present several Wikipedia-based approaches for improving NER systems in Section 4 and evaluate their performance in Section 5. We conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">English Wikipedia Entity Type Mapping</head><p>In this section, we focus on English Wikipedia. We divide Wikipedia pages into two types:</p><p>• Entity pages that describe an entity or object, either a named entity such as "Michael Jordan" or a common entity such as "Basketball."</p><p>• Non-entity pages that do not describe a certain entity, including disambiguation pages, redi- rection pages, list pages, etc.</p><p>We have developed an in-house English NER sys- tem ( <ref type="bibr" target="#b2">Florian et al., 2004</ref>). The system has 51 en- tity types, and the main motivation of deploying such a fine-grained entity type set is to build cog- nitive question answering applications on top of the NER system. An important check for a question an- swering system is the capability to detect whether a particular answer matches the expected type de- rived from the question. The entity type system used in this paper has been engineered to cover many of the frequent types that are targeted by naturally- phrased questions (such as PERSON, ORGANIZA- TION, GPE, TITLEWORK, FACILITY, EVENT, DATE, TIME, LOCATION, etc), and it was created over a long period of time, being updated as more types were found to be useful for question answer- ing, and to improve inter-annotator consistency.</p><p>We want to classify Wikipedia pages into one of the entity types used in the NER system. For non- entity pages and entity pages describing common entities, we assign them with a new type OTHER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Wikipedia Entity Type Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Features</head><p>We build maximum entropy classifiers <ref type="bibr" target="#b9">(Nigam et al., 1999</ref>) for Wikipedia entity type classification. We use both structured information and unstructured information of a Wikipedia page as features.</p><p>Each Wikipedia page has a unique title. The title of an entity page is usually the name of the entity, and may include auxiliary information in a bracket to distinguish entities with the same name. We use both the entity name and auxiliary information in a bracket (if any) of a Wikipedia title as features be- cause each could provide useful information for en- tity type classification. For example, based on the word "Prize" in the title "Nobel Prize" or the word "Awards" in the title "Academy Awards", one can infer that the entity type is AWARD. Likewise, the auxiliary information "company" in the title "Jordan (company)" indicates that the entity is an ORGA- NIZATION, and the auxiliary information "film" in the title "Alien (film)" indicates that the entity is a TITLEWORK.</p><p>The text in a Wikipedia page of an entity pro- vides rich information about the entity. A person can usually correctly infer the entity type by read- ing the first few sentences of the text in a Wikipedia page. Using more sentences provides additional in- formation about the entity which might be helpful, but it is also more likely to introduce noisy informa- tion which could affect the classification accuracy adversely. Therefore, we use the first 200 tokens of the text in a Wikipedia page and create n-gram word features out of them. We have also found that in- cluding additional n-gram word features of the first sentence in a Wikipedia page results in a better clas- sification accuracy.</p><p>Most Wikipedia pages also have a structured table called infobox, which is placed on the right top of a page. An infobox contains attribute-value pairs, of- ten providing summary information about an entity. The attributes in an infobox could be particularly useful for entity type classification. For example, the attribute "Born" in an infobox provides strong ev- idence that the corresponding entity is a PERSON; and the attribute "Headquarters" implies that the cor- responding entity is an ORGANIZATION. We in- clude the infobox attributes as classifier features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Training and Test Data</head><p>Entity linking (EL) or entity disambiguation is the task of determining the identities of entities men- tioned in text, by linking each entity to an entry (if exists) in an open knowledge base such as Wikipedia ( <ref type="bibr" target="#b3">Han et al., 2011;</ref><ref type="bibr" target="#b4">Hoffart et al., 2011</ref>). We apply an EL system <ref type="bibr" target="#b13">(Sil and Florian, 2014</ref>) to generate train- ing data for Wikipedia entity type classification as follows: if a named entity in our NER training data with entity type T is linked to a Wikipedia page, that page will be labeled with entity type T . Similarly, we apply the EL system to generate a set of test data by linking named entities in our NER test data to Wikipedia pages. The English Wikipedia snapshot  Notice that the automatically generated classifier training and test data are weakly labeled since the EL system may link an entity to a wrong Wikipedia page and thus the entity type assigned to that page could be wrong. Since the test data is crucial for evaluating the classification accuracy, we manually corrected the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Classifier Performance</head><p>To evaluate the prediction power of different types of features, we train a number of classifiers using only title features, only infobox features, only text features, and all features respectively. We show the F 1 score of the classifiers on different entity types in <ref type="table">Table 1</ref>. ALL is the overall performance, and PER (PERSON), ORG (ORGANIZATION), GPE, TITL (TITLEWORK), FAC (FACILITY) are the top five most frequently entity types in the test data.</p><p>From <ref type="table">Table 1</ref>, we can see that text features are the most important features for classifying Wikipedia pages, since the classifier trained with only text fea- tures achieves an overall F 1 score of 87.2, which is better than the classifier trained with either title or infobox features alone. Nevertheless, both infobox and title features provide additional useful informa- tion for entity type classification, and the classifier trained with all the features achieves an overall F 1 score of 90.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Improvement via Self-Training</head><p>Self-training is a semi-supervised learning tech- nique that can be used in applications where there is only a small number of labeled training examples but a large number of unlabeled examples. Since our weakly annotated classifier training data only cov- ers around 1% of all the Wikipedia pages, we are motivated to use self-training to further improve the  classification accuracy. We first apply a standard self-training approach. The classifier trained with the initial training data is used to decode (i.e., classify) all the unla- beled Wikipedia pages to predict their entity types with confidence scores. We add the self-decoded Wikipedia pages with high confidence scores to the training data and train a new classifier. Via exper- iments a threshold of 0.9 is used to sort out high- confident self-decoded examples. The F 1 score of the new classifier is improved to 91.1, as shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Under the standard approach, about 2.3M self- decoded examples are added, the size of which is about 500 times of the size of the original training data. The errors of the original classifier could be amplified with such a big increase of the training size with so many self-decoded examples.</p><p>To address this issue, we have developed a sampling-based self-training approach. Instead of adding all the self-decoded examples with confi- dence scores greater than or equal to 0.9, we do a random sampling of those high-confident examples. We use a sampling probability p(e) = q ·c(e), where q is a sampling ratio parameter and c(e) is the con- fidence score of example e. Under this approach, examples with higher confidence scores are more likely to be selected, while the total number of se- lected examples is controlled by the sampling ratio q. Via experiments we found that a small sampling ratio like q = 0.01 yields good improvement (al- though the improvement is not sensitive to q). As shown in <ref type="table" target="#tab_2">Table 2</ref>, the classification accuracy under the sampling-based approach is further improved to 91.8 F 1 score (the improvement is calculated by av- eraging over 5 random samples with q = 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Wikipedia Entity Type Mapping</head><p>We construct an English Wikipedia entity type mapping by applying the English Wikipedia entity type classifier on all the English Wikipedia pages (∼4.6M). Each entry of the mapping includes an entity name (which is extracted from the title of a Wikipedia page) and the associated entity type with confidence score (which is determined by the clas- sifier). We denote the English Wikipedia entity type mapping that includes all the pages by English-Wiki- Mapping.</p><p>To build a high-accuracy mapping, one may want to include only entities with confidence scores greater than or equal to a threshold t in the mapping, and we denote such a mapping by English-Wiki- Mapping(t). Notice that a mapping with a higher t will have more accurate entity types for its enti- ties, but it will include fewer entities. Therefore, there is a trade-off between accuracy and coverage of the mapping, which can be tuned by the confi- dence threshold t. There are about 2.9M entities in English-Wiki-Mapping(0.9), which covers about 63% of all the English Wikipedia pages.</p><p>We have also found that the length of an entity name (i.e., number of words in an entity name) also plays an important role for determining which enti- ties should be included in the mapping for improv- ing an NER system. Therefore, we use English- Wiki-Mapping(t, i) to denote the English Wikipedia entity type mapping that includes all the entities with confidence scores greater than or equal to t and at least i words in their names. English-Wiki- Mapping(0.9,2) covers about 55% of all the English Wikipedia pages, and English-Wiki-Mapping(0.9,3) covers about 25% of all the English Wikipedia pages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual Wikipedia Entity Type Mapping</head><p>Based on the English Wikipedia entity type map- ping, we want to build high-accuracy, high-coverage Wikipedia entity type mappings for other languages with minimum human annotation and language- dependent knowledge involved. We utilize the inter- language links of Wikipedia, which are the links between one entity's pages in different languages. The inter-language links between English Wikipedia pages and Wikipedia pages of another language pro- vide useful information for this task. Suppose we want to build a Wikipedia entity type mapping for a new language, and we use Portuguese as an example. A direct approach is projection us- ing the inter-language links between English and Portuguese Wikipedia pages: for each Portuguese Wikipedia page that has an inter-language link to an English Wikipedia page, we project the entity type of the English Wikipedia page (determined by the English entity type mapping) to the Portuguese Wikipedia page. The rationale is that both the En- glish and Portuguese pages are describing the same entity, even probably with different spelling (e.g., United States in English vs. Estados Unidos in Portuguese), the entity type of that entity does not change from one language to another.</p><p>However, the main limitation of the direct pro- jection approach is coverage. Only a fraction of all the Portuguese Wikipedia pages have inter-language links to English Wikipedia pages, and among those pages only a subset of them have classified en- tity types with confidence scores high enough (e.g., at least 0.9). For example, projecting English- Wiki-Mapping(0.9) to Portuguese Wikipedia returns 143K pages, which covers only 15% of all the Por- tuguese Wikipedia pages (around 920K in total).</p><p>We apply an alternative approach, which uses the 143K Portuguese Wikipedia pages (acquired by projection from English-Wiki-Mapping(0.9)) as weakly annotated training data to train a Portuguese Wikipedia entity type classifier. For feature en- gineering purpose, we also project the English Wikipedia entity type classifier training and test data (as described in Section 2.1.2) to Portuguese Wikipedia pages via inter-language links, and this produces 1,190 Portuguese Wikipedia pages which are used as the test data. Pages in the test data set are excluded from the 143K training data set.</p><p>We use similar features (title, infobox and text) as for the English classifiers to train the Portuguese classifiers. Again we find that the classifier trained with all the features achieves the best accuracy of 86.3 F 1 score. Notice that this is an approximated evaluation because the pages in the test data set are labeled via projection and not by human.</p><p>We build Portuguese Wikipedia entity type map- pings by applying the Portuguese Wikipedia en- tity type classifier on all the Portuguese Wikipedia pages. We use Portuguese-Wiki-Mapping(t) to de- note the mapping that includes entities with con- fidence scores greater than or equal to a thresh-old t. There are 525K entities in Portuguese-Wiki- Mapping(0.9), which covers about 57% of all the Portuguese Wikipedia pages, a significant improve- ment of coverage compared to the direct projection approach (15%).</p><p>The main advantage of our approach is that no hu- man annotation or language-dependent knowledge is required, so it can be easily applied to a new language. We have applied this approach to build high-accuracy, high-coverage Wikipedia entity type mappings for several new languages including Por- tuguese, Japanese, Spanish, Dutch and German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improving NER Systems</head><p>We have developed several approaches that utilize the Wikipedia entity type mappings to improve NER systems. Let M be a Wikipedia entity type map- ping. For an entity name x, let M(x) denote the set of possible entity types for x determined by the mapping. If an entity name x is in the mapping, then M(x) includes at least one entity type, i.e., |M(x)| ≥ 1, where |M(x)| is the cardinality of M(x). Otherwise if an entity name x is not in the mapping, then M(x) = ∅ is the empty set and |M(x)| = 0.</p><p>The first approach is to use a Wikipedia entity type mapping M as a decoding constraint for an NER system. Under this approach, the mapping is applied as a constraint during the decoding proce- dure: if a sequence of words in the text form an entity name x that is included in the mapping, i.e., |M(x)| ≥ 1, then the sequence of words will be identified as an entity, and its entity type is deter- mined by the decoding algorithm while being con- strained to one of the entity types in M(x).</p><p>The second approach is to use a Wikipedia entity type mapping M to post-process the output of an NER system. Under this approach, the mapping is applied after the decoding procedure: if the name of a system entity x is in the mapping and the en- tity type for that entity name is unique based on the mapping, i.e., |M(x)| = 1, then its entity type will be determined by the unique entity type in M(x).</p><p>The decoding constraint approach is more aggres- sive than the post-processing approach, because it may create new entities and change entity bound- aries. This approach is more reliable for entities with longer names. Via experiments we find that using Wiki-Mapping(0.9,2) or Wiki-Mapping(0.9,3) achieves the best improvement under the decoding constraint approach. Remember Wiki-Mapping(t, i) includes all the entities with confidence scores at least t and at least i words in their names.</p><p>In contrast, the post-processing approach is a more conservative approach since it relies on the system entity boundaries and only changes their en- tity types if determined by the mapping, so it will not create new entities. Via experiments we find that us- ing Wiki-Mapping(0.9,2) achieves the best improve- ment under the post-processing approach.</p><p>Based on the observation that the decoding con- straint approach is more reliable for longer enti- ties while the post-processing approach can better handle short entities, we have designed a joint ap- proach that combines the two approaches as fol- lows: it first applies Wiki-Mapping(0.9,3) as a de- coding constraint for an NER system to produce sys- tem entities, and then applies Wiki-Mapping(0.9,2) to post-process the system output. The joint ap- proach combines the advantages of both approaches and achieves robust performance in our experiments.</p><p>Finally, we can use a Wikipedia entity type map- ping to create dictionary features for training an NER system. The idea of using Wikipedia to create training features was explored before, e.g., <ref type="bibr" target="#b5">(Kazama and Torisawa, 2007;</ref><ref type="bibr" target="#b11">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b10">Radford et al., 2015)</ref>. The difference between our approach and the previous approaches is how the features are created: we first build high-accuracy, high-coverage multilingual Wikipedia entity type mappings and then use the mappings to generate dictionary features. Via experiments we find that using Wiki-Mapping(0.9,1) or Wiki-Mapping(0.9,2) achieves the best improvement under the dictionary feature approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the effectiveness of the proposed Wikipedia-based approaches via experi- ments on NER systems trained for 6 languages: English, Portuguese, Japanese, Spanish, Dutch and German. For each language, we compare the base- line NER system with the following approaches:</p><p>• DC(i): the decoding constraint approach with mapping Language-Wiki-Mapping(0.9,i).</p><p>• PP(i): the post-processing approach with map- ping Language-Wiki-Mapping(0.9,i).</p><p>• Joint: the joint approach that combines DC(3) and PP(2).</p><p>• DF(i): the dictionary feature approach with mapping Language-Wiki-Mapping(0.9,i).</p><p>To evaluate the generalization capability of an NER system, we compute the F 1 score on the un- seen entities (Unseen) as well as on all the entities (All) in a test data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English</head><p>The baseline English NER system is a CRF model trained with 328K tokens of human-annotated news articles. It uses standard NER features in the litera- ture including n-gram word features, word type fea- tures, prefix and suffix features, Brown cluster type features, gazetteer features, document-level cache features, etc.</p><p>We have two human-annotated test data sets: the first set, Test (News), consists of 40K tokens of human-annotated news articles; and the second set, Test (Political), consists of 77K tokens of human- annotated political party articles from Wikipedia. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>For Test (News) which is in the same domain as the training data, the baseline system achieves 88.2 F 1 score on all the entities, and a relatively low F 1 score of 78.7 on the unseen entities (38% of all the entities are unseen entities). The dictionary fea- ture approach DF(2) achieves the highest F 1 scores among the Wikipedia-based approaches. It improves the baseline system by 1.2 F 1 score on all the entities and by 3.1 F 1 score on the unseen entities. The joint approach achieves the second highest F 1 scores. It improves the baseline by 0.7 F 1 score on all the en- tities and by 2.0 F 1 score on the unseen entities.</p><p>For Test (Political) which is in a different domain from the training data, the fraction of unseen entities increases to 84%. In this case, the F 1 score of the baseline system drops to 64.1, and the Wikipedia- based approaches demonstrate larger improvements. For example, DF(2) improves the baseline system by 2.7 F 1 score on all the entities and by 3.6 F 1 score on the unseen entities.  score among all approaches in a column is shown in bold).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Portuguese</head><p>For Portuguese, we have applied a semi-supervised learning approach to build the baseline NER system. The training data set includes 31K tokens of human- annotated news articles, and 2M tokens of weakly annotated data. The weakly annotated data is gen- erated as follows. We have a large number of paral- lel sentences between English and Portuguese news articles. We apply the English NER system on the English sentences and project the entity type tags to the Portuguese sentences via alignments between the English and Portuguese sentences.</p><p>The baseline NER system is an MEMM model (CRF cannot handle such a big size of training data, since our NER system has 51 entity types, and the number of features and training time of CRF grow at least quadratically in the number of entity types). The test data set consists of 34K tokens of human- annotated Portuguese news articles.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Because the system is trained with little human-annotated train- ing data, the performance of the baseline system achieves only 60.1 F 1 score on all the entities and 50.2 F 1 score on the unseen entities (80% of all the entities). In this case, the more aggressive decod- ing constraint approach DC(2) achieves the best im- provement among the Wikipedia-based approaches, which improves the baseline by 5.9 F 1 score on all the entities and by 8.6 F 1 score on the unseen en- tities. The joint approach improves the baseline by 3.0 F 1 score on all the entities and by 4.3 F 1 score on the unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test (News) System</head><p>All Unseen 100% 80% Baseline 60.1 50.2 DC <ref type="formula">(2)</ref> 66.0 58.8 DC <ref type="formula">(3)</ref> 62.2 53.4 PP <ref type="formula">(2)</ref> 60.9 51.4 Joint 63.1 54.5 DF <ref type="formula">(1)</ref> 62.4 52.7 DF <ref type="formula">(2)</ref> 61.3 51.9  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Japanese</head><p>For Japanese, the baseline NER system is an MEMM model trained with 20K tokens of human- annotated news articles and 2.1M tokens of weakly annotated data. The weakly annotated data was gen- erated using similar steps as for the Portuguese NER system. The test data set consists of 22K tokens of human-annotated Japanese news articles.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Again, in this low-resource case, DC(2) achieves the best improve- ment among the Wikipedia-based approaches. It im- proves the baseline by 9.0 F 1 score on all the entities and by 18.3 F 1 score on the unseen entities (59% of all the entities). The joint approach improves the baseline by 4.8 F 1 score on all the entities and by 9.6 F 1 score on the unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Spanish, Dutch and German</head><p>We also evaluate the Wikipedia-based approaches on Spanish, Dutch and German NER systems trained with the CoNLL data sets <ref type="bibr" target="#b15">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b14">Tjong Kim Sang and De Meulder, 2003</ref>).</p><p>There are only 4 entity types in the CoNLL data: PER (person), ORG (organization), LOC (location), MISC (miscellaneous names). Accordingly, we have trained a CoNLL-style Wikipedia entity type classifier that produces the CoNLL entity types. The training data for the classifier is generated by using the CoNLL English training data set and the AIDA- YAGO2 data set that provides the Wikipedia titles for the named entities in the CoNLL English data set <ref type="bibr" target="#b4">(Hoffart et al., 2011</ref>). Applying the classifier on all the English Wikipedia pages, we construct a CoNLL-style English Wikipedia entity type map- ping. We then build CoNLL-style Wikipedia entity type mappings for Spanish, Dutch and German us- ing steps as described in Section 3.</p><p>For each of the three languages, the baseline NER system is a CRF model trained with human- annotated news data (∼200K tokens), and there are two test data sets, TestA and TestB, that are also human-annotated news data (ranging from 40K to 70K tokens). The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. For Dutch and German, DF(1) achieves the best im- provement among the Wikipedia-based approaches. For Spanish, the joint approach achieves the best im- provement among the Wikipedia-based approaches. Again, in all cases, the Wikipedia-based approaches demonstrate larger improvements (ranging from 1.0 to 3.4 F 1 score) on the unseen entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>From the experimental results, we have the follow- ing observations:</p><p>• NER systems are more likely to make mistakes on unseen entities. In all cases, the F 1 score of an NER system on all the entities is always higher than the F 1 score on the unseen entities.</p><p>• The Wikipedia-based approaches are effective in improving the generalization capability of NER systems (i.e., improving the accuracy on unseen entities), especially when a system is applied to a new domain (3.6 F 1 score improve- ment on political party articles/English NER) or it is trained with little human-annotated training data (18.3 F 1 score improvement on Japanese NER).</p><p>• In the low-resource scenario where an NER  system is trained with little human-annotated data (e.g., 20K-30K tokens of training data for the Portuguese and Japanese systems), the de- coding constraint approach, which uses a high- accuracy, high-coverage Wikipedia entity type mapping to create constraints during the decod- ing phase, achieves the best improvement.</p><p>• In the rich-resource scenario where an NER system is well trained (e.g., 200K-300K tokens of training data for the English, Dutch and Ger- man systems), the dictionary feature approach, which uses a Wikipedia entity type mapping to create dictionary type features, achieves the best improvement.</p><p>• In both scenarios, the joint approach, which combines the decoding constraint approach and the post-processing approach in a smart way, achieves relatively robust performance among the Wikipedia-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed and evaluated several ap- proaches that utilize high-accuracy, high-coverage Wikipedia entity type mappings to improve multi- lingual NER systems. These mappings are built from weakly annotated data, and can be easily ex- tended to new languages with no human annotation or language-dependent knowledge involved. Experimental results show that the Wikipedia- based approaches are effective in improving the gen- eralization capability of NER systems. When a sys- tem is well trained, the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with lit- tle human-annotated training data, a more aggres- sive decoding constraint approach achieves the best improvement. The improvements are larger on un- seen entities, and the approaches are especially use- ful when a system is applied to a new domain or it is trained with little training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Features</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Improving classifier accuracy via self-training.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Experimental results for English NER (the highest F1</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Experimental results for Portuguese NER. 

NER 
Test (News) 
System 
All 
Unseen 
100% 
59% 
Baseline 50.8 
27.3 
DC(2) 
59.8 
45.6 
DC(3) 
55.6 
36.9 
PP(2) 
50.8 
27.3 
Joint 
55.6 
36.9 
DF(1) 
52.9 
29.0 
DF(2) 
51.8 
28.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Experimental results for Japanese NER.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Experimental results for Spanish, Dutch, and German 

NER. 

</table></figure>

			<note place="foot" n="1"> An entity is an unseen entity if it does not appear in the training data used to train the NER model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Avirup Sil for helpful com-ments, and for collecting the Wikipedia data. We also thank the anonymous reviewers for their sug-gestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Augmenting Wikipedia with named entity tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wisam</forename><surname>Dakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silviu</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Joint Conference on Natural Language Processing</title>
		<meeting>the 3rd International Joint Conference on Natural Language Processing<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A statistical model for multilingual entity detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technologies Conference 2004 (HLT-NAACL&apos;04)</title>
		<meeting>the Human Language Technologies Conference 2004 (HLT-NAACL&apos;04)<address><addrLine>Boston, Massachusetts, USA, May</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Nicolas Nicolov, and Salim Roukos. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collective entity linking in web text: A graph-based method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11</title>
		<meeting>the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting Wikipedia as external knowledge for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT (NAACL 2016)</title>
		<meeting>NAACL-HLT (NAACL 2016)<address><addrLine>San Diego, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum entropy Markov models for information extraction and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dayne</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning, ICML &apos;00</title>
		<meeting>the Seventeenth International Conference on Machine Learning, ICML &apos;00<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="591" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI-99 Workshop on Machine Learning for Information Filtering</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="61" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Named entity recognition with documentspecific KB tag gazetteers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Radford, Xavier Carreras, and James Henderson</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-01" />
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="512" to="517" />
		</imprint>
	</monogr>
	<note>Learning multilingual named entity recognition from Wikipedia. Portugal, September. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining Wiki resources for multilingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">E</forename><surname>Richman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The IBM systems for English entity discovery and linking and Spanish entity linking at TAC 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Analysis Conference (TAC)</title>
		<meeting><address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introduction to the CONLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Introduction to the CONLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Conference on Natural Language Learning</title>
		<meeting>the Sixth Conference on Natural Language Learning<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>CONLL &apos;02</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A proposal to automatically build and maintain gazetteers for named entity recognition by using Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Toral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Muoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
