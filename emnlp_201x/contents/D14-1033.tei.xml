<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Go Climb a Dependency Tree and Correct the Grammatical Errors</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longkai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Computational Linguistics (Peking University) Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Go Climb a Dependency Tree and Correct the Grammatical Errors</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="266" to="277"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>State-of-art systems for grammar error correction often correct errors based on word sequences or phrases. In this paper, we describe a grammar error correction system which corrects grammatical errors at tree level directly. We cluster all error into two groups and divide our system into two modules correspondingly: the general module and the special module. In the general module, we propose a TreeNode Language Model to correct errors related to verbs and nouns. The TreeNode Language Model is easy to train and the decoding is efficient. In the special module, two extra classification models are trained to correct errors related to determiners and prepositions. Experiments show that our system outperforms the state-of-art systems and improves the F 1 score.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of grammar error correction is difficult yet important. An automatic grammar error cor- rection system can help second language (L2) learners improve the quality of their writing. In re- cent years, there are various competitions devoted to grammar error correction, such as the <ref type="bibr">HOO2011(Dale and</ref><ref type="bibr" target="#b9">Kilgarriff, 2011</ref>), HOO-2012( <ref type="bibr" target="#b8">Dale et al., 2012</ref>) and the CoNLL-2013 shared task ( <ref type="bibr" target="#b22">Ng et al., 2013)</ref>. There has been a lot of work ad- dressing errors made by L2 learners. A significant proportion of the systems for grammar error cor- rection train individual statistical models to cor- rect each special kind of error word by word and ignore error interactions. These methods assume no interactions between different kinds of gram- matical errors. In real problem settings errors are correlated, which makes grammar error correction much more difficult.</p><p>Recent research begins to focus on the error interaction problem. For example, <ref type="bibr" target="#b31">Wu and Ng (2013)</ref> decodes a global optimized result based on the individual correction confidence of each kind of errors. The individual correction confi- dence is still based on the noisy context.  uses a joint modeling approach, which considers corrections in phrase structures instead of words. For dependencies that are not covered by the joint learning model,  uses the results of Illi- nois system in the joint inference. These results are still at word level and are based on the noisy context. These systems can consider error inter- actions, however, the systems are complex and inefficient. In both <ref type="bibr" target="#b31">Wu and Ng (2013)</ref> and , Integer Linear Pro- gramming (ILP) is used for decoding a global op- timized result. In the worst case, the time com- plexity of ILP can be exponent.</p><p>In contrast, we think a better grammar error cor- rection system should correct grammatical errors at sentence level directly and efficiently. The sys- tem should correct as many kinds of errors as pos- sible in a generalized framework, while allowing special models for some kinds of errors that we need to take special care. We cluster all error into two groups and correspondingly divide our sys- tem into two modules: the general module and the special module. In the general module, our sys- tem views each parsed sentence as a dependency tree. The system generates correction candidates for each node on the dependency tree. The cor- rection can be made on the dependency tree glob- ally. In this module, nearly all replacement errors related to verb form, noun form and subject-verb agreement errors can be considered. In the spe- cial module, two extra classification models are used to correct the determiner errors and preposi- tion errors . The classifiers are also trained at tree node level. We take special care of these two kinds of errors because these errors not only include re- placement errors, but also include insertion and deletion errors. A classification model is more suitable for handling insertion and deletion errors. Besides, they are the most common errors made by English as a Second Language (ESL) learners and are much easier to be incorporated into a clas- sification framework.</p><p>We propose a TreeNode Language Model (TNLM) to efficiently measure the correctness of selecting a correction candidate of a node in the general module. Similar to the existing statistical language models which assign a probability to a linear chain of words, our TNLM assigns correct- ness scores directly on each node on the depen- dency tree. We select candidates for each node to maximize the global correctness score and use these candidates to form the corrected sentence. The global optimized inference can be tackled ef- ficiently using dynamic programming. Because the decoding is based on the whole sentence, error interactions can be considered. Our TNLM only needs to use context words related to each node on the dependency tree. Training a TreeNode lan- guage model costs no more than training ordinary language models on the same corpus. Experiments show that our system can outperform the state-of- art systems.</p><p>The paper is structured as follows. Section 1 gives the introduction. In section 2 we describe the task and give an overview of the system. In section 3 we describe the general module and in section 4 we describe the special module. Experiments are described in section 5. In section 6 related works are introduced, and the paper is concluded in the last section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task and System Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Description</head><p>The task of grammar error correction aims to cor- rect grammatical errors in sentences. There are various competitions devoted to the grammar er- ror correction task for L2 learners. The CoNLL- 2013 shared task is one of the most famous, which focuses on correcting five types of errors that are commonly made by non-native speakers of English, including determiner, preposition, noun number, subject-verb agreement and verb form er- rors. The training data released by the task orga- nizers come from the NUCLE corpus <ref type="bibr" target="#b7">(Dahlmeier et al., 2013)</ref>. This corpus contains essays writ- ten by ESL learners, which are then corrected by English teachers. The test data are 50 student es- says. Details of the corpus are described in <ref type="bibr" target="#b22">Ng et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Architecture</head><p>In our system, lists of correction candidates are first generated for each word. We generate can- didates for nouns based on their plurality. We gen- erate candidates for verbs based on their tenses. Then we select the correction candidates that max- imize the overall correctness score. An example process of correcting figure 1(a) is shown in table 1.</p><p>Correcting grammatical errors using local sta- tistical models on word sequence is insufficient. The local models can only consider the contexts in a fixed window. In the example of figure 1(a), the context of the verb "is" is "that boy is on the", which sounds reasonable at first glance but is in- correct when considering the whole sentence. The limitation of local classifiers is that long distance syntax information cannot be incorporated within the local context. In order to effectively use the syntax information to get a more accurate correct- ing result, we think a better way is to tackle the problem directly at tree level to view the sentence as a whole. From figure 1(a) we can see that the node "is" has two children on the dependency tree: "books" and "on". When we consider the node "is", its context is "books is on", which sounds in- correct. Therefore, we can make better corrections using such context information on nodes. Therefore, our system corrects grammatical er- rors on dependency trees directly. Because the correlated of words are more linked on trees than in a word sequence, the errors are more easier to be corrected on the trees and the agreement of dif- ferent error types is guaranteed by the edges. We follow the strategy of treating different kinds of errors differently, which is used by lots of gram- mar error correction systems. We cluster the five types of errors considered in CoNLL-2013 into two groups and divide our system into two mod- ules correspondingly.</p><p>• The general module, which is responsible for the verb form errors, noun number errors and subject-verb agreement errors. These er- rors are all replacement errors, which can be corrected by replacing the wrongly used word with a reasonable candidate word.   <ref type="table">Table 1</ref>: An example of the "correction candidate generation and candidate selection" framework.</p><p>• The special module, where two classifica- tion models are used to correct the determiner errors and preposition errors at tree level. We take special care of these two kinds of errors because these errors include both replace- ment errors and insertion/deletion errors. Be- sides, they are the most common errors made by ESL learners and is much easier to be in- corporated into a classification framework.</p><p>We should make it clear that we are not the first to use tree level correction models on ungrammat- ical sentences. <ref type="bibr" target="#b33">Yoshimoto et al. (2013)</ref> uses a Treelet Language model <ref type="bibr" target="#b24">(Pauls and Klein, 2012)</ref> to correct agreement errors. However, the perfor- mance of Treelet language model is not that good compared with the top-ranked system in CoNLL- 2013. The reason is that the production rules in the Treelet language model are based on complex con- texts, which will exacerbate the data sparseness problem. The "context" in Treelet language model also include words ahead of treelets, which are sometimes unrelated to the current node. In con- trast, our TreeNode Language model only needs to consider useful context words related to each node on the dependency tree. To train a TreeNode lan- guage model costs no more than training ordinary language models on the same corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Preparation</head><p>Our system corrects grammatical errors on de- pendency trees directly, therefore the sentences in training and testing data should have been parsed before being corrected. In our system, we use the Stanford parser 1 to parse the New York Times source of the Gigaword corpus 2 , and use the parsed sentences as our training data. We use the original training data provided by CoNLL-2013 as the develop set to tune all parameters.</p><p>Some sentences in the news texts use a differ- ent writing style against the sentences written by ESL learners. For example, sentences written by ESL learners seldom include dialogues between people, while very often news texts include para- graphs such as "'I am frightened!' cried Tom". We use heuristic rules to eliminate the sentences in the Gigaword corpus that are less likely to appear in the ESL writing. The heuristic rules include delet-ing sentences that are too short or too long 3 , delet- ing sentences that contains certain punctuations such as quotation marks, or deleting sentences that are not ended with a period.</p><p>In total we select and parse 5 million sen- tences of the New York Times source of English newswire in the Gigaword corpus. We build the system and experiment based on these sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The General Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The general module aims to correct verb form er- rors, noun number errors and subject-verb agree- ment errors. Other replacement errors such as spelling errors can also be incorporated into the general module. Here we focus on verb form er- rors, noun number errors and subject-verb agree- ment errors only. Our general module views each sentence as a dependency tree. All words in the sentence form the nodes of the tree. Nodes are linked through directed edges, annotated with the dependency relations.</p><p>Before correcting the grammatical errors, the general module should generate correction candi- dates for each node first. For each node we use the word itself as its first candidate. Because the general module considers errors related to verbs and nouns, we generate extra correction candi- dates only for verbs and nouns. For verbs we use all its verb forms as its extra candidates. For ex- ample when considering the word "speaks", we use itself and {speak, spoke, spoken, speaking} as its correction candidates. For nouns we use its singular form and plural form as its extra cor- rection candidates. For example when consider- ing the word "dog", we use itself and "dogs" as its correction candidate. If the system selects the original word as the final correction, the sentence remains unchanged. But for convenience we still call the newly generated sentence "the corrected sentence".</p><p>In a dependency tree, the whole sentence s can be formulized as a list of production rules r 1 , ..., r L of the form: [r = head → modif ier 1 , modif ier 2 ...]. An example of all production rules of figure 1(a) is shown in table 2. Because the production rules are made up of words, selecting a different correction candidate for only one node will result in a list of different production rules. For example, figure 1(b) selects the correction candidate "is" to replace the origi- nal "are". Therefore the production rules of <ref type="figure" target="#fig_0">figure  1</ref> <ref type="figure" target="#fig_0">figure 1(a)</ref>.</p><formula xml:id="formula_0">(b) include [are → books, on], instead of [is → books, on] in</formula><p>books → The, of of → boy boy → that is → books, on on → desk desk → the <ref type="table">Table 2</ref>: All the production rules in the example of figure 1(a)</p><p>The overall correctness score of s, which is score(s), can be further decomposed into L i=0 score(r i ). A reasonable score function should score the correct candidate higher than the incorrect one. Consider the node "is" in <ref type="figure" target="#fig_0">Figure  1</ref>(a), the production rule with head "is" is [is → books, on]. Because the correction of "is" is "are", a reasonable scorer should have score([is → books, on]) &lt; score([are → books, on]).</p><p>Given the formulation of sentence s = [r 1 , ..., r L ] and the candidates for each node, we are faced with two problems:</p><p>1. Score Function. Given a fixed selection of candidate for each node, how to compute the overall score of the dependency tree, i.e., score(s). Because score(s) is decomposed into L i=0 score(r i ), the problem becomes finding a score function to measure the cor- rectness of each r given a fixed selection of candidates.</p><p>2. Decoding. Given each node a list of correc- tion candidates and a reasonable score func- tion score(r) for the production rules, how to find the selection of candidates that maximize the overall score of the dependency tree.</p><p>For the first problem, we propose a TreeNode Language Model as the correctness measure of a fixed candidate selection. For the decoding prob- lem, we use a dynamic programming method to efficiently find the correction candidates that max- imize the overall score. We will describe the de- tails in the following sections.</p><p>One concern is whether the automatically parsed trees are reliable for grammar error cor- rection. We define "reliable" as follows. If we change some words in original sentence into their reasonable correction candidates (e.g. change "is" to "are") but the structure of the dependency tree does not change (except the replaced word and its corresponding POS tag, which are definitely changed), then we say the dependency tree is reli- able for this sentence. To verify this we randomly selected 1000 sentences parsed by the Stanford Parser. We randomly select the verbs and nouns and replace them with a wrong form. We parsed the modified sentences again and asked 2 annota- tors to examine whether the dependency trees are reliable for grammar error correction. We find that 99% of the dependency trees are reliable. There- fore we can see that the dependency tree can be used as the structure for grammar error correction directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TreeNode Language Model</head><p>In our system we use the score of TreeNode Lan- guage Model (TNLM) as the scoring function. Consider a node n on a dependency tree and as- sume n has K modifiers C 1 , ..., C K as its child nodes. We define Seq(n) = [C 1 , ..., n, ..., C K ] as an ordered sub-sequence of nodes that includes the node n itself and all its child nodes. The or- der of the sub-sequence in Seq(n) is sorted based on their position in the sentence. In this formula- tion, we can score the correctness of a production rule r by scoring the correctness of Seq(n). Be- cause Seq(n) is a word sequence, we can use a language model to measure its correctness. The sub-sequences are not identical to the original text. Therefore instead of using ordinary language models, we should train special language models using the sub-sequences to measure the correct- ness of a production rule.</p><p>Take the sentence in <ref type="figure" target="#fig_1">figure 2</ref> as an example. When considering the node "is" in the word se- quence, it is likely to be corrected into "are" be- cause it appear directly after the plural noun "par- ents". However, by the definition above, the sub- sequence corresponding to the node "damaged" is "car is damaged by ". In such context, the word "is" is less likely to be changed to "are". From the example we can see that the sub-sequence is suitable to be used to measure the correctness of a production rule. From this example we can also find that the sub-sequences are different with or- dinary sentences, because ordinary sentences are less likely to end with "by". <ref type="table">Table 3</ref> shows all the sub-sequences in the ex- ample of <ref type="figure" target="#fig_1">figure 2</ref>. If we collect all the sub- sequences in the corpus to form a new sub- sequence corpus, we can train a language model based on the new sub-sequence corpus. This is our TreeNode Language Model. One advantage of TLM is that once we have generated the sub- sequences, we can train the TLM in the same way as we train ordinary language models. Be- sides, the TLM is not limited to a fixed smoothing method. Any smoothing methods for ordinary lan- guage models are applicable for TLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node</head><p>Sub  <ref type="table">Table 3</ref>: All the sub-sentences in the example of figure 2</p><p>In our system we train the TLM using the same way as training tri-gram language model. For a sub-sequence S = w 0 ...w L , we calculate P (S) = L i=0 P (w i |w i−1 w i−2 ). The smoothing method we use is interpolation, which assumes the final P (w i |w i−1 w i−2 ) of the tri-gram language model follows the following decomposition:</p><formula xml:id="formula_1">P (w i |w i−1 w i−2 ) =λ 1 P (w i |w i−1 w i−2 ) +λ 2 P (w i |w i−1 ) +λ 3 P (w i )<label>(1)</label></formula><p>where λ 1 , λ 2 and λ 3 are parameters sum to 1. The parameters λ 1 , λ 2 and λ 3 are estimated through EM algorithm( <ref type="bibr" target="#b1">Baum et al., 1970;</ref><ref type="bibr" target="#b10">Dempster et al., 1977;</ref><ref type="bibr" target="#b15">Jelinek, 1980</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoding</head><p>The decoding problem is to select one correction candidate for each node that maximizes the over- all score of the corrected sentence. When the sen- tence is long and contains many verbs and nouns, enumerating all possible candidate selections is time-consuming. We use a bottom-up dynamic programming approach to find the maximized cor- rections within polynomial time complexity.</p><p>For a node n with L correction candidates n 1 , ...n L and K child nodes C 1 , ..., C K , we define n.scores <ref type="bibr">[i]</ref> as the maximum score if we choose the ith candidate n i for n. Because we decode from leaves to the root, C 1 .scores, ..., C K .scores have already been calculated before we calculate n.scores.</p><p>We assume the sub-sequence Seq(n i ) = [C 1 , ..., C M , n i , C M +1 , ..., C K ] without loss of generality, where C 1 , .., C M are the nodes before n i and C M +1 , ..., C K are the nodes after n i .</p><p>We define c i,j as the jth correction can- didate of child node C i .</p><p>Given a se- lection of candidates for each child node</p><formula xml:id="formula_2">seq = [c 1,j 1 , ..., c M,j M , n i , c M +1,j M +1 , ..., c K,j K ],</formula><p>we can calculate score(seq) as: </p><formula xml:id="formula_3">score(seq) = T N LM (seq) K i=1 C i .scores[j i ]<label>(</label></formula><p>Because seq is a word sequence, the maxi- mization can be efficiently calculated using Viterbi algorithm <ref type="bibr" target="#b11">(Forney Jr, 1973)</ref>. To be specific, the Viterbi algorithm uses the transition scores and emission scores as its input. The transition scores in our model are the tri-gram probabilities from our tri-gram TNLM. The emission scores in our model are the candidate scores of each child: C 1 .scores, ..., C K .scores, which have al- ready been calculated.</p><p>After the bottom-up calculation, we only need to look into the "ROOT" node to find the maxi- mum score of the whole tree. Similar to the Viterbi algorithm, back pointers should be kept to find which candidate is selected for the final corrected sentence. Detailed decoding algorithm is shown in table 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function decode(Node n)</head><p>if n is leaf set n.scores uniformly return for each child c of n decode(c) calculating n.scores using Viterbi End Function BEGIN decode(ROOT ) find the maximum score for the tree and back- track all candidates END In the real world implementations, we add a controlling parameter for the confidence of the correctness of the inputs. We multiply λ on P (w 0 |w −2 w −1 ) of the tri-gram TNLM if the cor- recting candidate w 0 is the same word in the orig- inal input. λ is larger than 1 to "emphasis" the confidence of the original word because the most of the words in the inputs are correct. The value of λ can be set using the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Special Module</head><p>The special module is designed for determiner er- rors and preposition errors. We take special care of these two kinds of errors because these errors include insertion and deletion errors, which can- not be corrected in the general module. Because there is a fixed number of prepositions and deter- miners, these two kinds of errors are much easier to be incorporated into a classification framework. Besides, they are the most common errors made by ESL learners and there are lots of previous works that leave valuable guidance for us to follow.</p><p>Similar to many previous state-of-art systems, we treat the correction of determiner errors and preposition errors as a classification problem. Al- though some previous works (e.g. ) use NPs and the head of NPs as features, they are basically local classifiers mak- ing predictions on word sequences. Difference to the local classifier approaches, we make predic- tions on the nodes of the dependency tree directly. In our system we correct determiner errors and preposition errors separately.</p><p>For the determiner errors, we consider the in- sertion, deletion and replacement of articles (i.e. 'a', 'an' and 'the'). Because the articles are used to modify nouns in the dependency trees, we can classify based on noun nodes. We give each noun node (node whose POS tag is noun) a label to in- dicate which article it should take. We use left po- sition (LP) and right position (RP) to specify the position of the article. The article therefore lo- cates between LP and RP. If a noun node already has an article as its modifier, then LP will be the position directly ahead of the article. In this case, RP = LP + 2. If an insertion is needed, the RP is the position of the first child node of the noun node. In this case LP = RP − 1. With this no- tation, detailed feature templates we use to correct determiner errors are listed in table 5. In our model we use 3 labels: 'a', 'the' and '∅'. We use 'a', 'the' to represent a noun node should be modified with 'a' or ''the' correspondingly. We use ''∅' to in- dicate that no article is needed for the noun node. We use rule-based method to distinguish between "a" and "an" as a post-process.</p><p>For the preposition errors, we only consider deletion and replacement of an existing preposi- tion. The classification framework is similar to determiner errors. We consider classification on preposition nodes (nodes whose POS tag is prepo- sition). We use prepositions as labels to indicate which preposition should be used. and use "∅" to denote that the preposition should be deleted. We use the same definition of LP and RP as the correction of determiner errors. Detailed feature templates we use to correct preposition errors are listed in table 6. Similar to the previous work <ref type="bibr" target="#b32">(Xing et al., 2013)</ref>, we find that adding more preposi- tions will not improve the performance in our ex- periments. Thus we only consider a fixed set of prepositions: {in, for, to, of, on}.</p><p>Previous works such as  show that Naive Bayes model and averaged perceptron model show better results than other classification models. These classifiers can give a reasonably good performance when there are lim- ited amount of training data. In our system, we use large amount of automatically generated training data based on the parsed Gigaword corpus instead of the limited training data provided by <ref type="bibr">CoNLL2013</ref>.</p><p>Take generating training data for determiner er- rors as an example. We generate training data based on the parsed Gigaword corpus C described in section 2. Each sentence S in C is a depen- dency tree T . We use each noun node N on T as one training instance. If N is modified by "the", its label will be "THE". If N is modified by "a" or "an", its label will be "A". Otherwise its label will be "NULL". Then we just omit the determiner modifier and generate features based on table 5. Generating training data for preposition errors is the same, except we use preposition nodes instead of noun nodes.</p><p>By generating training instances in this way, we can get large amount of training data. Therefore we think it is a good time to try different classifi- cation models with enough training data. We ex- periment on Naive Bayes, Averaged Perceptron, SVM and Maximum Entropy models (ME) in a 5- fold cross validation on the training data. We find ME achieves the highest accuracy. Therefore we use ME as the classification model in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head><p>In the experiments, we use our parsed Gigaword corpus as the training data, use the training data provided by CoNLL-2013 as the develop data, and use the test data of CoNLL-2013 as test data di- rectly. In the general module, the training data is used for the training of TreeNode Language Model. In the special module, the training data is used for training individual classification models.</p><p>We use the M2 scorer (Dahlmeier and Ng, 2012b) provided by the organizer of CoNLL-2013 for the evaluation of our system. The M2 scorer is widely used as a standard scorer in previous systems. Because we make comparison with the state-of-art systems on the CoNLL-2013 corpus, we use the same evaluation metric F 1 score of M2 scorer as the evaluation metric.</p><p>In reality, some sentences may have more than one kind of possible correction. As the example in "The books of that boy is on the desk.", the cor- responding correction can be either "The books of that boy are on the desk." or "The book of that boy is on the desk.". The gold test data can only con- <ref type="table">Table 5</ref>: Feature templates for the determiner errors. w i is the word at the ith position. N N is the current noun node. F a is the father node of the current noun node. Ch is a child node of the current noun node. <ref type="table">Table 6</ref>: Feature templates for preposition errors. w i is the word at the ith position. F a is the father node of the current preposition node. Ch is a child node of the current preposition node. sider a small portion of possible answers. To re- lieve this, the CoNLL-2013 shared task allows all participating teams to provide alterative answers if they believe their system outputs are also cor- rect. These alterative answers form the "Revised Data" in the shared task, which indeed help evalu- ate the outputs of the participating systems. How- ever, the revised data only include alterative cor- rections from the participating teams. Therefore the evaluation is not that fair for future systems. In our experiment we only use the original test data as the evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Features</head><formula xml:id="formula_5">w LP , w LP −1 , w LP −2 , w RP , w RP +1 , w RP +2 , w LP −2 w LP −1 , w LP −1 w LP , w LP w RP , w RP w RP +1 , w RP +1 w RP +2 , w LP −2 w LP −1 w LP , w LP −1 w LP w RP , w LP w RP w RP +1 , w RP w RP +1 w RP +2 Noun Node Features N N , w LP N N , w LP −1 w LP N N , w LP −2 w LP −1 w LP N N Father/Child Node Features F a, w RP F a, w RP w RP +1 F a, w RP w RP +1 w RP +2 F a, F a&amp;Ch</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Word Features</head><formula xml:id="formula_6">w LP , w LP −1 , w LP −2 , w RP , w RP +1 , w RP +2 , w LP −2 w LP −1 , w LP −1 w LP , w LP w RP , w RP w RP +1 , w RP +1 w RP +2 , w LP −2 w LP −1 w LP , w LP −1 w LP w RP , w LP w RP w RP +1 , w RP w RP +1 w RP +2 Father/Child Node Features F a, w RP F a, w RP w RP +1 F a, w RP w RP +1 w RP +2 F a, F a&amp;Ch</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Results</head><p>We first show the performance of each stage of our system. In our system, the general module and the special module correct grammar errors conse- quently. Therefore in  <ref type="table" target="#tab_3">Table 7</ref>: Results of each stage in our system. TNLM is the general module. "+Det" is the sys- tem containing the general module and determiner part of special module."+Prep" is the final system</p><p>We evaluate the effect of using TreeNode lan- guage model for the general module. We compare the TNLM with ordinary tri-gram language model. We use the same amount of training data and the same smoothing strategy (i.e. interpolation) for both of them.  Based on the result of the general module using TNLM, we compare our tree level special mod- ule against the local classification approach. The special module of our system makes predictions on the dependency tree directly, while local clas- sification approaches make predictions on linear chain of words and decide the article of a noun Phrase or the preposition of a preposition phrase. We use the same word level features for the two approaches except for the local classifiers we do not add tree level features. <ref type="table">Table 9</ref> shows the com- parison.</p><p>When using the parsed Gigaword texts as train- ing data, the quality of the sentences we select will influence the result. For comparison, we ran- domly select the same amount of sentences from the same source of Gigaword and parse them as a alterative training set. <ref type="table" target="#tab_7">Table 10</ref> shows the com- parison between random chosen training data and Method P R F1 score Local Classifier 26.38% 39.14% 31.51% Our Tree-based 32.64% 39.20% 35.62% <ref type="table">Table 9</ref>: Comparison for the special module on the test data. The input of the special module is the sentences corrected by the TNLM in the general module.</p><p>the selected training data of our system. We can see that the data selection (cleaning) procedure is important for the improvement of system F 1.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With Other Systems</head><p>We also compare our system with the state-of- art systems. The first two are the top-2 systems at CoNLL-2013 shared task :  and <ref type="bibr" target="#b16">Kao et al. (2013)</ref>. The third one is the Treelet Language Model in <ref type="bibr" target="#b33">Yoshimoto et al. (2013)</ref>. The fourth one is , which until now shows the best perfor- mance. The comparison on the test data is shown in table 11. In CoNLL-2013 only 5 kinds of errors are con- sidered. Our system can be slightly modified to handle the case where other errors such as spelling errors should be considered. In that case, we can modify the candidate generation of the general module. We only need to let the generate cor- rection candidates be any possible words that are similar to the original word, and run the same de- coding algorithm to get the corrected sentence. As a comparison, the ILP systems should add extra scoring system to score extra kind of errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Early grammatical error correction systems use the knowledge engineering approach <ref type="bibr" target="#b21">(Murata and Nagao, 1994;</ref><ref type="bibr" target="#b2">Bond and Ikehara, 1996;</ref><ref type="bibr" target="#b14">Heine, 1998)</ref>. However, manually designed rules usually have exceptions. Therefore, the ma- chine learning approach has become the dominant approach recently. Previous machine learning ap- proaches typically formulates the task as a clas- sification problem. Of all the errors, determiner and preposition errors are the two main research topics <ref type="bibr" target="#b18">(Knight and Chander, 1994;</ref><ref type="bibr" target="#b0">AEHAN et al., 2006;</ref><ref type="bibr" target="#b30">Tetreault and Chodorow, 2008;</ref><ref type="bibr" target="#b4">Dahlmeier and Ng, 2011)</ref>. Features used in the classifica- tion models include the context words, POS tags, language model scores <ref type="bibr" target="#b12">(Gamon, 2010)</ref>, and tree level features <ref type="bibr" target="#b29">(Tetreault et al., 2010)</ref>. Models used include maximum entropy <ref type="bibr" target="#b0">(AEHAN et al., 2006;</ref><ref type="bibr" target="#b30">Tetreault and Chodorow, 2008)</ref>, averaged percep- tron, Naive Bayes (Rozovskaya and Roth, 2011), etc. Other errors such as verb form and noun num- ber errors also attract some attention recently ( <ref type="bibr" target="#b19">Liu et al., 2010;</ref><ref type="bibr" target="#b28">Tajiri et al., 2012)</ref>.</p><p>Recent research efforts have started to deal with correcting different errors jointly <ref type="bibr" target="#b13">(Gamon, 2011;</ref><ref type="bibr" target="#b23">Park and Levy, 2011;</ref><ref type="bibr" target="#b5">Dahlmeier and Ng, 2012a;</ref><ref type="bibr" target="#b31">Wu and Ng, 2013;</ref>. Gamon (2011) uses a high-order sequential label- ing model to detect various errors. <ref type="bibr" target="#b23">Park and Levy (2011)</ref> models grammatical error correction using a noisy channel model. <ref type="bibr" target="#b5">Dahlmeier and Ng (2012a)</ref> uses a beam search decoder, which iteratively cor- rects to produce the best corrected output. <ref type="bibr" target="#b31">Wu and Ng (2013)</ref> and  use ILP to decode a global optimized result. The joint learning and joint inference are still at word/phrase level and are based on the noisy context. In the worst case, the time complexity of ILP can reach exponent. In contrast, our system corrects gram- mar errors at tree level directly, and the decoding is finished with polynomial time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future work</head><p>In this paper we describe our grammar error cor- rection system which corrects errors at tree level directly. We propose a TreeNode Language Model and use it in the general module to correct errors related to verbs and nouns. The TNLM is easy to train and the decoding of corrected sentence is ef- ficient. In the special module, two extra classifica- tion models are trained to correct errors related to determiners and prepositions at tree level directly. Because our current method depends on an auto- matically parsed corpus, future work may include applying some additional filtering (e.g. <ref type="bibr" target="#b20">Mejer and Crammer (2012)</ref>) of the extended training set ac- cording to some confidence measure of parsing ac- curacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Dependency parsing results of (a) the original sentence "The books of that boy is on the desk ." (b) the corrected sentence.</figDesc><graphic url="image-1.png" coords="3,154.77,62.81,288.02,120.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A illustrative sentence for TreeNode Language Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>2) where T N LM (seq) is the TreeNode Language Model score of seq. Then, n.scores[i] is calcu- lated as: n.scores[i] = max ∀seq score(seq)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : The Decoding algorithm</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>table 7 we</head><label>7</label><figDesc></figDesc><table>show the perfor-
mance when each component is added to the sys-
tem. 

Method P 
R 
F1 score 
TNLM 
33.96% 17.71% 23.28% 
+Det 
32.83% 38.28% 35.35% 
+Prep 
32.64% 39.20% 35.62% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 8 shows the comparison.</head><label>8</label><figDesc></figDesc><table>The 
TNLM can improve the F 1 by +2.1%. 

Method 
P 
R 
F1 score 
Ordinary LM 29.27% 16.68% 21.27% 
Our TNLM 
33.96% 17.71% 23.28% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Comparison for the general module 
between TNLM and ordinary tri-gram language 
model on the test data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 10 :</head><label>10</label><figDesc>Comparison of training using random chosen sentences and selected sentences.</figDesc><table>Method 
F1 score 
Rozovskaya et al. (2013) 
31.20% 
Kao et al. (2013) 
25.01% 
Yoshimoto et al. (2013) 
22.17% 
Rozovskaya and Roth (2013) 35.20% 
Our method 
35.62% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Comparison of F 1 of different systems 
on the test data . 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/lex-parser.shtml 2 https://catalog.ldc.upenn.edu/LDC2003T05</note>

			<note place="foot" n="3"> In our experiment, no less than 5 words and no more than 30 words.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partly supported by Na-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Detecting errors in english article usage by non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aehan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leacock</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">When and how to disambiguate?-countability in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ikehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Seminar on Multimodal Interactive Disambiguation: MIDDIM96</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Noun phrase reference in japanese-to-english machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ogura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawaoka</surname></persName>
		</author>
		<idno>cmp- lg/9601008</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grammatical error correction with alternating structure optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="915" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A beamsearch decoder for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better evaluation for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="568" to="572" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of learner english: The nus corpus of learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dahlmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hoo 2012: A report on the preposition and determiner error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anisimoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narroway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Building Educational Applications Using NLP</title>
		<meeting>the Seventh Workshop on Building Educational Applications Using NLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Helping our own: The hoo 2011 pilot shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kilgarriff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Workshop on Natural Language Generation</title>
		<meeting>the 13th European Workshop on Natural Language Generation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="242" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The viterbi algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Forney</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="268" to="278" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using mostly native data to correct errors in learners&apos; writing: a metaclassifier approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-order sequence modeling for language learner error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 6th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Definiteness predictions for japanese noun phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Heine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="519" to="525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Interpolated estimation of markov source parameters from sparse data. Pattern recognition in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Conll-2013 shared task: Grammatical error correction nthu system description. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated postediting of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="779" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Srl-based verb selection for esl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 conference on empirical methods in natural language processing</title>
		<meeting>the 2010 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1068" to="1076" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Are you sure? confidence in prediction of dependency tree edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mejer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="576" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Determination of referential property and number of nouns in japanese sentences for machine translation into english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nagao</surname></persName>
		</author>
		<idno>cmp-lg/9405019</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The conll-2013 shared task on grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated whole sentence grammar correction using a noisy channel model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="934" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale syntactic language modeling with treelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The university of illinois system in the conll-2013 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithm selection and model adaptation for esl correction tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="924" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint learning and inference for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="791" to="802" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tense and aspect error correction for esl learners using global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tajiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using parse features for preposition selection and error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chodorow</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl 2010 conference short papers</title>
		<meeting>the acl 2010 conference short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="353" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The ups and downs of preposition error detection in esl writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grammatical error correction using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Um-checker: A hybrid system for english grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria. As</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
	<note>sociation for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Naist at 2013 conll grammatical error correction shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yoshimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitsuzawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mizumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hayashibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
