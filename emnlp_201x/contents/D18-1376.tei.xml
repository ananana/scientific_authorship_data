<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thread Popularity Prediction and Tracking with a Permutation-invariant Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou</forename><forename type="middle">Pong</forename><surname>Chan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shenzhen Key Laboratory of Rich Media Big Data Analytics and Application</orgName>
								<orgName type="institution" key="instit1">Shenzhen Research Institute</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shatin, Hong Kong</settlement>
									<region>N.T</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shenzhen Key Laboratory of Rich Media Big Data Analytics and Application</orgName>
								<orgName type="institution" key="instit1">Shenzhen Research Institute</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thread Popularity Prediction and Tracking with a Permutation-invariant Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3392" to="3401"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3392</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of thread popularity prediction and tracking aims to recommend a few popular comments to subscribed users when a batch of new comments arrive in a discussion thread. This task has been formulated as a reinforcement learning problem, in which the reward of the agent is the sum of positive responses received by the recommended comments. In this work, we propose a novel approach to tackle this problem. First, we propose a deep neural network architecture to model the expected cumulative reward (Q-value) of a recommendation (action). Unlike the state-of-the-art approach, which treats an action as a sequence, our model uses an attention mechanism to integrate information from a set of comments. Thus, the prediction of Q-value is invariant to the permutation of the comments, which leads to a more consistent agent behavior. Second, we employ a greedy procedure to approximate the action that maximizes the predicted Q-value from a combinatorial action space. Different from the state-of-the-art approach , this procedure does not require an additional pre-trained model to generate candidate actions. Experiments on five real-world datasets show that our approach outperforms the state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online discussion forums allow people to join in- depth conversations about different topics in form of threads. Each thread corresponds to one con- versation, which is initiated by a post and users respond to it with comments. In addition, a com- ment can be further replied by another comment, forming a discussion tree. Users who are inter- ested in a particular thread will subscribe to it. Af- ter the subscription, users will receive a notifica- tion when a new comment arrives in that thread. However, the speed of content generation in a well-known discussion forum is breakneck. For instance, in Reddit 1 , there were more than 900 million comments posted in 2017 <ref type="bibr">(Reddit, 2017)</ref>. Hence, merely pushing every new comment to the subscribers leads to a poor user experience. Mo- tivated by this issue, <ref type="bibr" target="#b13">He et al. (2016c)</ref> proposed the task of thread popularity prediction and track- ing. When N new comments arrive in a thread, the system performs one step of recommendation by pushing K comments to the subscribers. We want to maximize the sum of popularities of the recommended comments over all recommendation steps. The popularity of a comment is measured by the number of positive reactions it received, e.g., the rating. With the assumption that a user needs to know the prior context in order to under- stand a comment, the system can only recommend new comments that are in the subtrees of previ- ously recommended comments. Thus, the selec- tion of comments at the current recommendation step will affect the comments that we can choose in the future recommendation steps.</p><p>To incorporate the long-term consequences of recommendations, the task of thread popularity prediction and tracking has been formulated as a reinforcement learning problem, in which an agent selects an action (a set of K comments) according to its current state (previous recommended com- ments), with the goal of maximizing the cumula- tive reward (total popularities of the recommended comments over all recommendation steps). The optimal action of the agent at each step is the action that maximizes the Q-function, Q(s, a), which denotes the long-term reward of choosing action a in state s. In practice, we learn this Q- function using a parametric function, Q(s, a; θ), where θ is the model parameter vector. Thus, the predicted optimal action of the agent is the action that maximizes Q(s, a; θ).</p><p>This reinforcement learning problem has two main challenges. First, we need to develop a para- metric model, Q(s, a; θ), to approximate the Q- function. Second, finding the action that maxi- mizes Q(s, a; θ) requires the prediction of all N K possible actions, which is intractable. Thus, we need a procedure to approximate the predicted op- timal action from a combinatorial action space.</p><p>To address the first challenge, <ref type="bibr" target="#b13">He et al. (2016c)</ref> proposed a neural network model, DRRN- BiLSTM, to approximate the Q-function. In this model, a bi-directional long short-term memory (LSTM) ( <ref type="bibr" target="#b9">Graves and Schmidhuber, 2005</ref>) is used to encode the set of K comments in an action. To address the second challenge, they proposed the two-stage Q-learning procedure to approxi- mate the predicted optimal action ( . In this procedure, the agent uses a pre-trained and less-sophisticated model to rank all possible ac- tions, then it uses the DRRN-BiLSTM to re-rank the top-M actions and selects the best one. How- ever, this approach has two limitations. First of all, bi-directional LSTM is a sequence model, which treats the set of K comments in an action as a sequence. Although they tried to fix this prob- lem by feeding randomly-permuted comments to the model, a different permutation of the same set of comments leads to a different Q-value predic- tion. Thus, the agent may not consistently select the predicted optimal action. Secondly, the two- stage Q-learning procedure requires an additional pre-trained model to generate candidate actions.</p><p>Our work addresses these two limitations as follows. We propose a novel neural network model, DRRN-Attention, to approximate the Q- function. In our model, we use an attention mech- anism ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) to integrate the information from a set of comment into an ac- tion embedding vector. In a nutshell, the atten- tion mechanism outputs a weighted sum of the comment representations, where the weights are learned by a subnetwork to indicate the impor- tance of each comment. Thus, the action embed- ding is invariant to the permutation of the com- ments, which leads to a permutation invariant Q- value prediction. Next, we employ a greedy pro- cedure to approximate the action that maximizes Q(s, a; θ). This procedure only requires the pre- diction of O(N K) actions, which is significantly lower than N K . Moreover, it does not require an additional pre-trained model to generate candidate actions.</p><p>In our experiments, we evaluate the perfor- mance of our DRRN-Attention model and the greedy approximation procedure against the base- lines on five real-world datasets. Experimental re- sults demonstrate that our approach beats the base- lines on four of the datasets and achieves a com- petitive performance on one of the datasets. Fur- thermore, we analyze the performance of our ap- proach across four action sizes (K = 2, 3, 4, 5). Our approach consistently achieves a higher cu- mulative reward than the baselines across all these action sizes.</p><p>We summarize our contributions as follow: (1) a new neural network architecture to model the Q- value of the agent which is invariant to the per- mutation of sub-actions; (2) a greedy procedure for the agent to select an action from the com- binatorial action space without an additional pre- trained model; and (3) the new state-of-the-art per- formances on five real-world datasets. In the task of thread popularity prediction and tracking, the agent selects a set of K comments from N available comments at every time step, where each comment is a free text. <ref type="bibr" target="#b13">He et al. (2016c)</ref> proposed two different approaches to tackle this task. In their first approach, the agent uses the Deep Reinforcement Relevance Network (DRRN) ( <ref type="bibr" target="#b11">He et al., 2016b</ref>) to model the Q- function of selecting a comment. In their second approach, the agent uses the DRRN-BiLSTM ( <ref type="bibr" target="#b13">He et al., 2016c</ref>) to model the Q-function of an ac-tion. To due with the combinatorial action space, the agent uses uniform sampling to generate a set of M candidate actions. To improve this random sampling scheme, they proposed the two-stage Q- learning procedure in their later work , which used a pre-trained model to gener- ate M candidate actions. Their experimental re- sults showed that using DRRN-BiLSTM with two- stage Q-learning procedure outperforms all other existing methods. The difference between our model and DRRN-BiLSTM is that we use atten- tion to encode a set of comments rather than using a bi-directional LSTM. Besides, the greedy pro- cedure in our approach does not require any extra pre-trained model.  also consid- ered a special case that the agent can access an ex- ternal knowledge source to augment the state rep- resentation. This setting is orthogonal to this work since we focus on the action encoding and the ap- proximation of predicted optimal action.</p><p>One line of research focused on the in- tegration of sequence-to-sequence (SEQ2SEQ) model <ref type="bibr" target="#b21">(Sutskever et al., 2014</ref>) and reinforcement learning framework, examples including dialogue generation ( <ref type="bibr" target="#b8">Dhingra et al., 2017;</ref><ref type="bibr" target="#b19">Su et al., 2016</ref>), question answering system <ref type="bibr" target="#b4">(Buck et al., 2017)</ref>, and machine translation ( <ref type="bibr" target="#b10">He et al., 2016a</ref>). In these tasks, the agent selects an action by generating a free text using a SEQ2SEQ model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Learning on Sets</head><p>Most of the deep learning models on sets em- ployed attention to integrate information from a set of input. This idea was first introduced in the read-process-and-write network ( <ref type="bibr" target="#b25">Vinyals et al., 2016)</ref>, which uses a process module to perform multiple steps of attention over a set of vectors to obtain a permutation-invariant embedding. Our work adapts this idea to aggregate a set of com- ment embedding vectors. In the domain of graph learning, several models ( <ref type="bibr" target="#b20">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b31">Zhang et al., 2017</ref>) learn an embedding of a node by attending over its neighboring nodes. All of the above models can be interpreted as a special case of memory network ( <ref type="bibr" target="#b20">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b31">Zhang et al., 2017</ref>), if we view the set of feature vectors as external mem- ory. Max-pooling is another promising technique for the problem of learning on sets. <ref type="bibr" target="#b18">Qi et al. (2017)</ref> used max-pooling to aggregate the feature vectors of a set of 3D geometry points. Recently, <ref type="bibr" target="#b30">Zaheer et al. (2017)</ref> derived the necessary and sufficient conditions for a neural network layer to be permu- tation invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Popularity Prediction</head><p>Another related line of research is popularity prediction problem in a supervised learning set- ting. <ref type="bibr" target="#b29">Yano and Smith (2010)</ref> used the LDA topic model ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref> to predict the number of comments of a blog post in a political blog. There are also several studies focused on the task of predicting the number of reshares on Face- book ( <ref type="bibr" target="#b7">Cheng et al., 2014</ref>) and the number of retweets in tweeter based on the text content ( <ref type="bibr" target="#b22">Tan et al., 2014;</ref><ref type="bibr" target="#b14">Hong et al., 2011</ref>). Recently, <ref type="bibr" target="#b6">Cheng et al. (2017)</ref> proposed a neural network model to learn comment embeddings for the task of com- munity endorsement prediction in a supervised learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discussion Tree</head><p>A discussion thread in an online forum can be rep- resented as a tree. Each node in the tree stores a free text. The root node represents the post of the thread and each non-root node represents a com- ment of the thread. There is a directed edge from node u to node v if and only if a comment (or post) u is replied by comment v. This tree keeps grow- ing as new comments are submitted to the thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition</head><p>The task of thread popularity prediction and track- ing is formally defined as a reinforcement learning problem. We use M t to denote the set of com- ments that are being tracked at time t. Given a discussion thread, we start an episode as follows. First, we initialize M 1 to be the post of a thread. Then, at each time step t the agent performs the following operations:</p><p>• Read the current state s t , which is all the pre- viously tracked comments {M 1 , . . . , M t }.</p><p>• Read N new comments, c t = {c t,1 , . . . , c t,N }, in the subtree of M t .</p><p>• Select a set of K comments from c t to rec- ommend, a t = {c 1</p><p>• Receive a reward,</p><formula xml:id="formula_0">r t+1 = K i=1 η c i t</formula><p>, where η c i t is the number of positive reactions re- ceived by comment c i t .</p><p>• Track the set of recommended comments in the next time step, M t+1 = a t .</p><p>The episode terminates when no more new com- ments appear in the subtree of M t . The goal of the agent is to maximize the cumulative reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Q-function</head><p>The state-action value function (or Q-function), Q(s, a), is defined as the expected cumulative re- ward starting from state s and taking action a.</p><formula xml:id="formula_1">More formally, Q(s, a) = E[ +∞ l=0 γ l r t+1+l |s t = s, a t = a]</formula><p>, where γ ∈ (0, 1] is a discount factor for future rewards. Since the goal of the agent is to maximize the cumulative reward, the opti- mal action for each state is the action that achieves the highest Q-value. Thus, the Q-function is as- sociated with an optimal policy: in every state, the agent selects the action that maximizes the Q-function, i.e., a t = argmax a Q(s t , a), ∀t. Since this Q-function is unknown to the agent, we approximate the Q-function using a paramet- ric model, Q(s, a; θ), and update the parameters θ using received rewards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exploration-Exploitation Trade-off</head><p>The agent needs to balance the exploration- exploitation trade-off when selecting an action. On one hand, the agent can choose the action with the highest estimated Q-value to exploit its cur- rent knowledge of the Q-function. On the other hand, the agent can choose a non-greedy action to get more information about the Q-value of other actions. The balance between exploration and ex- ploitation can be achieved by using the -greedy policy, in which the agent selects a random action with probability , and selects a greedy action with probability 1 − . Note that the term "greedy" in the -greedy policy means that the agent selects the action that is predicted to be optimal, i.e., se- lect a t = argmax a Q(s t , a; θ). It does not refer to the greedy procedure, which is used to approxi- mate the predicted optimal action in a combinato- rial action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DRRN-Attention Model</head><p>In this work, we propose a new deep neural net- work model, named DRRN-Attention, to approxi- mate the Q-function for the task of thread popular- ity prediction and tracking. The input to our model is a state, s t , and an action, a t = {c 1 t , . . . , c K t }, as defined in Section 3.2. The output is the prediction of Q-value, i.e., Q(s t , a t ; θ) ∈ R. <ref type="figure">Figure 1</ref> illus- trates the overall architecture of DRRN-Attention. We divide our model into three modules as fol- lows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Text Representation Module</head><p>The text representation module reads s t and</p><formula xml:id="formula_2">a t = {c 1 t , . . . , c K t }. We first convert s t , c 1 t , . . . , c K t into bag-of-words (BOW) representations, b st , b c 1 t , . . . , b c K t . Then,</formula><note type="other">we use a 2-layer feed- forward neural network to embed b st into a d-dimensional state embedding vector, m st ∈ R d . After that, we use another 2-layer feedforward neural network to embed b c i t into a d-dimensional comment embedding vector, m c i t ∈ R d , for i = 1, . . . , K. This module outputs m st and {m c 1 t , . . . , m c K t }.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Set Embedding Module</head><p>The input to this module is a set of d-dimensional comment embeddings, {m c 1 t , . . . , m c K t }. The out- put is an action embedding vector, m at ∈ R h+d , which is invariant to the ordering of comment em- beddings. The module consists of a single-layer LSTM with a hidden size of h, and a shared atten- tion mechanism, f : R h × R d → R. The initial hidden state, q 0 ∈ R h , of the LSTM is a trainable vector. Inspired by <ref type="bibr" target="#b25">(Vinyals et al., 2016)</ref>, we per- form L steps of computations over the comment embedding vectors. More specifically, at each step of computation l = 0, 1, . . . , L − 1:</p><p>• The query vector, q l ∈ R h , is the current hid- den state of the LSTM.</p><p>• Apply the attention mechanism to compute an attention coefficient, e i,l , between the query, q l , and a comment embedding, m c i t , for i = 1, . . . , K. In general, this framework is agnostic to the underlying attention mech- anism. In this work, we closely follow the attentional setup in ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>), as shown in the following equation.</p><formula xml:id="formula_3">e i,l = v T tanh(W e m c i t + U e q l ),<label>(1)</label></formula><p>where v ∈ R h , W e ∈ R h ×d , and U e ∈ R h ×h .  <ref type="figure">Figure 1</ref>: Architecture of DRRN-Attention model. The text representation module first converts state s t and each comment c i t in action a t into embedding vectors. Then, the comment embedding vectors are passed to the set embedding module to learn an action embedding vector. Finally, the state embedding and the action embedding are passed to the output module to output a prediction of Q-value.</p><p>• Apply softmax function to normalize the at- tention coefficients,</p><formula xml:id="formula_4">α i,l = exp(e i,l ) K j=1 exp(e j,l ) .<label>(2)</label></formula><p>• Use the normalized attention coefficients to compute a weighted sum of the comment em- bedding vectors, as the readout in this com- putation step,</p><formula xml:id="formula_5">r l = K i=1 α i,l m c i t .<label>(3)</label></formula><p>• The LSTM takes q l and r l as input and com- putes the next hidden state, q l+1 ,</p><formula xml:id="formula_6">q l+1 = LSTM([q l , r l ]).<label>(4)</label></formula><p>Note that swapping any two comment embed- ding vectors, m c i t and m c j t , will not affect the query vector q l as well as the attention readout r l . After L steps of computation, this module con- catenates q L and r L to yield the final output action embedding, m at = [q L , r L ] ∈ R h+d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Output Module</head><p>The input to this module is a state embedding vec- tor, m st ∈ R d , and an action embedding vec- tor, m at ∈ R h+d . We simply concatenate m st and m at and pass them through a fully-connected layer. The output is Q(s t , a t ; θ) ∈ R, which is the prediction of Q(s t , a t ).</p><formula xml:id="formula_7">Algorithm 1 Greedy(s t , c t , Q(·, ·; θ), K) 1: a = ∅ 2: for i = 1 → K do 3: c * = argmax c∈ct\a Q(s t , a ∪ c; θ) 4: a = a ∪ c * 5: return a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Greedy Procedure</head><p>The next challenge that we need to address is to approximate the predicted optimal action in a combinatorial action space. Finding the predicted optimal action, argmax a Q(s t , a; θ), is intractable since it requires the prediction of all N K actions. In this work, we use a greedy procedure to com- pute an approximation. The complete procedure is shown in Algorithm 1. We start from an empty action, a t = ∅, and then iteratively adds into a t the comments that leads to the largest increase in Q(s t , a t ; θ), until |a t | = K. The procedure con- sists of K iterations. In each iteration i, we need to predict the Q-value for n − i actions. In total, it only requires the prediction of O(N K) actions, which is tractable. The advantage of this proce- dure over existing methods is that it does not re- quire another pre-trained model to generate candi- date actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parameter Learning</head><p>We use the deep Q-learning algorithm ( <ref type="bibr" target="#b16">Mnih et al., 2015)</ref>, which is a variant of the traditional Q-learning algorithm ( <ref type="bibr" target="#b27">Watkins and Dayan, 1992)</ref>, to learn the model parameters of Q(s, a; θ) from the received rewards. The complete training proce- dure is shown in Algorithm 2 in the Appendix. The network parameters θ are first initialized ar- bitrarily. At each time step t, the agent selects an action a t according to the -greedy policy, re- ceives a reward r t+1 , and transits to the next state s t+1 . Thus, it yields a transition tuple, ζ t = (s t , a t , r t+1 , s t+1 ). Instead of using the current transition tuple, ζ t , to update the parameters, we first store ζ t into an experience memory, D. This experience memory has a limited capacity, |D|, and the stored transition tuples are rewritten in a first-in-first-out manner. Then, we sample mini- batches of transition tuples (s, a, r, s ) from D uni- formly at random. Using the sampled transition tuples, we perform a step of stochastic gradient de- scent to minimize the following loss function, <ref type="formula">(5)</ref> where y = r + γ max a Q(s , a ; θ − ) is the Q- learning target, θ − are the network parameters of the Q-learning target. We update θ − to match the network parameters, θ, after every F time steps, where F is a hyperparameter.</p><formula xml:id="formula_8">L(θ) = E (s,a,r,s )∼U (D) [(y − Q(s, a; θ)) 2 ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Setup</head><p>In the experiments, we analyze the performances of different neural network models and differ- ent approximation procedures. First, their perfor- mances are evaluated on five real-world datasets, with a fix action size K. Then, we evaluate their performances with different action sizes, on one dataset. For each experiment setting, we do the following comparative analysis:</p><p>• Compare the performance of our DRRN- Attention model with the baseline models us- ing different approximation procedures.</p><p>• Compare the performance of the greedy pro- cedure with the baseline approximation pro- cedures using different neural network mod- els.</p><p>• Find the approach (combination of neural network model and approximation proce- dure) that achieves the best performance.</p><p>Finally, we conduct a case study to better illus- trate the difference between our DRRN-Attention model and the DRRN-BiLSTM baseline.  <ref type="table">Table 1</ref>: Basic statics of discussion threads data from five subreddits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subreddit # Posts # Comments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Datasets</head><p>All the experiments are conducted on discussion thread data from the Reddit discussion forum. In Reddit, threads are grouped into different cate- gories, called subreddits, according to different discussion themes. Registered users are allowed to give up-votes or down-votes to a comment, these votes are then aggregated to compute a karma score for the comment. We use it as the re- ward for recommending that comment. Using the post IDs provided by <ref type="bibr" target="#b13">He et al. (2016c)</ref>, we crawl five datasets from five different subreddits respectively, including askscience, askmen, today- ilearned, worldnews, and nfl. These subreddits cover a wide range of discussion topics and lan- guage styles. The basic statistics of the datasets are presented in <ref type="table">Table 1</ref>. Since some of the posts and comments were deleted by Reddit, we re- move all the deleted posts and comments from the datasets. Thus, the statistics of our datasets are different from that in <ref type="bibr" target="#b13">He et al. (2016c)</ref>. For each dataset, we use the simulator provided by <ref type="bibr" target="#b13">He et al. (2016c)</ref> to partition 90% of the data as a training set, and 10% of the data as a testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Evaluation</head><p>The evaluation metric is the cumulative reward per episode averaged over 1,000 episodes ( <ref type="bibr" target="#b13">He et al., 2016c</ref>). For each setting, we evaluate an agent as follows. First, we train the agent on the training set using Algorithm 2 in the Appendix for 3,500 episodes. Then, we test the agent using the test- ing set for 1,000 episodes and choose every action according to the -greedy policy, but the agent can- not use the received rewards to update the model parameters. We repeat the testing for five repe- titions and report the mean and the standard de- viation of the evaluation metric. Throughout the training and testing, we fix = 0.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Baselines</head><p>Our DRRN-Attention model is compared with two baselines. The first one is DRRN-BiLSTM, which is the current state-of-the-art model to approxi- mate the Q-value for this task ( <ref type="bibr" target="#b13">He et al., 2016c</ref>). We modify the DRRN-BiLSTM model by replac- ing the Bi-directional LSTM with a mean opera- tor and call this new model DRRN-Mean. This DRRN-mean is used as the second baseline model. In addition, we compare the greedy procedure with two baseline approximation procedures. The first is random sampling procedure in ( <ref type="bibr" target="#b13">He et al., 2016c</ref>). The second is the two-stage Q-learning procedure in ( , which is the state- of-the-art approximation procedure for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Implementation Details</head><p>In preprocessing, we remove all punctuations and lowercase all alphabetic characters. To construct the bag-of-words representations, we use the dic- tionary provided by <ref type="bibr" target="#b13">He et al. (2016c)</ref>. This dictio- nary contains the most frequent 5,000 words in the data. All model parameters are initialized by a uni- form distribution within the interval [−0.1, 0.1].</p><p>In our DRRN-Attention model, we set the com- ment embedding size d to 16, the hidden size of the LSTM h to 16, the hidden size of the atten- tion mechanism h to 16, and the steps of atten- tion L to 2. In the text embedding module of DRRN-Attention, each fully-connected layer has a hidden size of 16. In the baseline models, we set the hidden size of the bidirectional LSTM to 20, the comment embedding size to 20. The text embedding module of the baselines has two lay- ers and each layer has a hidden size of 20. For the deep Q-learning algorithm, we set F = 1000. We update the model parameters using stochas- tic gradient descent with RMSprop (Tieleman and Hinton, 2012). We set different initial learning rates for different datasets (askscience: 0.00001; askmen: 0.00008; todayilearned, worldnews, and nfl: 0.00002). The mini batch size is 100. All the above hyperparameters are tuned by five-fold cross-validation. We also found that the model performances tuned by five-fold cross-validation are similar to that tuned by the testing set. We set the remaining hyperparameters according to ). The memory size |D| is set to 10000. The discount factor γ is set to 0.9. The candidate size m of the baseline approximation procedures is set to 10.</p><p>8 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Agent Performances on Various Datasets</head><p>In this section, the performances of different neu- ral network models and approximation procedures are evaluated on five datasets with N = 10, K = 3. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. We analyze the performances of our DRRN-Attention model in each approximation procedure. With the ran- dom sampling procedure, our model achieves a higher cumulative reward than the baseline mod- els across all datasets. When using the two- stage Q-learning procedure, or the greedy pro- cedure, our model outperforms the baselines on four of the datasets; its performance is compet- itive to the baselines in the remaining dataset. Next, we analyze the performances of the greedy procedure in each neural network model. When using the DRRN-BiLSTM model or the DRRN- Mean model to parameterize the Q-function, the greedy procedure achieves a higher cumulative re- ward than other two baseline procedures across all datasets. When using the DRRN-Attention model, the greedy procedure outperforms the baseline procedures in four of the datasets. In sum up, us-  ing DRRN-Attention model with the greedy pro- cedure outperforms all the baselines in four of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Agent Performances on Various Action Sizes</head><p>We evaluate all the neural network models and approximation procedures across various action sizes with K = 2, 3, 4, 5 and fix N = 10 on the askscience dataset. The results are presented in <ref type="table" target="#tab_5">Table 3</ref>. Our DRRN-Attention model outper- forms the baselines across all the action sizes form K = 2 to K = 5 with the random sampling pro- cedure or the greedy procedure. With the two- stage Q-learning procedure, our model achieves a higher cumulative reward than the baseline mod- els when K = 2, 3, 4. Then, we analyze the per- formance of the greedy procedure in each neural network model. When using the DRRN-BiLSTM to parameterize the Q-function, the greedy pro- cedure outperforms the baseline procedures when K = 3, 4, 5. When using our DRRN-Attention, the greedy procedure achieves a higher cumulative reward than the baselines when K = 2, 3, 5. Over- all, using the DRRN-Attention model with the greedy procedure achieves the best performances across all the action sizes form K = 2 to K = 5.  ferent Q-value prediction. The Q-value predic- tion of the permutation (1, 3, 2) almost triples that of the permutation (2, 3, 1). On the other hand, any permutation of the comments does not change the Q-value prediction when we use our DRRN-Attention model. This example demon- strates that the ordering of the comments can sig- nificantly affect the predicted Q-value when we use the DRRN-BiLSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Case Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Discussions</head><p>As mentioned in Section 7.1, the datasets that we use have a fewer number of comments than the datasets used by previous work.   pares the number of comments in the datasets used by <ref type="bibr" target="#b13">(He et al., 2016c</ref>) and us. The experiments in ) used three of the datasets (askscience, askmen, and todayilearned) from (He et al., 2016c). We compare our results and the re- sults reported in ( ) of the DRRN- BiLSTM + two-stage Q-learning baseline on these three datasets in <ref type="table" target="#tab_10">Table 6</ref>. On the askscience and to- dayilearned datasets, the results of our implemen- tation are worse than the results reported by them.</p><p>Since the number of comments in our askscience dataset is only half of that in , the results of our implementation on the askscience dataset are significantly worse than the results re- ported by them. On the askmen dataset, the num- ber of comments that we use is slightly less than the askmen dataset used by them. However, the results of our implementation on askmen are bet- ter than the results reported by them. We suspect that the deleted comments may have low karma scores, which cause the agent to achieve a higher cumulative reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we propose a new approach to the task of thread popularity prediction and tracking. In our approach, we propose a new neural network architecture, DRRN-Attention, to approximate the Q-function, which well respect the permutation in- variance of the comments in an action. Moreover, our approach employs the greedy procedure to approximate the predicted optimal action, which does not require an additional pre-trained model to generate candidate actions. Empirical studies on real data demonstrate that our approach beats the current state-of-the-art in most of the experi- mental settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2</head><label></label><figDesc>Related Work 2.1 Reinforcement Learning in Text-based Tasks Reinforcement learning has been widely applied in various text-based tasks. There are several ar- ticles in literature studying the tasks of mapping instruction manuals to a sequence of commands, such as game commands (Branavan et al., 2011), software commands (Branavan et al., 2010), and navigation directions (Vogel and Jurafsky, 2010). In the task of text-based game, an agent selects a textual command from a set of feasible com- mands at every time step. Narasimhan et al. (2016) considered a special case that all the tex- tual commands have a fixed structure, while He et al. (2016b) and Chen et al. (2017) considered another case that all commands are free text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Comparison of average episodic reward on different datasets.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of average episodic reward with different action sizes on askscience dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 presents</head><label>4</label><figDesc></figDesc><table>an example of Q-value prediction 
of a state and three sub-actions on the askscience 
dataset. In this study, we enumerate every per-
mutation of these three sub-actions, e.g., (1, 3, 2) 
denotes a permutation of comments that we place 
comment (1) in the first position, comment (3) in 
the second position, and comment (2) in the third 
position. Then, we use a trained DRRN-BiLSTM 
model and a trained DRRN-Attention model to 
predict the Q-value of each permutation of com-
ments. When we use the DRRN-BiLSTM model, 
a different permutation of comments yields a dif-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>A Q-value prediction example using DRRN-
BiLSTM and DRRN-Attention. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 com-</head><label>5</label><figDesc></figDesc><table>Subreddit 
# Cmts (their) # Cmts (our) 
askscience 
0.32M 
0.15M 
askmen 
1.06M 
0.94M 
todayilearned 
5.11M 
4.65M 
worldnews 
5.99M 
4.28M 
nfl 
6.12M 
5.72M 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The number of comments in the datasets used 
by He et al. (2016c) and us. 

Subreddit 
Reward (their) Reward (our) 
askscience 
833.9 
643.2 
askmen 
148.0 
184.3 
todayilearned 
697.9 
659.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>The cumulative reward achieved by the 
DRRN-BiLSTM + two-stage Q-learning baseline re-
ported by He et al. (2017) and us. 

</table></figure>

			<note place="foot" n="1"> https://www.reddit.com/</note>

			<note place="foot">t ,. .. , c K t }, where c i t ∈ c t for i = 1,. .. , K and c i t = c j t for i = j.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to win by reading manuals in a monte-carlo framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reading between the lines: Learning to map high-level instructions to commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ask the right questions: Active question reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Q-LDA: uncovering latent patterns in text-based sequential decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4984" to="4993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A factored neural network model for characterizing online discussions in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2296" to="2306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Can cascades be predicted? In WWW</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Alex</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Dow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="925" to="936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with a natural language action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Reinforcement learning with external knowledge and two-stage q-functions for predicting popular reddit threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1704.06217</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with a combinatorial action space for predicting popular reddit threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1838" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting popular messages in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ovidiu</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="57" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1192" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Charles Beattie</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
			<publisher>Shane Legg, and Demis Hassabis</publisher>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2355" to="2365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Continuously learning neural dialogue management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">Maria</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
		<idno>abs/1606.02689</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The effect of wording on message propagation: Topicand author-controlled natural experiments on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="175" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<idno>abs/1710.10903</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jch</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What&apos;s worthy of comment? content and comment volume in political blogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic key-value memory networks for knowledge tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<idno>abs/1803.07294</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
