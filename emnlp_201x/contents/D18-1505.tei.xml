<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting the Importance of Encoding Logic Rules in Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Bombay ♠ University of Massachusetts</orgName>
								<address>
									<settlement>Amherst ♣</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♠</forename><forename type="middle">♣</forename><surname>Preethi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Bombay ♠ University of Massachusetts</orgName>
								<address>
									<settlement>Amherst ♣</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothi</forename><forename type="middle">♠</forename></persName>
							<email>pjyothi@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Bombay ♠ University of Massachusetts</orgName>
								<address>
									<settlement>Amherst ♣</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Bombay ♠ University of Massachusetts</orgName>
								<address>
									<settlement>Amherst ♣</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting the Importance of Encoding Logic Rules in Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4743" to="4751"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4743</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We analyze the performance of different sentiment classification models on syntactically-complex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Pe-ters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo&apos;s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we explore the effectiveness of meth- ods designed to improve sentiment classification (positive vs. negative) of sentences that con- tain complex syntactic structures. While simple bag-of-words or lexicon-based methods <ref type="bibr">(Pang and Lee, 2005</ref>; <ref type="bibr">Wang and Manning, 2012;</ref><ref type="bibr">Iyyer et al., 2015</ref>) achieve good performance on this task, they are unequipped to deal with syntactic structures that affect sentiment, such as contrastive conjunc- tions (i.e., sentences of the form "A-but-B") or negations. Neural models that explicitly encode word order <ref type="bibr">(Kim, 2014</ref>), syntax <ref type="bibr">(Socher et al., 2013;</ref><ref type="bibr">Tai et al., 2015</ref>) and semantic features ( <ref type="bibr">Li et al., 2017</ref>) have been proposed with the aim of improving performance on these more compli- cated sentences. Recently, <ref type="bibr">Hu et al. (2016)</ref> in- corporate logical rules into a neural model and show that these rules increase the model's accu- racy on sentences containing contrastive conjunc- tions, while <ref type="bibr">Peters et al. (2018a)</ref> demonstrate in- creased overall accuracy on sentiment analysis by initializing a model with representations from a language model trained on millions of sentences.</p><p>In this work, we carry out an in-depth study of the effectiveness of the techniques in <ref type="bibr">Hu et al. (2016)</ref> and <ref type="bibr">Peters et al. (2018a)</ref> for sentiment clas- sification of complex sentences. Part of our con- tribution is to identify an important gap in the methodology used in <ref type="bibr">Hu et al. (2016)</ref> for perfor- mance measurement, which is addressed by av- eraging the experiments over several executions. With the averaging in place, we obtain three key findings: (1) the improvements in <ref type="bibr">Hu et al. (2016)</ref> can almost entirely be attributed to just one of their two proposed mechanisms and are also less pronounced than previously reported; (2) contex- tualized word embeddings ( <ref type="bibr">Peters et al., 2018a)</ref> incorporate the "A-but-B" rules more effectively without explicitly programming for them; and (3) an analysis using crowdsourcing reveals a big- ger picture where the errors in the automated sys- tems have a striking correlation with the inherent sentiment-ambiguity in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Logic Rules in Sentiment Classification</head><p>Here we briefly review background from <ref type="bibr">Hu et al. (2016)</ref> to provide a foundation for our reanalysis in the next section. We focus on a logic rule for sentences containing an "A-but-B" structure (the only rule for which <ref type="bibr">Hu et al. (2016)</ref> provide exper- imental results). Intuitively, the logic rule for such sentences is that the sentiment associated with the whole sentence should be the same as the senti- ment associated with phrase "B". 1 More formally, let p θ (y|x) denote the proba- bility assigned to the label y ∈ {+, −} for an input x by the baseline model using parameters θ. A logic rule is (softly) encoded as a variable r θ (x, y) ∈ [0, 1] indicating how well labeling x with y satisfies the rule. For the case of A-but-B sentences, r θ (x, y) = p θ (y|B) if x has the struc- ture A-but-B (and 1 otherwise). Next, we discuss the two techniques from <ref type="bibr">Hu et al. (2016)</ref> for in- corporating rules into models: projection, which directly alters a trained model, and distillation, which progressively adjusts the loss function dur- ing training.</p><p>Projection. The first technique is to project a trained model into a rule-regularized subspace, in a fashion similar to <ref type="bibr">Ganchev et al. (2010)</ref>. More precisely, a given model p θ is projected to a model q θ defined by the optimum value of q in the fol- lowing optimization problem: 2 min q,ξ≥0</p><formula xml:id="formula_0">KL(q(X, Y )||p θ (X, Y )) + C x∈X ξ x s.t. (1 − E y←q(·|x) [r θ (x, y)]) ≤ ξ x</formula><p>Here q(X, Y ) denotes the distribution of (x, y) when x is drawn uniformly from the set X and y is drawn according to q(·|x).</p><p>Iterative Rule Knowledge Distillation. The second technique is to transfer the domain knowl- edge encoded in the logic rules into a neural network's parameters. Following <ref type="bibr">Hinton et al. (2015)</ref>, a "student" model p θ can learn from the "teacher" model q θ , by using a loss function πH(p θ , P true ) + (1 − π)H(p θ , q θ ) during training, where P true denotes the distribution implied by the ground truth, H(·, ·) denotes the cross-entropy function, and π is a hyperparameter. <ref type="bibr">Hu et al. (2016)</ref> computes q θ after every gradient update by projecting the current p θ , as described above. Note that both mechanisms can be combined: Af- ter fully training p θ using the iterative distillation process above, the projection step can be applied one more time to obtain q θ which is then used as the trained model.</p><p>Dataset. All of our experiments (as well as those in <ref type="bibr">Hu et al. (2016)</ref>) use the SST2 dataset, a binarized subset of the popular Stanford Senti- ment Treebank (SST) <ref type="bibr">(Socher et al., 2013</ref>). The dataset includes phrase-level labels in addition to sentence-level labels (see <ref type="table">Table 1</ref> for detailed statistics); following <ref type="bibr">Hu et al. (2016)</ref>, we use both types of labels for the comparisons in Section 3.2. In all other experiments, we use only sentence- level labels, and our baseline model for all exper- iments is the CNN architecture from <ref type="bibr">Kim (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Reanalysis</head><p>In this section we reanalyze the effectiveness of the techniques of <ref type="bibr">Hu et al. (2016)</ref> and find that most of the performance gain is due to projection and not knowledge distillation. The discrepancy with the original analysis can be attributed to the relatively small dataset and the resulting variance across random initializations. We start by analyz- ing the baseline CNN by <ref type="bibr">Kim (2014)</ref> to point out the need for an averaged analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Importance of Averaging</head><p>We run the baseline CNN by <ref type="bibr">Kim (2014)</ref>   <ref type="table">Table 1</ref>: Statistics of SST2 dataset. Here "Discourse" in- cludes both A-but-B and negation sentences. The mean length of sentences is in terms of the word count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reported Test Accuracy (Hu et al., 2016) Averaged Test Accuracy</head><p>Averaged A-but-B accuracy  bels. We observe a large amount of variation from run-to-run, which is unsurprising given the small dataset size. The inset density plot in <ref type="figure" target="#fig_0">Figure 1</ref> shows the range of accuracies (83.47 to 87.20) along with 25, 50 and 75 percentiles. <ref type="bibr">3</ref> The figure also shows how the variance persists even after the average converges: the accuracies of 100 models trained for 20 epochs each are plotted in gray, and their average is shown in red.</p><formula xml:id="formula_1">no-distill distill no-distill distill no-distill distill no-project</formula><p>We conclude that, to be reproducible, only av- eraged accuracies should be reported in this task and dataset. This mirrors the conclusion from a detailed analysis by <ref type="bibr">Reimers and Gurevych (2017)</ref> in the context of named entity recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance of Hu et al. (2016)</head><p>We carry out an averaged analysis of the publicly available implementation 4 of <ref type="bibr">Hu et al. (2016)</ref>. Our analysis reveals that the reported performance of their two mechanisms (projection and distil- lation) is in fact affected by the high variability across random seeds. Our more robust averaged analysis yields a somewhat different conclusion of their effectiveness.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, the first two columns show the re- ported accuracies in <ref type="bibr">Hu et al. (2016)</ref> for models trained with and without distillation (correspond- ing to using values π = 1 and π = 0.95 t in the t th epoch, respectively). The two rows show the results for models with and without a final projec- tion into the rule-regularized space. We keep our hyper-parameters identical to <ref type="bibr">Hu et al. (2016)</ref>. <ref type="bibr">5</ref> The baseline system (no-project, no-distill) is identical to the system of <ref type="bibr">Kim (2014)</ref>. All the sys- tems are trained on the phrase-level SST2 dataset with early stopping on the development set. The number inside each arrow indicates the improve- ment in accuracy by adding either the projection or the distillation component to the training al- gorithm. Note that the reported figures suggest that while both components help in improving ac- curacy, the distillation component is much more helpful than the projection component.</p><p>The next two columns, which show the re- sults of repeating the above analysis after averag- ing over 100 random seeds, contradict this claim. The averaged figures show lower overall accuracy increases, and, more importantly, they attribute these improvements almost entirely to the projec- tion component rather than the distillation com- ponent. To confirm this result, we repeat our av- eraged analysis restricted to only "A-but-B" sen- tences targeted by the rule (shown in the last two columns). We again observe that the effect of pro- jection is pronounced, while distillation offers lit- tle or no advantage in comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Contextualized Word Embeddings</head><p>Traditional context-independent word embed- dings like word2vec (Mikolov et al., 2013) or GloVe ( <ref type="bibr">Pennington et al., 2014</ref>) are fixed vec- tors for every word in the vocabulary. In contrast, contextualized embeddings are dynamic represen- tations, dependent on the current context of the word. We hypothesize that contextualized word embeddings might inherently capture these logic rules due to increasing the effective context size for the CNN layer in <ref type="bibr">Kim (2014)</ref>. Following the recent success of ELMo ( <ref type="bibr">Peters et al., 2018a</ref>) in sentiment analysis, we utilize the TensorFlow Hub implementation of ELMo 6 and feed these contex- tualized embeddings into our CNN model. We fine-tune the ELMo LSTM weights along with the CNN weights on the downstream CNN task. As in Section 3, we check performance with and without the final projection into the rule-regularized space. We present our results in <ref type="table">Table 2</ref>.</p><p>Switching to ELMo word embeddings improves performance by 2.9 percentage points on an aver- age, corresponding to about 53 test sentences. Of these, about 32 sentences (60% of the improve- ment) correspond to A-but-B and negation style sentences, which is substantial when considering that only 24.5% of test sentences include these dis- course relations <ref type="table">(Table 1)</ref>. As further evidence that ELMo helps on these specific constructions, the non-ELMo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on av- erage, only 89 (34.8%) of which are A-but-B style or negations.</p><p>Statistical Significance: Using a two-sided Kolmogorov-Smirnov statistic <ref type="bibr">(Massey Jr, 1951</ref>) with α = 0.001 for the results in <ref type="table">Table 2</ref>, we find that ELMo and projection each yield statistically significant improvements, but distillation does not. Also, with ELMo, projection is not significant. Specific comparisons have been added in the Ap- pendix, in <ref type="table" target="#tab_4">Table A3</ref>.</p><p>KL Divergence Analysis: We observe no sig- nificant gains by projecting a trained ELMo model into an A-but-B rule-regularized space, unlike the other models. We confirm that ELMo's predic- tions are much closer to the A-but-B rule's man- ifold than those of the other models by computing KL(q θ ||p θ ) where p θ and q θ are the original and projected distributions: Averaged across all A-but- B sentences and 100 seeds, this gives 0.27, 0.26 and 0.13 for the <ref type="bibr">Kim (2014)</ref>, <ref type="bibr">Hu et al. (2016)</ref> with distillation and ELMo systems respectively.</p><p>Intra-sentence Similarity: To understand the information captured by ELMo embeddings for A-but-B sentences, we measure the cosine simi- larity between the word vectors of every pair of words within the A-but-B sentence ( <ref type="bibr">Peters et al., 2018b</ref>). We compare the intra-sentence similar- ity for fine-tuned word2vec embeddings (base- line), ELMo embeddings without fine-tuning and finally fine-tuned ELMo embeddings in <ref type="figure" target="#fig_5">Figure 3</ref>. In the fine-tuned ELMo embeddings, we notice the words within the A and within the B part of the A-but-B sentence share the same part of the vector space. This pattern is less visible in the  <ref type="table">Table 2</ref>: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sen- tences ("but"), negations ("neg").</p><p>ELMo embeddings without fine-tuning and absent in the word2vec embeddings. This observation is indicative of ELMo's ability to learn specific rules for A-but-B sentences in sentiment classifica- tion. More intra-sentence similarity heatmaps for A-but-B sentences are in <ref type="figure" target="#fig_0">Figure A1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Crowdsourced Experiments</head><p>We conduct a crowdsourced analysis that reveals that SST2 data has significant levels of ambiguity even for human labelers. We discover that ELMo's performance improvements over the baseline are robust across varying levels of ambiguity, whereas the advantage of <ref type="bibr">Hu et al. (2016)</ref> is reversed in sentences of low ambiguity (restricting to A-but-B style sentences). Our crowdsourced experiment was conducted on <ref type="figure">Figure Eight</ref>. 8 Nine workers scored the senti- ment of each A-but-B and negation sentence in the test SST2 split as 0 (negative), 0.5 (neutral) or 1 (positive). (SST originally had three crowdwork- ers choose a sentiment rating from 1 to 25 for ev- ery phrase.) More details regarding the crowd ex- periment's parameters have been provided in Ap- pendix A.</p><p>We average the scores across all users for each sentence. Sentences with a score in the range (x, 1] are marked as positive (where x ∈ [0.5, 1)), sentences in [0, 1 − x) marked as negative, and sentences in <ref type="bibr">[1 − x, x]</ref> are marked as neutral. For instance, "flat , but with a revelatory perfor- mance by michelle williams" (score=0.56) is neu- tral when x = 0.6. <ref type="bibr">9</ref> We present statistics of our dataset 10 in <ref type="table" target="#tab_4">Table 3</ref>. Inter-annotator agree- <ref type="bibr">7</ref> Trained on sentences and not phrase-level labels for a fair comparison with baseline and ELMo, unlike Section 3.2.</p><p>8 https://www.figure-eight.com/ 9 More examples of neutral sentences have been provided in the Appendix in <ref type="table" target="#tab_6">Table A1</ref>, as well as a few "flipped" sen- tences receiving an average score opposite to their SST2 label <ref type="table" target="#tab_7">(Table A2)</ref>. <ref type="bibr">10</ref> The dataset along with source code can be found in  <ref type="table">slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it  interesting   there  are  slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it</ref>   <ref type="table">4   there  are  slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it  interesting   there  are  slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it</ref>   <ref type="table">6   there  are  slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it  interesting   there  are  slow  and  repetitive  parts  ,  but  it  has  just  enough  spice  to  keep  it</ref>  ment was computed using Fleiss' Kappa (κ). As expected, inter-annotator agreement is higher for higher thresholds (less ambiguous sentences). Ac- cording to <ref type="bibr">Landis and Koch (1977)</ref>, κ ∈ (0.2, 0.4] corresponds to "fair agreement", whereas κ ∈ (0.4, 0.6] corresponds to "moderate agreement". We next compute the accuracy of our model for each threshold by removing the correspond- ing neutral sentences. Higher thresholds corre- spond to sets of less ambiguous sentences. <ref type="table" target="#tab_4">Table 3</ref> shows that ELMo's performance gains in <ref type="table">Table 2</ref> extends across all thresholds. In <ref type="figure" target="#fig_7">Figure 4</ref> we com- pare all the models on the A-but-B sentences in this set. Across all thresholds, we notice trends similar to previous sections: 1) ELMo performs the best among all models on A-but-B style sentences, and projection results in only a slight improvement; 2) models in <ref type="bibr">Hu et al. (2016)</ref> (with and without distil- lation) benefit considerably from projection; but 3) distillation offers little improvement (with or with- out projection). Also, as the ambiguity threshold increases, we see decreasing gains from projection on all models. In fact, beyond the 0.85 threshold, projection degrades the average performance, in- dicating that projection is useful for more ambigu- ous sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present an analysis comparing techniques for incorporating logic rules into sentiment classifi- cation systems. Our analysis included a meta- study highlighting the issue of stochasticity in performance across runs and the inherent ambi- guity in the sentiment classification task itself, which was tackled using an averaged analysis and https://github.com/martiansideofthemoon/ logic-rules-sentiment.   no-distill, no-project no-distill, project distill, no-project distill, project ELMo, no-project ELMo, project       For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the map.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Variation in models trained on SST-2 (sentenceonly). Accuracies of 100 randomly initialized models are plotted against the number of epochs of training (in gray), along with their average accuracies (in red, with 95% confidence interval error bars). The inset density plot shows the distribution of accuracies when trained with early stopping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparison of the accuracy improvements reported in Hu et al. (2016) and those obtained by averaging over 100 random seeds. The last two columns show the (averaged) accuracy improvements for A-but-B style sentences. All models use the publicly available implementation of Hu et al. (2016) trained on phrase-level SST2 data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The left figure has fine-tuned word2vec embeddings. The middle figure contains the original ELMo embeddings without any fine-tuning. The right figure contains fine-tuned ELMo embeddings. For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the heat map.</figDesc><graphic url="image-13.png" coords="5,100.17,87.96,98.57,98.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>(</head><label></label><figDesc>447 sentences) which got marked as neutral and which got the opposite of their labels in the SST2 dataset, using vari- ous thresholds. Inter-annotator agreement is computed using Fleiss' Kappa. Average accuracies of the baseline and ELMo (over 100 seeds) on non-neutral sentences are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average performance on the A-but-B part of the crowd-sourced dataset (210 sentences, 100 seeds)). For each threshold, only non-neutral sentences are used for evaluation. a crowdsourced experiment identifying ambiguous sentences. We present evidence that a recently proposed contextualized word embedding model (ELMo) (Peters et al., 2018a) implicitly learns logic rules for sentiment classification of complex sentences like A-but-B sentences. Future work includes a fine-grained quantitative study of ELMo word vectors for logically complex sentences along the lines of Peters et al. (2018b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>#</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>Figure A1: Heat map showing the cosine similarity between pairs of word vectors within a single sentence. The leftmost column has word2vec (Mikolov et al., 2013) embeddings, fine-tuned on the downstream task (SST2). The middle column contains the original ELMo embeddings (Peters et al., 2018a) without any fine-tuning. The representations from the three layers (token layer and two LSTM layers) have been averaged. The rightmost column contains ELMo embeddings fine-tuned on the downstream task. For better visualization, the cosine similarity between identical words has been set equal to the minimum value in the map.</figDesc><graphic url="image-31.png" coords="9,103.20,466.54,95.90,95.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Number of sentences in the crowdsourced study</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table A1 :</head><label>A1</label><figDesc></figDesc><table>Examples of neutral sentences for a threshold of 0.66 

# Judgments 
Average Original Sentence 
Positive Negative Neutral 

1 
5 
3 
0.28 
Positive 

de niro and mcdormand give solid perfor-
mances , but their screen time is sabotaged by 
the story 's inability to create interest 

6 
0 
3 
0.83 
Negative 

son of the bride may be a good half hour too 
long but comes replete with a flattering sense 
of mystery and quietness 

0 
5 
4 
0.22 
Positive 

wasabi is slight fare indeed , with the entire 
project having the feel of something tossed 
off quickly ( like one of hubert 's punches ) 
, but it should go down smoothly enough with 
popcorn 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table A2 :</head><label>A2</label><figDesc></figDesc><table>Examples of flipped sentiment sentences, for a threshold of 0.66 

Model 1 
vs 
Model 2 
Significant 

distill no-project 
distill 
project 
Yes 
no-distill no-project 
no-distill 
project 
Yes 
ELMo no-project 
ELMo 
project 
No 

no-distill no-project 
distill no-project 
No 
no-distill 
project 
distill 
project 
No 

no-distill no-project 
ELMo no-project 
Yes 
distill no-project 
ELMo no-project 
Yes 
no-distill 
project 
ELMo 
project 
Yes 
distill 
project 
ELMo 
project 
Yes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table A3 :</head><label>A3</label><figDesc></figDesc><table>Statistical significance using a two-sided Kolmogorov-Smirnov statistic (Massey Jr, 1951) with α = 0.001. all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 

all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 
0.1 

0.0 

0.1 

0.2 

0.3 

0.4 
all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 

all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 
0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 

all 
ends 
well 
, 
sort 
of 
, 
but 
the 
frenzied 
comic 
moments 
never 
click 

0.1 

0.2 

0.3 

0.4 

0.5 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 
0.1 

0.0 

0.1 

0.2 

0.3 

0.4 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 
0.2 

0.3 

0.4 

0.5 

0.6 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 

marisa 
tomei 
is 
good 
, 
but 
just 
a 
kiss 
is 
just 
a 
mess 
0.0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

the 
irwins 
emerge 
unscathed 

, 
but 
the 
fictional 
footage 

is 
unconvincing 

and 
criminally 
badly 
acted 

the 
irwins 
emerge 
unscathed 
, 
but 
the 
fictional 
footage 
is 
unconvincing 
and 
criminally 
badly 
acted 
0.1 

0.0 

0.1 

0.2 

0.3 

the 
irwins 
emerge 
unscathed 

, 
but 
the 
fictional 
footage 

is 
unconvincing 

and 
criminally 
badly 
acted 

the 
irwins 
emerge 
unscathed 
, 
but 
the 
fictional 
footage 
is 
unconvincing 
and 
criminally 
badly 
acted 
0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

the 
irwins 
emerge 
unscathed 

, 
but 
the 
fictional 
footage 

is 
unconvincing 

and 
criminally 
badly 
acted 

the 
irwins 
emerge 
unscathed 
, 
but 
the 
fictional 
footage 
is 
unconvincing 
and 
criminally 
badly 
acted 

</table></figure>

			<note place="foot" n="1"> The rule is vacuously true if the sentence does not have this structure.</note>

			<note place="foot" n="2"> The formulation in Hu et al. (2016) includes another hyperparameter λ per rule, to control its relative importance; when there is only one rule, as in our case, this parameter can be absorbed into C.</note>

			<note place="foot" n="3"> We use early stopping based on validation performance for all models in the density plot. 4 https://github.com/ZhitingHu/logicnn/ 5 In particular, C = 6 for projection.</note>

			<note place="foot" n="6"> https://tfhub.dev/google/elmo/1</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Crowdsourcing Details</head><p>Crowd workers residing in five English-speaking countries (United States, United Kingdom, New Zealand, Australia and Canada) were hired. Each crowd worker had a Level 2 or higher rating on <ref type="figure">Figure Eight</ref>, which corresponds to a "group of more experienced, higher accuracy contributors". Each contributor had to pass a test questionnaire to be eligible to take part in the experiment. Test questions were also hidden throughout the task and untrusted contributions were removed from the final dataset. For greater quality control, an upper limit of 75 judgments per contributor was enforced. Crowd workers were paid a total of $1 for 50 judg- ments. An internal unpaid workforce (including the first and second author of the paper) of 7 con- tributors was used to speed up data collection.</p></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
