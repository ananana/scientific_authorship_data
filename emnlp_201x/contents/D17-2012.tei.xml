<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Function Assistant: A Tool for NL Querying of APIs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Natural Language Processing</orgName>
								<orgName type="institution">University of Stuttgart</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Function Assistant: A Tool for NL Querying of APIs</title>
					</analytic>
					<monogr>
						<title level="j" type="main">EMNLP System Demonstrations</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="67" to="72"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we describe Function Assistant, a lightweight Python-based toolkit for querying and exploring source code repositories using natural language. The toolkit is designed to help end-users of a target API quickly find information about functions through high-level natural language queries and descriptions. For a given text query and background API, the tool finds candidate functions by performing a translation from the text to known representations in the API using the semantic parsing approach of Richard-son and Kuhn (2017). Translations are automatically learned from example text-code pairs in example APIs. The toolkit includes features for building translation pipelines and query engines for arbitrary source code projects. To explore this last feature, we perform new experiments on 27 well-known Python projects hosted on Github.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Software developers frequently shift between us- ing different third-party software libraries, or APIs, when developing new applications. Much of the development time is dedicated to understand- ing the structure of these APIs, figuring out where the target functionality lies, and learning about the peculiarities of how such software is structured or how naming conventions work. When the target API is large, finding the desired functionality can be a formidable and time consuming task. Of- ten developers resort to resources like Google or StackOverflow to find (usually indirect) answers to questions.</p><p>We illustrate these issues in <ref type="figure" target="#fig_0">Figure 1</ref> using two example functions from the well-known NLTK toolkit. Each function is paired with a short docstring, i.e., the quoted description under each function, which provides a user of the software a description of what the function does. While understanding the documentation and code re- quires technical knowledge of dependency pars- ing and graphs, even with such knowledge, the function naming conventions are rather arbitrary. The function add arc could just as well be called create arc. An end-user expecting an- other naming convention might be left astray when searching for this functionality. Similarly, the available description might deviate from how an end-user would describe such functionality. Understanding the remove by address function, in contrast, requires knowing the details of the particular DependencyGraph implementation being used. Nonetheless, the function corresponds to the standard operation of removing a node from a dependency graph. Here, the technical details about how this removal is specific to a given address might obfuscate the overall purpose of the function, making it hard to find or understand.</p><p>At a first approximation, navigating a given API requires knowing correspondences between tex- tual descriptions and source code representations. For example, knowing that the English expression Adds an arc in <ref type="figure" target="#fig_0">Figure 1</ref> translates (somewhat ar- bitrarily) to add arc, or that given address trans- lates to address. One must also know how to detect paraphrases of certain target entities or ac- tions, for example that adding an arc means the same as creating an arc in this context. Other technical correspondences, such as the relation be- tween an address and the target dependency graph implementation, must be learned.</p><p>In our previous work ( <ref type="bibr" target="#b5">Richardson and Kuhn (2017)</ref>, henceforth RK), we look at learning these types of correspondences from example API col- lections in a variety of programming languages and source natural languages. We treat each given API, consisting of text and function rep- resentation pairs, as a parallel corpus for train- ing a simple semantic parsing model. In addi- tion to learning translational correspondences, of the type described above, we achieve improve- ments by adding document-level features that help to learn other technical correspondences.</p><p>In this paper, we focus on using our models as a tool for querying API collections. Given a tar- get API, our model learns an MT-based seman- tic parser that translates text to code representa- tions in the API. End-users can formulate natural language queries to the background API, which our model will translate into candidate function representations with the goal of finding the de- sired functionality. Our tool, called Function Assistant can be used in two ways: as a black- box pipeline for building models directly from ar- bitrary API collections. As well, it can be cus- tomized and integrated with other outside compo- nents or models using the tool's flexible internal Python API.</p><p>In this paper, we focus on the first usage of our tool. To explore building models for new API col- lections, we run our pipeline on 27 open source Python projects from the well-known Awesome Python project list. <ref type="bibr">1</ref> As in previous work, we perform synthetic experiments on these datasets, which measure how well our models can generate function representations for unseen API descrip- tions, which mimic user queries.</p><p>Natural language querying of APIs has long been a goal in software engineering, related to the gen- eral problem of software reuse <ref type="bibr" target="#b3">(Krueger, 1992)</ref>. To date, a number of industrial scale products are available in this area. <ref type="bibr">2</ref> To our knowledge, most implementations use shallow term match- ing and/or information-extraction techniques ( <ref type="bibr" target="#b4">Lv et al., 2015)</ref>, differing from our methods that use more conventional NLP components and tech- niques. As we show in this paper and in RK, term matching and related techniques can sometimes serve as a competitive baseline, but are almost al- ways outperformed by our translation approach.</p><p>More recently, there has been increased interest in machine learning on learning code representa- tions from APIs, especially using resources such as GitHub or StackOverflow. However, this work tends to look at learning from many API collec- tions ( <ref type="bibr" target="#b2">Gu et al., 2016)</ref>, making such systems hard to evaluate and to apply to querying specific APIs.</p><p>Other work looks at learning to generate longer code from source code annotations for natural lan- guage programming <ref type="bibr" target="#b0">(Allamanis et al., 2015)</ref>, of- ten focusing narrowly on a specific programming language (e.g., Java) or set of APIs. To our knowl- edge, none of these approaches include companion software that facilitate building custom pipelines for specific APIs and querying.</p><p>Technically, our approach is related to work on semantic parsing, which looks at generating formal representations from text input for natu- ral language understanding applications, notably question-answering. Many existing methods take direct inspiration from work on MT ( <ref type="bibr" target="#b6">Wong and Mooney, 2006</ref>) and parsing <ref type="bibr" target="#b7">(Zettlemoyer and Collins, 2009)</ref>. Please see RK for more discussion and pointers to related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Technical Approach</head><p>In this paper, we focus on learning to generate function representations from textual descriptions inside of source code collections, or APIs. We will refer to these target function representations as API components. Each component specifies a function name, a list of arguments, and other op- tional information such as a namespace.</p><p>Given a set of example text-component pairs from an example API,</p><formula xml:id="formula_0">D = {(x i , z i )} n i=1</formula><p>, the goal is to learn how to generate correct, well-formed components z ∈ C for each text x. When viewed as a semantic parsing problem, we can view each z as analogous to a target logical form. In this paper, we focus narrowly on Python source code projects, and thus Python functions z, however our methods are agnostic to the input natural language and out- put programming language as shown in RK. When used for querying, our model takes a text input and attempts to generate the desired func- tion representation. Technically, our approach fol- lows our previous work and has two components: a simple and lightweight word-based translation model that generates candidate API components, and a discriminative model that reranks the trans- lation model output using additional phrase and document-level features. All of these models are implemented natively in our tool, and we describe each part in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translation Model</head><p>Given an input text (or query) sequence x = w 1 , .., w |x| , the goal is to generate an output API component z = u i , .., u |z| , which involves learn- ing a conditional distribution p(z | x). We pursue a noisy-channel approach, where</p><formula xml:id="formula_1">p(z | x) ∝ p(x | z)p(z)</formula><p>By assuming a uniform prior p(z) on output components, the model therefore involves com- puting p(x | z), which under a word-based trans- lation model can be expressed as:</p><formula xml:id="formula_2">p(x | z) = a p(x, a | z)</formula><p>where the summation ranges over the set of all many-to-one (word) alignments a from x → z.</p><p>While many different formulations of word- based models exist, we previously found that the simplest lexical translation model, or IBM Model 1 ( <ref type="bibr" target="#b1">Brown et al., 1993)</ref>, outperforms even higher- order alignment models with location parameters. This model computes all alignments exactly using the following equation:</p><formula xml:id="formula_3">p(x | z) ≈ |x| j=1 |z| i=0 p t (w j | u i )<label>(1)</label></formula><p>where p t defines a multinomial distribution over a given component term u j for all words w j .</p><p>While many parameter estimation strategies ex- ist for training word-based models, we similarly found that the simplest EM procedure of <ref type="bibr" target="#b1">Brown et al. (1993)</ref> works the best. In RK, we describe a linear-time decoding strategy (i.e., for generating components from input) over the number of com- ponents C, which we use in this paper. Our tool also implements our types of conventional MT de- coding strategies that are better suited for large APIs and more complex semantic languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discriminative Reranking</head><p>Following most semantic parsing approaches <ref type="bibr" target="#b7">(Zettlemoyer and Collins, 2009)</ref>, we use a dis- criminative log-linear model to rerank the com- ponents generated from the underlying translation model. Such a model defines a conditional distri- bution: p( z| x; θ) ∝ e θ·φ(x,z) , for a parameter vec- tor θ ∈ R b , and a set of feature functions φ(x, z).</p><p>Our tool implements several different training and optimization methods. For the purpose of this paper, we train our models using an online, stochastic gradient ascent algorithm under a max- imum conditional log-likelihood objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Features</head><p>For a given text input x and output component z, φ(x, z) defines a set of features between these two items. By default, our pipeline implementa- tion uses three classes of features, identical to the feature set used in RK. The first class includes ad- ditional word-level features, such as word/compo- nent match, overlap, component syntax informa- tion, and so on. The second includes phrase and hierarchical phrase features between text and com- ponent candidates, which are extracted standardly from symmetric word-level alignment heuristics.</p><p>The other category of features includes document-level features. This includes informa- tion about the underlying API class hierarchy, and relations between words/phrases and abstract classes within this hierarchy. Also, we use additional textual description of parameters in the docstrings to indicate whether word-components candidate pairs overlap in these descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation and Usage</head><p>All of the functionality above is implemented in the Function Assistant toolkit. The tool is part of the companion software release for our pre- vious work called Zubr. For efficiency, the core functionality is written in Cython 3 , which is a compiled superset of the Python language that fa- cilitates native C/C++ integration. The tool is designed to be used in two ways: first, as a black-box pipeline to build custom trans- lation pipelines and API query engines. The tool can also be integrated with other components us- ing our Cython and Python API. We focus on the first type of functionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## pipeline parameters p a r a m s = [ ( "−b a s e l i n e " , " b a s e l i n e " , F a l s e , " b o o l " , " Use b a s e l i n e model [ d e f a u l t = F a l s e ] " , " G P i p e l i n e " ) ]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Library Design and Pipelines</head><p>Our library uses dependency-injection OOP de- sign principles. All of the core components are implemented as wholly independent classes, each of which has a number of associated configura- tion values. These components interact via a class called Pipeline, which glues together various user-specified components and dependencies, and builds a global configuration from these compo- nents. Subsequent instantiation and sharing of ob- jects is dictated, or injected, by these global con- figurations settings, which can change dynami- cally throughout a pipeline run.</p><p>Pipelines are created by writing pipeline scripts, such as the one shown in <ref type="figure" target="#fig_1">Figure 2</ref>. This file is an ordinary Python file, with two mandatory vari- ables. The first params variable specifies vari- ous high-level configuration parameters associated with the pipeline. In this case, there is a set- ting --baseline, which can be evoked to run <ref type="bibr">3</ref> http://cython.org/ a baseline experiment, and will effect the subse- quent processing pipeline.</p><p>The second, and most important, variable is called tasks, and this specifies an ordering of subprocesses that should be executed. The fields in this list are pointers to either core utilities in the underlying Zubr toolkit (each with the pre- fix zubr.), or user defined functions. This par- ticular pipeline starts by building a dataset from a user specified source code repository, using DocExtractor, then builds a symmetric trans- lation model SymmetricAlignment, a fea- ture extractor FeatureExtractor, a discrim- inative reranker Optimizer, all via various in- termediate steps. It finishes by building a query interface and query server, QueryInterface and QueryServer, which can then be used for querying the input API.</p><p>As noted already, each subprocesses has a num- ber of associated configuration settings, which are joined into a global configuration object by the Pipeline instance. For the translation model, settings include, for example, the type of trans- lation model to use, the number of iterations to use when training models, and so on. All of these settings can be specified on the terminal, or in a separate configuration file. As well, the user is free to define custom functions, such as process data, or classes which can be used to modify the default processing pipeline or imple- ment new ML features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Web Server</head><p>The last step in this pipeline builds an HTTP web server that can be used to query the input API. In- ternally, the server makes calls to the trained trans- lation model and discriminative reranker, which takes user queries and attempts to translate them into API function representations. These candi- date translations are then returned to the user as potential answers to the query. Depending on the outcome, the user can either rephrase his/her question if the target function is not found, or look closer at the implementation by linking to the function's source code.</p><p>An example screen shot of the query server is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Here, the background API is the NLTK toolkit, and the query is Train a se- quence tagger model. While not mentioned ex- plicitly, the model returns training functions for the HiddenMarkovModelTagger. The right   side of the image shows the hyperlink path to the original source in Github for the train function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our current DocExtractor implementation supports building parallel datasets from raw Python source code collections. Internally, the tool reads source code using the abstract syntax tree utility, ast, in the Python standard library, and extracts sets of function and description pairs. In addition, the tool also extracts class descrip- tions, parameter and return value descriptions, and information about the API's internal class hierar- chy. This last type of information is then used to define document-level features.</p><p>To experiment with this feature, we built pipelines and ran experiments for 27 popular Python projects. The goal of these experiments is to test the robustness of our extractor, and see how well our models answer unseen queries for these resources using our previous experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The example projects are shown in <ref type="table" target="#tab_1">Table 1</ref>. Each dataset is quantified in terms of # Pairs, or the number of parallel function-component represen- tations, the # Symbols in the component output language, the # (NL) Words and Vocab size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Each dataset is randomly split into train, test, and dev. sets using a 70%-30% (or 15%/15%) split. We can think of the held-out sets as mimicking queries that users might ask the model. Stan- dardly, all models are trained on the training sets, and hyper-parameters are tuned to the dev. sets.</p><p>For a unseen text input during testing, the model generates a candidate list of component outputs. An output is considered correct if it matches ex- actly the gold function representation. As before, we measure the Accuracy @1, accuracy within top ten (Accuracy @10), and the MRR.</p><p>As in our previous work, three additional base- lines are used. The first is a simple bag-of-words  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>Test results are shown in <ref type="table" target="#tab_3">Table 2</ref>, and largely conform to our previous findings. The BoW and Term Match baselines are outperformed by all other models, which again shows that API querying is more complicated than simple word- component matching. The Reranker model leads to improvements on all datasets as compared with only using the Translation model, indicating that document-level and phrase features can help. We note that these experiments are synthetic, in the sense that it's unclear whether the held- out examples bear any resemblance to actual user queries. Assuming, however, that each held-out set is a representative sample of the queries that real users would ask, we can then interpret the re- sults as indicating how well our models answer queries. Whether or not these held-out examples reflect real queries, we believe that they still pro- vide a good benchmark for model construction. All code and data will be released to facilitate fur- ther experimentation and application building. Fu- ture work will look at eliciting more naturalistic queries (e.g., through StackOverflow), and doing usage studies via a permanent web demo 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce Function Assistant, a lightweight tool for querying API collections using uncon- 4 see demo here: http://zubr.ims.uni-stuttgart.de/ strained natural language. Users can supply our tool with target source code projects and build cus- tom translation or processing pipelines and query servers from scratch. In addition to the tool, we also created new resources for studying API querying, in the form of datasets built from 27 popular Github projects. While our approach uses simple components, we hope will that our tool and resources will serve as a benchmark for fu- ture work in this area, and ultimately help to solve everyday software search and reusability issues.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example function documentation in Python NLTK about dependency graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>. SymmetricAlignment " , # learn trans. model.Preprocess the extracted data using a custom function or outside library (e.g., nltk) :param config: The global configuration "Figure 2 :</head><label>2</label><figDesc>Figure 2: An example pipeline script for building a translation model and query server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example screen shot of the Function Assistant web server.</figDesc><graphic url="image-1.png" coords="5,98.74,63.33,400.05,217.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : New English Github datasets.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test results on our new Github datasets. 

(BoW) model, which uses word-component pairs 
as features. The second is a Term Match baseline, 
which ranks candidates according to the number 
of matches between input words and component 
words. We also compare the results of the Trans-
lation (model) without the reranker. 

</table></figure>

			<note place="foot" n="1"> github.com/vinta/awesome-python</note>

			<note place="foot" n="2"> e.g., www.krugle.com,www.searchcode.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bimodal modelling of source code and NL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Andrew D Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<title level="m">The mathematics of SMT. Computational linguistics</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08535</idno>
		<title level="m">Deep API Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Software reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Codehow: Effective code search based on api understanding and extended boolean model (e)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASE</title>
		<meeting>ASE</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Semantic Correspondences in Technical Documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning for Semantic Parsing with Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuk</forename><forename type="middle">Wah</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning context-dependent mappings from sentences to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
