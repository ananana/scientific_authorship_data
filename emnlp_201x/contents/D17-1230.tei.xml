<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Learning for Neural Dialogue Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">SÃ©bastien</forename><surname>Jean</surname></persName>
							<email>sebastien@cs.nyu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">New York University</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Ohio State University</orgName>
								<address>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Learning for Neural Dialogue Generation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2157" to="2169"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, drawing intuition from the Turing test, we propose using adversar-ial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator-analagous to the human evaluator in the Turing test-to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open domain dialogue generation ( <ref type="bibr" target="#b26">Ritter et al., 2011;</ref><ref type="bibr" target="#b37">Sordoni et al., 2015;</ref><ref type="bibr" target="#b45">Xu et al., 2016;</ref><ref type="bibr" target="#b15">Li et al., 2016b;</ref><ref type="bibr" target="#b30">Serban et al., 2016c</ref> aims at generating meaningful and coher- ent dialogue responses given the dialogue history. Prior systems, e.g., phrase-based machine trans- lation systems <ref type="bibr" target="#b26">(Ritter et al., 2011;</ref><ref type="bibr" target="#b37">Sordoni et al., 2015)</ref> or end-to-end neural systems ( <ref type="bibr" target="#b33">Shang et al., 2015;</ref><ref type="bibr" target="#b41">Vinyals and Le, 2015;</ref><ref type="bibr" target="#b14">Li et al., 2016a;</ref><ref type="bibr" target="#b46">Yao et al., 2015;</ref><ref type="bibr" target="#b22">Luan et al., 2016</ref>) approximate such a goal by predicting the next dialogue utterance given the dialogue history using the maximum likelihood estimation (MLE) objective. Despite its success, this over-simplified training objective leads to prob- lems: responses are dull, generic <ref type="bibr" target="#b37">(Sordoni et al., 2015;</ref><ref type="bibr" target="#b28">Serban et al., 2016a;</ref><ref type="bibr" target="#b14">Li et al., 2016a</ref>), repeti- tive, and short-sighted ( <ref type="bibr" target="#b18">Li et al., 2016d</ref>).</p><p>Solutions to these problems require answering a few fundamental questions: what are the cru- cial aspects that characterize an ideal conversation, how can we quantitatively measure them, and how can we incorporate them into a machine learning system? For example, <ref type="bibr" target="#b18">Li et al. (2016d)</ref> manually define three ideal dialogue properties (ease of an- swering, informativeness and coherence) and use a reinforcement-learning framework to train the model to generate highly rewarded responses. <ref type="bibr" target="#b6">Yu et al. (2016b)</ref> use keyword retrieval confidence as a reward. However, it is widely acknowledged that manually defined reward functions can't possibly cover all crucial aspects and can lead to suboptimal generated utterances. A good dialogue model should generate utter- ances indistinguishable from human dialogues. Such a goal suggests a training objective resem- bling the idea of the Turing test <ref type="bibr" target="#b40">(Turing, 1950)</ref>. We borrow the idea of adversarial training <ref type="bibr" target="#b9">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b7">Denton et al., 2015</ref>) in com- puter vision, in which we jointly train two mod- els, a generator (a neural SEQ2SEQ model) that defines the probability of generating a dialogue se- quence, and a discriminator that labels dialogues as human-generated or machine-generated. This discriminator is analogous to the evaluator in the Turing test. We cast the task as a reinforcement learning problem, in which the quality of machine- generated utterances is measured by its ability to fool the discriminator into believing that it is a human-generated one. The output from the dis- criminator is used as a reward to the generator, pushing it to generate utterances indistinguishable from human-generated dialogues.</p><p>The idea of a Turing test-employing an evalu- ator to distinguish machine-generated texts from human-generated ones-can be applied not only to training but also testing, where it goes by the name of adversarial evaluation. Adversarial evalua- tion was first employed in <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> to evaluate sentence generation quality, and prelimi- narily studied for dialogue generation by <ref type="bibr" target="#b12">Kannan and Vinyals (2016)</ref>. In this paper, we discuss poten- tial pitfalls of adversarial evaluations and necessary steps to avoid them and make evaluation reliable.</p><p>Experimental results demonstrate that our ap- proach produces more interactive, interesting, and non-repetitive responses than standard SEQ2SEQ models trained using the MLE objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dialogue generation Response generation for dialogue can be viewed as a source-to-target trans- duction problem. <ref type="bibr" target="#b26">Ritter et al. (2011)</ref> frame the gen- eration problem as a machine translation problem. <ref type="bibr" target="#b37">Sordoni et al. (2015)</ref> improved Ritter et al.'s sys- tem by rescoring the outputs of a phrasal MT-based conversation system with a neural model incorpo- rating prior context. Recent progress in SEQ2SEQ models have inspired several efforts ( <ref type="bibr" target="#b41">Vinyals and Le, 2015;</ref><ref type="bibr">Serban et al., 2016a,d;</ref><ref type="bibr" target="#b22">Luan et al., 2016)</ref> to build end-to-end conversational systems that first apply an encoder to map a message to a distributed vector representing its meaning and then generate a response from the vector.</p><p>Our work adapts the encoder-decoder model to RL training, and can thus be viewed as an exten- sion of <ref type="bibr" target="#b18">Li et al. (2016d)</ref>, but with more general RL rewards. <ref type="bibr" target="#b18">Li et al. (2016d)</ref> simulate dialogues between two virtual agents, using policy gradient methods to reward sequences that display three useful conversational properties: informativity, co- herence, and ease of answering. Our work is also related to recent efforts to integrate the SEQ2SEQ and reinforcement learning paradigms, drawing on the advantages of both ( ). For example,  combine reinforcement learning with neural generation on tasks with real users. <ref type="bibr" target="#b1">Asghar et al. (2016)</ref> train an end-to-end RL dialogue model using human users.</p><p>Dialogue quality is traditionally evaluated <ref type="bibr" target="#b37">(Sordoni et al., 2015</ref>, e.g.) using word-overlap metrics such as BLEU and METEOR scores used for ma- chine translation. Some recent work ( ) has started to look at more flexible and reli- able evaluation metrics such as human-rating pre- diction (  and next utterance clas- sification ( .</p><p>Adversarial networks The idea of generative adversarial networks has enjoyed great success in computer vision ( <ref type="bibr" target="#b24">Radford et al., 2015;</ref><ref type="bibr" target="#b5">Chen et al., 2016a;</ref><ref type="bibr" target="#b27">Salimans et al., 2016)</ref>. Training is formal- ized as a game in which the generative model is trained to generate outputs to fool the discrimina- tor; the technique has been successfully applied to image generation.</p><p>However, to the best of our knowledge, this idea has not achieved comparable success in NLP. This is due to the fact that unlike in vision, text gener- ation is discrete, which makes the error outputted from the discriminator hard to backpropagate to the generator. Some recent work has begun to ad- dress this issue: <ref type="bibr" target="#b13">Lamb et al. (2016)</ref> propose provid- ing the discriminator with the intermediate hidden vectors of the generator rather than its sequence outputs. Such a strategy makes the system differen- tiable and achieves promising results in tasks like character-level language modeling and handwriting generation. <ref type="bibr" target="#b47">Yu et al. (2016a)</ref> use policy gradient reinforcement learning to backpropagate the error from the discriminator, showing improvement in multiple generation tasks such as poem generation, speech language generation and music generation. Outside of sequence generation, <ref type="bibr" target="#b6">Chen et al. (2016b)</ref> apply the idea of adversarial training to sentiment analysis and <ref type="bibr">Zhang et al. (2017)</ref> apply the idea to domain adaptation tasks.</p><p>Our work is distantly related to recent work that formalizes sequence generation as an action-taking problem in reinforcement learning. <ref type="bibr" target="#b25">Ranzato et al. (2016)</ref> train RNN decoders in a SEQ2SEQ model using policy gradient to obtain competitive ma- chine translation results. <ref type="bibr" target="#b2">Bahdanau et al. (2017)</ref> take this a step further by training an actor-critic RL model for machine translation. Also related is recent work <ref type="bibr" target="#b35">(Shen et al., 2016;</ref><ref type="bibr" target="#b44">Wiseman and Rush, 2016)</ref> to address the issues of exposure bias and loss-evaluation mismatch in neural translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adversarial Training for Dialogue Generation</head><p>In this section, we describe in detail the compo- nents of the proposed adversarial reinforcement learning model. The problem can be framed as fol- lows: given a dialogue history x consisting of a se- quence of dialogue utterances, 1 the model needs to generate a response y = {y 1 , y 2 , ..., y T }. We view the process of sentence generation as a sequence of actions that are taken according to a policy defined by an encoder-decoder recurrent neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adversarial REINFORCE</head><p>The adversarial REINFORCE algorithm consists of two components: a generative model G and a discriminative model D.</p><p>Generative model The generative model G de- fines the policy that generates a response y given dialogue history x. It takes a form similar to SEQ2SEQ models, which first map the source input to a vector representation using a recurrent net and then compute the probability of generating each token in the target using a softmax function.</p><p>Discriminative model The discriminative model D is a binary classifier that takes as input a se- quence of dialogue utterances {x, y} and outputs a label indicating whether the input is generated by humans or machines. The input dialogue is encoded into a vector representation using a hi- erarchical encoder ( <ref type="bibr" target="#b33">Li et al., 2015;</ref><ref type="bibr" target="#b29">Serban et al., 2016b)</ref>, 2 which is then fed to a 2-class softmax function, returning the probability of the input dia- logue episode being a machine-generated dialogue (denoted Q â ({x, y})) or a human-generated dia- logue (denoted Q + ({x, y})).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy Gradient Training</head><p>The key idea of the system is to encourage the generator to generate utterances that are indistinguishable from human generated dialogues. We use policy gradient meth- ods to achieve such a goal, in which the score of current utterances being human-generated ones assigned by the discriminator (i.e., Q + ({x, y})) is used as a reward for the generator, which is trained to maximize the expected reward of gener- ated utterance(s) using the REINFORCE algorithm <ref type="bibr" target="#b43">(Williams, 1992)</ref>:</p><formula xml:id="formula_0">J(Î¸) = E yâ¼p(y|x) (Q + ({x, y})|Î¸)<label>(1)</label></formula><p>1 We approximate the dialogue history using the concate- nation of two preceding utterances. We found that using more than 2 context utterances yields very tiny performance im- provements for SEQ2SEQ models. <ref type="bibr">2</ref> To be specific, each utterance p or q is mapped to a vector representation hp or hq using LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>). Another LSTM is put on sentence level, map- ping the context dialogue sequence to a single representation.</p><p>Given the input dialogue history x, the bot gener- ates a dialogue utterance y by sampling from the policy. The concatenation of the generated utter- ance y and the input x is fed to the discriminator. The gradient of <ref type="formula" target="#formula_0">(1)</ref> is approximated using the like- lihood ratio trick <ref type="bibr" target="#b43">(Williams, 1992;</ref><ref type="bibr" target="#b8">Glynn, 1990;</ref><ref type="bibr" target="#b0">Aleksandrov et al., 1968)</ref>:</p><formula xml:id="formula_1">J(Î¸) â [Q + ({x, y}) â b({x, y})] log Ï(y|x) = [Q + ({x, y}) â b({x, y})] t log p(y t |x, y 1:tâ1 ) (2)</formula><p>where Ï denotes the probability of the generated responses. b({x, y}) denotes the baseline value to reduce the variance of the estimate while keeping it unbiased. <ref type="bibr">3</ref> The discriminator is simultaneously updated with the human generated dialogue that contains dialogue history x as a positive example and the machine-generated dialogue as a negative example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reward for Every Generation</head><p>Step (REGS)</p><p>The REINFORCE algorithm described has the dis- advantage that the expectation of the reward is ap- proximated by only one sample, and the reward associated with this sample (i.e., <ref type="formula">(2)</ref>) is used for all actions (the gen- eration of each token) in the generated sequence. Suppose, for example, the input history is what's your name, the human-generated response is I am John, and the machine-generated response is I don't know. The vanilla REINFORCE model assigns the same negative reward to all tokens within the human-generated response (i.e., I, don't, know), whereas proper credit assignment in training would give separate rewards, most likely a neutral reward for the token I, and negative rewards to don't and know. We call this reward for every generation step, abbreviated REGS. Rewards for intermediate steps or partially de- coded sequences are thus necessary. Unfortunately, the discriminator is trained to assign scores to fully generated sequences, but not partially decoded ones. We propose two strategies for computing in- termediate step rewards by (1) using Monte Carlo (MC) search and (2) training a discriminator that is able to assign rewards to partially decoded se- quences.</p><formula xml:id="formula_2">[Q + ({x, y}) â b({x, y})] in Eq</formula><p>In (1) Monte Carlo search, given a partially de- coded s P , the model keeps sampling tokens from the distribution until the decoding finishes. Such a process is repeated N (set to 5) times and the N generated sequences will share a common prefix s P . These N sequences are fed to the discrimi- nator, the average score of which is used as a re- ward for the s P . A similar strategy is adopted in <ref type="bibr" target="#b47">Yu et al. (2016a)</ref>. The downside of MC is that it requires repeating the sampling process for each prefix of each sequence and is thus significantly time-consuming. <ref type="bibr">4</ref> In <ref type="formula">(2)</ref>, we directly train a discriminator that is able to assign rewards to both fully and partially decoded sequences. We break the generated se- quences into partial sequences, namely {y</p><formula xml:id="formula_3">+ 1:t } N Y + t=1</formula><p>and {y</p><formula xml:id="formula_4">â 1:t } N Y â t=1</formula><p>and use all instances in {y</p><formula xml:id="formula_5">+ 1:t } N Y + t=1</formula><p>as positive examples and instances {y</p><formula xml:id="formula_6">â 1:t } N Y â t=1</formula><p>as negative examples. The problem with such a strat- egy is that earlier actions in a sequence are shared among multiple training examples for the discrimi- nator (for example, token y + 1 is contained in all par- tially generated sequences, which results in overfit- ting. To mitigate this problem, we adopt a strategy similar to when training value networks in AlphaGo ( <ref type="bibr" target="#b36">Silver et al., 2016)</ref>, in which for each collection of subsequences of Y , we randomly sample only one example from {y + 1:t } N Y + t=1 and one example from</p><formula xml:id="formula_7">{y â 1:t } N Y â t=1</formula><p>, which are treated as positive and neg- ative examples to update the discriminator. Com- pared with the Monte Carlo search model, this strat- egy is significantly more time-effective, but comes with the weakness that the discriminator becomes less accurate after partially decoded sequences are added in as training examples. We find that the MC model performs better when training time is less of an issue.</p><p>For each partially-generated sequence Y t = y 1:t , the discriminator gives a classification score Q + (x, Y t ). We compute the baseline b(x, Y t ) us- ing a similar model to the vanilla REINFORCE model. This yields the following gradient to update the generator:</p><formula xml:id="formula_8">J(Î¸) â t (Q + (x, Y t ) â b(x, Y t )) log p(y t |x, Y 1:tâ1 ) (3)</formula><p>Comparing <ref type="formula">(3)</ref> with <ref type="formula">(2)</ref>, we can see that the val- ues for rewards and baselines are different among generated tokens in the same response.</p><p>Teacher Forcing Practically, we find that updat- ing the generative model only using Eq. 1 leads to unstable training for both vanilla Reinforce and REGS, with the perplexity value skyrocket- ing after training the model for a few hours (even when the generator is initialized using a pre-trained SEQ2SEQ model). The reason this happens is that the generative model can only be indirectly exposed to the gold-standard target sequences through the reward passed back from the discriminator, and this reward is used to promote or discourage its (the generator's) own generated sequences. Such a training strategy is fragile: once the generator (acci- dentally) deteriorates in some training batches and the discriminator consequently does an extremely good job in recognizing sequences from the gener- ator, the generator immediately gets lost. It knows that its generated sequences are bad based on the rewards outputted from the discriminator, but it does not know what sequences are good and how to push itself to generate these good sequences (the odds of generating a good response from random sampling are minute, due to the vast size of the space of possible sequences). Loss of the reward signal leads to a breakdown in the training process.</p><p>To alleviate this issue and give the generator more direct access to the gold-standard targets, we propose also feeding human generated responses to the generator for model updates. The most straight- forward strategy is for the discriminator to auto- matically assign a reward of 1 (or other positive values) to the human generated responses and for the generator to use this reward to update itself on human generated examples. This can be seen as having a teacher intervene with the generator some fraction of the time and force it to generate the true responses, an approach that is similar to the professor-forcing algorithm of <ref type="bibr" target="#b13">Lamb et al. (2016)</ref>.</p><p>A closer look reveals that this modification is the same as the standard training of SEQ2SEQ mod-</p><formula xml:id="formula_9">For number of training iterations do . For i=1,D-steps do . Sample (X,Y) from real data . SamplÃª Y â¼ G(Â·|X) .</formula><p>Update D using (X, Y ) as positive examples and</p><formula xml:id="formula_10">(X, Ë Y ) as negative examples. . End . . For i=1,G-steps do . Sample (X,Y) from real data . SamplÃª Y â¼ G(Â·|X) . Compute Reward r for (X, Ë Y ) using D. . Update G on (X, Ë Y ) using reward r .</formula><p>Teacher-Forcing: Update G on (X, Y ) . End End <ref type="figure">Figure 1</ref>: A brief review of the proposed adversarial reinforcement algorithm for training the generator G and discriminator D. The reward r from the discriminator D can be computed using different strategies according to whether using REINFORCE or REGS. The update of the generator G on (X, Ë Y ) can be done by either using Eq.2 or Eq.3. D-steps is set to 5 and G-steps is set to 1. els, making the final training alternately update the SEQ2SEQ model using the adversarial objec- tive and the MLE objective. One can think of the professor-forcing model as a regularizer to regu- late the generator once it starts deviating from the training dataset.</p><p>We also propose another workaround, in which the discriminator first assigns a reward to a human generated example using its own model, and the generator then updates itself using this reward on the human generated example only if the reward is larger than the baseline value. Such a strategy has the advantage that different weights for model updates are assigned to different human generated examples (in the form of different reward values produced by the generator) and that human gen- erated examples are always associated with non- negative weights.</p><p>A sketch of the proposed model is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Details</head><p>We first pre-train the generative model by predict- ing target sequences given the dialogue history.</p><p>We trained a SEQ2SEQ model ( <ref type="bibr" target="#b39">Sutskever et al., 2014</ref>) with an attention mechanism ( <ref type="bibr" target="#b3">Bahdanau et al., 2015;</ref>) on the OpenSubti- tles dataset. We followed protocols recommended by <ref type="bibr" target="#b39">Sutskever et al. (2014)</ref>, such as gradient clip- ping, mini-batch and learning rate decay. We also pre-train the discriminator. To generate negative examples, we decode part of the training data. Half of the negative examples are generated using beam- search with mutual information reranking as de- scribed in <ref type="bibr" target="#b14">Li et al. (2016a)</ref>, and the other half is generated from sampling.</p><p>For data processing, model training and decod- ing (both the proposed adversarial training model and the standard SEQ2SEQ models), we employ a few strategies that improve response quality, in- cluding: (2) Remove training examples with length of responses shorter than a threshold (set to 5). We find that this significantly improves the general re- sponse quality. <ref type="bibr">5</ref> (2) Instead of using the same learn- ing rate for all examples, using a weighted learning rate that considers the average tf-idf score for to- kens within the response. Such a strategy decreases the influence from dull and generic utterances. 6 (3) Penalizing intra-sibling ranking when doing beam search decoding to promote N-best list diversity as described in <ref type="bibr" target="#b17">Li et al. (2016c)</ref>. <ref type="formula">(4)</ref> Penalizing word types (stop words excluded) that have already been generated. Such a strategy dramatically decreases the rate of repetitive responses such as no. no. no. no. no. or contradictory responses such as I don't like oranges but i like oranges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adversarial Evaluation</head><p>In this section, we discuss strategies for success- ful adversarial evaluation. Note that the proposed adversarial training and adversarial evaluation are separate procedures. They are independent of each other and share no common parameters.</p><p>The idea of adversarial evaluation, first proposed by <ref type="bibr" target="#b4">Bowman et al. (2016)</ref>, is to train a discriminant function to separate generated and true sentences, in an attempt to evaluate the model's sentence gen- eration capability. The idea has been preliminarily studied by <ref type="bibr" target="#b12">Kannan and Vinyals (2016)</ref> in the con- text of dialogue generation. Adversarial evaluation also resembles the idea of the Turing test, which <ref type="bibr">5</ref> To compensate for the loss of short responses, one can train a separate model using short sequences. <ref type="bibr">6</ref> We treat each sentence as a document. Stop words are removed. Learning rates are normalized within one batch. For example, suppose t1, t2, ..., ti, ... ,tN denote the tf-idf scores for sentences within current batch and lr denotes the original learning rate. The learning rate for sentence with index i is N Â· lr Â· t i i t i</p><p>. To avoid exploding learning rates for sequences with extremely rare words, the tf-idf score of a sentence is capped at L times the minimum tf-idf score in the current batch. L is empirically chosen and is set to 3. requires a human evaluator to distinguish machine- generated texts from human-generated ones. Since it is time-consuming and costly to ask a human to talk to a model and give judgements, we train a machine evaluator in place of the human evaluator to distinguish the human dialogues and machine dialogues, and we use it to measure the general quality of the generated responses.</p><p>Adversarial evaluation involves both training and testing. At training time, the evaluator is trained to label dialogues as machine-generated (negative) or human-generated (positive). At test time, the trained evaluator is evaluated on a held-out dataset. If the human-generated dialogues and machine- generated ones are indistinguishable, the model will achieve 50 percent accuracy at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adversarial Success</head><p>We define Adversarial Success (AdverSuc for short) to be the fraction of instances in which a model is capable of fooling the evaluator. AdverSuc is the difference between 1 and the accuracy achieved by the evaluator. Higher values of AdverSuc for a dialogue generation model are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Testing the Evaluator's Ability</head><p>One caveat with the adversarial evaluation methods is that they are model-dependent. We approximate the human evaluator in the Turing test with an au- tomatic evaluator and assume that the evaluator is perfect: low accuracy of the discriminator should indicate high quality of the responses, since we interpret this to mean the generated responses are indistinguishable from the human ones. Unfor- tunately, there is another factor that can lead to low discriminative accuracy: a poor discriminative model. Consider a discriminator that always gives random labels or always gives the same label. Such an evaluator always yields a high AdverSuc value of 0.5. <ref type="bibr" target="#b4">Bowman et al. (2016)</ref> propose two different discriminator models separately using unigram fea- tures and neural features. It is hard to tell which feature set is more reliable. The standard strategy of testing the model on a held-out development set is not suited to this case, since a model that overfits the development set is necessarily superior.</p><p>To deal with this issue, we propose setting up a few manually-invented situations to test the ability of the automatic evaluator. This is akin to setting up examinations to test the ability of the human evaluator in the Turing test. We report not only the AdverSuc values, but also the scores that the evalu- ator achieves in these manually-designed test cases, indicating how much we can trust the reported Ad- verSuc. We develop scenarios in which we know in advance how a perfect evaluator should behave, and then compare AdverSuc from a discriminative model with the gold-standard AdverSuc. Scenarios we design include:</p><p>â¢ We use human-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an AdverSuc of 0.5 (accuracy 50%), which is the gold- standard result.</p><p>â¢ We use machine-generated dialogues as both positive examples and negative examples. A perfect evaluator should give an AdverSuc of 0.5 (accuracy 50%).</p><p>â¢ We use original human-generated dialogues as positive examples and dialogues consisting of random utterances as negative examples. A perfect evaluator should give an AdverSuc of 0 (accuracy 100%).</p><p>â¢ We use original human-generated dialogues as positive examples and dialogues with re- sponses replaced by the utterances immedi- ately following the true responses. A perfect evaluation should give an AdverSuc of 0 (ac- curacy 100%). The evaluator reliability error (ERE) is the average deviation of an evaluator's adversarial error from the gold-standard error in the above tasks, with equal weight for each task. The smaller the error, the more reliable the evaluator is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine-vs-Random Accuracy</head><p>Evaluator reliability error uses scenarios con- structed from human-generated dialogues to assess feature or hyper-parameter choice for the evaluator. Unfortunately, no machine-generated responses are involved in the ERE metric. The following exam- ple illustrates the serious weakness resulting from this strategy: as will be shown in the experiment section, when inputs are decoded using greedy or beam search models, most generation systems to date yield an adversarial success less than 10 per- cent (evaluator accuracy 90 percent). But when using sampling for decoding, the adversarial suc- cess skyrockets to around 40 percent, 7 only 10 percent less than what's needed to pass the Turing test. A close look at the decoded sequences using sampling tells a different story: the responses from Input tell me ... how long have you had this falling sickness ? Vanilla-MLE i 'm not a doctor . Vanilla-Sample well everything you did was totally untrue . REINFORCE i don 't know how long it 's been .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGS Monte Carlo</head><p>A few months, I guess . Input so i had the doctors test sammy 's response to conditioning . Vanilla-MLE sammy wrote the test sammy wrote the test . Vanilla-Sample objects pick objects up objects objects objects objects objects REINFORCE i 'm not sure that 's relevant .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGS Monte Carlo</head><p>so he took the pills . Input what are you going to do ? Vanilla-MLE i 'm going to the movies . Vanilla-Sample get him outta here first ! REINFORCE i 'm going to get you .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGS Monte Carlo</head><p>i 'm going to get you a new car . Input they fear your power your intellect . Vanilla-MLE you 're the only one who knows what 's going on . Vanilla-Sample when they are conquered and you surrender they will control all of us . REINFORCE i 'm afraid i 'm not ready yet .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REGS Monte Carlo</head><p>i 'm not afraid of your power .  <ref type="table" target="#tab_2">Tables 5 and 6.</ref> sampling are sometimes incoherent, irrelevant or even ungrammatical. We thus propose an additional sanity check, in which we report the accuracy of distinguishing be- tween machine-generated responses and randomly sampled responses (machine-vs-random for short). This resembles the N-choose-1 metric described in <ref type="bibr" target="#b34">Shao et al. (2017)</ref>. Higher accuracy indicates that the generated responses are distinguishable from randomly sampled human responses, indicat- ing that the generative model is not fooling the generator simply by introducing randomness. As we will show in Sec. 5, using sampling results in high AdverSuc values but low machine-vs-random accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we detail experimental results on adversarial success and human evaluation.   <ref type="bibr" target="#b11">(Joachims, 2002</ref>). multi-utterance dialogue (i.e., input messages and responses) is transformed to a unigram represen- tation; (2) Concat Neural: a neural classification model with a softmax function that takes as input the concatenation of representations of constituent dialogues sentences; (3) Hierarchical Neural: a hierarchical encoder with a structure similar to the discriminator used in the reinforcement; and (4) SVM+Neural+multi-lex-features: a SVM model that uses the following features: unigrams, neural representations of dialogues obtained by the neural model trained using strategy (3), <ref type="bibr">9</ref> the forward like- lihood log p(t|s) and backward likelihood p(s|t).</p><p>ERE scores obtained by different models are re- ported in <ref type="table" target="#tab_1">Table 2</ref>. As can be seen, the hierarchical neural evaluator (model 3) is more reliable than simply concatenating the sentence-level represen- tations (model 2). Using the combination of neural features and lexicalized features yields the most reliable evaluator. For the rest of this section, we report results obtained by the Hierarchical Neu- ral setting due to its end-to-end nature, despite its inferiority to SVM+Neural+multil-features. <ref type="table">Table 3</ref> presents AdverSuc values for different models, along with machine-vs-random accuracy described in Section 4.3. Higher values of Adver- Suc and machine-vs-random are better.</p><p>Baselines we consider include standard SEQ2SEQ models using greedy decoding (MLE- greedy), beam-search (MLE+BS) and sampling, as well as the mutual information reranking model of <ref type="bibr" target="#b14">Li et al. (2016a)</ref> with two algorithmic variations: (1) MMI+p(t|s), in which a large N-best list is first Model AdverSuc machine-vs-random MLE-BS 0.037 0.942 MLE-Greedy 0.049 0.945 MMI+p(t|s) 0.073 0.953 MMI-p(t) 0.090 0.880 Sampling 0.372 0.679 Adver-Reinforce 0.080 0.945 Adver-REGS 0.098 0.952 <ref type="table">Table 3</ref>: AdverSuc and machine-vs-random scores achieved by different training/decoding strategies.</p><p>generated using a pre-trained SEQ2SEQ model and then reranked by the backward probability p(s|t) and (2) MMIâp(t), in which language model probability is penalized during decoding. Results are shown in <ref type="table">Table 3</ref>. What first stands out is decoding using sampling (as discussed in Sec- tion 4.3), achieving a significantly higher AdverSuc number than all the rest models. However, this does not indicate the superiority of the sampling decod- ing model, since the machine-vs-random accuracy is at the same time significantly lower. This means that sampled responses based on SEQ2SEQ mod- els are not only hard for an evaluator to distinguish from real human responses, but also from randomly sampled responses. A similar, though much less extreme, effect is observed for MMIâp(t), which has an AdverSuc value slightly higher than Adver- Reinforce, but a significantly lower machine-vs- random score.</p><p>By comparing different baselines, we find that MMI+p(t|s) is better than MLE-greedy, which is in turn better than MLE+BS. This result is in line with human-evaluation results from <ref type="bibr" target="#b14">Li et al. (2016a)</ref>. The two proposed adversarial algorithms achieve better performance than the baselines. We expect this to be the case, since the adversarial algorithms are trained on an objective function more similar to the evaluation metric (i.e., adversarial success). REGS performs slightly better than the vanilla RE- INFORCE algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Human Evaluation</head><p>For human evaluation, we follow protocols de- fined in <ref type="bibr" target="#b18">Li et al. (2016d)</ref>, employing crowdsourced judges to evaluate a random sample of 200 items. We present both an input message and the gener- ated outputs to 3 judges and ask them to decide which of the two outputs is better (single-turn gen- eral quality). Ties are permitted. Identical strings are assigned the same score. We also present the judges with multi-turn conversations simulated be- tween the two agents. Each conversation consists Setting adver-win adver-lose tie single-turn 0.62 0.18 0.20 multi-turn 0.72 0.10 0.18 <ref type="table">Table 4</ref>: The gain from the proposed adversarial model over the mutual information system based on pairwise human judgments.</p><p>of 3 turns. Results are presented in <ref type="table">Table 4</ref>. We observe a significant quality improvement on both single-turn quality and multi-turn quality from the proposed adversarial model. It is worth noting that the reinforcement learning system described in <ref type="bibr" target="#b18">Li et al. (2016d)</ref>, which simulates conversations be- tween two bots and is trained based on manually designed reward functions, only improves multi- turn dialogue quality, while the model described in this paper improves both single-turn and multi- turn dialogue generation quality. This confirms that the reward adopted in adversarial training is more general, natural and effective in training dialogue systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, drawing intuitions from the Turing test, we propose using an adversarial training ap- proach for response generation. We cast the model in the framework of reinforcement learning and train a generator based on the signal from a dis- criminator to generate response sequences indis- tinguishable from human-generated dialogues. We observe clear performance improvements on multi- ple metrics from the adversarial training strategy. The adversarial training model should theo- retically benefit a variety of generation tasks in NLP. Unfortunately, in preliminary experiments applying the same training paradigm to machine translation, we did not observe a clear performance boost. We conjecture that this is because the adversarial training strategy is more beneficial to tasks in which there is a big discrepancy between the distributions of the generated sequences and the reference target sequences. In other words, the adversarial approach is more beneficial on tasks in which entropy of the targets is high. Exploring this relationship further is a focus of our future work.</p><p>crosoft Research, as well as Sumit Chopra and Marc'Aurelio Ranzato from Facebook AI Research for helpful discussions and comments. Jiwei Li is supported by a Facebook Fellowship, which we gratefully acknowledge. This work is also partially supported by the NSF under award IIS-1514268, and the DARPA Communicating with Comput- ers (CwC) program under ARO prime contract no. W911NF-15-1-0462, IIS-1464128. Any opinions, findings, and conclusions or recommendations ex- pressed in this material are those of the authors and do not necessarily reflect the views of DARPA, the NSF, or Facebook.</p><p>Zhou Yu, Ziyu Xu, Alan W Black, and Alex I Rud- nicky. 2016b. Strategy and policy learning for non- task-oriented conversational systems. In 17th An- nual Meeting of the Special Interest Group on Dis- course and Dialogue. page 404.</p><p>Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2017. Aspect-augmented adversarial net- works for domain adaptation.</p><p>arXiv preprint arXiv:1701.00188 .</p><p>Input maybe his eyes were too big for his stomach .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla-MLE</head><p>he 's got a big mouth . Vanilla-Sample i 'm sure he 's unhappy with the fact . Reinforce he 's a little bit of a pain . MC i don 't think so . Input they didn 't have much success with this problem commander . Vanilla-MLE they 're not the only ones who have been in the system . Vanilla-Sample it 's the new priority only . Reinforce they 're not the only ones who can help us . MC can we find someone else ? Input and he carried them all the time wherever he went . Vanilla-MLE i don 't know who he is . Vanilla-Sample the church ... suffered the winds of 900 meters with horse aid . Reinforce he was a good man . MC he was a very good man . Input somebody probably flushed their pet snake down the toilet . Vanilla-MLE i 'm sure the dog will be here soon . Vanilla-Sample i could at least think . Reinforce i don 't think so . MC i don 't think so . Input excuse me but how did we pay for these suits ? Vanilla-MLE i 'm not a beggar . Vanilla-Sample is this the money eugene ? Reinforce i don 't know what to do . MC how much money you owe me ? Input ... while the beautiful and happy world was madly destroyed ; Vanilla-MLE i was born in the world of the world Vanilla-Sample as in childhood i took you to one tenth of a postcard Reinforce and the world was full of joy . MC and the world was full of joy . Input i was kinda hoping that you were my dad .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla-MLE i 'm just a little bit of a little bit of a little bit of a little bit of a Vanilla-Sample</head><p>shiny gold gold and silver in the mountains . Reinforce i 'm not gonna be a baby . MC i 'm not going to be a father . Input the man who does that is not afraid to die .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla-MLE</head><p>he 's not afraid of the dark . Vanilla-Sample even the program is the only truth Reinforce he 's also not afraid of me . MC he 's such a brave man ! Input do you have any idea what caused the explosion ? Vanilla-MLE i think it 's the same thing . Vanilla-Sample i 'm still very nervous . Reinforce maybe it 's just a coincidence . MC i don 't know if it 's a coincidence . Input don 't bother the citizens in hong kong island anymore Vanilla-MLE i 'll be back in a few days Vanilla-Sample you wiii be transferred to the target court Reinforce i 'm not sure if you can handle it without bothering them . MC i 'm not going to be a nuisance Input you know you ought to find yourself a girl .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla-MLE</head><p>you know what i mean ? Vanilla-Sample it 's full of crime ... Reinforce i don 't know what you 're talking about . MC i don 't want to be a girl . Input i got you the big screen tv deluxe UNknown machine Vanilla-MLE i 'm gonna be a little late for the movie . Vanilla-Sample you mean you 're living ? Reinforce i 'm gonna get you a new one . MC i 'm gonna get you a new tv set . Input we won 't give her the chance to surprise us .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla-MLE</head><p>she 's not going to be a party member . Vanilla-Sample the timing 's good and bad . Reinforce we 'll have to get her back . MC i don 't think we have to worry about her . Input does it live in this matter we call dna ? Vanilla-MLE i don 't think so . Vanilla-Sample and it requires time to be wrong and no one believes it . Reinforce it 's a genetic disorder . MC i don 't think so .  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Sampled responses from different models. More in Appendix</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>ERE scores obtained by different models. 

5.1 Adversarial Evaluation 

ERE We first test adversarial evaluation models 
with different feature sets and model architectures 
for reliability, as measured by evaluator reliability 
error (ERE). We explore the following models: (1) 
SVM+Unigram: SVM using unigram features. 8 A 

8 Trained using the SVM-Light package </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 5 : Appendix: Responses sampled from different models.</head><label>5</label><figDesc></figDesc><table>Input 

i 'll have to drive home . 
Vanilla-MLE 
i 'll be back in a minute . 
Vanilla-Sample 
let 's go for some fun . 
Reinforce 
i 'll drive you home . 
MC 
i don 't have a car . 
Input 
i had to work late and then i went for a walk . 
Vanilla-MLE 
i was in the middle of the road . 
Vanilla-Sample 
my body was just pounding . 
Reinforce 
i was just trying to help you . 
MC 
i was just trying to get a little sleep . 
Input 
lots of cream lots of sugar . 
Vanilla-MLE 
i 'm gonna be in the kitchen . 
Vanilla-Sample 
was this gay or red ? 
Reinforce 
i don 't want to be a vegetable . 
MC 
i 'm not a very good cook . 
Input 
so they just let him go ? 
Vanilla-MLE 
he 's not gonna let them go through the motions . 
Vanilla-Sample 
they asked him whether we could go see ernesto ? 
Reinforce 
i don 't know what to do . 
MC 
he 's not going to be here any longer . 
Input 
you must not bomb this town . 
Vanilla-MLE 
you have to go back to the city . 
Vanilla-Sample 
the one is a multiple choice . 
Reinforce 
I think I have to . 
MC 
I think I have to . 
Input 
communism recently failed in eastern europe . 
Vanilla-MLE 
the war is not over . 
Vanilla-Sample 
there are people who hope that in tokyo they won 't risk losing 
their departing . 
Reinforce 
and the people who are not the same . 
MC 
the war is still going on . 
Input 
you should stop in more often . 
Vanilla-MLE 
I don 't think so . 
Vanilla-Sample 
is it possible as old as you ? 
Reinforce 
i 'm not going to be a burden on you . 
MC 
i don 't want to be a burden . 
Input 
he couldn 't have heard us . 
Vanilla-MLE 
he was in the middle of the night . 
Vanilla-Sample 
he should have been somewhere out here . 
Reinforce 
he 's not a good guy . 
MC 
he 's got a lot of nerve. 
Input 
that 's our wake up call . 
Vanilla-MLE 
we 're gonna be late for the meeting . 
Vanilla-Sample 
c 'mon c 'mon let 's get outta here . 
Reinforce 
i 'll be right back . 
MC 
we 're gonna have to get to the station . 
Input 
i had 'em when i started . 
Vanilla-MLE 
i 'm gonna get 'em . 
Vanilla-Sample 
those weak rotten guys had me in a rush on a broken swoop . 
Reinforce 
i don 't know what to do . 
MC 
i was just a little bit nervous . 
Input 
oatmeal delicious start into a new day 
Vanilla-MLE 
i 'll be right back 
Vanilla-Sample 
sure if you don 't put it into the water 
Reinforce 
i 'm gonna be a little busy with the dishes . 
MC 
i 'm gonna make you a little dinner . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 : Appendix: More responses sampled from different models.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157-2169 Copenhagen, Denmark, September 7-11, 2017. c 2017 Association for Computational Linguistics</note>

			<note place="foot" n="3"> Like Ranzato et al. (2016), we train another neural network model (the critic) to estimate the value (or future reward) of current state (i.e., the dialogue history) under the current policy Ï. The critic network takes as input the dialogue history, transforms it to a vector representation using a hierarchical network and maps the representation to a scalar. The network is optimized based on the mean squared loss between the estimated reward and the real reward.</note>

			<note place="foot" n="4"> Consider one target sequence with length 20, we need to sample 5*20=100 full sequences to get rewards for all intermediate steps. Training one batch with 128 examples roughly takes roughly 1 min on a single GPU, which is computationally intractable considering the size of the dialogue data we have. We thus parallelize the sampling processes, distributing jobs across 8 GPUs.</note>

			<note place="foot" n="7"> Similar results are also reported in Kannan and Vinyals (2016).</note>

			<note place="foot" n="9"> The representation before the softmax layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aleksandrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Sysoyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Shemeneva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic optimization. Engineering Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="11" to="16" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Online sequence-to-sequence reinforcement learning for open-domain conversational agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabiha</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasca</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03929</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An actor-critic algorithm for sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep generative image models using a? laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Likelihood ratio gradient estimation for stochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter W Glynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to classify text using support vector machines: Methods, theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial evaluation of dialogue models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Professor forcing: A new algorithm for training recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4601" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Spithourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1094" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple, fast diverse decoding algorithm for neural generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for dialogue generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards an automatic turing test: Learning to evaluate dialogue responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Angelard-Gontier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the evaluation of dialogue systems with next utterance classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Iulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SIGDIAL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09457</idno>
		<title level="m">LSTM based conversation models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-driven response generation in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2011</title>
		<meeting>EMNLP 2011</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="583" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian V Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence (AAAI-16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multiresolution recurrent neural networks: An application to dialogue response generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00776</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Generative deep neural networks for dialogue: A short review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A hierarchical latent variable encoder-decoder model for generating dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP</title>
		<meeting>ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Generating long and diverse responses with neural conversational models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A neural network approach to context-sensitive generation of conversational responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meg</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<title level="m">Continuously learning neural dialogue management. arxiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Computing machinery and intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Turing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="433" to="460" />
			<date type="published" when="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A neural conversational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Deep Learning Workshop</title>
		<meeting>ICML Deep Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04562</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Incorporating loose-structured knowledge into LSTM with recall gate for conversation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05110</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention with intention for a neural network conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on Machine Learning for Spoken Language Understanding and Interaction</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Seqgan: sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05473</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
