<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Contextual Information via Dynamic Memory Network for Event Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobo</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Science (CAS)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">The University of CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Science (CAS)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Control Engineering</orgName>
								<orgName type="institution">The University of CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Science (CAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">CAS Key Laboratory of Network Data Science and Technology</orgName>
								<orgName type="institution">Chinese Academy of Science (CAS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Contextual Information via Dynamic Memory Network for Event Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1030" to="1035"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1030</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of event detection involves identifying and categorizing event triggers. Contex-tual information has been shown effective on the task. However, existing methods which utilize contextual information only process the context once. We argue that the context can be better exploited by processing the context multiple times, allowing the model to perform complex reasoning and to generate better context representation, thus improving the overall performance. Meanwhile, dynamic memory network (DMN) has demonstrated promising capability in capturing contextual information and has been applied successfully to various tasks. In light of the multi-hop mechanism of the DMN to model the context, we propose the trigger detection dynamic memory network (TD-DMN) to tackle the event detection problem. We performed a five-fold cross-validation on the ACE-2005 dataset and experimental results show that the multi-hop mechanism does improve the performance and the proposed model achieves best F 1 score compared to the state-of-the-art methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>According to ACE (Automatic Content Extrac- tion) event extraction program, an event is iden- tified by a word or a phrase called event trigger which most represents that event. For example, in the sentence "No major explosion we are aware of", an event trigger detection model is able to identify the word "explosion" as the event trigger word and further categorize it as an Attack event.</p><p>The ACE-2005 dataset also includes annotations for event arguments, which are a set of words or phrases that describe the event. However, in this work, we do not tackle the event argument classi- fication and focus on event trigger detection.</p><p>The difficulty of the event trigger detection task lies in the complicated interaction between the event trigger candidate and its context. For in- stance, given a sentence at the end of a passage:</p><p>they are going over there to do a mission they believe in and as we said, 250 left yesterday.</p><p>It's hard to directly classify the trigger word "left" as an "End-Position" event or a "Transport" event because we are not certain about what the number "250" and the pronoun "they" are refer- ring to. But if we see the sentence:</p><p>we are seeing these soldiers head out. which is several sentences away from the for- mer one, we now know the "250" and "they" refer to "the soldiers", and from the clue "these soldiers head out" we are more confident to classify the trigger word "left" as the "Transport" event.</p><p>From the above, we can see that the event trigger detection task involves complex reason- ing across the given context. Exisiting methods ( <ref type="bibr" target="#b7">Liu et al., 2017;</ref><ref type="bibr" target="#b0">Chen et al., 2015;</ref><ref type="bibr" target="#b5">Li et al., 2013;</ref><ref type="bibr" target="#b9">Nguyen et al., 2016;</ref><ref type="bibr" target="#b11">Venugopal et al., 2014)</ref> mainly exploited sentence-level features and ( <ref type="bibr" target="#b6">Liao and Grishman, 2010;</ref><ref type="bibr" target="#b13">Zhao et al., 2018)</ref> proposed document-level models to utilize the context. The methods mentioned above either not di- rectly utilize the context or only process the con- text once while classifying an event trigger candi- date. We argue that processing the context multi- ple times with later steps re-evaluating the context with information acquired from the previous steps improves the model performance. Such a mech- anism allows the model to perform complicated reasoning across the context. As in the example, we are more confident to classify "left" as a "Tran- port" event if we know "250" and "they" refer to "soldiers" in previous steps.</p><p>We utilize the dynamic memory network (DMN) ( <ref type="bibr" target="#b12">Xiong et al., 2016;</ref><ref type="bibr" target="#b3">Kumar et al., 2016)</ref> to capture the contextual information of the given trigger word. It contains four modules: the input module for encoding reference text where the an- swer clues reside, the memory module for storing knowledge acquired from previous steps, the ques- tion module for encoding the questions, and the answer module for generating answers given the output from memory and question modules. DMN is proposed for the question answering task, however, the event trigger detection problem does not have an explicit question. The original DMN handles such case by initializing the ques- tion vector produced by the question module with a zero or a bias vector, while we argue that each sentence in the document could be deemed as a question. We propose the trigger detection dy- namic memory network (TD-DMN) to incorporate this intuition, the question module of TD-DMN treats each sentence in the document as implicitly asking a question "What are the event types for the words in the sentence given the document con- text". The high-level architecture of the TD-DMN model is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We compared our results with two models: DM- CNN( <ref type="bibr" target="#b0">Chen et al., 2015)</ref> and DEEB-RNN( <ref type="bibr" target="#b13">Zhao et al., 2018</ref>) through 5-fold cross-validation on the ACE-2005 dataset. Our model achieves best F 1 score and experimental results further show that processing the context multiple times and adding implicit questions do improve the model perfor- mance. The code of our model is available online. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Approach</head><p>We model the event trigger detection task as a multi-class classification problem following exist- ing work. In the rest of this section, we describe four different modules of the TD-DMN separately along with how data is propagated through these modules. For simplicity, we discuss a single docu- ment case. The detailed architecture of our model is illustrated in <ref type="figure" target="#fig_1">Figure 2.</ref> 1 https://github.com/AveryLiu/TD-DMN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Module</head><p>The input module further contains two layers: the sentence encoder layer and the fusion layer. The sentence encoder layer encodes each sentence into a vector independently, while the fusion layer gives these encoded vectors a chance to exchange information between sentences.</p><p>Sentence encoder layer Given document d with l sentences (s 1 , . . . , s l ), let s i denotes the i-th sentence in d with n words (w i1 , . . . , w in ). For the j-th word w ij in s i , we concatenate its word em- bedding w ij with its entity type embedding 2 e ij to form the vector W ij as the input to the sentence encoder Bi-GRU( <ref type="bibr" target="#b1">Cho et al., 2014</ref>) of size H s . We obtain the hidden state h ij by merging the forward and backward hidden states from the Bi-GRU:</p><formula xml:id="formula_0">h ij = −−−→ GRU s (W ij ) + ←−−− GRU s (W ij ) (1)</formula><p>where + denotes element-wise addition.</p><p>We feed h ij into a two-layer perceptron to gen- erate the unnormalized attention scalar u ij :</p><formula xml:id="formula_1">u ij = tanh(h ij · W s 1 ) · W s 2 (2)</formula><p>where W s 1 and W s 2 are weight parameters of the perceptron and we omitted bias terms. u ij is then normalized to obtain scalar attention weight α ij :</p><formula xml:id="formula_2">α ij = exp(u ij ) n k=1 exp(u ik )<label>(3)</label></formula><p>The sentence representation s i is obtained by:</p><formula xml:id="formula_3">s i = n j=1 α ij h ij (4)</formula><p>Fusion layer The fusion layer processes the en- coded sentences and outputs fact vectors which contain exchanged information among sentences. Let s i denotes the i-th sentence representation ob- tained from the sentence encoder layer. We gen- erate fact vector f i by merging the forward and backward states from fusion GRU:</p><formula xml:id="formula_4">f i = − −−− → GRU f (s i ) + ← −− − GRU f (s i )<label>(5)</label></formula><p>Let H f denotes the hidden size of the fusion GRU, we concatenate fact vectors f 1 to f l to ob- tain the matrix F of size l by H f , where the i-th row in F stores the i-th fact vector f i .  The memory module initializes m 0 with q * and iteratively processes for t times, at each time k it produces a memory vector m k using fact matrix F , question vector q * and previous memory state m k−1 . The answer module outputs the predicted trigger type for each word in s i using the concatenated tensor of the hidden states of the question module and the last memory state m t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Module</head><formula xml:id="formula_5">… W i1 W ij … W in Bi-GRU Bi-GRU Bi-GRU … … … … Sentence Encoder Layer h i1 h ij h in … … α i1 α ij α in s i s i+1 s l … s 1 s i-1 … Bi-GRU Bi-GRU Bi-GRU Bi-GRU Bi-GRU … Fusion Layer … … … f i … … f 1 f i-1 f i+1 f l m t w i1 w ij e ij w in e in w in e in w ij e ij w i1 e i1 … W i1 W ij … W in Bi-GRU Bi-GRU Bi-GRU … … … … Question</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Question Module</head><p>The question module treats each sentence s in d as implicitly asking a question: What are the event types for each word in the sentence s given the document d as context? For simplicity, we only discuss the single sentence case. Iteratively pro- cessing from s 1 to s l will give us all encoded ques- tions in document d. Let W ij be the vector representation of j-th word in s i , the question GRU generates hidden state q ij by:</p><formula xml:id="formula_6">q ij = − −− → GRU q (W ij ) + ← −− − GRU q (W ij )<label>(6)</label></formula><p>The question vector q * is obtained by averaging all hidden states of the question GRU:</p><formula xml:id="formula_7">q * = 1 n n j=1 q ij<label>(7)</label></formula><p>Let H q denotes the hidden size of the question GRU, q * is a vector of size H q , the intuition here is to obtain a vector that represents the question sentence. q * will be used for the memory module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Memory Module</head><p>The memory module has three components: the attention gate, the attentional GRU( <ref type="bibr" target="#b12">Xiong et al., 2016</ref>) and the memory update gate. The attention gate determines how much the memory module should attend to each fact given the facts F , the question q * , and the acquired knowledge stored in the memory vector m t−1 from the previous step.</p><p>The three inputs are transformed by:</p><formula xml:id="formula_8">u = [F * q * ; |F − q * |; F * m t−1 ; |F − m t−1 |] (8)</formula><p>where ; is concatenation. * , − and |.| are element- wise product, subtraction and absolute value re- spectively. F is a matrix of size (m, H f ), while q * and m t−1 are vectors of size (1, H q ) and (1, H m ), where H m is the output size of the memory update gate. To allow element-wise operation, H f , H q and H m are set to a same value H. Meanwhile, q * and m t−1 are broadcasted to the size of (m, H). In equation 8, the first two terms measure the sim- ilarity and difference between facts and the ques- tion. The last two terms have the same functional- ity for facts and the last memory state. Let β of size l denotes the generated attention vector. The i-th element in β is the attention weight for fact f i . β is obtained by transforming u using a two-layer perceptron: <ref type="table">P  R  F1  P  R  F1  P  R  F1  P  R  F1  P  R  F1  F1  DMCNN</ref>  where W m 1 and W m 2 are parameters of the per- ceptron and we omitted bias terms.</p><formula xml:id="formula_9">β = softmax(tanh(u · W m 1 ) · W m 2 ) (9)</formula><note type="other">Methods Fold 1 Fold 2 Fold 3 Fold 4 Fold 5 Avg</note><p>The attentional GRU takes facts F , fact atten- tion β as input and produces context vector c of size H c . At each time step t, the attentional GRU picks the f t as input and uses β t as its update gate weight. For space limitation, we refer reader to ( <ref type="bibr" target="#b12">Xiong et al., 2016</ref>) for the detailed computation.</p><p>The memory update gate outputs the updated memory m t using question q * , previous memory state m t−1 and context c:</p><formula xml:id="formula_10">m t = relu([q * ; m t−1 ; c] · W u )<label>(10)</label></formula><p>where W u is the parameter of the linear layer.</p><p>The memory module could be iterated several times with a new β generated for each time. This allows the model to attend to different parts of the facts in different iterations, which enables the model to perform complicated reasoning across sentences. The memory module produces m t as the output at the last iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Answer Module</head><p>Answer module predicts the event type for each word in a sentence. For each question GRU hidden state q ij , the answer module concatenates it with the memory vector m t as the input to the answer GRU with hidden size H a . The answer GRU out- puts a ij by merging its forward and backward hid- den states. The fully connected dense layer then transforms a ij to the size of the number of event labels O and the softmax layer is applied to output the probability vector p ij . The k-th element in p ij is the probability for the word w ij being the k-th event type. Let y ij be the true event type label for word w ij . Assuming all sentences are padded to the same length n, the cross-entropy loss for the single document d is applied as:</p><formula xml:id="formula_11">J(y, p) = − l i=1 n j=1 O k=1 I(y ij = k)log p (k) ij (11)</formula><p>where I(·) is the indicator function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset and Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dateset</head><p>Different from prior work, we performed a 5-fold cross-validation on the ACE 2005 dataset. We par- titioned 599 files into 5 parts. The file names of each fold can be found online <ref type="bibr">3</ref> . We chose a differ- ent fold each time as the testing set and used the remaining four folds as the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We compared our model with two other mod- els: <ref type="bibr">DMCNN (Chen et al., 2015)</ref> and DEEB- RNN ( <ref type="bibr" target="#b13">Zhao et al., 2018)</ref>. DMCNN is a sentence- level event detection model which enhances tradi- tional convolutional networks with dynamic mul- tiple pooling mechanism customized for the task. The DEEB-RNN is a state-of-the-art document- level event detection model which firstly generate a document embedding and then use it to aid the event detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>We report precision, recall and F 1 score of each fold along with the averaged F 1 score of all folds. We evaluated all the candidate trigger words in each testing set. A candidate trigger word is cor- rectly classified if its event subtype and offsets match its human annotated label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>To avoid overfitting, we fixed the word embedding and added a 1 by 1 convolution after the embed- ding layer to serve as a way of fine tuning but with a much smaller number of parameters. We removed punctuations, stop words and sentences which have length less equal than 2. We used the Stanford corenlp toolkit( <ref type="bibr" target="#b8">Manning et al., 2014</ref>) to separate sentences. We down-sampled negative samples to ease the unbalanced classes problem.</p><p>The setting of the hyperparameters is the same for different hops of the TD-DMN model. We set H, H s , H c , and H a to 300, the entity type embed- ding size to 50, W s 1 to 300 by 600, W s 2 to 600 by 1, W m 1 to 1200 by 600, W m 2 to 600 by 1, W u to 900 by 300, the batch size 4 to 10. We set the down- sampling ratio to 9.5 and we used Adam optimizer <ref type="bibr" target="#b2">(Kingma and Ba, 2014</ref>) with weight decay set to 1e −5 . We set the dropout( <ref type="bibr" target="#b10">Srivastava et al., 2014)</ref> rate before the answer GRU to 0.4 and we set all other dropout rates to 0.2. We used the pre-trained word embedding from ( <ref type="bibr" target="#b4">Le and Mikolov, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on the ACE 2005 Corpus</head><p>The performance of each model is listed in ta- ble 1. The first observation is that models us- ing document context drastically outperform the model that only focuses on the sentence level fea- ture, which indicates document context is helpful in event detection task. The second observation is that increasing number of hops improves model performance, this further implies that processing the context for multiple times does better exploit the context. The model may have exhausted the context and started to overfit, causing the perfor- mance drop at the fourth hop.</p><p>The performance of reference models is much lower than that reported in their original papers. Possible reasons are that we partitioned the dataset randomly, while the testing set of the original par- tition mainly contains similar types of documents and we performed a five-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Impact of the Question Module</head><p>To reveal the effects of the question module, we ran the model in two different settings. In the first setting, we initialized the memory vector m 0 and question vector q * with a zero vector, while in the second setting, we ran the model untouched. The results are listed in the table 2. The two models perform comparably under the 1-hop setting, this implies that the model is unable to distinguish the initialization values of the question vector well in the 1-hop setting. For higher number of hops, the untouched model outperforms the modified one. This indicates that with a higher number of mem- ory iterations, the question vector q * helps the model to better exploit the context information. <ref type="bibr">4</ref> In each batch, there are 10 documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>F 1 F * 1 TD-DMN 1-hop 65.48 65.52 TD-DMN 2-hop 65.69 65.46 TD-DMN 3-hop 65.78 65.51 TD-DMN 4-hop 65.57 65.40 indicates results with empty questions.</p><p>We still observe the increase and drop pattern of the F 1 for the untouched model. However, such a pattern is not obvious with empty questions. This implies that we are unable to have a steady gain without the question module in this specific task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Future Work</head><p>In this work, we explored the TD-DMN archi- tecture to exploit document context. Extending the model to include wider contexts across several similar documents may also be of interest. The detected event trigger information can be incor- porated into question module when extending the TD-DMN to the argument classification problem.</p><p>Other tasks with document context but without ex- plicit questions may also benefit from this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed the TD-DMN model which utilizes the multi-hop mechanism of the dy- namic memory network to better capture the con- textual information for the event trigger detec- tion task. We cast the event trigger detection as a question answering problem. We carried five- fold cross-validation experiments on the ACE- 2005 dataset and results show that such multi-hop mechanism does improve the model performance and we achieved the best F 1 score compared to the state-of-the-art models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the TD-DMN architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The detailed architecture of the TD-DMN model. The figure depicts a simplified case where a single document d with l sentences is the input to the input module and a sentence s i of d with n words is the input to the question module. The input module encodes document d into a fact matrix {F |f 1 ,. .. , f l }. The question module encodes sentence s i into the question vector q * . The memory module initializes m 0 with q * and iteratively processes for t times, at each time k it produces a memory vector m k using fact matrix F , question vector q * and previous memory state m k−1. The answer module outputs the predicted trigger type for each word in s i using the concatenated tensor of the hidden states of the question module and the last memory state m t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>1-hop 67.3 62.1 64.6 65.4 61.7 63.5 72.0 60.0 65.4 66.6 68.0 67.3 68.3 65.0 66.6 65.5 TD-DMN 2-hop 69.2 61.0 64.8 64.6 63.4 64.0 64.3 66.4 65.3 68.7 65.9 67.3 68.5 65.7 67.1 65.7 TD-DMN 3-hop 66.3 63.7 64.9 66.9 60.6 63.6 68.3 64.0 66.1 67.9 66.3 67.1 70.2 64.3 67.1 65.8 TD-DMN 4-hop 66.7 63.4 65.0 61.4 65.5 63.4 66.4 66.0 66.2 64.7 69.1 66.8 70.0 63.4 66.5 65.6 Table 1: 5-fold cross-validation results on the ACE-2005 dataset. The results are rounded to a single digit. The F 1 of the last column are calculated by averaging F 1 scores of all folds.</figDesc><table>67.6 60.5 63.9 62.6 63.1 62.9 68.9 62.1 65.3 68.9 65.0 66.9 66.0 65.5 65.8 64.9 
DEEB-RNN 
64.9 64.1 64.5 63.4 64.7 64.0 66.1 64.3 65.2 66.0 67.3 66.6 65.5 67.2 66.3 65.3 
TD-DMN </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The impact of the question module, F  *  

1 

</table></figure>

			<note place="foot" n="2"> The ACE-2005 includes entity type (including type &quot;NA&quot; for none-entity) annotations for each word, the entity type embedding is a vector associated with each entity type.</note>

			<note place="foot" n="3"> https://github.com/AveryLiu/TD-DMN/data/splits</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Prof. Huawei Shen for providing men-torship in the rebuttal phase. We thank Jinhua Gao for discussion on the paper presentation. We thank Yixing Fan and Xiaomin Zhuang for providing ad-vice regarding hyper-parameter tuning. We thank Yue Zhao for the initial discussion on event ex-traction. We thank Yaojie Lu for providing pre-processing scripts and the results of DMCNN. We thank anonymous reviewers for their advice. The first author personally thank Wei Qi for being sup-portive when he was about to give up.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Event extraction via dynamic multi-pooling convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using document level cross-event inference to improve event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint event extraction via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Thien Huu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relieving the computational bottleneck: Joint inference for event extraction with high-dimensional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="831" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document embedding enhanced event detection with hierarchical and supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="414" to="419" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
