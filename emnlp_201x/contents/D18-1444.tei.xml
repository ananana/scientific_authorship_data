<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controlling Length in Abstractive Summarization Using a Convolutional Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Controlling Length in Abstractive Summarization Using a Convolutional Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4110" to="4119"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Convolutional neural networks (CNNs) have met great success in abstractive summariza-tion, but they cannot effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in ab-stractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this approach generates high-quality summaries with user defined length, and outperforms the baselines consistently in terms of ROUGE score, length variations and semantic similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Great progress ( <ref type="bibr" target="#b16">Rush et al., 2015;</ref><ref type="bibr" target="#b3">Chopra et al., 2016;</ref><ref type="bibr" target="#b13">Nallapati et al., 2016;</ref><ref type="bibr" target="#b17">See et al., 2017;</ref><ref type="bibr" target="#b15">Paulus et al., 2017</ref>) has been made recently on abstractive summarization. Many use sequence- to-sequence model based on RNN and attention mechanism ( <ref type="bibr" target="#b16">Rush et al., 2015)</ref>, which was orig- inally used for machine translation <ref type="bibr" target="#b19">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). Recently, <ref type="bibr" target="#b6">Gehring et al. (2017)</ref> proposed a convolutional se- quence to sequence model equipped with Gated Linear Units ( ), residual con- nections ( <ref type="bibr" target="#b7">He et al., 2016</ref>) and attention mecha- nism. Such a convolutional model achieves state- of-the-art accuracies in abstractive summarization on single sentence summarization, and it is much faster than the previous recurrent models as it can be easily parallelized. Furthermore, unlike recur- rent models, the convoluational model has more stable gradients because of its backpropagation path.</p><p>Constraining summary length, while largely ne- glected in the past, is actually an important aspect of abstractive summarization. For example, given the same input document, if the summary is to be displayed on mobile devices, or within a fixed area of advertisement slot on a website, we may want to produce a much shorter summary. Unfor- tunately, most existing abstractive summarization models are not trained to react to summary length constraints. When the constraint is given at test time, the current practice is i) to truncate the gener- ated summary after N tokens are generated when you want the summaries of length no more than N , and ii) ignore EOS (end of summary) token until the first M tokens are generated when you want the summaries of length at least M . Such a crude way of controlling summary length makes the output summary incomplete or incoherent.</p><p>Previous research on controlling length of ab- stractive summary has been scarce. , who applies convolutional sequence to se- quence model on multi-sentence summarization, converts length range as some special markers which are predefined and fixed. These markers are included in the training vocabulary. At train- ing time, the model prepends the input of the sum- marizer with marker indicating the length of input sequence. At test time, it controls the length of the generated summary also by prepending length marker indicating the desired length. Unfortu- nately, this approach can not generate summaries of arbitrary lengths. It only generates summaries in predefined ranges of length, thus only meets the length constraints approximately. This is shown in <ref type="table">Table 1</ref>. The above truncation practice can be used in conjunction with any of the length control meth- ods but the excessive parts (red) will be truncated leaving incomplete sentences.</p><p>In our work, we extend the convolutional se- quence to sequence model ( <ref type="bibr" target="#b6">Gehring et al., 2017)</ref> by controlling the length of summarization. Our approach seeks to generate summaries of any de- <ref type="table">Table 1: Example summaries generated by different  models with a desired length of 10 (red parts exceed  the 10 token limit)</ref>.</p><p>Reference summary (53 tokens) david de gea and victor valdes enjoyed an afternoon off at a theme park . spanish duo donned shades as they made the most of the rare sunshine . it has certainly been a rollercoaster season for manchester united . united are third in the premier league after an impressive recent run . Basic CNN summary (35 tokens) david de gea and victor valdes made the most of the rare english sun with a trip to a theme park . david de gea and victor valdes enjoyed some fun in the sun . ) summary (30 tokens) david de gea and victor valdes enjoyed a trip to a theme park . the pair enjoyed a relaxing time just days after united 's win against manchester city . Our Length Control summary (LC) (10 tokens) david de gea and victor valdes enjoy some fun . sired number of tokens (also shown in <ref type="table">Table 1</ref>). To do this, a length constraint is added to each con- volutional block of the initial layer of the model. This information is propagated layer by layer dur- ing training. Our contributions are as follows:</p><p>1. We propose a simple but effective method to generate summaries with arbitrary desired length (Section 2.2).</p><p>2. Our approach outperforms the state-of-art baseline methods substantially by all evalua- tion metrics, i.e., ROUGE scores, length vari- ation and semantic similarity (Section 3).</p><p>3. The generated summaries from our model are natural and complete, especially when the de- sired length is short (Section 3).</p><p>Next, we present the basic convolutional se- quence to sequence model and our extension, fol- lowed by the evaluation of our approach and a dis- cussion of related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this section, we will describe the model archi- tecture used for our experiments and propose our length control method which is implemented by extending the basic model. For summarization problems based on seq2seq model, given a sequence of tokens x = (x 1 , x 2 , ..., x m ) in the source document and a se- quence of tokens y = (y 1 , y 2 , ..., y n ) in the target summary (i.e. m &gt; n), the goal is to estimate the conditional probability p(y|x):</p><formula xml:id="formula_0">p(y|x) = T t p(y t |y 1 , y 2 , ..., y t−1 , x)<label>(1)</label></formula><p>We aim at getting the above conditional prob- abaility which can generate summaries with arbi- trary desired length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic CNN seq2seq Model</head><p>Our basic model consists of a multi-layer con- volutional sequence to sequence model (CNN seq2seq) <ref type="bibr">1 (Gehring et al., 2017;</ref><ref type="bibr" target="#b10">LeCun et al., 1989)</ref> and an attention mechanism. <ref type="figure" target="#fig_0">Figure 1</ref> il- lustrates the model. In the CNN seq2seq model, we obtain the input sequence X = (X 1 , ..., X m ) and output sequence Y = (Y 1 , ..., Y n ) after combining word vectors with their absolute positions in the document. We use z = (z l 1 , z l 2 , ..., z l m ) and h = (h l 1 , h l 2 , ..., h l n ) to denote the convolutional output of the encoder and decoder in l-th layer. Each element of the output sequence generated by the decoder network is fed back into the next layer of decoder network. Next, we add GLU ( ) and residual connections ( <ref type="bibr" target="#b7">He et al., 2016</ref>) in each layer:</p><formula xml:id="formula_1">h l i = GLU (W l [h l−1 s , ..., h l−1 t ]+b l )+h l−1 i (2)</formula><p>where [h l−1 s , ..., h l−1 t ] corresponds to the h l i in the convolutional layers. The choice of s and t is based on kernel width and the padding method used to match the output of convolutional layers to the input length. We compute the probability distribution of generating the next elements y i+1 based on the current state and transform the top decoder output h L i via softmax:</p><formula xml:id="formula_2">p(y i+1 |y 1 , ..., y i , x) = sof tmax(W o h L i +b o ) (3)</formula><p>In addition, a multi-step attention mechanism that connects the encoder and decoder is used in each decoder layer. We define the decoder state d l i for attention as following:</p><formula xml:id="formula_3">d l i = W l d h l i +b l d + Y i<label>(4)</label></formula><p>The attention c l i is a weighted sum of the en- coder outputs. The weights a l ij are based on the decoder states.</p><formula xml:id="formula_4">a l ij = exp(d l i · z u j ) m t=1 exp(d l i · z u t )<label>(5)</label></formula><formula xml:id="formula_5">c l i = m j=1 a l ij (z u j + X j )<label>(6)</label></formula><p>At last, we add c l i to the current decoder ele- ments h l i , which forms the final output or the input of the next layer in the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modified Model with Length Control (LC)</head><p>We propose an approach which can control the summary length in CNN seq2seq model. The model can generate different summaries by setting desired length. It has the ability to generate the EOS tag at the appropriate time point in a natural manner.</p><p>To produce a summary of a given desired length, we modify the basic model by feeding the desired length as a parameter into the decoder of the CNN seq2seq model. At training time, we use the true length of the gold summary as the de- sired length. At test time, we can give any desired length len to the model and obtain a summary with length approximate to len. The modified decoder is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The CNN seq2seq model creates hierarchical structure over the input sequence. It is capable of capturing the correlation between elements over short distances at lower layers and between ele- ments over long distances at higher layers. The useful information among the elements is aggre- gated after GLU. Therefore, we set the desired length as an input to the initial state of the decoder:</p><formula xml:id="formula_6">h 1 i = v(W 1 [h 0 s , ..., h 0 t ]+b 1 )+h 0 i * len (7)</formula><p>where W is a trainable parameter, len is the de- sired length, v is GLU funciton and h 0 i is the i-th element in the initial layer.</p><p>In the above function, we add length informa- tion at first layer in CNN model. GLU is like a gate. It can filter some information from a particu- lar unit in each layer. The information attenuation occurs in GLU layer by layer. Different desired lengths have different degrees of information at- tenuation. Therefore the model is able to learn the probability of generating EOS with its own length information attenuation. This operation enables the model to produce a natural and complete sum- mary for a given length constraint naturally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>In this section, our benchmark is the CNN/Daily Mail DMQA dataset ( <ref type="bibr" target="#b8">Hermann et al., 2015;</ref><ref type="bibr" target="#b13">Nallapati et al., 2016;</ref><ref type="bibr" target="#b17">See et al., 2017)</ref>  <ref type="bibr">2</ref> , consisting of pairs of a single source document and a multi- sentence summary. The dataset includes 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs. We follow the same pre-processing step used by <ref type="bibr" target="#b17">See et al. (2017)</ref>, and fill in the blanks with answer named entities. We show an example of such pairs in <ref type="table">Table 4</ref>(a).</p><p>We compare our length constrained summariza- tion model with the basic CNN seq2seq model and the state-of-the-art length controllable summariza- tion model   <ref type="bibr">3</ref> . Following Fan et al., we distribute the dataset into a set of disjoint buckets that correspond to summaries of different lengths. Each bucket contains roughly equal num- ber of documents. The distribution is shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>All competing methods have three flavors: free, truncated and exact. In the free version(Free), given the desired length N , each method gener- ates summaries naturally until an EOS is gener- ated. In the truncated version(Trunc), each method artificially inserts an EOS if EOS has not been generated in the first N tokens. In the exact ver- sion(Exact), each method generates N non-EOS tokens by assigning a score of -∞ to the EOS and inserts an EOS after the N -th token. The pur- pose of Free version is to evaluate the method's ability to generate summaries with desired length; the purpose of the other two versions is to en- able fair comparison of the summaries in terms of their content given that the summaries are of equal length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>In the following experiments, all the competing models have 8 convolutional layers in both en- coder and decoder parts with kernel width as 3.</p><p>For each convolutional layer, we set the hidden vector size as 512 and the embedding size as 256.</p><p>To alleviate the overfitting problem, we add the dropout (p = 0.2) layer for all convolutional lay- ers and fully connected layers.</p><p>To optimize the proposed model, we use Nes- terov's accelerated gradient method <ref type="bibr" target="#b18">(Sutskever et al., 2013</ref>) with gradient clipping 0.1 ( <ref type="bibr" target="#b14">Pascanu et al., 2013)</ref>, momentum 0.99, and learning rate 0.2. We terminate the training process when the learning rate drops below 10e-5. We set beam size as 5 for the beam search algorithm in the testing step. Next, we introduce the evaluation metrics in the following experiments:</p><p>1. ROUGE scores (F1 score) of the pro- duced summaries, including ROUGE-1(R-1), ROUGE-2(R-2) and ROUGE-L(R-L) <ref type="bibr" target="#b11">(Lin, 2004)</ref>. ROUGE-2 is the most popular metric for summarization.</p><p>2. Variance(Var) of the summary lengths against the desired length len:</p><formula xml:id="formula_7">var = 0.001 * 1 n n i=0 |l i − len| 2 , (8)</formula><p>where n is the number of pairs in the dataset, and l i is the length of the generated summary i. We introduce the variance to evaluate the ability of exact control of the output length.</p><p>3. Similarity(Sim) between generated sum- maries and their corresponding reference summaries:</p><formula xml:id="formula_8">sim = 1 n n i=0 y i · y i ||y i ||||y i || (9)</formula><p>where n is the number of pairs. y i is the vec- tor representation of the reference summary i and y i is vector of the corresponding gener- ated summary i. Both y i and y i are the sum of GloVe 4 word vectors of the words in these summaries.</p><p>We introduce the similarity metric here to com- plement the ROUGE scores because <ref type="bibr" target="#b20">Yao et al. (2017a)</ref> showed that the standard ROUGE scores cannot capture semantic similarity beyond n- grams. Given the same source document, abstrac- tive summarization may create summaries that don't share many words but mean the same. To show the effectiveness of this Sim metric, we de- sign a dataset from the summarization tasks of TAC 2010∼2011 5 . The TAC dataset consists of 90 topics in total, each with 2 subset. Each subset has 4 reference summaries by different humans. We assume reference summaries about the same topic to be semantically similar to each other, while summaries across topics are unrelated. Thus we created 2,160 pairs of similar summaries as pos- tive data and 2,160 pairs of unrelated summaries as negative data. We then compute the Pearson corre- lation between the ROUGE score and the ground truth as well as between Sim and the ground truth and show the results in <ref type="table" target="#tab_0">Table 2</ref>. Sim metric cer- tainly resembles semantic similarity better than ROUGE by this experiment.</p><p>In this paper, we don't use manual evaluation as the major metric. The reason is that <ref type="bibr" target="#b11">Lin (2004)</ref> showed that the manual evaluation is unstable and the inter-human agreement is low due to the vari- ety in abstractive summaries. The ROUGE scores and Similarity scores can respectively measure the syntactic similarity and semantic similarity. They are complementary to each other and give bet- ter quantitative assessment of the summarization quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment 1: Gold Summary Lengths</head><p>In the first experiment, for each test document- summary pair, we set the desired length as the length of the gold summary and ask the competing methods to generate a summary with the desired length. As shown in <ref type="table" target="#tab_1">Table 3</ref>, the proposed model (LC) outperforms the other models on all of the evaluation metrics. The ROUGE score shows the accuracy of these models. Lower variance reflects better length control of the model. Higher similar- ity reflects better quality of generated summaries from the semantic point of view. The LC model achieves the highest ROUGE and similarity scores as well as the lowest vari- ance in both Free and Exact version, which shows the effectiveness of LC for generating high quality summaries under length constraint. In the Trunc version, the LC model outperforms the other com- parable models on all evaluation metrics except for the ROUGE score. Note that, the ROUGE scores of LC model are very stable, indicating its effec- tive length control. As for the other two mod- els, they have better ROUGE score on Trunc ver- sion. However, as the example shown in <ref type="table">Table  4</ref> 6 , higher ROUGE scores do not necessarily mean <ref type="bibr">6</ref> The entities in different color indicate two important roles in the text. The words in bold type mean correct content. high quality abstractive summaries.</p><p>The ROUGE score consists of Recall(R), Preci- sion(P) and F1-measure(F). The summary tends to achieve a better ROUGE score when the length of generated summary is slightly shorter than the de- sired length. In <ref type="table">Table 4</ref>(b), the CNN model has the same R score as LC model and a higher P score than LC model because of its slightly shorter length. We can see that the CNN model achieve a higher F score even its generated summary is not good. Moreover, for the basic model, the gener- ated summary always repeats the sentences when the length of generated summary is longer than the desired length. In <ref type="table">Table 4</ref>(c), the P score of its Trunc version would be improved by a large mar- gin. Thus, the ROUGE score for the Trunc version biases toward the models with weak length con- trol. The generated summaries of the LC model in <ref type="table">Table 4</ref>(d), which capture the semantic of the ref- erence summary and satisfy the constraint length very well, are better than the other two models even with a slightly lower ROUGE score. The topic of this example is that Louis Jordan, who is the son of Frank Jordan, got lost during sailing and was finally rescued from his boat. Our model gen- erates the summary with correct information, but other two models get the Louis Jordan and Frank Jordan mixed up. This is correctly measured by the similarity scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experiment 2: Arbitrary Lengths</head><p>In the second experiment, we ask the methods to generate summaries with arbitrary lengths. We re- port the results of all three methods with five arbi- trary lengths: 10, 30, 50, 70 and 90. We show the performance of each model with different length constraints in <ref type="table" target="#tab_3">Table 5, Table 6</ref>, <ref type="figure">Figure 4</ref> and <ref type="figure">Figure  5</ref>. The basic CNN model has the same ROUGE scores in the Free version since it cannot control the length of generated summaries on its own. For , the desired length is mapped to the model's predefined fixed length range(s) that contains the desired length before it produces its summaries. For example, the desired length 10 is mapped to the first bucket <ref type="bibr">(0,</ref><ref type="bibr">33]</ref>.</p><p>To demonstrate the effectiveness of LC model and further illustrate the results, we show an ex- ample of generated summaries by LC(Free) model with different lengths. As shown in <ref type="table" target="#tab_5">Table 7</ref>, when the desired length (e.g., 10) is very different from the length of the reference summary, the ROUGE <ref type="table">Table 4</ref>: Example summaries generated in Experiment 1.</p><p>(a) Source document and reference summary (36 tokens) Source document the last time frank jordan spoke with his son, louis jordan was fishing on a sailboat a few miles off the south carolina coast. the next time ... more than two months had passed and the younger jordan was on a contrainer ship 200 miles from north carolina, just rescued from his disabled boat . "i thought i lost you,"the relieved father said. louis jordan, 37, took his sailboat out in late january and hadn't been heard from in 66 days ... the younger jordan said he took his sailboat out to the gulf stream to find some better fishing ... the boat capsized two more times before he was rescued, according to jordan. Reference summary louis jordan says his sailboat capsized three times . he survived by collecting rainwater and eating raw fish . frank jordan told cnn his son is n't an experienced sailor but has a strong will .</p><p>(b) Free summary <ref type="formula">(29 tokens</ref> Trunc louis jordan was on a sailboat a few miles off the south carolina coast . he had n't been heard from in 66 days when he was rescued . he was rescued from his boat .</p><p>6.06 6.06 6.06 0.0 0.9293 Exact louis jordan was on a sailboat a few miles off the south carolina coast . he had n't been heard from in 66 days when he was rescued . he was rescued from his boat .</p><p>6.06 6.06 6.06 - 0.9293 score may not be good even though the gener- ated summary matches the reference quite well semantically. The generated summaries from LC model are natural and complete. The summaries with short desired length on Trunc and Exact ver- sion would be more vulnerable to the incomplete problem. We randomly sample 100 summaries generated by each model under Trunc and Ex- act with desired length of 10 and 30, and manu- ally inspect their readibility. This is a simplified human-evaluation of summarization, which just determines whether the sentences in summaries under length control are complete or not. If com- plete, the score is 1; if not, it is 0. It is easier to accomplish and more reliable than other sophis- ticated human-evaluation. <ref type="table">Table 8</ref> shows that the LC model has a clear advantage over the other two models in terms of summary fluency.</p><p>In this experiment, the desired length is fixed for all the documents which is independent from the corresponding lengths of reference summaries such that the generated summaries may include more versatile words and phrases different from the reference summaries. Thus, the similar-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50</head><p>" i thought i lost you , " jordan says . the younger jordan was on a sailboat a few miles off the south carolina coast . " i thought i lost you , " jordan tells his son . jordan says he was grateful to the people .</p><p>ity score is more reasonable for evaluation than ROUGE score. As shown in <ref type="table" target="#tab_4">Table 6</ref> and <ref type="figure">Figure  4</ref>, the LC model achieves the highest similarity score except for the length of 10 and 30 in the Free version. The reason is that there is only 5% of testing data with the length of reference summary shorter than 30. Due to the effective length con- trol of LC model, the lengths of generated sum- maries from LC model are usually much shorter than those from the other models and the length of corresponding reference summaries when we   <ref type="figure">Figure 5</ref>, the LC model achieves the lowest variance. In <ref type="figure">Figure  5</ref>(a), as the length of most summaries is around 50 and the number of summaries with a length of 10 or 90 is small, the CNN model and Fan model has lowest variance at 50 and highest variance at 90. In <ref type="figure">Figure 5(</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Significance Test on Similarity Result</head><p>We use significance test to prove that similarity metric is reliable even though the numerical dif- ference of similarity scores in experiment is little. Because the similarity scores of generated sum- maries do not follow normal distribution, we take Kruskal-Wallis test <ref type="bibr" target="#b12">(Loukina et al., 2014;</ref><ref type="bibr" target="#b0">Albert, 2017)</ref> as our significance test to measure that the difference of similarity results of three methods is significant or not. As shown in <ref type="table" target="#tab_8">Table 9</ref>, all p- values are less than 0.05. The smaller p-value, the higher significant. Thus, the difference of the sim- ilarity results is significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>In this section, we discuss some previous work on length control in abstractive summarization and explain why we choose CNN as our basic summa- rization model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Length Control for Abstractive Summarization</head><p>When summarizing a document, it is desirable to be able to control the length of summary so as to cater to different users and scenarios. Most abstractive summarization systems are based on encoder-decoder models and generate summaries whose length depends on the training summaries. Due to the variability of the sequence genera- tion models, such as the different structures and functions, it is hard to design a length constraint method on all summarization models.</p><p>Previous methods control summary length by generating EOS token at a particular time. <ref type="bibr" target="#b16">Rush et al. (2015)</ref> used an ad-hoc method, in which the system is inhibited from generating the EOS tag by assigning a score of -∞ to the tag and gener- ats a fixed number of words. <ref type="bibr" target="#b9">Kikuchi et al. (2016)</ref> proposed two different methods for RNN seq2seq model which can control the summary length by taking length embedding as an additional input for the LSTM and adding desired length into initial memory cell for the LSTM. In this model, they use the Gigawords as dataset and focus on the abstrac- tive summarization in sentence level which gen- erates one sentence as the summary. For CNN seq2seq model,  put some spe-cial markers into the vocabulary which denote dif- ferent length ranges. It prepends the input of the summarizer with the marker during training and testing. These special markers are predefined and fixed. In this paper, we aim at generating complete summaries with arbitrary desired length naturally for CNN seq2seq model. We use multi-layers CNN seq2seq model on both encoder and decoder. We set the length constraint at the first layer of de- coder to implement the length control of the sum- marization. Compared with other methods, our approach can effectively control the length of gen- erated summary in a natural manner. Meanwhile, it can generate summaries with length approxi- mate to the desired length without semantic losing in less time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Encoder-Decoder for Abstractive Summarization</head><p>Automatic document summarization generates short summaries for original documents. A sum- mary should cover the key topics of the origi- nal document(s). A good summary should be coherent, non-redundant and readable( <ref type="bibr" target="#b21">Yao et al., 2017b</ref>). The research in abstractive summariza- tion with encoder-decoder model <ref type="bibr" target="#b19">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b16">Rush et al., 2015;</ref><ref type="bibr" target="#b3">Chopra et al., 2016;</ref><ref type="bibr" target="#b13">Nallapati et al., 2016;</ref><ref type="bibr" target="#b17">See et al., 2017;</ref><ref type="bibr" target="#b15">Paulus et al., 2017;</ref>) has made some progress. Most of them use RNN with different atten- tion mechanisms ( <ref type="bibr" target="#b13">Nallapati et al., 2016;</ref><ref type="bibr" target="#b17">See et al., 2017;</ref><ref type="bibr" target="#b15">Paulus et al., 2017)</ref>. <ref type="bibr" target="#b16">Rush et al. (2015)</ref> used RNN with soft-attention, while <ref type="bibr" target="#b15">Paulus et al. (2017)</ref> used the RNN with intra-attention. Re- cently, research on CNN based summarization has gained momentum. <ref type="bibr" target="#b6">Gehring et al. (2017)</ref> pro- posed the CNN seq2seq model with multi-step at- tention, which was extended in . <ref type="bibr" target="#b2">Bai et al. (2018)</ref> showed that CNN is more pow- erful than RNN for sequence modeling. What's more, CNN enables much faster training and more stable gradients than RNN. Therefore we select CNN seq2seq model as our basic model and do not compare our model with RNN seq2seq model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a simple approach to modify exist- ing CNN seq2seq model with a summary length input and were able to train a model that pro- duces summaries of desired length that are flu- ent and coherent. This is a better solution than the current practice of summary truncation. Com- pared with the existing summarization methods, we show that our model has the ability to control the output length on its own using its internal state without losing semantic information or sacrificing the ROUGE score.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CNN seq2seq model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Modified Decoder</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The buckets distribution of the dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>8 :</head><label>8</label><figDesc>The proportion of summaries that are natural and complete with desired length 10 and 30 Trunc Exact 10 30 10 30 CNN 0.41 0.37 0.48 0.47 Fan 0.50 0.42 0.53 0.57 LC 0.62 0.59 0.88 0.86 set the desired length as 10 or 30. This leads to a relative lower similarity score shown in Figure 4(b) and Figure 4(c). As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Similarity of different length</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Pearson correlation with the true semantic re- latedness</head><label>2</label><figDesc></figDesc><table>TAC 2010 TAC 2011 
R-2 
0.6107 
0.6034 
Sim 
0.6653 
0.7165 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Desired Length: Gold Summary Lengths 

R-1 
R-2 
R-L 
Var 
Sim 

Free 

CNN 34.49 14.38 25.78 0.3465 0.9220 
Fan 
34.53 14.40 25.78 0.3446 0.9216 
LC 
35.45 14.50 26.02 0.0005 0.9272 

Trunc 

CNN 34.76 14.53 26.00 0.3045 0.9201 
Fan 
34.74 14.52 25.97 0.3031 0.9197 
LC 
35.44 14.48 26.02 0.0002 0.9268 

Exact 

CNN 35.39 14.43 26.07 0.0 
0.9249 
Fan 
35.37 14.42 26.03 0.0 
0.9246 
LC 
35.44 14.50 26.02 0.0 
0.9268 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 : Desired Length: 10, 30, 50, 70, 90 (a) Free version</head><label>5</label><figDesc></figDesc><table>Free 
10 
30 
50 
70 
90 

CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
R-1 34.49 34.28 19.03 34.49 34.28 32.26 34.49 34.60 34.71 34.49 34.65 33.83 34.49 30.56 32.17 
R-2 14.38 14.18 8.45 14.38 14.18 13.60 14.38 14.41 14.24 14.38 14.50 13.67 14.38 12.20 13.00 
R-L 25.78 25.60 16.47 25.78 25.60 24.64 25.78 25.79 25.62 25.78 25.82 24.67 25.78 22.08 23.28 

(b) Trunc version 

Trunc 
10 
30 
50 
70 
90 

CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
R-1 20.14 20.12 18.77 32.96 32.99 32.25 35.14 35.07 35.60 34.49 34.67 33.83 31.27 34.70 32.16 
R-2 
9.27 9.22 8.31 14.06 14.04 13.60 14.46 14.40 14.30 14.38 14.50 13.67 12.40 14.55 13.00 
R-L 17.35 17.34 16.28 25.11 25.06 24.62 26.09 26.05 25.90 25.78 25.82 24.67 22.69 25.86 23.29 

(c) Exact version 

Exact 
10 
30 
50 
70 
90 

CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
CNN Fan 
LC 
R-1 20.14 20.14 20.06 33.05 32.83 32.94 34.71 34.72 34.81 32.24 33.35 33.82 31.27 31.37 32.04 
R-2 
9.27 9.23 9.23 14.08 13.79 14.00 14.78 14.17 14.23 13.33 13.39 13.59 12.41 12.47 12.86 
R-L 17.36 17.36 17.30 25.15 24.87 25.02 25.65 25.63 25.60 24.31 24.35 24.56 22.69 22.76 23.14 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 : Similarity of different length</head><label>6</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Generated summaries of LC (Free) model 

10 
the younger jordan was rescued from his 
disabled boat . 

30 

louis jordan was rescued from his disabled . 
he boat had n't been heard from in 66 days 
in late january . he was rescued from his 
disabled boat . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 9 : p-value of significance test</head><label>9</label><figDesc></figDesc><table>Free 
Trunc 
Exact 
Exp.1 3.4e-32 2.12e-45 
0.01 
Exp.2 
0.0 
4.6e-39 
1.0e-4 

</table></figure>

			<note place="foot" n="1"> https://github.com/facebookresearch/ fairseq-py.</note>

			<note place="foot" n="2"> https://cs.nyu.edu/kcho/DMQA/ 3 All datasets, source code and generated summaries can be downloaded from http://202.120.38.146/ sumlen.</note>

			<note place="foot" n="4"> https://nlp.stanford.edu/projects/glove/. 5 https://tac.nist.gov/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Kenny Q. Zhu is the contact author and was sup-ported by NSFC grants 91646205 and 61373031. Thanks to the anonymous reviewers for their valu-able feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploring teacher forcing techniques for sequence-to-sequence abstractive headline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corbin</forename><surname>Albert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia, 6-11 Au</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05217</idno>
		<title level="m">Controllable abstractive summarization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
		<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Kocisk´ykocisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<imprint>
			<publisher>Quebec</publisher>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-11-27" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Rouge: a package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic evaluation of spoken summaries: the case of language assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastassia</forename><surname>Loukina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Zechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on innovative use of NLP for building educational applications</title>
		<meeting>the ninth workshop on innovative use of NLP for building educational applications</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-813" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recent advances in document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Recent advances in document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="336" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
