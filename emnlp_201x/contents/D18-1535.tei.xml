<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging Knowledge Gaps in Neural Entailment via Symbolic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
							<email>{dongyeok}@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging Knowledge Gaps in Neural Entailment via Symbolic Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4940" to="4945"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4940</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most textual entailment models focus on lexical gaps between the premise text and the hypothesis , but rarely on knowledge gaps. We focus on filling these knowledge gaps in the Science Entailment task, by leveraging an external structured knowledge base (KB) of science facts. Our new architecture combines standard neural entailment models with a knowledge lookup module. To facilitate this lookup, we propose a fact-level decomposition of the hypothesis , and verifying the resulting sub-facts against both the textual premise and the struc-tured KB. Our model, NSnet, learns to aggregate predictions from these heterogeneous data formats. On the SciTail dataset, NSnet outperforms a simpler combination of the two predictions by 3% and the base entailment model by 5%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual entailment, a key challenge in natural lan- guage understanding, is a sub-problem in many end tasks such as question answering and infor- mation extraction. In one of the earliest works on entailment, the PASCAL Recognizing Textual Entailment Challenge, <ref type="bibr" target="#b3">Dagan et al. (2005)</ref> define entailment as follows: text (or premise) P en- tails a hypothesis H if typically a human reading P would infer that H is most likely true. They note that this informal definition is "based on (and assumes) common human understanding of lan- guage as well as common background knowledge".</p><p>While current entailment systems have achieved impressive performance by focusing on the lan- guage understanding aspect, these systems, es- pecially recent neural models (e.g. <ref type="bibr" target="#b15">Parikh et al., 2016;</ref>, do not directly address the need for filling knowledge gaps by leveraging common background knowledge. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates an example of P and H from SciTail, a recent science entailment dataset (Khot P: The aorta is a large blood vessel that moves blood away from the heart to the rest of the body.</p><p>H (entailed): Aorta is the major artery carrying re- cently oxygenated blood away from the heart.   <ref type="bibr">et al., 2018)</ref>, that highlights the challenge of knowledge gaps-sub-facts of H that aren't stated in P but are universally true. In this exam- ple, an entailment system that is strong at filling lexical gaps may align large blood vessel with major artery to help conclude that P entails H. Such a system, however, would equally well-but incorrectly-conclude that P entails a hypothetical variant H' of H where artery is replaced with vein. A typical human, on the other hand, could bring to bear a piece of background knowledge, that aorta is a major artery (not a vein), to break the tie.</p><p>Motivated by this observation, we propose a new entailment model that combines the strengths of the latest neural entailment models with a structured knowledge base (KB) lookup module to bridge such knowledge gaps. To enable KB lookup, we use a fact-level decomposition of the hypothesis, and verify each resulting sub-fact against both the premise (using a standard entail- ment model) and against the KB (using a struc- tured scorer). The predictions from these two modules are combined using a multi-layer "ag- gregator" network. Our system, called NSnet, achieves 77.9% accuracy on SciTail, substantially improving over the baseline neural entailment model, and comparable to the structured entail- ment model proposed by .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural-Symbolic Learning</head><p>A general solution for combining neural and sym- bolic modules remains a challenging open prob- lem. As a step towards this, we present a system in the context of neural entailment that demon- strates a successful integration of the KB lookup model and simple overlap measures, opening up a path to achieve a similar integration in other mod- els and tasks. The overall system architecture of our neural-symbolic model for textual entailment is presented in <ref type="figure" target="#fig_2">Figure 2</ref>. We describe each layer of this architecture in more detail in the following sub-sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Inputs</head><p>We decompose the hypothesis and identify rele- vant KB facts in the bottom "inputs" layer ( <ref type="figure" target="#fig_2">Fig. 2</ref>).</p><p>Hypothesis Decomposition: To identify knowl- edge gaps, we must first identify the facts stated in the hypothesis h = (h 1 , h 2 ..). We use <ref type="bibr">ClausIE (Del et al., 2013</ref>) to break h into sub-facts. ClausIE tuples need not be verb-mediated and generate multiple tuples derived from conjunctions, lead- ing to higher recall than alternatives such as Open IE ( <ref type="bibr" target="#b0">Banko et al., 2007)</ref>. <ref type="bibr">1</ref> Knowledge Base (KB): To verify these facts, we use the largest available clean knowledge base for the science domain ( <ref type="bibr" target="#b4">Dalvi et al., 2017)</ref>, with 294K simple facts, as input to our system. The knowledge base contains subject-verb-object (SVO) tuples with short, one or two word argu- ments (e.g., hydrogen; is; element). Using these simple facts ensures that the KB is only used to fill the basic knowledge gaps and not directly prove the hypothesis irrespective of the premise.</p><p>KB Retrieval: The large number of tuples in the knowledge base makes it infeasible to evalu- ate each hypothesis sub-fact against the entire KB. Hence, we retrieve the top-100 relevant knowledge tuples, K ′ , for each sub-fact based on a simple Jac- card word overlap score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modules</head><p>We use a Neural Entailment model to compute the entailment score based on the premise, as well as two symbolic models, Symbolic Matcher and Symbolic Lookup, to compute entailment scores based on the premise and the KB respectively (middle layer in <ref type="figure" target="#fig_2">Fig. 2</ref>).</p><p>Neural Entailment We use a simple neural en- tailment model, Decomposable Attention ( <ref type="bibr" target="#b15">Parikh et al., 2016)</ref>, one of the state-of-the-art models on the SNLI entailment dataset <ref type="bibr" target="#b1">(Bowman et al., 2015)</ref>. However, our architecture can just as easily use any other neural entailment model. We initial- ize the model parameters by training it on the Sci- ence Entailment dataset. Given the sub-facts from the hypothesis, we use this model to compute an entailment score n(h i , p) from the premise to each sub-fact h i .</p><p>Symbolic Matcher In our initial experiments, we noticed that the neural entailment models would often either get distracted by similar words in the distributional space (false positives) or com- pletely miss an exact mention of h i in a long premise (false negatives). To mitigate these errors, we define a Symbolic Matcher model that com- pares exact words in h i and p, via a simple asym- metric bag-of-words overlap score:</p><formula xml:id="formula_0">m(h i , p) = |h i ∩ p| |p|</formula><p>One could instead use more complex symbolic alignment methods such as integer linear program- ming ( <ref type="bibr" target="#b11">Khashabi et al., 2016;</ref><ref type="bibr" target="#b12">Khot et al., 2017)</ref>.</p><p>Symbolic Lookup This module verifies the presence of the hypothesis sub-fact h i in the re- trieved KB tuples K ′ , by comparing the sub-fact to each tuple and taking the maximum score. Each field in the KB tuple kb j is scored against the cor- responding field in h i (e.g., subject to subject) and averaged across the fields. To compare a field, we use a simple word-overlap based Jaccard similar- ity score, Sim(a, b) = |a∩b| |a∪b| . The lookup match score for the entire sub-fact and kb-fact is:</p><formula xml:id="formula_1">Sim f (h i , kb j ) =         ∑ k Sim(h i [k], kb j [k])         /3</formula><p>and the final lookup module score for h i is:</p><formula xml:id="formula_2">l(h i ) = max kb j ∈K ′ Sim f (h i , kb j )</formula><p>Note that the Symbolic Lookup module assesses whether a sub-fact of H is universally true. Neural models, via embeddings, are quite strong at medi- ating between P and H. The goal of the KB lookup module is to complement this strength, by verify- ing universally true sub-facts of H that may not be stated in P (e.g. "aorta is a major artery" in our motivating example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aggregator Network</head><p>For each sub-fact h i , we now have three scores: n(h i , p) from the neural model, m(h i , p) from the symbolic matcher, and l(h i ) from the symbolic lookup model. The task of the Aggregator net- work is to combine these to produce a single en- tailment score. However, we found that using only the final predictions from the three modules was not effective. Inspired by recent work on skip/highway connections ( <ref type="bibr" target="#b8">He et al., 2016;</ref><ref type="bibr" target="#b17">Srivastava et al., 2015)</ref>, we supplement these scores with intermediate, higher-dimensional representa- tions from two of the modules. We define a hybrid layer that takes as input a simple concatenation of these representation vec- tors from the different modules:</p><formula xml:id="formula_3">in(h i , p) =[h enc i ; l(h i ); m(h i , p); n(h i , p); emb i ; n v (h i , p)]</formula><p>The hybrid layer is a single layer MLP for each sub-fact h i that outputs a sub-representation out i = MLP(in(h i , p)). A compositional layer then uses a two-layer MLP over a concatenation of the hybrid layer outputs from different sub-facts, {h 1 , . . . , h I }, to produce the final label,</p><formula xml:id="formula_4">label = MLP([out 1 ; out 2 ; · · · out I ])</formula><p>Finally, we use the cross-entropy loss to train the Aggregator network jointly with representations in the neural entailment and symbolic lookup mod- els, in an end-to-end fashion. We refer to this en- tire architecture as the NSnet network. To assess the effectiveness of the aggregator net- work, we also use a simpler baseline model, En- semble, that works as follows. For each sub-fact h i , it combines the predictions from each model using a probabilistic-OR function, assuming the model score P m as a probability of entailment. This function computes the probability of at least one model predicting that h i is entailed, i.e. P(</p><formula xml:id="formula_5">h i ) = 1 − Π m (1 − P m ) where m ∈ n(h i , p), m(h i , p), l(h i ).</formula><p>We average the probabilities from all the facts to get the final entailment probability. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We use the SciTail dataset 3 (  for our experiments, which contains 27K entailment examples with a 87.3%/4.8%/7.8% train/dev/test split. The premise and hypothesis in each example are natural sentences authored independently as well as independent of the entailment task, which makes the dataset particularly challenging. We fo- cused mainly on the SciTail dataset, since other crowd-sourced datasets, large enough for training, contained limited linguistic variation <ref type="bibr" target="#b7">(Gururangan et al., 2018</ref>) leading to limited gains achievable via external knowledge.</p><p>For background knowledge, we use version v4 of the aforementioned Aristo TupleKB 4 ( <ref type="bibr" target="#b4">Dalvi et al., 2017)</ref>, containing 283K basic science facts. We compare our proposed models to Decomposed Graph Entailment Model (DGEM) (  and Decomposable Attention Model (De- compAttn) (Parikh et al., 2016). <ref type="table" target="#tab_1">Table 1</ref> summarizes the validation and test accu- racies of various models on the SciTail dataset. The DecompAttn model achieves 74.3% on the test set but drops by 1.6% when the hypotheses are decomposed. The Ensemble approach uses the same hypothesis decomposition and is able to re- cover 2.1% points by using the KB. The end-to- end NSnet network is able to further improve the score by 3.1% and is statistically significantly (at p-value 0.05) better than the baseline neural entail- ment model. The model is marginally better than DGEM, a graph-based entailment model proposed by the authors of the SciTail dataset We show sig- nificant gains over our base entailment model by using an external knowledge base, which are com- parable to the gains achieved by DGEM through the use of hypothesis structure. These are orthog- onal approaches and one could replace the base DecompAttn model with DGEM or more recent models ( <ref type="bibr" target="#b18">Tay et al., 2017;</ref><ref type="bibr" target="#b19">Yin et al., 2018</ref>). In <ref type="table" target="#tab_2">Table 2</ref>, we evaluate the impact of the Sym- bolic Matcher and Symbolic Lookup module on the best reported model. As we see, removing the symbolic matcher, despite its simplicity, results in a 3.2% drop. Also, the KB lookup model is able to fill some knowledge gaps, contributing 2.1% to the final score. Together, these symbolic matching models contribute 4% to the overall score. <ref type="figure">Figure 3</ref> shows few randomly selected examples in test set. The first two examples show cases when the symbolic models help to change the neu- ral alignment's prediction (F) to correct prediction (T) by our proposed Ensemble or NSnet models. The third question shows a case where the NSnet architecture learns a better combination of the neu- ral and symbolic methods to correctly identify the entailment relation while Ensemble fails to do so. Few randomly selected examples in the test set between symbolic only, neural only, Ensemble and NSnet inference. The symbolic only model shows its the most similar knowledge from knowledge base inside parenthesis. The first two example shows when knowledge helps fill the gap where neural model can't. The third example shows when NSnet predicts correctly while Ensemble fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Qualitative Analysis</head><p>Premise: plant cells possess a cell wall , animals never .</p><p>Hypothesis: a cell wall is found in a plant cell but not in an animal cell . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Compared to neural only ( <ref type="bibr" target="#b1">Bowman et al., 2015;</ref><ref type="bibr" target="#b15">Parikh et al., 2016</ref>) or symbolic only ( <ref type="bibr" target="#b12">Khot et al., 2017;</ref><ref type="bibr" target="#b11">Khashabi et al., 2016</ref>) systems, our model takes advantage of both systems, often called neural-symbolic learning ( <ref type="bibr" target="#b6">Garcez et al., 2015)</ref>. Various neural-symbolic models have been pro- posed for question answering ( <ref type="bibr" target="#b14">Liang et al., 2016)</ref> and causal explanations ( <ref type="bibr" target="#b9">Kang et al., 2017)</ref>. We focus on end-to-end training of these models specifically for textual entailment.</p><p>Contemporaneous to this work, <ref type="bibr" target="#b2">Chen et al. (2018)</ref> have incorporated knowledge-bases within the attention and composition functions of a neural entailment model, while <ref type="bibr" target="#b10">Kang et al. (2018)</ref> gen- erate adversarial examples using symbolic knowl- edge (e.g., WordNet) to train a robust entailment model. We focused on integrating knowledge- bases via a separate symbolic model to fill the knowledge gaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a new entailment model that attempts to bridge knowledge gaps in textual entailment by incorporating structured knowledge base lookup into standard neural entailment models. Our ar- chitecture, NSnet, can be trained end-to-end, and achieves a 5% improvement on SciTail over the baseline neural model used here. The methodol- ogy can be easily applied to more complex entail- ment models (e.g., DGEM) as the base neural en- tailment model. Accurately identifying the sub- facts from a hypothesis is a challenging task in it- self, especially when dealing with negation. Im- provements to the fact decomposition should fur- ther help improve the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>H</head><label></label><figDesc>' (not entailed): Aorta is the major vein carrying recently oxygenated blood away from the heart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Knowledge gap: Aorta is a major artery (not a vein). Large blood vessel soft-aligns with major artery but also with major vein.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural-symbolic learning in NSnet. The bottom layer has QA and their supporting text in SciTail, and the knowledge base (KB). The middle layer has three modules: Neural Entailment (blue) and Symbolic Matcher and Symbolic Lookup (red). The top layer takes the outputs (black and yellow) and intermediate representation from the middle modules, and hierarchically trains with the final labels. All modules and aggregator are jointly trained in an end-to-end fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>From the Symbolic Lookup model, we use the representation of each sub-fact h enc i = Enc(h i ) obtained by averaging word embeddings (Pen- nington et al., 2014) and individual similarity scores over the top-100 KB tuples emb i = [. . . , Sim f (h i , kb j ), . . .]. From the neural entail- ment model, we use the intermediate representa- tion of both the sub-fact of hypothesis and premise text from the final layer (before the softmax com- putation), n v (h i , p) = [v 1 ; v 2 ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the pupil is a hole in the iris that allows light into the eye . Hypothesis: the pupil of the eye allows light to enter . Sub-fact of hypothesis neural only symbolic only Ensemble NSnet the pupil of the eye allows light to enter F(0.43) T(0.12), (light enter eye) T(0.50) - Prediction (true label: T (entail)) F T T T Premise: binary fission in various single-celled organisms ( left ) . Hypothesis: binary fission is a form of cell division in prokaryotic organisms that produces identical offspring .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Entailment accuracies on the SciTail dataset. NSnet substantially improves upon its base model and marginally outperforms DGEM.</figDesc><table>Entailment Model 
Valid. Test 

Majority classifier 
63.3 60.3 
DecompAttn (Base model) 
73.1 74.3 
DecompAttn + HypDecomp 
71.8 72.7 
DGEM 
79.6 77.3 
Ensemble (this work) 
75.2 74.8 
NSnet (this work) 
77.4 77.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Ablation: Both Symbolic Lookup 
and Symbolic Matcher have significant impact on 
NSnet performance. 

Valid. 
Test 

NSnet 
77.39 
77.94 
-Symbolic Matcher 76.46 74.73 (-3.21%) 
-Symbolic Lookup 75.95 75.80 (-2.14%) 
-Both 
75.10 73.98 (-3.96%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> While prior work on question answering in the science domain has successfully used Open IE to extract facts from sentences (Khot et al., 2017), one of the key reasons for errors was the lossy nature of Open IE.</note>

			<note place="foot" n="2"> While more intuitive, performing an AND aggregation resulted in worse performance (cf. Appendix ?? for details). 3 http://data.allenai.org/scitail 4 http://data.allenai.org/tuple-kb</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers as well as Luke Zettlemoyer, Matt Gardner, Oyvind Tafjord, Julia Hockenmaier and Eduard Hovy for their comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language inference with external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>MLCW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain-targeted, high precision knowledge extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ClausIE: clause-based open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corro Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural-symbolic learning and reasoning: contributions and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davila Garcez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Földiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiuwe</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kühnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel L</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches</title>
		<meeting>the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches<address><addrLine>Stanford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Detecting and explaining causes from text for a time series event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adventure: Adversarial training for textual entailment with knowledge-guided examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Question answering via integer programming over semistructured knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Answering complex questions using open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on freebase with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Kenneth D Forbus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A compare-propagate architecture with alignment factorization for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00102</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-task oriented textual entailment via deep exploring inter-sentence interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
