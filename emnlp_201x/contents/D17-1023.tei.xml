<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<email>tliu@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Chinese Information Processing</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">UltraPower-BNU Joint Laboratory for Artificial Intelligence</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
							<email>libofang@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="244" to="253"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper , we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix, and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram co-occurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burdens brought by ngrams.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, deep learning approaches have achieved state-of-the-art results on a range of NLP tasks. One of the most fundamental work in this field is word embedding, where low-dimensional word representations are learned from unlabeled corpo- ra through neural models. The trained word em- beddings reflect semantic and syntactic informa- tion of words. They are not only useful in reveal- ing lexical semantics, but also used as inputs of various downstream tasks for better performance <ref type="bibr" target="#b10">(Kim, 2014;</ref><ref type="bibr" target="#b2">Collobert et al., 2011;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>).</p><p>Most of the word embedding models are trained upon &lt;word, context&gt; pairs in the local win- dow. Among them, word2vec gains its popu- larity by its amazing effectiveness and efficien- cy ( <ref type="bibr">Mikolov et al., 2013b,a)</ref>. It achieves state- of-the-art results on a range of linguistic tasks with only a fraction of time compared with pre- vious techniques. A challenger of word2vec is GloVe ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>). Instead of train- ing on &lt;word, context&gt; pairs, GloVe directly uti- lizes word co-occurrence matrix. They claim that the change brings the improvement over word2vec on both accuracy and speed. <ref type="bibr" target="#b13">Levy and Goldberg (2014b)</ref> further reveal that the attractive properties observed in word embeddings are not restricted to neural models such as word2vec and GloVe. They use traditional count-based method (PPMI matrix with hyper-parameter tuning) to represent word- s, and achieve comparable results with the above neural embedding models.</p><p>The above models limit their information source to word co-occurrence statistics ( <ref type="bibr" target="#b15">Levy et al., 2015)</ref>. To learn improved word representation- s, we extend the information source from co- occurrence of 'word-word' type to co-occurrence of 'ngram-ngram' type. The idea of using ngrams is well supported by language modeling, one of the oldest problems studied in statistical NLP. In lan- guage models, co-occurrence of words and ngrams is used to predict the next word ( <ref type="bibr" target="#b11">Kneser and Ney, 1995;</ref><ref type="bibr" target="#b9">Katz, 1987)</ref>. Actually, the idea of word em- bedding models roots in language models. They are closely related but are used for different pur- poses. Word embedding models aim at learning useful word representations instead of word pre- diction. Since ngram is a vital part in language modeling, we are inspired to integrate ngram sta- tistical information into the recent word represen- tation methods for better performance.</p><p>The idea of using ngrams is intuitive. However, there is still rare work using ngrams in recent rep- resentation methods. In this paper, we introduce ngrams into SGNS, GloVe, PPMI, and its SVD factorization. To evaluate the ngram-based mod- els, comprehensive experiments are conducted on word analogy and similarity tasks. Experimental results demonstrate that the improved word repre- sentations are learned from ngram co-occurrence statistics. Besides that, we qualitatively evaluate the trained ngram representations. We show that they are able to reflect ngrams' meanings and syn- tactic patterns (e.g. 'be + past participle' pattern). The high-quality ngram representations are useful in many ways. For example, ngrams in negative form (e.g. 'not interesting') can be used for find- ing antonyms (e.g. 'boring').</p><p>Finally, a novel method is proposed to build n- gram co-occurrence matrix. Our method reduces the disk I/O as much as possible, largely alle- viating the costs brought by ngrams. We uni- fy different representation methods in a pipeline. The source code is organized as ngram2vec toolk- it and released at https://github.com/ zhezhaoa/ngram2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>SGNS, GloVe, PPMI, and its SVD factorization are used as baselines. The information used by them does not go beyond word co-occurrence s- tatistics. However, their approaches to using the information are different. We review these meth- ods in the following 3 sections. In section 2.4, we revisit the use of ngrams in the deep learning con- text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SGNS</head><p>Skip-gram with negative sampling (SGNS) is a model in word2vec toolkit ( <ref type="bibr">Mikolov et al., 2013b,a)</ref>. Its training procedure follows the ma- jority of neural embedding models ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>: (1) Scan the corpus and use &lt;word, context&gt; pairs in the local window as training samples. (2) Train the models to make words use- ful for predicting contexts (or in reverse). The de- tails of SGNS is discussed in Section 3.1. Com- pared to previous neural embedding models, S- GNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess attractive proper- ties. They are able to reflect relations between two words accurately, which is evaluated by a fancy task called word analogy.</p><p>Due to the above advantages, many models are proposed on the basis of SGNS. For example, <ref type="bibr" target="#b6">Faruqui et al. (2015)</ref> introduce knowledge in lex- ical resources into the models in word2vec.  extend the contexts from the local window to the entire documents. <ref type="bibr" target="#b17">Li et al. (2015)</ref> use supervised information to guide the training. Dependency parse-tree is used for defining contex- t in ( <ref type="bibr" target="#b12">Levy and Goldberg, 2014a)</ref>. LSTM is used for modeling context in ( <ref type="bibr" target="#b20">Melamud et al., 2016</ref>) Sub-word information is considered in ( <ref type="bibr" target="#b27">Sun et al., 2016;</ref><ref type="bibr" target="#b26">Soricut and Och, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">GloVe</head><p>Different from typical neural embedding model- s which are trained on &lt;word, context&gt; pairs, GloVe learns word representation on the basis of co-occurrence matrix ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>). GloVe breaks traditional 'words predict contexts' paradigm. Its objective is to reconstruct non-zero values in the matrix. The direct use of matrix is reported to bring improved results and higher speed. However, there is still dispute about the advantages of GloVe over word2vec ( <ref type="bibr" target="#b15">Levy et al., 2015;</ref><ref type="bibr" target="#b24">Schnabel et al., 2015</ref>). GloVe and other em- bedding models are essentially based on word co- occurrence statistics of the corpus. The &lt;word, context&gt; pairs and co-occurrence matrix can be converted to each other. <ref type="bibr" target="#b28">Suzuki and Nagata (2015)</ref> try to unify GloVe and SGNS in one framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PPMI &amp; SVD</head><p>When we are satisfied with the huge promotions achieved by embedding models on linguistic tasks, a natural question is raised: where the superior- ities come from. One conjecture is that it's due to the neural networks. However, <ref type="bibr" target="#b14">Levy and Goldberg (2014c)</ref> reveal that SGNS is just factoring P- MI matrix implicitly. Also, <ref type="bibr" target="#b13">Levy and Goldberg (2014b)</ref> show that positive PMI (PPMI) matrix still rivals the newly proposed embedding mod- els on a range of linguistic tasks. Properties like word analogy are not restricted to neural model- s. To obtain dense word representations from PP- MI matrix, we factorize PPMI matrix with SVD, a classic dimensionality reduction method for learn- ing low-dimensional vectors from sparse matrix <ref type="bibr" target="#b4">(Deerwester et al., 1990</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Ngram in Deep Learning</head><p>In the deep learning literature, ngram has shown to be useful in generating text representations. Re- cently, convolutional neural networks (CNNs) are reported to perform well on a range of NLP tasks <ref type="bibr" target="#b1">(Blunsom et al., 2014;</ref><ref type="bibr" target="#b7">Hu et al., 2014;</ref><ref type="bibr" target="#b25">Severyn and Moschitti, 2015)</ref>. CNNs are essentially using n- gram information to represent texts. They use 1-D convolutional layers to extract ngram features and the distinct features are selected by max-pooling layers. In ( , ngram embedding is in- troduced into Paragraph Vector model, where tex- t embedding is trained to be useful to predict n- grams in the text. In the word embedding liter- ature, a related work is done by <ref type="bibr" target="#b19">Melamud et al. (2014)</ref>, where word embedding models are used as baselines. They propose to use ngram language models to model the context, showing the effec- tiveness of ngrams on similarity tasks. Another work that is related to ngram is from <ref type="bibr" target="#b22">Mikolov et al. (2013b)</ref>, where phrases are embedded into vec- tors. It should be noted that phrases are different from ngrams. Phrases have clear semantics and the number of phrases is much less than the num- ber of ngrams. Using phrase embedding has little impact on word embedding's quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we introduce ngrams into SGNS, GloVe, PPMI, and SVD. Section 3.1 reviews the SGNS. Section 3.2 and 3.3 show the details of in- troducing ngrams into SGNS. In section 3.4, we show the way of using ngrams in GloVe, PPMI, and SVD, and propose a novel way of building n- gram co-occurrence matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Predicts Word: the Revisit of SGNS</head><p>First we establish some notations. The raw input is a corpus T = {w 1 ,w 2 ,......,w |T | }. Let W and C denote word and context vocabularies. θ is the pa- rameters to be optimized. SGNS's parameters in- volve two parts: word embedding matrix and con- text embedding matrix. With embedding w ∈ R d , the total number of parameters is (|W|+|C|)*d.</p><p>The SGNS's objective is to maximize the condi- tional probabilities of contexts given center words:</p><formula xml:id="formula_0">|T | t=1 c∈C(wt) log p(c|w t ; θ) (1)</formula><p>where C(wt) = {wi, t − win ≤ i ≤ t + win and i = t} and win denotes the window size. As illustrat- ed in <ref type="figure" target="#fig_0">figure 1</ref>, the center word 'written' predict- s its surrounding words 'Potter', 'is', 'by', and 'J.K.'. In this paper, negative sampling ( <ref type="bibr" target="#b22">Mikolov et al., 2013b</ref>) is used to approximate the condition- al probability:</p><formula xml:id="formula_1">p(c|w) = σ( w T c) k j=1 E c j ∼Pn(C) σ(− w T c j ) (2)</formula><p>where σ is sigmoid function. k samples (from c 1 to c k ) are drawn from context distribution raised to the power of n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Predicts Ngram</head><p>In this section, we introduce ngrams into context vocabulary. We treat each ngram as a normal word and give it a unique embedding. During the train- ing, the center word should not only predict its sur- rounding words, but also predict its surrounding n- grams. As shown in <ref type="figure" target="#fig_1">figure 2</ref>, center word 'written' predicts the bigrams in the local window such as 'by J.K.'. The objective of 'word predicts ngram' is similar with the original SGNS. The only differ- ence is the definition of the C(w). In ngram case, C(w) is formally defined as follows:</p><formula xml:id="formula_2">C(wt) = N n=1 {wi:i+n|wi:i+n is not wt AN D t − win ≤ i ≤ t + win − n + 1} (3)</formula><p>where w i:i+n denotes the ngram w i w i+1 ...w i+n−1 and N is the order of context ngram. Two points need to be noticed from the above definition. The first is how to determine the distance between cen- ter word and context ngram. In this paper, we use the distance between the word and the ngram's far-end word. As show in <ref type="figure" target="#fig_1">figure 2</ref>, the distance between 'written' and 'Harry Potter' is 3. As a result, 'Harry Potter' is not included in the cen- ter word's context. This distance definition en- sures that the ngram models don't use the infor- mation beyond the pre-specified window, which guarantees fair comparisons with baselines. An- other point is whether the overlap of word and n- gram is allowed or not. In the overlap situation, ngrams are used as context even they contain the center word. As the example in figure 2 shows, ngram 'is written' and 'written by' are predicted by the center word 'written'. In the non-overlap case, these ngrams are excluded. The properties of word embeddings are different when overlap is allowed or not, which will be discussed in experi- ments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ngram Predicts Ngram</head><p>We further extend the model to introduce ngram- s into center word vocabulary. During the train- ing, center ngrams (including words) predict their surrounding ngrams. As shown in <ref type="figure" target="#fig_2">figure 3</ref>, center bigram 'is written' predicts its surrounding word- s and bigrams. The objective of 'ngram predicts ngram' is as follows:</p><formula xml:id="formula_3">|T | t=1 Nw nw=1 c∈C(w t:t+nw ) log p(c|w t:t+nw ; θ)<label>(4)</label></formula><p>where N w is the order of center ngram. The defi- nition of C(w t:t+nw ) is as follows:</p><formula xml:id="formula_4">Nc nc=1 {wi:i+n c |wi:i+n c is not wt:t+n w AN D t − win + nw − 1 ≤ i ≤ t + win − nc + 1} (5)</formula><p>where N c is the order of context ngram. To this end, the word embeddings are not only affected by the ngrams in the context, but also indirect- ly affected by co-occurrence statistics of 'ngram- ngram' type in the corpus. SGNS is proven to be equivalent with factor- izing pointwise mutual information (PMI) ma- trix ( <ref type="bibr" target="#b14">Levy and Goldberg, 2014c)</ref>. Following their work, we can easily show that models in section 3.2 and 3.3 are implicitly factoring PMI matrix of 'word-ngram' and 'ngram-ngram' type. In the next section, we will discuss the content of intro- ducing ngrams into positive PMI (PPMI) matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Co-occurrence Matrix Construction</head><p>Introducing ngrams into GloVe, PPMI, and SVD is straightforward: the only change is to replace word co-occurrence matrices with ngram ones. In the above three sections, we have discussed the way of taking out &lt;word(ngram), word(ngram)&gt; pairs from a corpus. Afterwards, we build the co- occurrence matrix upon these pairs. The rest steps are identical with the original baseline models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Win</head><p>Type However, building the co-occurrence matrix is not an easy task as it apparently looks like. The introduction of ngrams brings huge burdens on the hardware. The matrix construction cost is closely related to the number of pairs (#Pairs). <ref type="table">Table 1</ref> shows the statistics of pairs extracted from corpus wiki2010 <ref type="bibr">1</ref> . We can observe that #Pairs is huge when ngrams are considered.</p><p>To speed up the process of building ngram co-occurrence matrix, we take advantages of 'mixture' strategy ( <ref type="bibr" target="#b23">Pennington et al., 2014</ref>) and 'stripes' strategy ( <ref type="bibr" target="#b5">Dyer et al., 2008;</ref><ref type="bibr" target="#b18">Lin, 2008)</ref>. The two strategies optimize the process in differ- ent aspects. Computational cost is reduced signif- icantly when they are used together.</p><p>When words (or ngrams) are sorted in descend- ing order by frequency, the co-occurrence matrix's top-left corner is dense while the rest part is s- parse. Based on this observation, the 'mixture' of two data structures are used for storing ma- trix. Elements in the top-left corner are stored in a 2D array, which stays in memory. <ref type="bibr" target="#b18">Lin, 2008)</ref>, we can build the matrix of 'bi bi' type even in a laptop. It only re- quires 12GB to store temporary files (win=2, sub- sampling=0, memory size=4GB), which is much smaller than the implementations in ( <ref type="bibr" target="#b23">Pennington et al., 2014;</ref><ref type="bibr" target="#b15">Levy et al., 2015)</ref> . More detailed analysis about these strategies can be found in the ngram2vec toolkit.</p><note type="other">The rest of the elements are stored in the form of &lt;ngram, H&gt;, where H&lt;context, count&gt; is an associative array recording the number of times the ngram and context co-occurs ('stripes' strategy). Compared with storing &lt;ngram, context&gt; pairs explicitly, the 'stripes' strategy provides more opportunities to aggregate pairs outside of the top-left corner. Algorithm 1 shows the way of using the 'mix- ture' and 'stripes' strategies together. In the first stage, pairs are stored in different data structures according to topLeft function. Intermediate results are written to temporary files when memory is full. In the second stage, we merge these sorted tempo- rary files to generate co-occurrence matrix. The getSmallest function takes out the pair &lt;ngram, H&gt; with the smallest key from temporary files. In practice, algorithm 1 is efficient. Instead of using computer clusters (</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The tasks used in this paper is the same with the work of <ref type="bibr" target="#b15">Levy et al. (2015)</ref>, including six similarity and two analogy datasets. In similarity task, a s- calar (e.g. a score from 0 to 10) is used to measure the relation between the two words. For example, in a similarity dataset, the 'train, car' pair is giv- en the score of 6.31. A problem of similarity task is that scalar only reflects the strength of the rela- tion, while the type of relation is totally ignored ( <ref type="bibr" target="#b24">Schnabel et al., 2015)</ref>.</p><p>Due to the deficiency of similarity task, anal- ogy task is widely used as benchmark recently for evaluation of word embedding models. To answer analogy questions, relations between the two words are reflected by a vector, which is usually obtained by the difference between word Algorithm 1: An algorithm for building n- gram co-occurrence matrix</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input : Pairs P , Sorted vocabulary V Output: Sorted and aggregated pairs 1 The 2D array A[ ][ ];</head><p>2 The dictionary D &lt; ngram, H &gt;; <ref type="bibr">3</ref> The temporary files array tf s[ ]; f id=1; 4 for pair p &lt; n, c &gt; in P do embeddings. Different from a scalar, the vec- tor provides more accurate descriptions of rela- tions. For example, capital-country relation is encoded in vec(Athens)-vec(Greece), vec(Tokyo)- vec(Japan) and so on. More concretely, the ques- tions in the analogy task are in the form of 'a is to b as c is to d'. 'd' is an unknown word in the test phase. To correctly answer the questions, the models should embed the two relations, vec(a)- vec(b) and vec(c)-vec(d), into similar positions in the space. Following the work of Levy and Gold- berg (2014b), both additive (add) and multiplica- tive (mul) functions are used for finding word 'd'. The latter one is more suitable for sparse represen- tation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pipeline and Hyper-parameter Setting</head><p>We implement SGNS, GloVe, PPMI, and SVD in a pipeline, allowing the reuse of code and intermedi- ate results. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the overview of the pipeline. Firstly, &lt;word(ngram), word(ngram)&gt; pairs are extracted from the corpus as the input of SGNS. Afterwards, we build the co-occurrence matrix upon the pairs. GloVe and PPMI learn word representations on the basis of co-occurrence    matrix. SVD factorizes the PPMI matrix to obtain low-dimensional representation. Most hyper-parameters come from 'corpus to pairs' part and four representation models. 'corpus to pairs' part determines the source of information for the subsequent models and its hyper-parameter setting is as follows: low-frequency words (n- grams) are removed with a threshold of 10. High- frequency words (ngrams) are removed with sub- sampling at the degree of 1e-5 2 . Window size is set to 2 and 5. Clean strategy ( <ref type="bibr" target="#b15">Levy et al., 2015</ref>) is used to ensure no information beyond pre-specified window is included. Overlap setting is used in default. For hyper-parameters of four representation models, we use the embeddings of 300 dimensions in dense representations. SGNS is trained by 3 iterations. The rest strictly follow the baseline models <ref type="bibr">3</ref> . We consider unigrams (words), bigrams, and trigrams in this work. The imple- mentation of higher-order models and their results will be released with ngram2vec toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ngrams on SGNS</head><p>SGNS is a popular word embedding model. Even compared with its challengers such as GloVe, S- GNS is reported to have more robust performance with faster training speed ( <ref type="bibr" target="#b15">Levy et al., 2015</ref>). Ta- ble 2 lists the results on analogy datasets. We can observe that the introduction of bigrams pro- vides significant improvements at different hyper- parameter settings. The SGNS of 'bi bi' type pro- vides the highest results. It is very effective on capturing semantic information (Google seman- tic). Around 10 percent improvements are wit-    <ref type="table">Table 6</ref>: Performance of (ngram) SVD on analogy and similarity datasets.</p><note type="other">Win Type Google MSR Sim. Rel. Bruni Radinsky Luong Hill Add Mul Add Mul 2 uni</note><p>nessed on semantic questions compared with u- ni uni baseline. For syntactic questions (Google syntactic and MSR datasets), around 5 percent im- provements are obtained on average. The effect of overlap is large on analogy datasets. Semantic questions prefer the overlap setting. Around 10 and 3 percent improvements are witnessed compared with non-overlap setting at the window size of 2 and 5. While in syntac- tic case, non-overlap setting performs better by a margin of around 5 percent.</p><p>The introduction of trigrams deteriorates the models' performance on analogy datasets (espe- cially at the window size of 2). It is probably be- cause that trigram is sparse on wiki2010, a rela- tively small corpus with 1 billion tokens. We con- jecture that high order ngrams are more suitable for large corpora and will report the results in our future work. It should be noticed that trigram is not included in vocabulary in non-overlap case at the window size of 2. The shortest distance be- tween a word and a trigram is 3, which exceeds the window size. <ref type="table" target="#tab_3">Table 3</ref> illustrates the SGNS's performance on similarity task. The conclusion is similar with the case in analogy datasets. The use of bigrams is effective while the introduction of trigrams deteri- orates the performance in most cases. In general, the bigrams bring significant improvements over SGNS on a range of linguistic tasks. It is gener- ally known that ngram is a vital part in tradition- al language modeling problem. Results in table 2 and 3 confirm the effectiveness of ngrams again on SGNS, a more advanced word embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ngrams on PPMI, GloVe, SVD</head><p>In this section, we only report the results of mod- els of 'uni uni' and 'uni bi' types. Using high- er order co-occurrence statistics brings immense costs (especially at the window size of 5). <ref type="bibr" target="#b13">Levy and Goldberg (2014b)</ref> demonstrate that traditional count-based models can still achieve competitive results on many linguistic tasks, challenging the dominance of neural embedding models. <ref type="table" target="#tab_5">Table 4</ref> lists the results of PPMI matrix on analogy and similarity datasets. PPMI prefers Multiplicative (Mul) evalution. To this end, we focus on analyz- ing the results on Mul columns. When bigrams are used, significant improvements are witnessed on analogy task. On Google dataset, bigrams bring over 10 percent increase on the total accuracies. At the window size of 2, the accuracy in semantic questions even reaches 0.854, which is the state- of-the-art result to the best of our knowledge. On MSR dataset, around 20 percent improvements are achieved. The use of bigrams does not always bring improvements on similarity datasets. PPMI matrix of 'uni bi' type improves the results on 5 datasets at the window size of 2. At the window size of 5, using bigrams only improves the results on 2 datasets. <ref type="table" target="#tab_6">Table 5</ref> and 6 list GloVe and SVD's results. For GloVe, consistent (but minor) improvements are achieved on analogy task with the introduction of bigrams. On similarity datasets, improvements are witnessed on most cases. For SVD, bigram- s sometimes lead to worse results in both anal- ogy and similarity tasks. In general, significan- t improvements are not witnessed on GloVe and SVD. Our preliminary conjecture is that the de- fault hyper-parameter setting should be blamed. We strictly follow the hyper-parameters used in baseline models, making no adjustments to cater to the introduction of ngrams. Besides that, some common techniques such as dynamic window, de- creasing weighting function, dirty sub-sampling are discarded. The relationships between ngrams and various hyper-parameters require further ex- ploration. Though trivial, it may lead to much bet- ter results and give researchers better understand- ing of different representation methods. That will be the focus of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Evaluations of Ngram Embedding</head><p>In this section, we analyze the properties of n- gram embeddings trained by SGNS of 'bi bi' type. Ideally, the trained ngram embeddings should re- flect ngrams' semantic meanings. For example, vec(wasn't able) should be close to vec(unable). vec(is written) should be close to vec(write) and vec(book). Also, the trained ngram embeddings should preserve ngrams' syntactic patterns. For example, 'was written' is in the form of 'be + past participle' and the nearest neighbors should pos- sess similar patterns, such as 'is written' and 'was transcribed'. <ref type="table">Table 7</ref> lists the target ngrams and their top n- earest neighbours. We divide the target ngram- s into six groups according to their patterns. We can observe that the returned words and ngram- s are very intuitive. As might be expected, syn- onyms of the target ngrams are returned in top po- sitions (e.g. 'give off' and 'emit'; 'heavy rain' and 'downpours'). From the results of the first group, it can be observed that bigram in negative form 'not X' is useful for finding the antonym of word 'X'. Besides that, the trained ngram embeddings also preserve some common sense. For example, the returned result of 'highest mountain' is a list of mountain names (with a few exceptions such as 'unclimbed'). In terms of syntactic patterns, we can observe that in most cases, the returned ngrams are in the similar form with target ngram- s. In general, the trained embeddings basically re- flect semantic meanings and syntactic patterns of ngrams.</p><p>With high-quality ngram embeddings, we have the opportunity to do more interesting things in our future work. For example, we will construct a antonym dataset to evaluate ngram embeddings systematically. Besides that, we will find more scenarios for using ngram embeddings. In our view, ngram embeddings have potential to be used in many NLP tasks. For example, <ref type="bibr" target="#b8">Johnson and Zhang (2015)</ref> use one-hot ngram representation as the input of CNN.  use ngram em- beddings to represent texts. Intuitively, initializing these models with pre-trained ngram embeddings may further improve the accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce ngrams into four representation methods. The experimental results demonstrate n- grams' effectiveness for learning improved word representations. In addition, we find that the trained ngram embeddings are able to reflect their semantic meanings and syntactic patterns. To al- leviate the costs brought by ngrams, we propose a novel way of building co-occurrence matrix, en- abling the ngram-based models to run on cheap hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pattern</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head><p>Word Bigram Negative Form wasn't able unable(.745), couldn't(.723), didn't(.680) was unable(.832), didn't manage(.799) don't need don't(.773), dont(.751), needn't(.715) dont need(.790), don't have(.785), dont want(.769) not enough enough(.708), insufficient(.701), sufficient(.629) not sufficient(.750), wasn't enough(.729) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of 'word predicts word'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of 'word predicts ngram'.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,218.25,68.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of 'ngram predicts ngram'.</figDesc><graphic url="image-3.png" coords="4,307.28,62.81,218.25,57.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>if</head><label></label><figDesc>Memory is full or P is empty then 10 Sort D by key (ngram); 11 Write D to tf s[f id]; 12 f id += 1; 13 end 14 end 15 end 16 Write A to tf s[0] in the form of &lt; ngram, H &gt;; 17 old = getSmallest(tf s) ; 18 while !(All files in tf s are empty) do 19 new = getSmallest(tf s) ; 20 if old.ngram == new.ngram then 21 old = &lt; old.ngram, merge(old.H, new.H) &gt;; 22 else 23 Write old to disk; 24 old = new 25 end 26 end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The pipeline.</figDesc><graphic url="image-4.png" coords="6,72.00,409.78,218.26,98.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Performance of (ngram) SGNS on analogy datasets.</head><label>2</label><figDesc></figDesc><table>Win 
Type 
Sim. Rel. Bruni Radinsky Luong Hill 

2 
uni uni .745 .586 
.713 
.635 
.387 
.419 
uni bi 
.739 .600 
.698 
.627 
.395 
.429 
uni tri 
.700 .535 
.658 
.591 
.380 
.415 
bi bi 
.757 .574 
.724 
.644 
.408 
.407 
bi tri 
.724 .564 
.669 
.605 
.403 
.412 

5 
uni uni .789 .648 
.756 
.652 
.407 
.401 
uni bi 
.794 .681 
.752 
.653 
.437 
.431 
uni tri 
.783 .673 
.743 
.652 
.432 
.436 
bi bi 
.816 .703 
.760 
.671 
.446 
.421 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance of (ngram) SGNS on similarity datasets.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 : Performance of (ngram) PPMI on analogy and similarity datasets.</head><label>4</label><figDesc></figDesc><table>Win 
Type 
Google 
MSR 
Sim. 
Rel. 
Bruni 
Radinsky 
Luong 
Hill 
Add 
Mul 
Add 
Mul 

2 
uni uni 
.535 / .599 / .482 
.540 / .610 / .481 
.444 
.445 
.681 
.529 
.698 
.608 
.381 
.351 
uni bi 
.543 / .601 / .493 
.549 / .612 / .496 
.464 
.472 
.686 
.545 
.695 
.631 
.389 
.352 

5 
uni uni 
.625 / .689 / .572 
.626 / .696 / .568 
.476 
.490 
.747 
.600 
.735 
.657 
.389 
.347 
uni bi 
.631 / .699 / .575 
.633 / .703 / .574 
.477 
.504 
.752 
.610 
.737 
.631 
.395 
.342 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Performance of (ngram) GloVe on analogy and similarity datasets. 

Win 
Type 
Google 
MSR 
Sim. 
Rel. 
Bruni 
Radinsky 
Luong 
Hill 
Add 
Mul 
Add 
Mul 

2 
uni uni 
.419 / .388 / .446 
.439 / .394 / .477 
.321 
.353 
.714 
.593 
.712 
.625 
.410 
.344 
uni bi 
.387 / .322 / .440 
.410 / .327 / .479 
.372 
.402 
.739 
.546 
.688 
.636 
.427 
.347 

5 
uni uni 
.433 / .426 / .439 
.460 / .463 / .458 
.290 
.321 
.752 
.633 
.731 
.623 
.411 
.326 
uni bi 
.410 / .340 / .468 
.446 / .365 / .513 
.374 
.416 
.751 
.559 
.698 
.639 
.426 
.363 

</table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/data/WestburyLab.wikicorp.201004.txt.bz2</note>

			<note place="foot" n="2"> Sub-sampling is not used in GloVe, which follows its original setting.</note>

			<note place="foot" n="3"> http://bitbucket.org/omerlevy/ hyperwords for SGNS, PPMI and SVD; http://nlp.stanford.edu/projects/glove/ for GloVe.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Table 7: Target bigrams and their nearest neighbours associated with similarity scores</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASIS</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast, easy, and cheap: Construction of statistical machine translation models with mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Cordova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2015</title>
		<meeting>NAACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2014</title>
		<meeting>NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2042" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effective use of word order for text categorization with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rie</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2015</title>
		<meeting>NAACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICASSP 1995</title>
		<meeting>ICASSP 1995</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (2)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2014</title>
		<meeting>NIPS 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weighted neural bag-of-n-grams model: New baselines for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1591" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word embedding revisited: A new representation learning and explicit matrix factorization perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2015</title>
		<meeting>IJCAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3650" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable language processing algorithms for the masses: A case study in computing word co-occurrence matrices with mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2008</title>
		<meeting>EMNLP 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic modeling of joint-context in distributional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>CoNLL; Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning generic context embedding with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2013</title>
		<meeting>NIPS 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2015</title>
		<meeting>SIGIR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised morphology induction using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2015</title>
		<meeting>NAACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inside out: Two jointly predictive models for word representations and phrase representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2016</title>
		<meeting>AAAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2821" to="2827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A unified learning framework of skip-grams and global vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cluster-driven model for improved word and text embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECAI 2016</title>
		<meeting>ECAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
