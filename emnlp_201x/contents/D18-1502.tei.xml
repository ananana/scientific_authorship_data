<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sedtawut</forename><surname>Watcharawittayakul</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Lassonde School of Engineering</orgName>
								<orgName type="institution" key="instit2">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Lassonde School of Engineering</orgName>
								<orgName type="institution" key="instit2">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Lassonde School of Engineering</orgName>
								<orgName type="institution" key="instit2">York University</orgName>
								<address>
									<addrLine>4700 Keele Street</addrLine>
									<settlement>Toronto</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4725" to="4730"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4725</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE. The main idea behind dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either small or large values for the single forgetting factor in the original FOFE. In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs. Our results on the challenging Google Billion Words corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs. Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modelling is an essential task for many natural language processing (NLP) applications including speech recognition, machine translation and text summarization. The goal of language modelling is to learn the distribution over a se- quence of characters or words; this distribution may be utilized for encoding the language struc- ture (e.g. the grammatical structure) as well as extracting information from the corpora <ref type="bibr" target="#b7">(Jozefowicz et al., 2016</ref>). In the recent years, the popularity of neural networks (NN) has been a significant driving force for language modelling (LM) research; the well-known NN-LM models includes the feedforward NN-LMs (FNN-LMs) ( <ref type="bibr" target="#b0">Bengio et al., 2001</ref><ref type="bibr" target="#b1">Bengio et al., , 2003</ref>, recurrent NN-LMs (RNN-LMs) ( <ref type="bibr" target="#b8">Mikolov et al., 2010</ref>; Mikolov * Equal contribution. and Zweig, 2012) and the long short-term mem- ory (LSTM-LMs) <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997</ref>). Among all, FNN-LMs often have a simpler and more efficient learning process, but they tend to underperform the other NN-LMs due to the lim- ited capability to memorize the long term depen- dency in natural languages ( <ref type="bibr" target="#b18">Zhang et al., 2015b)</ref>. However this drawback could be addressed by ap- plying the fixed-size ordinally-forgetting encoding (FOFE) to FNN's inputs. FOFE is an encoding method, which relies on the ordinally-forgetting mechanism to encode any word sequence based on the positions of words; this also allows the FOFE code to capture the long-term dependency ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>). As shown in Zhang (2015b), FNN- LMs with FOFE can easily yield comparable per- formance as other NN-LMs. The key parameter in the FOFE method is the forgetting factor, which is responsible for determining the degree of sensitiv- ity of the encoding with respect to the past con- text. However, the choice of a good value for the forgetting factor could be tricky since both small and large forgetting factors are offering different benefits.</p><p>In this paper, we propose a simple alteration to FOFE method, which allows to incorporate two forgetting factors into the fixed-size encoding of the variable-length word sequences. We name this approach as dual-FOFE. Our hypothesis is that by incorporating both the small and large forget- ting factors in the FOFE encoding, the dual-FOFE is able to simultaneously optimize the abilities to capture the positional information as well as to model long term dependency. In our experiments, we have evaluated the proposed dual FOFE mod- els on two large scale language modeling tasks, namely enwiki9 and Google Billion Words (GBW) corpora. Experimental results have shown that both FOFE models yield very competitive perfor- mance on these tasks, comparable with the state-of-the-art systems but with significantly reduced learning complexity. Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will briefly review the NN-LMs and the original FOFE method. The general idea behind NN-LM is to project the discrete words onto a continuous space, then learn to estimate the conditional probabilities of each known word within the projected space. The training of NN- LMs are often incredibly slow due to the ineffi- ciency of softmax normalization when applied to the extremely large output layer. The solution cur- rently used by many NN-LMs (including our mod- els in this work) is to use noise contrastive estima- tion (NCE) ( <ref type="bibr" target="#b4">Gutmann and Hyvrinen, 2010)</ref>. The basic idea of NCE is to reduce the probability esti- mation problem into a probabilistic binary classi- fication problem <ref type="bibr" target="#b11">(Mnih and Teh, 2012;</ref><ref type="bibr" target="#b10">Mnih and Kavukcuoglu, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fixed-Size Ordinally Forgetting Encoding</head><p>Fixed-size ordinally-forgetting Encoding (FOFE) is an encoding method which generates a fixed- size representation, namely the FOFE code, for any variable-length word sequence ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>). For a given word sequence S = {w 1 , w 2 , ..., w T }, let e t denote the one-hot repre- sentation of the word w t , z t for the FOFE code of the partial word sequence up to word w t , z t is computed as follows:</p><formula xml:id="formula_0">z t = α · z t−1 + e t (1 ≤ t ≤ T )<label>(1)</label></formula><p>where α (0 &lt; α &lt; 1) denotes the forgetting factor, a parameter responsible for determining the de- gree of influence each time step of the past context has on the FOFE code. Obviously, FOFE can con- vert any variable-length sequence into a fixed-size code with length equal to the size of vocabulary.</p><p>In regard to uniqueness of FOFE code, the code is said to be (almost) unique under the two theo- rems (proven in Zhang (2015a)): Theorem 1 If 0 &lt; α ≤ 0.5, then FOFE code is guarantee uniqueness for any values of vocabu- lary's size and sequence's length. Theorem 2 If 0.5 &lt; α &lt; 1, then FOFE code is guarantee almost uniqueness for any finite values of vocabulary's size and sequence's length, except for a finite set of countable choices of α.</p><p>Furthermore, the chance of actually having any collisions for α between 0.5 and 1 is nearly impos- sible in practice, due to quantization errors in real computer systems. Hence in practice, it is safe to argue that FOFE is able to uniquely encodes any variable-length sequence into a fixed-size repre- sentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FOFE for FNN-LMs</head><p>The idea of FOFE based NN-LMs is to use FOFE to encode the partial history sequence of past words in a sentence, then feed this fixed-size FOFE code to a feedforward neural network as an input to predict next word. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the FOFE code could be efficiently computed via time-delayed recursive structure, where the sym- bol z −1 in the figure represents a unit time delay (or equivalently a memory unit) from z t to z t−1 . The basic architecture for FOFE based FNN- LMs (called 1st-order) is the standard FNN archi- tecture with an additional layer for encoding the input into FOFE code. However, in this work, we use the 2nd-order and 3rd-order FOFE FNN- LMs, which are shown to produce slightly better results ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>). In a 2nd-order FOFE model, both the current partial sequences FOFE code (denoted as z t ) and the previous partial se- quences FOFE code (denoted as z t−1 ) are utilized to predict next word. In a 3rd-order FOFE model, all z t , z t−1 and z t−2 are used as inputs to neural networks.</p><p>More recently, the FOFE methods have been successfully applied to many NLP tasks, including word embedding ( <ref type="bibr" target="#b12">Sanu et al., 2017)</ref>, named entity recognition ( <ref type="bibr" target="#b14">Xu et al., 2017a</ref>), entity discovery and linking ( <ref type="bibr" target="#b16">Xu et al., 2016</ref><ref type="bibr" target="#b15">Xu et al., , 2017b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual-FOFE</head><p>The main idea of dual-FOFE is to generate aug- mented FOFE encoding codes by concatenating two FOFE codes using two different forgetting factors. Each of these FOFE codes is still com- puted in the same way as the mathematical for- mulation shown in Equation (1). The difference between them is that we may select to use two dif- ferent values for the forgetting factor (denoted as α) for additional modeling benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intuition behind Dual-FOFE</head><p>As mentioned in the subsection 2.1, the values in a FOFE code are used to encode both the content and the order information in a sequence. This is achieved by a recursive encoding method where at each recursive step the code will be multiplied by the forgetting factor (α) whose value is bounded by 0 &lt; α &lt; 1. In a practical computer with fi- nite precision, this has an impact on the FOFE's abilities to precisely memorize the long-term de- pendency of past context as well as to properly represent the positional information.</p><p>The FOFE's ability to represent the positional information would improve with smaller forget- ting factors. The reason is that that when α is small, the FOFE code (z t ) for each word vastly differs from its neighbour in magnitude. If α is too large (close to 1), the contribution of a word may not change too much no matter where it is. This may hamper the following neural networks to model the positional information. Conversely, the FOFE's ability to model the long-term depen- dency of the older context would improve with larger forgetting factors. This is because when α is small, the contribution of a word from the older history may quickly underflow to become irrele- vant (i.e. forgotten) when computing the current word.</p><p>In the original FOFE with just a single forget- ting factor, we would have to determine the best trade-off between these two benefits. On the other hand, the dual-FOFE does not face such issues since it is composed of two FOFE codes: the half of the dual-FOFE code using a smaller forgetting factor is solely optimized and responsible for rep- resenting the positional information of all words in the sequence; meanwhile the other half of the dual-FOFE code using a larger forgetting factor is optimized and responsible for maintaining the long-term dependency of past context. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the architecture of dual- FOFE based FNN-LMs is very similar to the orig- inal FOFE FNN-LMs. <ref type="bibr">1</ref> In the Dual-FOFE FNN- LMs, the input word sequence would have to pass through two branches of the FOFE layers (using two different forgetting factors) and each encod- ing branch will produce a FOFE code represent- ing the input sequence. These two FOFE codes are then joined to produce the dual-FOFE code, which would be fed to FNNs to predict the next word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-FOFE based FNN-LM</head><p>It might also be worth noting that in our imple- mentation we do not explicitly reset FOFE codes, i.e. z t value, at sentence boundaries. However, far- away histories will be gradually forgotten by the recursive calculation in FOFE due to 0 &lt; α &lt; 1 and finite precision in computers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-FOFE vs. Higher Order FOFE</head><p>As mentioned previously in the subsection 2.2, the higer order FOFE codes would utilize both the current and the previous sequence FOFE codes for prediction. Hence similar to dual-FOFE, the higher order FOFE could also maintain the sen- sitivity to both nearby and faraway context. Ob- viously a much higher order FOFE code may be required in order to achieve the same effect as dual-FOFE in terms of modelling long-term de- pendency. In this case, the higher order FOFE may also significantly increase the number of param- eters in the input layer. At last, the dual FOFE <ref type="bibr">1</ref> The difference in the location of the projection layer between <ref type="figure" target="#fig_0">Figure 1</ref> and 2 simply indicates two equivalent ways to do word projection. <ref type="figure" target="#fig_0">Figure 1</ref> was originally from Zhang (2015b), but they mentioned in text (without a <ref type="figure">figure)</ref> that it is more efficient to do projection as in <ref type="figure" target="#fig_1">Figure 2</ref> and both methods are mathematically equivalent since both projection and FOFE steps are linear. and the higher order FOFE are largely comple- mentary since we have observed consistent per- formance gains when combining dual FOFE with either 2nd-order or 3rd-order FOFE in our experi- ments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this work, we have evaluated the proposed dual-FOFE based FNN-LMs against various tra- ditional neural language models on two corpora: i) enwik9 corpus: it consists of the first 1 billion bytes of English wikipedia dump, having total size of 170.8 million words; the corpus was divided into three parts: the training set (153M words), the test set (8.9M words), and the validation set (8.9M words); the vocabulary size is limited to 80k words ( <ref type="bibr" target="#b18">Zhang et al., 2015b)</ref>. ii) Google Bil- lion Words (GBW) corpus: it contains about 800 million words and the corpus is divided into two parts: the training set (792M words) and the test set (8M words); the vocabulary size for this cor- pus is limited to 800k words ( <ref type="bibr" target="#b2">Chelba et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on enwiki9</head><p>In the experiments on the enwiki9 corpus, we have trained three dual-FOFE FNN-LMs with differ- ent forgetting factor pairs, one FOFE FNN-LM, and one tri-FOFE FNN-LM. All five models adopt a 2nd-order FOFE structure, employing a word embeddings of 256 dimensions, three hidden lay- ers of 400, 600, 600 neurons and an output layer of 80k words (reflecting the vocabulary). <ref type="bibr">2</ref> Note that the dual-FOFE FNN-LMs have to double the size of input context windows since dual-FOFE essentially contain two FOFE codes. But this in- crease only accounts for a negligible faction of to- tal model parameters.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, all three dual-FOFE FNN- LMs, using three pairs of forgetting factors as (0.5, 0.7) and (0.7, 0.9) and (0.5, 0.9), can significantly outperform other traditional models previously re- ported on this corpus. We also note that it is bene- ficial to include a relatively large forgetting factor, such as 0.9, in the dual FOFE models since such a large alpha may help to memorize much longer context in the inputs. When compared with the original FOFE counterpart, the best dual-FOFE model using forgetting factors (0.5, 0.9) offers a relative gain of around 8% in test PPL.</p><p>It is worth noting that our dual-FOFE mod- els can be extended to incorporate more than two alpha values. In fact after we have obtained a strong result supporting our dual-FOFE hypoth- esis, we have performed additional experiments using three alpha values, the so-called tri-FOFE model. The result on <ref type="table" target="#tab_0">Table 1</ref> has shown that the tri-FOFE FNN-LMs still slightly outperforms the dual-FOFE models. However, the gain is marginal. This leads us to believing that further extension of more alpha values in FOFE would be of limited use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Google Billion Words (GBW)</head><p>In the experiments on the GBW corpus, we have trained one dual-FOFE FNN-LM and one FOFE FNN-LM. Following the best dual-FOFE model configuration on the previous corpus, this dual- FOFE FNN-LM uses the same pair of dual for- getting factors (0.5, 0.9). Both models adopt a 3rd-order structure, employing word embeddings of 256 dimensions, three hidden layers each of 4096 neurons, a compression layer with 720 neu- rons, and an output layer of 800k words (reflect- ing the vocabulary). Although dual-FOFE FNN- LM has doubled the size of input context windows of FOFE FNN-LM, the total number of model pa- rameters in both models are almost equal, roughly 0.82 billion parameters.</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, the dual-FOFE FNN-LM is able to produce a very competitive performance, comparable with the best previously reported re- sults on this task, such as GCNN-13 ( <ref type="bibr" target="#b3">Dauphin et al., 2016)</ref> and LSTM-LM ( <ref type="bibr" target="#b7">Jozefowicz et al., 2016)</ref>. The dual-FOFE FNN-LM are among the few single-model systems that are able to achieve test PPL below 40 on this task. Furthermore, our proposed dual FOFE model can significantly reduce the computational complexity, e.g., our model has a relatively smaller number of param- eter (0.82B parameters) and it requires much less hardware resource to train (using only 1 GPU in our experiments). When compared with the orig- inal FOFE counterpart, the dual-FOFE FNN-LM is able to provide approximately 11% relative im- provement in PPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have proposed a new approach of utilizing the fixed-size ordinally-forgetting en- coding (FOFE) method for neural network lan- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Architecture PPL KN 3-gram ( <ref type="bibr" target="#b18">Zhang et al., 2015b)</ref> - 156 <ref type="bibr">KN 5-gram (Zhang et al., 2015b)</ref> - 132 FNN-LM 2-gram ( <ref type="bibr" target="#b18">Zhang et al., 2015b)</ref> [2*200]-600-600-80k 150 FNN-LM 3-gram ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>) <ref type="bibr">[3*200]</ref>-400-400-80k 131 FNN-LM 4-gram ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>) <ref type="bibr">[4*200]</ref>-400-400-80k 125 RNN-LM ( <ref type="bibr" target="#b18">Zhang et al., 2015b)</ref> [1*600]-80k 112 FOFE[α=0.7] FNN-LM ( <ref type="bibr" target="#b18">Zhang et al., 2015b</ref>) <ref type="bibr">[2*200]</ref>-600-600-80k 107 FOFE[α=0.7] FNN-LM <ref type="bibr">[2*256]</ref>-400-600-600-80k 104.8 Dual-FOFE[α=0.5,0.7] FNN-LM [2*2*256]-400-600-600-80k 101.7 Dual-FOFE[α=0.7,0.9] FNN-LM [2*2*256]-400-600-600-80k 97.0 Dual-FOFE[α=0.5,0.9] FNN-LM [2*2*256]-400-600-600-80k 96.6 Tri-FOFE[α=0.5,0.7,0.9] FNN-LM [3*2*256]-400-600-600-80k 95.9  <ref type="bibr">LSTM-2048</ref><ref type="bibr">-512 (Jozefowicz et al., 2016</ref> 43.7 0.83B 40 GPUs LSTM + CNN input ( <ref type="bibr" target="#b7">Jozefowicz et al., 2016)</ref> 30.0 1.04B 40 GPUs GCNN-13 ( <ref type="bibr" target="#b3">Dauphin et al., 2016)</ref> 38 As the name implies, this approach involves to produce a new fixed-sized representation for any variable-length sequence from a concatenation of two FOFE codes. This would have allowed us to select two values for the forgetting factors. One FOFE code with a smaller forgetting factor is re- sponsible for representing the positional informa- tion of all words in the sequence while the other using a larger forgetting factor is responsible for modelling the even longer term dependency in far away history. Our experiments on both en- wiki9 and Google Billion Words (GBW) tasks have both demonstrated the effectiveness of the dual-FOFE modeling approach. Experimental re- sults on the challenging GBW corpus have shown that the dual-FOFE FNN-LM has achieved over 10% improvement in perplexity over the original FOFE FNN-LM, without any significant drawback in model and learning complexity. When com- pared with other traditional neural language mod- els, the dual-FOFE FNN-LM has achieved com- petitive performance with significantly lower com- putational complexity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) 1st-order FOFE FNN-LM; (Center) 2nd-order FOFE FNN-LM; (Right) 3rd-order FOFE FNN-LM.</figDesc><graphic url="image-1.png" coords="2,307.28,345.32,218.24,118.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Left) 2nd-order Dual-FOFE FNN-LM; (Right) 3rd-order Dual-FOFE FNN-LM.</figDesc><graphic url="image-2.png" coords="3,307.28,88.93,218.26,130.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>guage models (NN-LMs), known as dual-FOFE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Test PPL of various LMs on enwiki9.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test PPL of various LMs on Google Billion Words. 

model 
PPL #param hardware 
Sigmoid-RNN-2048 (Ji et al., 2015) 
68.3 
4.1B 
1 CPU 
Interpolated KN 5-gram &amp; 1.1B n-grams (Chelba et al., 2013) 67.6 
1.8B 
100 CPUs 
Sparse Non-Negative Matrix LM (Shazeer et al., 2015) 
52.9 
33B 
-
RNN-1024 + MaxEnt 9-gram (Chelba et al., 2013) 
51.3 
20B 
24 GPUs 
LSTM-1024-512 (Jozefowicz et al., 2016) 
48.2 0.82B 
40 GPUs 
</table></figure>

			<note place="foot" n="2"> Comparing with Zhang (2015b), our single FOFE FNNLM baseline use a slightly larger model, which lead to slightly better perplexity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported mainly by a research do-nation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natu-ral Sciences and Engineering Research Council (NSERC) of Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="932" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno>abs/1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<idno>abs/1612.08083</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvrinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>Proceedings Track</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Blackout: Speeding up recurrent neural network language models with very large vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Shihao Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadathur</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dubey</surname></persName>
		</author>
		<idno>abs/1511.06909</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Word embeddings based on fixed-size ordinally forgetting encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Sanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="310" to="315" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sparse non-negative matrix language modeling for skip-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><surname>Pelemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1428" to="1432" />
		</imprint>
	</monogr>
	<note>Proceedings of Interspeech 2015</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A local detection approach for named entity recognition and mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sedtawut</forename><surname>Watcharawittayakul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1237" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">FOFE-based deep neural networks for entity discovery and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nargiza</forename><surname>Nosirova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Analysis Conference</title>
		<meeting>the Text Analysis Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The YorkNRM systems for trilingual EDL tasks at TAC KBP 2016</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sedtawut</forename><surname>Watcharawittayakul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Text Analysis Conference</title>
		<meeting>the Text Analysis Conference</meeting>
		<imprint>
			<publisher>TAC</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A fixed-size encoding method for variable-length sequences with its application to neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<idno>abs/1505.01504</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The fixed-size ordinallyforgetting encoding method for neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="495" to="500" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
