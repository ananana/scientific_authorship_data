<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Heuristically Informed Unsupervised Idiom Usage Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Liu</surname></persName>
							<email>{changsheng,hwa}@cs.pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15260</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Heuristically Informed Unsupervised Idiom Usage Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1723" to="1731"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1723</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Depending on the surrounding context, an idiomatic expression may be interpreted figuratively or literally. This paper proposes an unsupervised learning method for recognizing the intended usages of idioms. We treat the possible usages as a latent variable in proba-bilistic models and train them in a linguistically motivated feature space. Crucially, we show that distributional semantics serves as a helpful heuristic for formulating a literal usage metric to estimate the likelihood that the idiom is intended literally. This information can then guide the unsupervised training process for the probabilistic models. Experiments show that our overall model performs competitively against supervised methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many idiomatic expressions may be interpreted both figuratively or literally. Their intended us- ages depend on how they fit with their contexts. For example, the idiom "spill the beans" is used figuratively in the first instance below, and liter- ally in the second:</p><p>(1) <ref type="bibr">[</ref> 60 idioms have a clear literal meaning as well as a figurative one <ref type="bibr" target="#b9">(Fazly et al., 2009)</ref>. Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., ma- chine translation and sentiment analysis <ref type="bibr" target="#b24">(Salton et al., 2014;</ref><ref type="bibr" target="#b28">Williams et al., 2015)</ref>.</p><p>While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples ( <ref type="bibr" target="#b21">Peng et al., 2014;</ref><ref type="bibr" target="#b2">Byrne et al., 2013;</ref><ref type="bibr" target="#b18">Liu and Hwa, 2017)</ref>. A more challenging problem is to recog- nize idiom usages without a dictionary or some annotated examples ( <ref type="bibr" target="#b16">Korkontzelos et al., 2013)</ref>. Some previous unsupervised models tried to ex- ploit linguistic differences in usages. For exam- ple, <ref type="bibr" target="#b9">Fazly et al.(2009)</ref> observed that an idiom ap- pearing in its canonical form is usually used fig- uratively;  relied on the break in lexical coherence between the idioms and the context to signal a figurative usage. These heuristics, however, are not always applicable be- cause the distinctions they depend upon may not be present or obvious. To improve generaliza- tion across different idioms and usage contexts, we need a more reliable heuristic, and appropri- ately incorporate it into an unsupervised learning framework.</p><p>We propose a heuristic that differentiates usages based on distributional semantics <ref type="bibr" target="#b13">(Harris, 1954;</ref><ref type="bibr" target="#b27">Turney and Pantel, 2010)</ref>. Our key insight is that when an idiom is used literally, its relationship with its context is more predictable than when it is used figuratively. This is because the literal mean- ing of an idiom is compositional <ref type="bibr" target="#b15">(Katz and Giesbrecht, 2006</ref>), and the constituent words that make up the idiom are also meant literally. For exam- ple, in instance (2), spill is meant literally and can take on objects other than beans; moreover, one of the context words, mess, can often be seen to co-occur with spill in other text, even without beans. Our strategy is to represent an idiom's literal usage in terms of the word embeddings of the idiom's constituent words and other words they frequently co-occur with. Then, for any instance in which the idiom's usage is not known, we only need to deter- mine the semantic similarity between that instance and the idiom's literal representation. We define a literal usage metric that estimates the likelihood that an instance would be labeled "literal".</p><p>While the literal usage metric captures the dis- tributional semantic information of the context, we find that some other linguistic cues are also sig- nificant for usage detection (such as whether the subject of the sentence is a person); therefore, we allow our model to further refine through unsu- pervised methods. Specifically, we treat the usage (figurative or literal) as a hidden variable in proba- bilistic latent variable models, and we define a set of features that are linguistically relevant for idiom usage detection as observables. We integrate our literal usage metric with the latent variable mod- els by treating the metric outputs as soft labels to guide the latent variable models toward grouping by usages.</p><p>We hypothesize that unsupervised learning in a more linguistically motivated feature space, in- formed by soft labels from a semantically driven metric, will produce more robust classifiers. We conduct experiments comparing our approach against other supervised and unsupervised base- lines. Results suggest that our approach achieves performances that are competitive to supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Despite the common perception that idioms are mainly used figuratively, many can also be meant literally. A number of models have been pro- posed in the literature to recognize an idiom's us- ages under different context. Many rely on spe- cific linguistic property to draw a clear-cut deci- sion boundary between literal and figurative us- ages. For example, <ref type="bibr" target="#b9">Fazly et al. (2009)</ref> proposed a method that relies on the concept of canonical form. Based on the observation that while literal usages are less syntactically restricted, figurative usages tend to occur in a small number of canon- ical form(s). As shown in the examples above, however, this rule of thumb does not always hold.  proposed a method by building a cohesion graph to include all content words in the context; if removing the idiom im- proves cohesion, they assume the instance is figu- rative. Later, <ref type="bibr" target="#b17">Li and Sporleder (2009)</ref> used their cohesion graph method to label a subset of the test data with high confidence. This subset is then passed on as training data to the supervised classi- fier, which then labels the remainder of the dataset.</p><p>When manually annotated examples are avail- able, supervised classifiers are effective. <ref type="bibr" target="#b22">Rajani et al. (2014)</ref> extracted all non-stop-words in the context and used them as "bag of words" fea- tures to train a L2 regularized Logistic Regres- sion (L2LR) classifier <ref type="bibr" target="#b8">(Fan et al., 2008)</ref>. As local context of an idiom holds clues for discriminat- ing between its literal and figurative usages, <ref type="bibr" target="#b18">Liu and Hwa (2017)</ref> find that context representation also plays a significant role in idiom usage recog- nition. They took an adaptive approach, applying supervised ensemble learning over three classifiers based on different context representations ( <ref type="bibr" target="#b21">Peng et al., 2014;</ref><ref type="bibr" target="#b0">Birke and Sarkar, 2006;</ref><ref type="bibr" target="#b22">Rajani et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>Given a target idiomatic expression and a collec- tion of instances in which the idiom occurs, our proposed system <ref type="figure">(Figure 1</ref>) determines whether the idiom in each instance is meant figuratively or literally. We first build a Literal Usage Rep- resentation for each idiom by leveraging the dis- tributional semantics of its constituents (Sec 3.1). Given an instance of idiom, we can determine its usage by the semantic similarity between the con- text of the instance and the Literal Usage Repre- sentation. We define a Literal Usage Metric to transform the semantic similarity score into soft label, i.e., an initial rough estimation of the in- stance's usage (Sec 3.2). Finally, we treat the soft labels as distant supervision for downstream prob- abilistic latent variable models, in which the us- ages are considered as the hidden variables and are represented over a set of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Literal Usage Representation</head><p>An idiom co-occurs with different sets of words depending on whether it is meant literally or fig- uratively. For example, when used literally, get wind is more likely to co-occur with words such as rain, storm or weather; in contrast, when used figuratively, it frequently co-occurs with rumor or <ref type="figure">Figure 1</ref>: An overview of our unsupervised idiom usage recognition model story, etc. Comparing the two sets of words asso- ciated with the idiom, we see that the literal set of words also tend to co-occur with just wind, a con- stituent word within the idiom. Therefore, even without annotated data or dictionary, we may still approximate a representation for the literal mean- ing of an idiom by the idiom's constituent words and their semantic relationship to other words. To do so, we begin by initializing a literal meaning set to just the idiom's main constituent words <ref type="bibr">3</ref> ; we then grow the set by adding two types of semanti- cally related words. First, we look for co-occuring words in a large textual corpus (e.g., <ref type="bibr" target="#b7">(David et al., 2005</ref>)): for each constituent word w, we randomly sample s sentences that contain w from the corpus; we extract the top n most frequent words (exclud- ing stop words) and add them to the literal mean- ing set. Second, we look for words that are se- mantically close in a word embedding space: we train a continuous bag-of-words (CBOW) embed- ding model <ref type="bibr" target="#b19">(Mikolov et al., 2013</ref>) and add addi- tional t words that are the most related to w using cosine similarity.</p><p>All together, the literal usage representation is a collection of vectors, i.e., the embeddings of the words in the final extended literal meaning set. The size of the set depends on parameters s, n, and t; if the chosen values are too small, we do not end up with a word collection that is representative enough; if the numbers are too large, we would only be wasting computing resources chasing Zip- fian tails. Parameter setting choices are discussed further in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Literal Usage Metrics</head><p>Among all the instances to be classified, we expect the context words of the literal cases to be more semantically close to the literal usage representa- tion we just formed. Let L denote the set of words in the literal usage representation for the target id- iom. For each instance, let C be the set of non-stop context words in the instance. We calculate s, the semantic similarity score between the context of the instance and the literal usage representation as follows:</p><formula xml:id="formula_0">s = 1 |C| c∈C 1 |L| l∈L sim(c, l)<label>(1)</label></formula><p>where c denotes a word in C, l denotes a word in L and sim(c, l) refers to the cosine similarity between the word embeddings of c and l. Let S = {s 1 , s 2 , ...s n } be the set of semantic similarity scores for all the instances we wish to classify. Instances with higher scores are more likely to use the idiom literally. A naive literal usage metrics is to choose a predefined thresh- old for all idioms and label all the instances with score above the threshold as literal usages. This approach is unlikely to work well in practice. As noted by previous work, idioms have different lev- els of semantic analyzability ( <ref type="bibr" target="#b11">Gibbs et al., 1989;</ref><ref type="bibr" target="#b3">Cacciari and Levorato, 1998)</ref>. When an idiom has a high degree of semantic analyzability, its contex- tual words will be more semantically close to the literal usage representation, thus a higher thresh- old is needed.</p><p>In this work, we select a different decision threshold for each idiom adaptively based on the similarity scores distribution. And most im- portantly, rather than generate a hard label, we transform these scores into a probabilistic metric, where 0 means the usage in the instance is almost certainly figurative while 1.0 means it is literal.</p><p>We propose a metric based on the principle of Minimum Variance (MinV). That is, we first sort the scores in S and choose the threshold (from these scores) that minimizes the sum of variances of the two resulting clusters. For each instance i, we then apply the following metric to estimate the probability that the idiom in instance i is meant literally based on its semantic similarity score s i :</p><formula xml:id="formula_1">P r i = 1 1 + e −k * (s i −t)<label>(2)</label></formula><p>where k is a constant weighting factor and t in- dicates the learned threshold. The intuition is that the larger the difference between s i and the thresh- old is, the more likely the instance i is literal; the probability of literal usage is not linearly corre- lated to the difference, we use the sigmoid func- tion to account for this non-linearity. We incorpo- rate k to scale the value of the difference since it is generally very small (close to 0). Without k, all the P r values gravitate toward 0.5, rendering the soft label being equivalent to random guess. We set k to 5 for all the idioms based on a development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Heuristically Informed Usage Recognition</head><p>The soft label, generated by MinV (the literal us- age metric), captures the distributional semantic information of the context. In practice, there are a variety of other linguistic features which are also informative of the intended usage of idiom. We explore probabilistic latent variable models over a collection of features that are linguistically rele- vant for idiom usage detection. The soft label is integrated into the unsupervised learning of hid- den usages as a distant supervision. In this section, we will describe the proposed features in the latent variable models and how we integrate the soft la- bel into the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Latent Variable Models</head><p>To predict an idiom's usage in instances, we con- sider two representative probabilistic latent vari- able models: Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>  <ref type="bibr">4</ref> and unsupervised Naive Bayes (NB). For both models, the latent variable is the id- iom usage (figurative vs. literal); the observables are linguistic features that can be extracted from the instances, described below: Subordinate Clause We encode a binary fea- ture indicating whether the target expression is fol- lowed by a subordinate clause (the Stanford Parser <ref type="bibr" target="#b4">(Chen and Manning, 2014</ref>) is used). This feature is useful for some idioms such as in the dark. It usu- ally suggests a figurative usage as in You've kept us totally in the dark about what happened that night.</p><p>Selectional Preference Violation of selectional preference is normally a signal of figurative usage (e.g., having an abstract entity as the subject of play with fire). We encode this feature if the head word of the idiom is a verb and focus on the sub- ject of the verb. We apply Stanford Name Entity tagger ( <ref type="bibr" target="#b10">Finkel et al., 2005</ref>) with 3 classes ("Loca- tion", "Person", "Organization") on the sentence containing the idiom. If the subject is labeled as an Entity, its class will be encoded in the feature vec- tor. Pronouns such as "I" and "he" also indicate the subject is a "Person". However, they are nor- mally not tagged by Stanford Name Entity tagger. To overcome this issue, we add Part-of-Speech of the subject into the feature vector.</p><p>Abstractness Abstract words refer to things which are hard to perceive directly with our senses. Abstractness has been shown to be useful in the detection of metaphor, another type of figu- rative language <ref type="bibr" target="#b26">(Turney et al., 2011)</ref>. A figurative usage of an idiomatic phrase may have relatively more abstract contextual words. For example, in the sentence She has lived life in the fast lane, the word life is considered as an abstract word. This is a useful indicator that in the fast lane is used figuratively. We use the MRC Psycholinguistic Database Machine Usable Dictionary <ref type="bibr" target="#b5">(Coltheart, 1981)</ref> which contains a list of 4295 words with their abstractness measure between 100 and 700. We calculate the average abstractness score for all the contextual words (with stop words being re- moved) in the sentence containing the idiom. The score is then transformed into categorical feature to overcome sparsity problem based on the follow- ing criteria: concrete (450 -700), medium (350 - 450), abstract (100 -350).</p><p>Neighboring Words Words preceding and fol- lowing the idiomatic expression can be very in- formative in terms of usage recognition. For ex- ample, words such as relax or shower before the idiom in hot water often signal a literal usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part-of-Speech of the Neighboring Words</head><p>Class of neighboring words might be useful as well. For example, a pronoun preceding dog's age generally indicates a literal usage, as in I think my dog's age is starting to catch up. She sometimes needs help to jump on to my bed, while a deter- miner usually marks a figurative usage, as in It's been a dog's age since I've used Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Incorporating Soft Label into Usage</head><p>Recognition Given a collection of instances and their features, either LDA or NB can separate the instances into two groups (hopefully, by usages), but it does not associate the right label (i.e., "figurative" or "lit- eral") to the groups. We do not want to rely on any manual annotations for this step. Therefore, we integrate the automatically generated soft la- bels (based on MinV, our literal usage metric) into the unsupervised learning procedure as a weak form of supervision. Formally, we want to es- timate each instance's posterior distribution over (literal/figurative) usages θ du and usage-feature distribution φ uf . For LDA, we derive a Gibbs sampling algorithm which incorporates the soft la- bel into the learning procedure. We refer it as in- formed Gibbs sampling (infGibbs). For unsuper- vised naive Bayes model, we adapt the classical Expectation-Maximization algorithm to integrate the soft label. We refer it as informed Expectation- Maximization (infEM).</p><p>Informed Gibbs Sampling The Gibbs sam- pling algorithm ( <ref type="bibr" target="#b12">Griffiths and Steyvers, 2004</ref>) used in traditional LDA initializes each word to- ken a random hidden topic. The system needs to interpret the learned topics post-hoc, e.g., by hu- man annotation. In our case, for each feature f in each instance, an initial random usage biased by the instance's soft label is assigned to f (i.e., a Bernoulli trial). Since the soft label explicitly en- codes an instance's literal and figurative usage dis- tribution, we do not need to interpret the learned usages at the end of the algorithm. Based on these assignments, we build a feature-usage counting matrix C F U and instance-usage counting matrix C DU with dimensions |F | × 2 and |D| × 2 re- spectively (|F | is the feature size and |D| is the number of instances): C F U i,j is the count of fea- ture i assigned to usage j; C DU d,j is the count of features assigned to usage j in instance d. Then for each feature f in each instance, we resample a new usage for f and matrices C F U and C DU will be updated accordingly. This step will be repeated for T times. The resampling equation is:</p><formula xml:id="formula_2">p(u i = j|u −i , f ) ∝ p j · C f i −i,j +β C ( * ) −i,j +|F |β · C d i −i,j +α C d i −i, * +|U |α (3)</formula><p>where i indexes features in the instance d, j is an index into literal and figurative usages, * in- dicates a summation over that dimension and − means excluding the corresponding instance. The first factor p j is the soft label encoding prior us- age distribution. The second factor represents the probability of feature f under usage j (C f i −i,j is the count of the feature f assigned to usage j, excluding the current usage assignment u i ). The third factor represents the probability of usage j in the current instance (C d i −i,j is the count of linguis- tic features which are assigned to usage j in the current instance, excluding the current feature f ). The value of |U | is 2, representing the number of usages (i.e., figurative and literal). α and β are the hyper-parameters from the Dirichlet priors (we set both of them to 1). The core idea of Equation 3 is to integrate both distribution semantic information (soft label, the first factor) and linguistically mo- tivated features (the second and third factors) into the inference procedure.</p><p>The matrices of C F U and C DU from the last 10% * T iterations are averaged and then nor- malized to approximate the true usage-feature dis- tribution φ uf and instance-usage distribution θ du respectively. The final result is determined by θ du , i.e., assigning each instance with the usage of probability higher than 0.5. We do average to have a more stable result because an accidental bad sampling would affect our model negatively if we only use the C F U and C DU from the last iteration. This procedure is important for some id- ioms if their feature space is sparse. The iteration number T is set to 500 based on a development set.</p><p>Informed Expectation Maximization Com- bining a Naive Bayes classifier with the EM algo- rithm has been widely used in text classification and word sense disambiguation <ref type="bibr" target="#b14">(Hristea, 2013;</ref><ref type="bibr" target="#b20">Nigam et al., 2000</ref>). In our case, we want to con- struct a model to recover the missing literal and figurative labels of the instances of the target id- iom. This section describes two extensions to the basic EM algorithm for idiom usage recognition. The extensions help improve parameter estimation by taking the automatically learned soft labels into consideration.</p><p>Our informed EM method extends a basic ver- sion for NB <ref type="bibr" target="#b14">(Hristea, 2013)</ref>, where the initial pa- rameter values θ du and φ uf are chosen randomly. At each iteration, the E-step of the algorithm esti- mates the expectations of the missing values (i.e. the literal and figurative usage) given the latest it- eration of the model parameters; the M-step max- imizes the likelihood of the model parameters us- ing the previously-computed expectations of the missing values. As we've done with extending Gibbs sampling for LDA, we also perform two similar adaptations on conventional EM for NB to incorporate soft labels. First, we assign each instance an initial usage distribution θ du directly using the soft label, and then initialize the usage- feature distribution φ uf using these assignments. We refer it as informed initialization. Second, in the E-step, we multiply the expectation result of the basic EM with the soft label as the new ex- pected usage for each instance (i.e., updating θ du ). The M-step is the same as basic EM to update the usage-feature distribution φ uf .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We conduct experiments to address three ques- tions:</p><p>1. How effective is our overall approach? How does it compare against previous work?</p><p>2. How effective is our literal usage metric (i.e., MinV) compared to other heuristics?</p><p>3. How effective is our literal usage metric at informing downstream learning processes?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Models Our main experiments will evaluate the two variants of the proposed fully unsupervised model as described in section 3: MinV+infGibbs and MinV+infEM. We report the average per- formance of our models over 5 runs. Perform- ing multiple runs is necessary because we have a sampling process. They are compared with three baseline unsupervised models: , <ref type="bibr" target="#b17">Li and Sporleder (2009)</ref>  <ref type="bibr">5 and Fazly et al. (2009)</ref>; and two baseline supervised models: <ref type="bibr" target="#b22">Rajani et al. (2014)</ref> and <ref type="bibr" target="#b18">Liu and Hwa (2017)</ref> (us- ing 5-fold cross validation).</p><p>Parameter setting Recall that in order to build the literal usage representation of an idiom, we need to sample s sentences that contain each con- stituent word w from an external corpus; extract from them the top n most frequently co-occurring words with w; then separately find t words that are semantically similar to w using word embed- dings. To set parameters with values in reasonable ranges, we evaluated MinV on a small develop- ment set. We picked 10 idioms that are differ- ent from the evaluation set, scraped 50 instances from the web for each idiom, and labeled them ourselves. We find that s &gt;= 100, n=10, and t=5 yield good results.</p><p>We use the gensim toolkit <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref> and train our word embedding model using the continuous bag of word model on Text8 Cor- pus <ref type="bibr">6</ref> . Negative sampling is applied as the training method; the min count is set to 2. For the other parameters, we use the default settings in gensim. Evaluative Data Our goal is to compare all the methods under two public available corpora: Se- mEval 2013 Task 5B corpus ( <ref type="bibr" target="#b16">Korkontzelos et al., 2013</ref>), which is used by prior supervised meth- ods ( <ref type="bibr" target="#b18">Liu and Hwa, 2017;</ref><ref type="bibr" target="#b22">Rajani et al., 2014</ref>) and verb-noun combination (VNC) dataset <ref type="bibr" target="#b6">(Cook et al., 2008</ref>), which is used by a prior unsupervised method <ref type="bibr" target="#b9">(Fazly et al., 2009</ref>). However, there are some methods-datasets conflicts that have to be re- solved. Because the idioms in the SemEval dataset are all in their canonical forms, and because the id- ioms are not restricted to the verb-noun combina- tion, we cannot evaluate the method by Fazly et al. on this dataset (as their method is tailored to verb- noun combination). Some idioms from the VNC dataset are almost always used figuratively (or lit- erally), which presents a problem for supervised methods. To facilitate full comparisons, we select the subset of idioms from the VNC corpus whose number of literal and figurative instances are both higher than 10. A summary of the two corpora is shown in <ref type="table">Table 1</ref>. Note that each instance in Se- mEval corpus has about 3∼5 sentences; for con- sistency, we use 3 sentences as the context: the sentence with the target idiom and two neighbor- ing sentences. Evaluation metric Following the convention in prior works, we report the F-score for the recogni- tion of figurative usages and the overall accuracy.  <ref type="table" target="#tab_3">Table 2</ref> shows the result of our models and the other comparative methods. Our proposed mod- els show consistent performance across the two corpora, outperforming the unsupervised base- lines from Sporleder and Li <ref type="formula" target="#formula_1">(2009)</ref>, <ref type="bibr" target="#b17">Li and Sporleder (2009)</ref> and the supervised model from <ref type="bibr" target="#b22">Rajani et al. (2014)</ref>. Moreover, there is no statisti- cal significance in the F-score difference between the supervised ensemble model from <ref type="bibr" target="#b18">Liu and Hwa (2017)</ref> and our models.</p><p>On the VNC corpus, our models have compa- rable average scores as that of <ref type="bibr" target="#b9">Fazly et al. (2009)</ref>; our scores are more stable across different idioms. While the method of Fazly et al. is nearly perfect for some idioms (0.98 on "take heart"), it performs poorly for others (e.g., 0.33 on "pull leg"). Their algorithm has trouble with idioms whose canoni- cal and non-canonical forms can appear frequently both in literal and figurative usages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effectiveness of MinV</head><p>The core of our approach is MinV, the literal us- age metric we developed to generate soft labels to guide the unsupervised learning. This experiment examines its effectiveness by creating usage clas- sifications directly from it (i.e., if MinV predicts a probability of &gt;0.5, predict "literal"). We com- pare MinV against two alternative heuristics.</p><p>MinV is based on two core ideas. First, if an idiom is used figuratively, we expect to see a big difference (low similarity scores) between its con- text and the semantic representation of idiom's lit- eral usage. The idea is similar to that of Sporleder and Li (2009), but they relied on lexical chain instead of distributional semantics. Second, in- stead of choosing a predefined threshold to sep- arate the raw semantic similarity scores, we se- lect a different decision threshold for each idiom adaptively based on the distribution of the scores. So as an alternative, we compare MinV against a Fixed-Threshold heuristic that labels an instance as "literal" if its raw score is higher than some global threshold (set to 0.346 based on develop- ment data).</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we observe that Minv outperforms both Sporleder and Li's model as well as Fixed- Threshold, but using MinV by itself is not suffi- cient. It has great fluctuations, e.g., the F-Score for individual idioms varies from 0.43 to 0.88. Re- call that MinV +infGibbs has a smaller fluctuation across different idioms in <ref type="table" target="#tab_3">Table 2</ref>. These results suggest that the subsequent learning process is ef- fective.</p><p>Through error analysis, we find two major fac- tors contributing to the performance fluctuation. First, the context itself could be misleading. An error case of play ball by MinV is:</p><p>All 10-year-old Minnie Cruttwell wants to do is play with the boys , but the Football Association are not playing ball. She is a member of a mixed team called Balham Blazers , but the FA say she must join a girls' team when she is 12.</p><p>The context words in bold (which are related to the word "ball") mislead MinV to predict a "lit- eral" usage when it is actually a "figurative" usage (since an organization such as the Football Asso- ciation cannot literally play ball). Second, not all content words in the context are relevant for dis- tinguishing the idiom's usage. A future direction is to prune contextual words more intelligently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Integration of MinV into Learning</head><p>We have argued that an advantage of using a met- ric with a probabilistic interpretation instead of a binary class heuristic is that its scores can be incorporated into subsequent learning models as soft labels. In this set of experiments, we evaluate the impact of the metric on the learning methods. First, we consider unsupervised learning without input from the literal usage metric. We cluster the instances with the original Gibbs sampling and EM algorithms and then label the two clusters with the majority usage within the clusters. Second, we explore using the information from the literal usage metric as "noisy gold standard" to perform supervised training on a nearest neighbors (NN) classifier. Specifically, the literal and figurative instances labeled by MinV with high confidence (top 30%) are used as example set. Then for each test instance, we calculate its cosine similarity (in feature space) to the literal and figurative example sets and assign the label of the closest set. We refer this model as MinV +NN.      <ref type="table">Table 4</ref>: The performance of MinV+NN and mod- els without soft label on all the idioms in the two corpora <ref type="table">Table 4</ref> shows the performances of the new models, which are all worse than our full models MinV +infGibbs and MinV +infEM. This high- lights the advantage of integrating distributional semantic information and local features into one single learning procedure. Without the informed prior (encoded by the soft labels), the Gibbs sam- pling and EM algorithms only seek to maximize the probability of the observed data, and may fail to learn the underlying usage structure.</p><p>The model MinV +NN is not as competitive as our full models. It is too sensitive to the selected instances. Even though the training examples are instances that MinV is the most confident about, there are still mislabelled instances. These "noisy training examples" would lead the NN classifier to make unreliable predictions. In contrast, our unsupervised learning is less sensitive to the per- formance of MinV; it can achieve a decent perfor- mance for an idiom even when the quality of the soft labels is poor. For example, when using MinV as a stand-alone model for break a leg, its figura- tive F-score is only 0.43, but through further train- ing, the full model MinV+infGibbs achieves 0.64. <ref type="figure" target="#fig_2">Fig. 2</ref> shows the training curve. A possible rea- son for this phenomenon is that the soft label is integrated into the learning process by biasing the sampling procedure (see Equation 3). We only en- courage our model to follow the distributional se- mantic evidence captured by soft label and do not force it. So if there are strong evidences encoded by the linguistically motivated features in the in- stances to overcome the soft label it still has the freedom to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an unsupervised method for id- iom usage recognition built upon the heuristic that instances that use the idiom literally are semanti- cally closer to constituent words of the idiom. Ex- perimental results on two different corpora suggest that our models are competitive against supervised methods and prior unsupervised methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance of MinV+infGibbs on the idiom "break a leg"</figDesc><graphic url="image-2.png" coords="8,307.28,247.94,204.09,155.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The performances of different models. Avg. F f ig denotes average figurative F-score, Avg.Acc 
denotes average accuracy. We report the range in the parenthesis. * indicates the difference is significant 
with our MinV+ infGibbs model at the 95% confidence level. Since the method from Fazly et al. (2009) 
restricted their experiment to VNC type, we only report their performance on the VNC corpus. 

Model 
Avg. F f ig 
Avg.Acc 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>A comparison of classifying by different heuristics. Results are averaged across all the id- ioms in the two corpora.</figDesc><table>Model 
Avg. F f ig 
Avg.Acc 
</table></figure>

			<note place="foot" n="3"> We observe that the nouns tend to be the most indicative of the idiom&apos;s literal meaning, but if the idiom does not contain any noun, we back off to any constituent word that is not a stop word.</note>

			<note place="foot" n="4"> Although originally conceived for modeling document content, LDA can be applied to any kind of discrete input</note>

			<note place="foot" n="5"> We replace Normalized Google Distance (NGD) with word embeddings to measure the semantic relatedness between words due to the query frequency restriction on the API of NGD.</note>

			<note place="foot" n="6"> From http://mattmahoney.net/dc/text8. zip</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A clustering approach for nearly unsupervised recognition of nonliteral language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Birke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IIRG: A naive approach to evaluating phrasal semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorna</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Fenlon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dunnion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The effect of semantic analyzability of idioms in metalinguistic tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cacciari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Chiari</forename><surname>Levorato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metaphor and Symbol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="177" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The mrc psycholinguistic database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Coltheart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The vnc-tokens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">English gigaword second edition ldc2005t12. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graff</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kong</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maeda</forename><surname>Kazuaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Wei</forename><surname>Rong En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised type and token identification of idiomatic expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="103" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How to kick the bucket and not decompose: Analyzability and idiom processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond W Gibbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cooper</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cutting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="576" to="593" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Na¨ıveNa¨ıve Bayes Model in the Context of Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentina</forename><forename type="middle">T</forename><surname>Hristea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic identification of non-compositional multiword expressions using latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenie</forename><surname>Giesbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</title>
		<meeting>the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Ioannis Korkontzelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">Massimo</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Zanzotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Biemann</surname></persName>
		</author>
		<title level="m">Semeval-2013 task 5: Evaluating phrasal semantics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifier combination for contextual idiom detection without labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Representations of context in recognizing the figurative and literal usages of idioms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text classification from labeled and unlabeled documents using em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="103" to="134" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classifying idiomatic and literal expressions using topic models and intensity of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2027" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Using abstract context to detect figurative language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edaena</forename><surname>Nazneen Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<publisher>Valletta</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An empirical study of the impact of idioms on phrase based statistical machine translation of en</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kelleher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>glish to brazilian-portuguese</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised recognition of literal and non-literal use of idiomatic expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Sporleder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Literal and metaphorical sense identification through concrete and abstract context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Peter D Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohai</forename><surname>Assaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="680" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The role of idioms in sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lowri</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bannister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arribasayllon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alun</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Spasi´cspasi´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="7375" to="7385" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
