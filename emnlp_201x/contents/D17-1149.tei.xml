<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Translating Phrases in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>tuzhaopeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou, Shenzhen</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Translating Phrases in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1421" to="1431"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Phrases play an important role in natural language understanding and machine translation (Sag et al., 2002; Villavicencio et al., 2005). However, it is difficult to integrate them into current neural machine translation (NMT) which reads and generates sentences word by word. In this work, we propose a method to translate phrases in NMT by integrating a phrase memory storing target phrases from a phrase-based statistical machine translation (SMT) system into the encoder-decoder architecture of NMT. At each decoding step, the phrase memory is first rewritten by the SMT model, which dynamically generates relevant target phrases with contextual information provided by the NMT model. Then the proposed model reads the phrase memory to make probability estimations for all phrases in the phrase memory. If phrase generation is carried on, the NMT decoder selects an appropriate phrase from the memory to perform phrase translation and updates its decoding state by consuming the words in the selected phrase. Otherwise, the NMT decoder generates a word from the vocabulary as the general NMT decoder does. Experiment results on the Chinese→English translation show that the proposed model achieves significant improvements over the baseline on various test sets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) has been re- ceiving increasing attention due to its impressive * Corresponding author translation performance <ref type="bibr" target="#b16">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b6">Cho et al., 2014;</ref><ref type="bibr" target="#b32">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref>. Sig- nificantly different from conventional statistical machine translation (SMT) ( <ref type="bibr" target="#b3">Brown et al., 1993;</ref><ref type="bibr" target="#b18">Koehn et al., 2003;</ref><ref type="bibr" target="#b4">Chiang, 2005</ref>), NMT adopts a big neural network to perform the entire trans- lation process in one shot, for which an encoder- decoder architecture is widely used. Specifically, the encoder encodes a source sentence into a con- tinuous vector representation, then the decoder uses the continuous vector representation to gen- erate the corresponding target translation word by word.</p><p>The word-by-word generation philosophy in NMT makes it difficult to translate multi-word phrases. Phrases, especially multi-word expres- sions, are crucial for natural language understand- ing and machine translation ( <ref type="bibr" target="#b27">Sag et al., 2002;</ref><ref type="bibr" target="#b37">Villavicencio et al., 2005</ref>) as the meaning of a phrase cannot be always deducible from the mean- ings of its individual words or parts. Unfortu- nately current NMT is essentially a word-based or character-based ( <ref type="bibr" target="#b7">Chung et al., 2016;</ref><ref type="bibr" target="#b9">Costa-jussà and Fonollosa, 2016;</ref><ref type="bibr" target="#b21">Luong and Manning, 2016)</ref> translation system where phrases are not consid- ered as translation units. In contrast, phrases are much better than words as translation units in SMT and have made a significant advance in trans- lation quality. Therefore, a natural question arises: Can we translate phrases in NMT?</p><p>Recently, there have been some attempts on multi-word phrase generation in NMT ( <ref type="bibr" target="#b31">Stahlberg et al., 2016b;</ref><ref type="bibr" target="#b43">Zhang and Zong, 2016)</ref>. However these efforts constrain NMT to generate either syntactic phrases or domain phrases in the word- by-word generation framework. To explore the phrase generation in NMT beyond the word-by- word generation framework, we propose a novel architecture that integrates a phrase-based SMT model into NMT. Specifically, we add an auxil- iary phrase memory to store target phrases in sym- bolic form. At each decoding step, guided by the decoding information from the NMT decoder, the SMT model dynamically generates relevant target phrase translations and writes them to the mem- ory. Then the NMT decoder scores phrases in the phrase memory and selects a proper phrase or word with the highest probability. If the phrase generation is carried out, the NMT decoder gen- erates a multi-word phrase and updates its decod- ing state by consuming the words in the selected phrase.</p><p>Furthermore, in order to enhance the ability of the NMT decoder to effectively select appropriate target phrases, we modify the encoder of NMT to make it fit for exploring structural information of source sentences. Particularly, we integrate syn- tactic chunk information into the NMT encoder, to enrich the source-side representation. We vali- date our proposed model on the Chinese→English translation task. Experiment results show that the proposed model significantly outperforms the conventional attention-based NMT by 1.07 BLEU points on multiple NIST test sets.</p><p>The rest of this paper is organized as fol- lows. Section 2 briefly introduces the attention- based NMT as background knowledge. Section 3 presents our proposed model which incorporates the phrase memory into the NMT encoder-decoder architecture, as well as the reading and writing procedures of the phrase memory. Section 4 presents our experiments on the Chinese→English translation task and reports the experiment results. Finally we discuss related work in Section 5 and conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Neural machine translation often adopts the encoder-decoder architecture with recurrent neu- ral networks (RNN) to model the translation pro- cess. The bidirectional RNN encoder which con- sists of a forward RNN and a backward RNN reads a source sentence x = x 1 , x 2 , ..., x Tx and transforms it into word annotations of the entire source sentence h = h 1 , h 2 , ..., h Tx . The de- coder uses the annotations to emit a target sentence y = y 1 , y 2 , ..., y Ty in a word-by-word manner.</p><p>In the training phase, given a parallel sentence (x, y), NMT models the conditional probability as follows,</p><formula xml:id="formula_0">P (y|x) = Ty i=1 P (y i |y &lt;i , x)<label>(1)</label></formula><p>where y i is the target word emitted by the decoder at step i and y &lt;i = y 1 , y 2 , ..., y i−1 . The condi- tional probability P (y i |y &lt;i , x) is computed as</p><formula xml:id="formula_1">P (y i |y &lt;i , x) = sof tmax(f (s i , y i−1 , c i )) (2)</formula><p>where f (·) is a non-linear function and s i is the hidden state of the decoder at step i:</p><formula xml:id="formula_2">s i = g(s i−1 , y i−1 , c i )<label>(3)</label></formula><p>where g(·) is a non-linear function. Here we adopt Gated Recurrent Unit ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>) as the re- current unit for the encoder and decoder. c i is the context vector, computed as a weighted sum of the annotations h:</p><formula xml:id="formula_3">c i = Tx j=1 α t,j h j (4)</formula><p>where h j is the annotation of source word x j and its weight α t,j is computed by the attention model. We train the attention-based NMT model by maximizing the log-likelihood:</p><formula xml:id="formula_4">C(θ) = N n=1 Ty i=1 log P (y n i |y n &lt;i , x n )<label>(5)</label></formula><p>given the training data with N bilingual sentences <ref type="bibr" target="#b5">(Cho, 2015)</ref>. In the testing phase, given a source sentence x, we use beam search strategy to search a target sen- tencê y that approximately maximizes the condi- tional probability P (y|x)</p><formula xml:id="formula_5">ˆ y = argmax y P (y|x)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we introduce the proposed model which incorporates a phrase memory into the encoder-decoder architecture of NMT. Inspired by the recent work on attaching an external struc- ture to the encoder-decoder architecture <ref type="bibr" target="#b13">(Gulcehre et al., 2016;</ref><ref type="bibr" target="#b12">Gu et al., 2016;</ref><ref type="bibr" target="#b33">Tang et al., 2016;</ref><ref type="bibr" target="#b38">Wang et al., 2017)</ref>, we adopt a similar approach to incorporate the phrase memory into NMT.  <ref type="figure" target="#fig_0">Figure 1</ref> shows an example. Given the gener- ated words "President Bush emphasized that", the model generates the next fragment either from a word generation mode or a phrase generation mode. If the model selects the word generation mode, it generates a word by the NMT decoder as in the standard NMT framework. Otherwise, it generates a multi-word phrase by enquiring a phrase memory, which is written by an SMT de- coder based on the dynamic decoding information from the NMT model for each step. The trade-off between word generation mode and phrase gener- ation mode is balanced by a weight λ, which is produced by a neural network based balancer. Formally, a generated translation y = {y 1 , y 2 , . . . , y Ty } consists of two sets of frag- ments: words generated by NMT decoder w = {w 1 , w 2 , . . . , w K } and phrases generated from the phrase memory p = {p 1 , p 2 , . . . , p L } . The prob- ability of generating y is calculated by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><formula xml:id="formula_6">P (y|x) = w k ∈w (1 − λ t(w k ) )P word (w k ) × p l ∈p λ t(p l ) P phrase (p l )<label>(7)</label></formula><p>where P word (w k ) is the probability of generating the word w k (see Equation 2), P phrase (p l ) is that of generating the phrase p l which will be described in Section 3.2, and t(·) is the decoding step to gen- erate the corresponding fragment.</p><p>The balancing weight λ is produced by the bal- ancer -a multi-layer network. The balancer net- work takes as input the decoding information, in- cluding the context vector c i , the previous decod- ing state s i−1 and the previous generated word y i−1 :</p><formula xml:id="formula_7">λ i = σ(f b (s i , y i−1 , c i ))<label>(8)</label></formula><p>where σ(·) is a sigmoid function and f b (·) is the activation function. Intuitively, the weight λ can be treated as the estimated importance of the phrase to be generated. We expect λ to be high if the phrase is appropriate at the current decoding step.</p><p>Well-Formed Phrases We employ a source- side chunker to chunk the source sentence, and only phrases that corresponds to a source chunk are used in our model. We restrict ourselves to the well-formed chunk phrases based on the fol- lowing considerations: (1) In order to take ad- vantage of dynamic programming, we restrict our- selves to non-overlap phrases. 1 (2) We explicitly utilize the boundary information of the source-side chunk phrases, to better guide the proposed model to adopt a target phrase at an appropriate decoding step. (3) We enable the model to exploit the syn- tactic categories of chunk phrases to enhance the proposed model with its selection preference for special target phrases. With these information, we enrich the context vector c i to enable the proposed model to make better decisions, as described be- low.</p><p>Following the commonly-used strategy in se- quence tagging tasks <ref type="bibr" target="#b41">(Xue and Shen, 2003)</ref>, we al- low the words in a phrase to share the same chunk tag and introduce a special tag for the beginning word. For example, the phrase " &amp;E S (in- formation security)" is tagged as a noun phrase "NP", and the tag sequence should be "NP B NP". Partially motivated by the work on integrating lin- guistic features into NMT (Sennrich and Haddow, 2016), we represent the encoder input as the com- bination of word embeddings and chunking tag embeddings, instead of word embeddings alone in the conventional NMT. The new input is formu- lated as follows:</p><formula xml:id="formula_8">[E w x i , E t t i ]<label>(9)</label></formula><p>where E w ∈ R dw×|V N M T | is a word embedding matrix and dw is the word embedding dimension- ality, E t ∈ R dt×|V T AG | is a tag embedding matrix and dt is the tag embedding dimensionality.</p><p>[·] is the vector concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phrase Memory</head><p>The phrase memory stores relevant target phrases provided by an SMT model, which is trained on the same bilingual corpora. At each decoding step, the memory is firstly erased and re-written by the SMT model, the decoding of which is based on the translation information provided by the NMT model. Then, the proposed model enquires phrases along with their probabilities P phrase from the memory.</p><p>Writing to Phrase Memory Given a partial translation y &lt;i = {y 1 , y 2 , . . . , y t−1 } gener- ated from NMT, the SMT model picks potential phrases extracted from the translation table. The phrases are scored with multiple SMT features, including the language model score, the trans- lation probabilities, the reordering score, and so on. Specially, the reordering score depends on alignment information between source and target words, which is derived from attention distribution produced by the NMT model ( <ref type="bibr" target="#b38">Wang et al., 2017)</ref>. SMT coverage vector in ( <ref type="bibr" target="#b38">Wang et al., 2017</ref>) is also introduced to avoid repeat phrasal recommen- dations. In our work, the potential phrase is phrase with high SMT score which is defined as follow- ing:</p><formula xml:id="formula_9">SM T score (p l |y &lt;t , x) = M m=1 w m h m (p l , x(p l ))<label>(10)</label></formula><p>where p l is a target phrase and x(p l ) is its cor- responding source span. h m (p l , x(p l )) is a SMT feature function and w m is its weight. The feature weights can be tuned by the minimum error rate training (MERT) algorithm <ref type="bibr" target="#b25">(Och, 2003)</ref>.</p><p>This leads to a better interaction between SMT and NMT models. It should be emphasized that our memory is dynamically updated at each de- coding step based on the decoding history from both SMT and NMT models.</p><p>The proposed model is very flexible, where the phrase memory can be either fully dynamically generated by an SMT model or directly extracted from a bilingual dictionary, or any other bilingual resources storing idiomatic translations or bilin- gual multi-word expressions, which may lead to a further improvement. <ref type="bibr">2</ref> Reading Phrase Memory When phrases are read from the memory, they are rescored by a neu- ral network based score function. The score func- tion takes as input the phrase itself and decoding information from NMT (i = t(p l ) denotes the cur- rent decoding step):</p><formula xml:id="formula_10">score phrase (p l ) = g s e(p l ), s i , y i−1 , c i<label>(11)</label></formula><p>where g s (·) is either an identity or a non-linear function. e(p l ) is the representation of phrase p l , which is modeled by a recurrent neural networks. Again, s i is the decoder state, y i−1 is the lastly generated word, and c i is the context vector. The scores are normalized for all phrases in the phrase memory, and the probability for phrase p l is calcu- lated as</p><formula xml:id="formula_11">P phrase (p l ) = sof tmax(score phrase (p l )) (12)</formula><p>The probability calculation is controlled with pa- rameters, which are trained together with the pa- rameters from the NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>Formally, we train both the default parameters of standard NMT and the new parameters associated with phrase generation on a set of training exam- ples {[x n , y n ]} N n=1 :</p><formula xml:id="formula_12">C(θ) = N n=1</formula><p>log P (y n |x n )</p><p>where P (y n |x n ) is defined in Equation 7. Ideally, the trained model is expected to produce a higher balance weight λ and phrase probability P phrase when a phrase is selected from the memory, and lower scores in other cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding</head><p>During testing, the NMT decoder generates a tar- get sentence which consists of a mixture of words and phrases. Due to the different granularities of words and phrases, we design a variant of beam search strategy: At decoding step i, we first com- pute P phrase for all phrases in the phrase memory and P word for all words in NMT vocabulary. Then the balancer outputs a balancing weight λ i , which is used to scale the phrase and word probabilities : λ i × P phrase and (1 − λ i ) × P word . Now outputs are normalized probabilities on the concatenation of phrase memory and the general NMT vocabu- lary. At last, the NMT decoder generates a proper phrase or word of the highest probability. If a target phrase in the phrase memory has the highest probability, the decoder generates the tar- get phrase to complete the multi-word phrase gen- eration process, and updates its decoding state by consuming the words in the selected phrase as de- scribed in Equation 3. All translation hypotheses are placed in the corresponding beams according to the number of generated target words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluated the effectiveness of our model on the Chinese→English machine translation task. The training corpora consisted of about 1.25 million sentence pairs <ref type="bibr">3</ref>  We compared our proposed model with two state-of-the-art systems:</p><p>* Moses: a state-of-the-art phrase-based SMT system ( <ref type="bibr" target="#b17">Koehn et al., 2007</ref>) with its default settings, where feature function weights are tuned by the minimum error rate training (MERT) algorithm <ref type="bibr" target="#b25">(Och, 2003)</ref>.</p><p>* RNNSearch: an in-house implementation of the attention-based NMT system (Bahdanau et al., 2015) with its default settings.</p><p>For Moses, we used the full bilingual train- ing data to train the phrase-based SMT model and the target portion of the bilingual training data to train a 4-gram language model using KenLM <ref type="bibr">5</ref> . We ran Giza++ on the training data in both Chinese-to-English and English-to-Chinese directions and applied the "grow-diag-final" re- finement rule ( <ref type="bibr" target="#b18">Koehn et al., 2003</ref>) to obtain word alignments. The maximum phrase length is set to 7. For RNNSearch, we generally followed settings in the previous work ( <ref type="bibr" target="#b2">Bahdanau et al., 2015;</ref><ref type="bibr">Tu et al., 2017a,b)</ref>. We only kept a shortlist of the most frequent 30,000 words in Chinese and En- glish, covering approximately 97.7% and 99.3% of the data in the two languages respectively. We constrained our source and target sequences to have a maximum length of 50 words in the train- ing data. The size of embedding layer of both sides was set to 620 and the size of hidden layer was set to 1000. We used a minibatch stochastic gradient descent (SGD) algorithm of size 80 together with Adadelta <ref type="bibr" target="#b42">(Zeiler, 2012)</ref> to train the NMT mod- els. The decay rates ρ and were set as 0.95 and 10 −6 . We clipped the gradient norm to 1.0 (Pas- canu et al., 2013). We also adopted the dropout technique. Dropout was applied only on the out- put layer and the dropout rate was set to 0.5. We used a simple beam search decoder with beam size 10 to find the most likely translation.</p><p>For the proposed model, we used a Chinese chunker 6 ( <ref type="bibr" target="#b46">Zhu et al., 2015</ref>) to chunk the source- side Chinese sentences. 13 chunking tags ap- peared in our chunked sentences and the size of chunking tag embedding was set to 10. We used the trained phrase-based SMT to translate the source-side chunks. The top 5 translations accord- ing to their translation scores (Equation 10) were kept and among them multi-word phrases were used as phrasal recommendations for each source chunk phrase. For a source-side chunk phrase, if there exists phrasal recommendations from SMT, the output chunk tag was used as its chunking tag feature as described in Section 3.1. Otherwise, the words in the chunk were treated as general words by being tagged with the default tag. In the phrase memory, we only keep the top 7 target translations with highest SMT scores at each decoding step. We used a forward neural network with two hid- den layers for both the balancer (Equation 8) and the scoring function (Equation 11). The numbers of units in the hidden layers were set to 2000 and 500 respectively. We used a backward RNN en- coder to learn the phrase representations of target phrases in the phrase memory.   <ref type="table">Table 2</ref>: Percentages of sentences that contain phrases generated by the proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Sentences Affected by Generated</head><p>Phrases We also check the number of transla- tions that contain phrases generated by the pro- posed model, as shown in <ref type="table">Table 2</ref>. As seen, a large portion of translations take the recom- mended phrases, and the number increases when the chunking tag feature is used. <ref type="bibr">7</ref> Considering BLEU scores reported in <ref type="table" target="#tab_2">Table 1</ref>, we believe that the chunking tag feature benefits the proposed model on its phrase generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis on Generated Phrases</head><p>Syntactic Categories of Generated Phrases We first investigate which category of phrases is more likely to be selected by the proposed approach. There are some phrases, such as <ref type="bibr">7</ref> The numbers on NIST08 are relatively lower since part of the test set contains sentences from Web forums, which contain less multi-word expressions.  <ref type="table">Table 3</ref>: Percentages of phrase categories to the total number of generated ones. "All" denotes all generated phrases, and "New" means new phrases that cannot be found in translations generated by the baseline system. "Total" is the total number of generated phrases and "Correct" denotes the fully correct ones.</p><p>noun phrases (NPs, e.g., "national laboratory" and "vietnam airlines") and quantifier phrases (QPs, e.g., "15 seconds" and "two weeks") , that we expect to be favored by our approach. Statistics shown in <ref type="table">Table 3</ref> confirm our hypothesis. Let's first concern all generated phrases (i.e., column "All"): most selected phrases are noun phrases (81.0%) and quantifier phrases (10.8%). Among them, 44.5% percent of them are fully correct 8 . Specifically, NPs have relative higher generation accuracy (i.e., 47.8% = 38.7%/81.0%) while VPs have lower accuracy (i.e., 21.2% = 1.7%/8.0%).</p><p>By looking into the wrong cases, we found most errors are related to verb tense, which is the draw- back of SMT models.</p><p>Concerning the newly introduced phrases that cannot be found in baseline translations (i.e., col- umn "New"), 13.2% of generated phrases are both new and fully correct, which contribute most to the performance improvement. We can also find that most newly introduced verb phrases and quantifier phrases are not correct, the patterns of which can be well learned by word-based NMT models. <ref type="table" target="#tab_2">All  New  Total Correct  Total Correct  2  66.2%  33.6% 34.9%  9.1%  3</ref> 20.7% 8.4% 13.4% 3.2% 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Words</head><p>7.4% 1.9% 5.4% 0.6% 5 5.7% 0.6% 5.2% 0.3% <ref type="table">Table 4</ref>: Percentages of phrases with different word counts to the total number of generated ones. <ref type="table">Ta- ble 4</ref> lists the distribution of generated phrases based on the number of inside words. As seen, most generated phrases are short phrases (e.g., 2- gram and 3-gram phrases), which also contribute most to the new and fully correct phrases (i.e., 12.3% = 9.1%+3.2%). Focusing on long phrases (e.g., order 4), most of them are newly intro- duced (10.6% out of 13.1%). Unfortunately, only a few portion of these phrases are fully correct, since long phrases have higher chance to contain one or two unmatched words.  <ref type="table">Table 5</ref>: Additional experiment results on the translation task to directly measure the im- provement obtained by the phrase generation. "+NULL" denotes that we replace the generated target phrases with a special symbol /NULL0 in test sets. BLEU scores in the table are case in- sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Words in Generated Phrases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SYSTEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Generated Phrases on Translation</head><p>Performance Note that the proposed model benefits not only from fully matched phrases, but also from partially matched phrases. For example, the baseline system translates " I[ Ê˜øõÝÊ˜ø Ê˜øõÝ" in a word-by-word manner and outputs "state aviation and space department". The gener- ated phrase provided by SMT is "national aviation and space administration", but the only correct ref- erence is "national aeronautics and space adminis- tration". The generated phrase is not fully correct but still useful.</p><p>To directly measure the improvement obtained by the phrase generation, we replace the generated target phrases with a special symbol "NULL" in test sets. As shown in <ref type="table">Table 5</ref>, when deleting the generated target phrases, ("+memory+chunking tag") and ("+memory") translation performances decrease by 2.74 BLEU points and 1.32 BLEU points respectively. Moreover, translation perfor- mances on NIST08 decrease less than those on NIST04 and NIST05 in both settings. The rea- son is that NIST08 which contains sentences from web data has little influence on generating target phrases which are provided from a different do- main <ref type="bibr">9</ref> . The overall results demonstrate that neu- ral machine translation benefits from phrase trans- lation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Balancer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight</head><p>Test Dynamic 33.55 Constant (λ = 0.1) 31.35 <ref type="table">Table 6</ref>: Translation performance with a variety of balancing weight strategies. "Dynamic" is the pro- posed approach and "Constant (λ = 0.1)" denotes fixing the balancing weight to 0.1. BLEU scores in the table are case insensitive.</p><p>The balancer which is used to coordinate the phrase generation and word generation is very cru- cial for the proposed model. We conducted an ad- ditional experiment to validate the effectiveness of the neural network based balancer. We use the set- ting "+memory +chunking tag" as baseline system to conduct the experiments. In this experiment, we fixed the balancing weight λ (Equation 8) to 0.1 during training and testing and report the re- sults. As shown in <ref type="table">Table 6</ref>, we find that using the fixed value for the balancing weight (Constant (λ = 0.1) ) decreases the translation performance sharply. This demonstrates that the neural network based balancer is an essential component for the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to Word-Level Recommendations and Discussions</head><p>Our approach is related to our previous work ( <ref type="bibr" target="#b38">Wang et al., 2017</ref>) which integrates the SMT word-level knowledge into NMT. To make a com- parison, we conducted experiments followed set- tings in ( <ref type="bibr" target="#b38">Wang et al., 2017</ref>). The comparison re- sults are reported in <ref type="table">Table 7</ref>. We find that our approach is marginally better than the word-level SYSTEM Test +word level recommendation 33.27 +memory +chunking tag 33.55 <ref type="table">Table 7</ref>: Experiment results on the translation task. "+word level recommendation" is the proposed model in ( <ref type="bibr" target="#b38">Wang et al., 2017)</ref>. BLEU scores in the table are case insensitive.</p><p>model proposed in ( <ref type="bibr" target="#b38">Wang et al., 2017</ref>) by 0.28 BLEU points. In our approach, the SMT model translates source-side chunk phrases using the NMT decod- ing information. Although we use high-quality target phrases as phrasal recommendations, our approach still suffers from the errors in segmenta- tion and chunking. For example, the target phrase "laptop computers" cannot be recommended by the SMT model if the Chinese phrase "Ã J &gt; M" is not chunked as a phrase unit. This is the rea- son why some sentences do not have correspond- ing phrasal recommendations <ref type="table">(Table 2)</ref>. There- fore, our approach can be further enhanced if we can reduce the error propagations from the seg- menter or chunker, for example, by using n-best chunk sequences instead of the single best chunk sequence.</p><p>Additionally, we also observe that some target phrasal recommendations have been also gener- ated by the baseline system in a word-by-word manner. These phrases, even taken as parts of fi- nal translations by the proposed model, do not lead to improvements in terms of BLEU as they have already occurred in translations from the baseline system. For example, the proposed model suc- cessfully carries out the phrase generation mode to generate a target phrase "guangdong province" (the translation of Chinese phrase "2 À Ž") which has appeared in the baseline system.</p><p>As external resources, e.g., bilingual dictionary, which are complementary to the SMT phrasal rec- ommendations, are compatible with the proposed model, we believe that the proposed model will get further improvement by using external resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Our work is related to the following research top- ics on NMT:</p><p>Generating phrases for NMT In these stud- ies, the generated NMT multi-word phrases are either from an SMT model or a bilingual dictio- nary. In syntactically guided neural machine trans- lation (SGNMT), the NMT decoder uses phrase translations produced by the hierarchical phrase- based SMT system Hiero, as hard decoding con- straints. In this way, syntactic phrases are gener- ated by the NMT decoder ( <ref type="bibr" target="#b31">Stahlberg et al., 2016b)</ref>. <ref type="bibr" target="#b43">Zhang and Zong (2016)</ref> use an SMT translation system, which is integrated an additional bilin- gual dictionary, to synthesize pseudo-parallel sen- tences and feed the sentences into the training of NMT in order to translate low-frequency words or phrases. <ref type="bibr" target="#b33">Tang et al. (2016)</ref> propose an external phrase memory that stores phrase pairs in sym- bolic forms for NMT. During decoding, the NMT decoder enquires the phrase memory and properly generates phrase translations. The significant dif- ferences between these efforts and ours are 1) that we dynamically generate phrase translations via an SMT model, and 2) that at the same time we modify the encoder to incorporate structural infor- mation to enhance the capability of NMT in phrase translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating linguistic information into NMT</head><p>NMT is essentially a sequence to sequence map- ping network that treats the input/output units, eg., words, subwords , charac- ters ( <ref type="bibr" target="#b7">Chung et al., 2016;</ref><ref type="bibr" target="#b9">Costa-jussà and Fonollosa, 2016)</ref>, as non-linguistic symbols. However, linguistic information can be viewed as the task- specific knowledge, which may be a useful sup- plementary to the sequence to sequence mapping network. To this end, various kinds of linguis- tic annotations have been introduced into NMT to improve its translation performance.  enrich the input units of NMT with various linguistic features, including lem- mas, part-of-speech tags, syntactic dependency la- bels and morphological features. <ref type="bibr" target="#b11">García-Martínez et al. (2016)</ref> propose factored NMT using the mor- phological and grammatical decomposition of the words (factors) in output units. <ref type="bibr" target="#b10">Eriguchi et al. (2016)</ref> explore the phrase structures of input sen- tences and propose a tree-to-sequence attention model for the vanilla NMT model.  propose to linearize source-side parse trees to obtain structural label sequences and explicitly incorporated the structural sequences into NMT, while <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> propose to incorporate target-side syntactic information into NMT by serializing the target sequences into lin- earized, lexicalized constituency trees.  integrate topic knowledge into NMT for domain/topic adaptation.</p><p>Combining NMT and SMT A variety of ap- proaches have been explored for leveraging the advantages of both NMT and conventional SMT. <ref type="bibr" target="#b14">He et al. (2016)</ref> integrate SMT features with the NMT model under the log-linear framework in or- der to help NMT alleviate the limited vocabulary problem ( <ref type="bibr" target="#b22">Luong et al., 2015;</ref><ref type="bibr" target="#b15">Jean et al., 2015)</ref> and coverage problem ( <ref type="bibr" target="#b36">Tu et al., 2016)</ref>. <ref type="bibr" target="#b1">Arthur et al. (2016)</ref> observe that NMT is prone to mak- ing mistakes in translating low-frequency content words and therefore attempt at incorporating dis- crete translation lexicons into the NMT model, to alliterate the imprecise translation problem ( <ref type="bibr" target="#b38">Wang et al., 2017)</ref>. Motivated by the complementary strengths of syntactical SMT and NMT, differ- ent combination schemes of Hiero and NMT have been exploited to form SGNMT ( <ref type="bibr">Stahlberg et al., 2016a,b)</ref>. <ref type="bibr" target="#b38">Wang et al. (2017)</ref> propose an approach to incorporate the SMT model into attention-based NMT. They combine NMT posteriors with SMT word recommendations through linear interpola- tion implemented by a gating function which dy- namically assigns the weights. <ref type="bibr" target="#b24">Niehues et al. (2016)</ref> propose to use SMT to pre-translate the in- puts into target translations and employ the target pre-translations as input sequences in NMT.  propose a neural system combination framework to directly combine NMT and SMT outputs. The combination of NMT and SMT has been also introduced in interactive machine trans- lation to improve the system's suggestion quality <ref type="bibr" target="#b40">(Wuebker et al., 2016)</ref>. In addition, word align- ments from the traditional SMT pipeline are also used to improve the attention mechanism in NMT ( <ref type="bibr" target="#b8">Cohn et al., 2016;</ref><ref type="bibr" target="#b23">Mi et al., 2016;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we have presented a novel model to translate source phrases and generate target phrase translations in NMT by integrating the phrase memory into the encoder-decoder architecture. At decoding, the SMT model dynamically generates relevant target phrases with contextual informa- tion provided by the NMT model and writes them to the phrase memory. Then the proposed model reads the phrase memory and uses the balancer to make probability estimations for the phrases in the phrase memory. Finally the NMT decoder selects a phrase from the phrase memory or a word from the vocabulary of the highest probability to gen- erate. Experiment results on Chinese→English translation have demonstrated that the proposed model can significantly improve the translation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the NMT decoder with the phrase memory. The NMT decoder performs phrase generation using the balancer and the phrase memory.</figDesc><graphic url="image-1.png" coords="3,71.55,62.81,219.17,186.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>SYSTEM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 reports</head><label>1</label><figDesc></figDesc><table>main results of different models 
measured in terms of BLEU score. We observe 
that our implementation of RNNSearch outper-
forms Moses by 2.34 BLEU points. (+memory) 
which is the proposed model with the phrase mem-
ory obtains an improvement of 0.47 BLEU points 
over the baseline RNNSearch. With the source-
side chunking tag feature, (+memory+chunking 
tag) outperforms the baseline RNNSearch by 1.07 
BLEU points, showing the effectiveness of chunk-
ing syntactic categories on the selection of ap-
propriate target phrases. From here on, we use 
"+memory+chunking tag" as the default setting in 
the following experiments if not otherwise stated. 

</table></figure>

			<note place="foot" n="1"> Overlapped phrases may result in a high dimensionality in translation hypothesis representation and make it hard to employ shared fragments for efficient dynamic programming.</note>

			<note place="foot" n="2"> Bilingual resources can be utilized in two ways: First, we can store the bilingual resources in a static memory and keep all items available to NMT in the whole decoding period. Second, we can integrate the bilingual resources into SMT and then dynamically feed them into the phrase memory.</note>

			<note place="foot" n="3"> The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 4 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 5 https://kheafield.com/code/kenlm/</note>

			<note place="foot" n="6"> http://www.niuparser.com/</note>

			<note place="foot" n="8"> Fully correct means that the generated phrases can be retrieved in corresponding references as a whole unit.</note>

			<note place="foot" n="9"> The parallel training data are mainly from news domain.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank three anonymous review-ers for their insightful comments, and also ac-knowledge Zhengdong Lu, Lili Mou for useful discussions. This work was supported by the National Natural Science Foundation of China (Grants No.61525205, 61373095 and 61622209).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04743</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on EMNLP</title>
		<meeting>the 2016 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACL</title>
		<meeting>the 43rd ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<title level="m">Natural language understanding with distributed representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on EMNLP</title>
		<meeting>the 2014 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Character-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonollosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="357" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04621</idno>
		<title level="m">Factored neural machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequence-to-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with smt features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 30th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL and the 7th IJCNLP</title>
		<meeting>the 53rd ACL and the 7th IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on EMNLP</title>
		<meeting>the 2013 Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th ACL Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th ACL Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 NAACL</title>
		<meeting>the 2003 NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01020</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3093" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL and the 7th IJCNLP</title>
		<meeting>the 53rd ACL and the 7th IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pre-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st ACL</title>
		<meeting>the 41st ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiword expressions: A pain in the neck for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Ivan A Sag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Copestake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flickinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural machine translation by minimising the bayes-risk with respect to syntactic translation lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03791</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural machine translation with external phrase memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Lh</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01792</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on multiword expressions: Having a crack at a hard nut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aline</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="377" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural machine translation advised by statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 31th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Models and inference for prefix-constrained machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chinese word segmentation as lmr tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libin</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sapporo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="176" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bridging neural machine translation and bilingual dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07272</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Topic-informed neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1807" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06393</idno>
		<title level="m">Neural system combination for machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Niuparser: A chinese syntactic and semantic parsing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-IJCNLP 2015 System Demonstrations</title>
		<meeting>ACL-IJCNLP 2015 System Demonstrations<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
