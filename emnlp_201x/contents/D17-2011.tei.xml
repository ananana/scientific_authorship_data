<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">QUINT: Interpretable Question Answering over Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishiraj</forename><surname>Saha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
							<email>myahya6@bloomberg.net</email>
							<affiliation key="aff1">
								<orgName type="department">Bloomberg L.P</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep2">Saarland Informatics Campus</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">QUINT: Interpretable Question Answering over Knowledge Bases</title>
					</analytic>
					<monogr>
						<title level="j" type="main">EMNLP System Demonstrations</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="61" to="66"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present QUINT, a live system for question answering over knowledge bases. QUINT automatically learns role-aligned utterance-query templates from user questions paired with their answers. When QUINT answers a question, it visualizes the complete derivation sequence from the natural language utterance to the final answer. The derivation provides an explanation of how the syntactic structure of the question was used to derive the structure of a SPARQL query, and how the phrases in the question were used to instantiate different parts of the query. When an answer seems unsatisfactory, the derivation provides valuable insights towards refor-mulating the question.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Motivation. A KB-QA system takes a natural lan- guage utterance as input and produces one or more crisp answers as output ( <ref type="bibr" target="#b1">Bast and Haussmann, 2015;</ref><ref type="bibr" target="#b2">Berant et al., 2013;</ref><ref type="bibr" target="#b10">Reddy et al., 2014;</ref><ref type="bibr" target="#b17">Yih et al., 2015)</ref>. This is usually done through seman- tic parsing: translating the utterance to a formal query in a language such as SPARQL, and execut- ing this query over a KB like <ref type="bibr">Freebase (Bollacker et al., 2008</ref>) or YAGO ( <ref type="bibr" target="#b11">Suchanek et al., 2007)</ref> to return one or more answer entities.</p><p>In addition to answering questions, a KB-QA system should ideally be able to explain how an answer was derived i.e., how the system under- stood the users' questions. While rapid progress is being made on the KB-QA task, the quality of an- swers obtained from KB-QA systems are far from perfect. This is due to a combination of factors related to the ambiguity of natural language, the underlying data (e.g., KB incompleteness, gaps in lexicon coverage) and the KB-QA systems them- selves (e.g., errors in named entity recognition and disambiguation, query ranking). Explanations help address this gap in two ways: (i) helping users gain confidence when correct answers are re- turned, and (ii) making sense of the limitations of the system by looking at explanations for wrong answers, possibly providing cues to work around them. For an expert user, explanations also con- tribute to traceability: identifying the exact point of failure in the KB-QA system pipeline, which can be used for subsequent debugging.</p><p>In this work, we demonstrate QUINT ( <ref type="bibr" target="#b0">Abujabal et al., 2017)</ref>, a state-of-the-art KB-QA system that gives step-by-step explanations of how it derives answers for questions. Furthermore, when QUINT is unable to link a specific phrase in the question to a KB item, it asks the user to reformulate the phrase. Such reformulations can be used to im- prove various components in the KB-QA pipeline such as underlying lexicons. QUINT takes the first step towards enabling interactive QA in the future, where the system can ask the user about parts of the question that it is unsure about.</p><p>Example. Take the question "Where was Mar- tin Luther raised?": QUINT returns Eisleben in Germany as the top answer.</p><p>A quick look by the user at the derivation reveals that (i) 'Martin Luther' was mapped to the KB entity MartinLuther, the theologist, and (ii) 'raised' was interpreted as the KB predicate placeOfBirth. For (i), if the user had intended the US activist MartinLutherKing instead, a sim- ple reformulation with "martin luther king" in the input returns Atlanta, the US city where Luther King was born. On the other hand, for (ii), if the birthplace was not the specific intent, a quick rephrasing of the question to "Where did Martin Luther live?" results in Saxony-Anhalt, which is derived from the predicate placesLived.</p><p>Motivated by the need for interpretable question answering, QUINT's approach to KB-QA relies on role-aligned templates, where each template consists of an utterance template based on a de- pendency parse pattern and a corresponding query template based on the SPARQL query language. The template (i) specifies how to chunk an utter- ance into phrases, (ii) guides how these phrases map to KB primitives by specifying their seman- tic roles as predicates, entities, or types, and (iii) aligns syntactic structure in the utterance to the se- mantic predicate-argument structure of the query. Limitations of past work. Prior template-based approaches rely on a set of manually defined rules or templates to handle user questions <ref type="bibr" target="#b2">(Berant et al., 2013;</ref><ref type="bibr" target="#b7">Fader et al., 2013</ref><ref type="bibr" target="#b8">Fader et al., , 2014</ref><ref type="bibr" target="#b12">Unger et al., 2012;</ref><ref type="bibr" target="#b14">Yahya et al., 2013;</ref><ref type="bibr" target="#b16">Yao and Durme, 2014;</ref><ref type="bibr" target="#b18">Zou et al., 2014</ref>). The main drawback of these approaches is the limited coverage of templates, making them brittle when it comes to unconventional question formulations. In con- trast, QUINT automatically learns templates from question-answer pairs.</p><p>Embedding-based methods ( <ref type="bibr" target="#b4">Bordes et al., 2014;</ref><ref type="bibr" target="#b6">Dong et al., 2015;</ref><ref type="bibr" target="#b15">Yang et al., 2014;</ref><ref type="bibr" target="#b13">Xu et al., 2016</ref>) map questions, KB entities, and subgraphs to a shared space for KB-QA without explic- itly generating a semantic representation. This makes it difficult for such systems to generate fine- grained explanations to users.</p><p>Other approaches to KB-QA ( <ref type="bibr" target="#b1">Bast and Haussmann, 2015;</ref><ref type="bibr" target="#b17">Yih et al., 2015</ref>) over-generate query candidates for a given utterance with no fine-grained alignments to map natural language phrases in a question onto different KB items, making explainability challenging. Contribution. The key contribution of this demo paper is a live online KB-QA system that visu- alizes the derivation steps for generating an an- swer, and thus takes the first steps towards ex- plainable question-answering. The demo is avail- able at the following URL: https://gate. d5.mpi-inf.mpg.de/quint/quint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">QUINT</head><p>We now give a brief overview of QUINT ( <ref type="bibr" target="#b0">Abujabal et al., 2017)</ref>, the KB-QA system driving our demonstration. QUINT has a training phase   <ref type="figure">Figure 1</ref>: Overview of QUINT.</p><p>for automatically learning templates and a query ranking function, and an answering phase where templates are used to instantiate queries that are ranked by the learned function. <ref type="figure">Figure 1</ref> shows a block diagram for QUINT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training phase</head><p>The input to QUINT's training phase is a set of natural language utterances u ∈ U and the corresponding gold answer set A u from a KB such as the one shown in <ref type="figure" target="#fig_1">Figure 2</ref>. An ex- ample of a training utterance is u ="Where was Obama educated?", which is paired with the answer set A u = {ColumbiaUniversity, HarvardUniversity, PunahouSchool}.</p><p>First, entity mentions in each utterance u are detected and disambiguated to Freebase entities using the AIDA system <ref type="bibr" target="#b9">(Hoffart et al., 2011</ref>).</p><p>Next, QUINT heuristically constructs a query to capture the question, the guiding intuition be- ing that the correct query connects entities in a question u to an answer entity a ∈ A u . To do this, QUINT starts by finding, for each answer en- tity a ∈ A u , the smallest subgraph in the KB that contains all the entities detected in u and a (black nodes in <ref type="figure" target="#fig_1">Fig. 2</ref> for a = ColumbiaUniversity). This subgraph is then extended by augmenting it with all type nodes connected to a (gray nodes Organization and EducationalInstitution in <ref type="figure" target="#fig_1">Fig. 2</ref>). This subgraph is transformed into a back- bone queryˆqqueryˆ queryˆq by replacing the answer node (a) and any cvt nodes with distinct variables (cvt nodes are used to represent n-ary relations in Freebase). The resultingˆqresultingˆ resultingˆq is shown in <ref type="figure">Figure 3</ref>. This is fol- lowed by aligning the components ofˆqofˆ ofˆq and the de- pendency parse of u. The alignment problem is formulated as a constrained optimization and the best alignment m is obtained using Integer Linear Programming (ILP). To connect natural language phrases in a ques- tion to KB items, QUINT uses two kinds of weighted lexicons, a predicate lexicon L P and a type lexicon L C . Entities, as mentioned above, are dealt with using an off-the-shelf named en- tity recognition and disambiguation system. The output of the ILP solver tells us which tokens in u are used to instantiate which KB items in the backbone queryˆqqueryˆ queryˆq. Nodes in the dependency parse of u as well as nodes and edges inˆqinˆ inˆq that are not part of the alignment m are removed from the de- pendency parse of u andˆqandˆ andˆq, respectively. In our running example, this results in dropping the node EducationalInstitution fromˆqfromˆ fromˆq <ref type="figure">(Fig. 3</ref>). The obtained alignment is then generalized by drop- ping concrete values in both u andˆqandˆ andˆq, which are referred to as an utterance template u t and a query template q t , respectively. This role-aligned tem- plate pair of (u t , q t ) is added to a template repos- itory T . The process is repeated for each train- ing instance (u, A u ). However, since several ut- terances are likely to have similar syntactic struc- ture, the number of templates |T | |U |. <ref type="figure">Figure 4</ref> shows the generated template from this instance.</p><p>Finally, as part of the training phase, QUINT trains a ranking function to rank the queries gener- ated from matching a question against the template repository T . A learning-to-rank framework with a random forest classifier ( <ref type="bibr" target="#b1">Bast and Haussmann, 2015</ref>) is used to model a preference function for a pair of queries, given an input utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Answering phase</head><p>When the trained system receives a new utterance u from the user, the dependency parse of u is matched against the utterance templates in T . For every match, the paired query template is instanti- ated using the alignment information together with the underlying lexicons. Thus, a set of candidate queries are obtained which are then ranked using the learning-to-rank framework. Finally, the an- swer of the top-ranked query is shown to the user. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Demonstration</head><p>We now give a walkthrough of our demonstra- tion which shows QUINT's ability to explain its answers to both normal and technically proficient users through brief and detailed explanations, re- spectively. We use the question "Where was Mar- tin Luther raised?" to drive this section. • Whether to add answer type to queries: most KB-QA systems do not capture fine-grained types, while QUINT does so by design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question and Answers</head><p>• The number of top-ranked queries to show an- swers for.</p><p>• The number of decision trees for the learning- to-rank module: this is used for query ranking during the answering phase (Sec. 2.2).</p><p>The answers to each of the top-k ranked queries </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brief Explanation</head><p>Figure 6: Retrieved answers from the KB.</p><p>in response to a question are given in individual tabs, as shown in <ref type="figure">Figure 6</ref>. To give more context to the user, wherever applicable, each answer is accompanied by a picture, a Wikipedia link and a short description from Freebase. For our run- ning example question, the answer is Eisleben, where Martin Luther was born, for the best query. If we explore the rank-2 query, the answer is Saxony-Anhalt, which is the province in Ger- many where he lived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explanations</head><p>Explanations show how the given answers were derived. The ability to generate and display ex- planations is the core contribution of this system demonstration. QUINT generates two types of ex- planations: (i) a brief explanation geared towards non-technical users and (ii) a detailed explanation geared towards more advanced users. Brief explanations provide an accessible and quick way for users to see how QUINT understood various parts of the question, resulting in the given set of answers. <ref type="figure" target="#fig_4">Figure 7</ref> shows such an explana- tion for our running example. Brief explanations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Brief Explanation</head><p>QUINT understood your question as follows:</p><p>• The phrase "martin luther" is interpreted as Martin Luther • The words "was, raised" are interpreted as the relation Place of birth are particularly useful for normal users who are interested in confirming whether a given answer comes from interpreting the question as they in- tended. Where this is not the case, such an expla- nation guides users in reformulating their question to allow QUINT to better understand it.</p><p>Detailed explanations are geared towards ad- vanced users who are familiar with dependency parses and SPARQL queries. A detailed expla- nation shows the derivation steps which roughly correspond to the right hand side of the diagram in <ref type="figure">Figure 1</ref>. First, the dependency parse of the in- put question is shown <ref type="figure">(Fig. 8)</ref>. Below that is the matching template consisting of an utterance tem- plate that fits the utterance, and the correspond- ing query template that will be instantiated to gen- erate a query. Shared numbers between the ut- terance template and the query template indicate alignment, i.e., which tokens in the utterance are used to instantiate which KB items in the query. In this example, we can see that the verb phrase 'was raised' and the noun phrase 'Martin Luther' are used to instantiate the KB-predicate and the KB-entity, respectively.</p><p>For a user to understand why QUINT maps a certain syntactic structure to a certain semantic form in a template, the user can view some train- ing instances that produced the template. This is achieved by clicking Template Provenance <ref type="figure">(Fig.  9</ref>). For our example, two such instances are the questions "Where was Obama educated?" and "Where are Riddell Helmets manufactured?".</p><p>Finally, the user is shown the SPARQL query that was executed to retrieve the shown answers <ref type="figure">(Fig. 10)</ref>. The node labeled ?ANS is the tar- get answer node, and nodes marked as ?VAR are intermediate join variables. Additionally, an alignment table with KB items is shown based on the alignments induced from the template (lower part of <ref type="figure">Fig. 10)</ref>. For KB items, we find that 'Martin Luther' was mapped to the theologist MartinLuther (other possibilities in- clude the German diplomat with the same name and MartinLutherKing), and the lemmatized phrase 'be raise' was mapped to the KB-predicate placeOfBirth. Looking at the rank-2 query, we find that 'raise' was interpreted as placesLived in the KB. This is an alternative intention when the slightly ambiguous phrase 'raised' is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Use Cases</head><p>When QUINT returns the correct answer, expla- nations allow the user to confirm its correctness by seeing how it was derived. When, on the other hand, things go wrong and an answer seems in- correct, explanations can give the user a better un- derstanding of the limitations of the system (e.g., data incompleteness), and insights into overcom- ing these limitations (e.g., question rephrasing). We discuss some of these possible scenarios be- low, accompanied by real examples. Incomplete lexicon. QUINT uses different lex- icons to map the phrases in an utterance onto KB items (Sec. 2). These lexicons are inherently in- complete. For example, QUINT could not cor- rectly answer the utterance "countries of euro- union" (implying the European Union) since the phrase 'euro-union' does not have an entry in QUINT's lexicons. Therefore it shows the fol- Training Utterance: where was obama educated? lowing message urging the user to reformulate the phrase: The phrase euro-union in your input ques- tion could not be interpreted. Please reformulate the phrase. Incorrect query ranking. QUINT uses a learning-to-rank framework for ranking SPARQL queries. Sometimes, the most appropriate query is not ranked at the top. For example, for the utterance "What is the former currency of Ger- many?", the top answer is Euro, which is incor- rect. This is because the alignment information in the matching template fails to capture 'former'. However, if the user explores the top-5 SPARQL queries, she finds that a query with the correct predicate currencyFormerlyUsed (as opposed to the incorrect currencyUsed) is indeed there and retrieves the desired answers DeutscheMark, GermanRentenmark and GermanPapiermark.</p><p>Incorrect disambiguation to KB items. Sometimes, the phrases in the input utterance are linked to wrong KB items as originally intended by the user. For example, for the utterance "What language group does English belong to?", 'En- glish' gets mapped to the wrong entity England, resulting in an unexpected answer. When the user sees the brief explanation <ref type="figure" target="#fig_4">(Fig. 7)</ref>, she can immediately observe this incorrect mapping. Subsequently, she may rephrase it as 'English language' which may produce the desired answer.</p><p>Missed answer type constraints. Answer typ- ing plays an important role in ensuring precise QA. For example, in "Which college did Michelle Obama go to?", the user explicitly specifies that she is looking for colleges, as opposed to "Where did Michelle Obama study?", which, for exam- ple, could include her high school as well. Here 'college' is mapped to the KB type University and only when this constraint is added to the SPARQL query do we get the desired answer set HarvardLawSchool and PrincetonUniversity.</p><p>No matching templates. Sometimes an utter- ance is syntactically out of scope for QUINT: it has never seen similar instances during training. For example: "Germany's capital during the for- ties?". In such cases, QUINT raises the message We could not find any matching templates. Please reformulate your question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a demonstration of QUINT, a sys- tem that uses automatically learned templates to provide interpretable answers to natural language questions over knowledge bases. When a user gets an answer to her question, our demonstration al- lows her to view the details of how the answer was derived. This improves her confidence in case of correct answers, while giving her a better under- standing of the limitations of the QA system in case of mistakes, and how to work around them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Role</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example KB fragment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Backbone queryˆqqueryˆ queryˆq generated from the utterance "Where was Obama educated?" and the answer entity ColumbiaUniversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 shows</head><label>5</label><figDesc>Figure 5 shows the main window of QUINT where the starting point for a user is the question box or one of the sample questions provided. An expert user can make several configuration choices (Fig. 5, bottom right): • Model to load: a model includes a set of templates learned offline, and a corresponding learned query ranking function. The choices correspond to the training part of the WebQuestions (Berant et al., 2013) and Free917 (Cai and Yates, 2013) datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: QUINT's brief explanation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>shared numbers indicate alignment among utterance template's nodes and query template's nodes and edges W/VERB- 2 auxpassFigure 8 :</head><label>28</label><figDesc>Figure 8: Question dependency parse and matched utterance and query template.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Structurally similar training instances to "Where was Martin Luther raised?".</figDesc></figure>

			<note place="foot" n="2"> Work done while at the Max Planck Institute for Informatics</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated template generation for question answering over knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirek</forename><surname>Riedewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">More accurate question answering on freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elmar</forename><surname>Haussmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Question answering with subgraph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing via schema matching and lexicon extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqing</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multicolumn convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Paraphrase-driven learning for open question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Open Question Answering over Curated and Extracted Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust disambiguation of named entities in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale semantic parsing without questionanswer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Yago: A core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Template-based question answering over RDF data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenz</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Question Answering on Freebase via Relation Extraction and Textual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust question answering over the web of linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Yahya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natural language question answering over RDF: a graph data driven approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Xu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
