<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2830</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Bekoulis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IDLab Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IDLab Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IDLab Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IDLab Department of Information Technology</orgName>
								<orgName type="institution">Ghent University -imec</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial training for multi-context joint entity and relation extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2830" to="2836"/>
							<date type="published">October 31-November 4, 2018. 2018. 2830</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Adversarial training (AT) is a regularization method that can be used to improve the ro-bustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular , we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many neural network methods have recently been exploited in various natural language processing (NLP) tasks, such as parsing ( <ref type="bibr" target="#b4">Zhang et al., 2017)</ref>, POS tagging ( <ref type="bibr">Lample et al., 2016)</ref>, relation extrac- tion (dos <ref type="bibr">Santos et al., 2015)</ref>, translation <ref type="bibr">(Bahdanau et al., 2015)</ref>, and joint tasks <ref type="bibr">(Miwa and Bansal, 2016</ref>). However, <ref type="bibr" target="#b1">Szegedy et al. (2014)</ref> observed that intentional small scale perturbations (i.e., adversarial examples) to the input of such models may lead to incorrect decisions (with high confidence). <ref type="bibr">Goodfellow et al. (2015)</ref> proposed adversarial training (AT) (for image recognition) as a regularization method which uses a mixture of clean and adversarial examples to enhance the robustness of the model. Although AT has recently been applied in NLP tasks (e.g., text classifica- tion ( <ref type="bibr">Miyato et al., 2017)</ref>), this paper -to the best of our knowledge -is the first attempt investigat- ing regularization effects of AT in a joint setting for two related tasks.</p><p>We start from a baseline joint model that per- forms the tasks of named entity recognition (NER) and relation extraction at once. Previously pro- posed models (summarized in Section 2) exhibit several issues that the neural network-based base- line approach (detailed in Section 3.1) overcomes: (i) our model uses automatically extracted features without the need of external parsers nor manually extracted features (see <ref type="bibr">Gupta et al. (2016)</ref>; <ref type="bibr">Miwa and Bansal (2016)</ref>; <ref type="bibr">Li et al. (2017)</ref>), (ii) all enti- ties and the corresponding relations within the sen- tence are extracted at once, instead of examining one pair of entities at a time (see <ref type="bibr">Adel and Schütze (2017)</ref>), and (iii) we model relation extraction in a multi-label setting, allowing multiple relations per entity (see <ref type="bibr">Katiyar and Cardie (2017)</ref>; <ref type="bibr">Bekoulis et al. (2018a)</ref>). The core contribution of the paper is the use of AT as an extension in the training pro- cedure for the joint extraction task (Section 3.2).</p><p>To evaluate the proposed AT method, we per- form a large scale experimental study in this joint task (see Section 4), using datasets from different contexts (i.e., news, biomedical, real estate) and languages (i.e., English, Dutch). We use a strong baseline that outperforms all previous models that rely on automatically extracted features, achieving state-of-the-art performance (Section 5). Com- pared to the baseline model, applying AT during training leads to a consistent additional increase in joint extraction effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Joint entity and relation extraction: Joint mod- els ( <ref type="bibr">Li and Ji, 2014;</ref><ref type="bibr">Miwa and Sasaki, 2014</ref>) that are based on manually extracted features have been proposed for performing both the named en- tity recognition (NER) and relation extraction sub- tasks at once. These methods rely on the availabil- ity of NLP tools (e.g., POS taggers) or manually designed features leading to additional complex- ity. Neural network methods have been exploited to overcome this feature design issue and usu- ally involve <ref type="bibr">RNNs and CNNs (Miwa and Bansal, 2016;</ref><ref type="bibr" target="#b5">Zheng et al., 2017)</ref>. Specifically, Miwa and Bansal (2016) as well as <ref type="bibr">Li et al. (2017)</ref> ap- ply bidirectional tree-structured RNNs for differ- ent contexts (i.e., news, biomedical) to capture syntactic information (using external dependency parsers). <ref type="bibr">Gupta et al. (2016)</ref> propose the use of various manually extracted features along with RNNs. <ref type="bibr">Adel and Schütze (2017)</ref> solve the sim- pler problem of entity classification (EC, assum- ing entity boundaries are given), instead of NER, and they replicate the context around the entities, feeding entity pairs to the relation extraction layer. <ref type="bibr">Katiyar and Cardie (2017)</ref> investigate RNNs with attention without taking into account that relation labels are not mutually exclusive. Finally, <ref type="bibr">Bekoulis et al. (2018a)</ref> use LSTMs in a joint model for extracting just one relation at a time, but increase the complexity of the NER part. Our baseline model enables simultaneous extraction of multi- ple relations from the same input. Then, we fur- ther extend this strong baseline using adversarial training.</p><p>Adversarial training (AT) (Goodfellow et al., 2015) has been proposed to make classifiers more robust to input perturbations in the context of im- age recognition. In the context of NLP, several variants have been proposed for different tasks such as text classification ( <ref type="bibr">Miyato et al., 2017)</ref>, re- lation extraction ( <ref type="bibr" target="#b2">Wu et al., 2017)</ref> and POS tag- ging ( <ref type="bibr" target="#b3">Yasunaga et al., 2018)</ref>. AT is considered as a regularization method. Unlike other regu- larization methods (i.e., dropout <ref type="bibr" target="#b0">(Srivastava et al., 2014</ref>), word dropout ( <ref type="bibr">Iyyer et al., 2015)</ref>) that in- troduce random noise, AT generates perturbations that are variations of examples easily misclassified by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint learning as head selection</head><p>The baseline model, described in detail in <ref type="bibr">Bekoulis et al. (2018b)</ref>, is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. It aims to detect (i) the type and the boundaries of the en- tities and (ii) the relations between them. The in- put is a sequence of tokens (i.e., sentence) w = w 1 , ..., w n . We use character level embeddings to implicitly capture morphological features (e.g., prefixes and suffixes), representing each character by a vector (embedding). The character embed- dings are fed to a bidirectional LSTM (BiLSTM) to obtain the character-based representation of the word. We also use pre-trained word embeddings. Word and character embeddings are concatenated to form the final token representation, which is then fed to a BiLSTM layer to extract sequential information.</p><p>For the NER task, we adopt the BIO (Begin- ning, Inside, Outside) encoding scheme. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the B-PER tag is assigned to the beginning token of a 'person' (PER) entity. For the prediction of the entity tags, we use: (i) a softmax approach for the entity classification (EC) task (assuming entity boundaries given) or (ii) a CRF approach where we identify both the type and the boundaries for each entity. During decoding, in the softmax set- ting, we greedily detect the entity types of the to- kens (i.e., independent prediction). Although in- dependent distribution of types is reasonable for EC tasks, this is not the case when there are strong correlations between neighboring tags. For in- stance, the BIO encoding scheme imposes several constraints in the NER task (e.g., the B-PER and I- LOC tags cannot be sequential). Motivated by this intuition, we use a linear-chain CRF for the NER task ( <ref type="bibr">Lample et al., 2016)</ref>. For decoding, in the CRF setting, we use the Viterbi algorithm. Dur- ing training, for both EC (softmax) and NER tasks (CRF), we minimize the cross-entropy loss L NER . The entity tags are later fed into the relation ex- traction layer as label embeddings (see <ref type="figure" target="#fig_0">Fig. 1</ref>), as- suming that knowledge of the entity types is ben- eficial in predicting the relations between the in- volved entities.</p><p>We model the relation extraction task as a multi-label head selection problem <ref type="bibr">(Bekoulis et al., 2018b;</ref><ref type="bibr" target="#b4">Zhang et al., 2017</ref>). In our model, each word w i can be involved in multiple relations with other words. For instance, in the example il- lustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, "Smith" could be involved not only in a Lives in relation with the token "Cali- fornia" (head) but also in other relations simulta- neously (e.g., Works for, Born In with some corre- sponding tokens). The goal of the task is to predict for each word w i , a vector of headsˆyheadsˆ headsˆy i and the vec- tor of corresponding relationsˆrrelationsˆ relationsˆr i . We compute the score s(w j , w i , r k ) of word w j to be the head of w i given a relation label r k using a single layer neural network. The corresponding probability is defined as:</p><formula xml:id="formula_0">P(w j , r k | w i ; θ) = σ(s(w j , w i , r k )),</formula><p>where σ(.) is the sigmoid function. During train- ing, we minimize the cross-entropy loss L rel as:</p><formula xml:id="formula_1">n i=0 m j=0 − log P(y i,j , r i,j | w i ; θ)<label>(1)</label></formula><p>where m is the number of associated heads (and thus relations) per word w i . During decoding, the most probable heads and relations are selected us- ing threshold-based prediction. The final objective for the joint task is computed as L JOINT (w; θ) = L NER + L rel where θ is a set of parameters. In the case of multi-token entities, only the last token of the entity can serve as head of another token, to eliminate redundant relations. If an entity is not involved in any relation, we predict the auxiliary "N" relation label and the token itself as head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial training (AT)</head><p>We exploit the idea of AT ( <ref type="bibr">Goodfellow et al., 2015</ref>) as a regularization method to make our model robust to input perturbations. Specifically, we generate examples which are variations of the original ones by adding some noise at the level of the concatenated word representation ( <ref type="bibr">Miyato et al., 2017)</ref>. This is similar to the concept intro- duced by <ref type="bibr">Goodfellow et al. (2015)</ref> to improve the robustness of image recognition classifiers. We generate an adversarial example by adding the worst-case perturbation η adv to the original em- bedding w that maximizes the loss function:</p><formula xml:id="formula_2">η adv = argmax η≤ǫ L JOINT (w + η; ˆ θ)<label>(2)</label></formula><p>wherê θ is a copy of the current model parameters. Since Eq. (2) is intractable in neural networks, we use the approximation proposed in <ref type="bibr">Goodfellow et al. (2015)</ref> defined as:</p><formula xml:id="formula_3">η adv = ǫg/ g , with g = ∇ w L JOINT (w; ˆ θ),</formula><p>where ǫ is a small bounded norm treated as a hyperparameter. Similar to <ref type="bibr" target="#b3">Yasunaga et al. (2018)</ref>, we set ǫ to be α √ D (where D is the dimension of the embeddings). We train on the mixture of original and adversarial examples, so the final loss is computed as:</p><formula xml:id="formula_4">L JOINT (w; ˆ θ) + L JOINT (w + η adv ; ˆ θ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>We evaluate our models on four datasets, us- ing the code as available from our github code- base. <ref type="bibr">1</ref> Specifically, we follow the 5-fold cross- validation defined by Miwa and Bansal <ref type="formula" target="#formula_1">(2016)</ref>  We use three types of evaluation, namely: (i) S(trict): we score an entity as correct if both the entity boundaries and the entity type are correct (ACE04, ADE, CoNLL04, DREC), (ii) B(oundaries): we score an entity as correct if only the entity boundaries are correct while the entity type is not taken into account (DREC) and (iii) R(elaxed): a multi-token entity is considered correct if at least one correct type is assigned to the tokens comprising the entity, assuming that the   boundaries are known (CoNLL04), to compare to previous works. In all cases, a relation is consid- ered as correct when both the relation type and the argument entities are correct.   <ref type="bibr">Miwa and Bansal (2016)</ref>, who rely on NLP tools, the baseline performs within a reasonable margin (less than 1%) on the joint task. On the other hand, <ref type="bibr">Li et al. (2017)</ref> use the same model for the ADE biomedical dataset, where we report a 2.5% overall improvement. This indicates that NLP tools are not always accurate for various con- texts. For the CoNLL04 dataset, we use two eval- uation settings. We use the relaxed evaluation similar to <ref type="bibr">Gupta et al. (2016)</ref>; <ref type="bibr">Adel and Schütze (2017)</ref> on the EC task. The baseline model outper- forms the state-of-the-art models that do not rely on manually extracted features (&gt;4% improve- ment for both tasks), since we directly model the whole sentence, instead of just considering pairs of entities. Moreover, compared to the model of <ref type="bibr">Gupta et al. (2016)</ref> that relies on complex fea- tures, the baseline model performs within a margin of 1% in terms of overall F 1 score. We also re- port NER results on the same dataset and improve overall F 1 score with ∼1% compared to <ref type="bibr">Miwa and Sasaki (2014)</ref>, indicating that our automatically extracted features are more informative than the hand-crafted ones. These automatically extracted features exhibit their performance improvement mainly due to the shared LSTM layer that learns to automatically generate feature representations of entities and their corresponding relations within a single model. For the DREC dataset, we use two evaluation methods. In the boundaries evaluation, the baseline has an improvement of ∼3% on both tasks compared to <ref type="bibr">Bekoulis et al. (2018a)</ref>, whose quadratic scoring layer complicates NER. <ref type="table">Table 1</ref> and <ref type="figure">Fig. 2</ref> show the effectiveness of the adversarial training on top of the baseline model. In all of the experiments, AT improves the pre- dictive performance of the baseline model in the joint setting. Moreover, as seen in <ref type="figure">Fig. 2</ref>, the performance of the models using AT is closer to maximum even from the early training epochs. Specifically, for ACE04, there is an improvement in both tasks as well as in the overall F 1 perfor- mance (0.4%). For CoNLL04, we note an im- provement in the overall F 1 of 0.4% for the EC and 0.8% for the NER tasks, respectively. For the DREC dataset, in both settings, there is an overall improvement of ∼1%. <ref type="figure">Figure 2</ref> shows that from the first epochs, the model obtains its maximum performance on the DREC validation set. Finally, for ADE, our AT model beats the baseline F 1 by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Our results demonstrate that AT outperforms the neural baseline model consistently, consider- ing our experiments across multiple and more di- verse datasets than typical related works. The im-Giannis Bekoulis, Johannes Deleu, Thomas Demeester, and Chris Develder. 2018a. An attentive neural ar- chitecture for joint segmentation and parsing and its application to real estate ads. Expert Systems with Applications, 102:100-112. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our model for joint entity and relation extraction with adversarial training (AT) comprises (i) a word and character embedding layer, (ii) a BiLSTM layer, (iii) a CRF layer and (iv) a relation extraction layer. In AT, we compute the worst-case perturbations η of the input embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>Comparison of our method with the state- of-the-art in terms of F 1 score. The proposed mod- els are: (i) baseline, (ii) baseline EC (predicts only entity classes) and (iii) baseline (EC) + AT (reg- ularized by AT). The ✓and ✗ symbols indicate whether the models rely on external NLP tools. We include different evaluation types (S, R and B).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 shows our experimental results. The name of the dataset is presented in the first column while the models are listed in the second column. The proposed models are the following: (i) baseline: the baseline model shown in Fig. 1 with the CRF layer and the sigmoid loss, (ii) baseline EC: the proposed model with the softmax layer for EC, (iii) baseline (EC) + AT: the baseline regular- ized using AT. The final three columns present the F 1 results for the two subtasks and their av- erage performance. Bold values indicate the best results among models that use only automatically extracted features. For ACE04, the baseline outperforms Katiyar and Cardie (2017) by ∼2% in both tasks. This improvement can be explained by the use of: (i) multi-label head selection, (ii) CRF-layer and (iii) character level embeddings. Compared to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://github.com/bekou/multihead_ joint_entity_relation_extraction</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations<address><addrLine>Banff, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial training for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1778" to="1783" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust multilingual part-of-speech tagging via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans</title>
		<meeting>the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency parsing as head selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="665" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint entity and relation extraction based on a hybrid neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">257</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
