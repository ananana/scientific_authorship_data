<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Syntactic Structure with Invertible Neural Projections</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute School of Computer Science Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute School of Computer Science Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Language Technologies Institute School of Computer Science Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Syntactic Structure with Invertible Neural Projections</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1292" to="1302"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1292</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel gener-ative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction , and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data annotation is a major bottleneck for the appli- cation of supervised learning approaches to many problems. As a result, unsupervised methods that learn directly from unlabeled data are increasingly important. For tasks related to unsupervised syn- tactic analysis, discrete generative models have dominated in recent years -for example, for both part-of-speech (POS) induction <ref type="bibr" target="#b10">(Blunsom and Cohn, 2011;</ref><ref type="bibr" target="#b52">Stratos et al., 2016</ref>) and unsuper- vised dependency parsing (Klein and Manning,  (trained on one billion words with context window size equal to 1) and latent embeddings learned by our approach with a Markov-structured prior. Each node represents a word and is colored according to the most likely gold POS tag from the Penn Treebank (best seen in color).</p><p>2004; <ref type="bibr" target="#b17">Cohen and Smith, 2009;</ref><ref type="bibr" target="#b43">Pate and Johnson, 2016)</ref>. While similar models have had success on a range of unsupervised tasks, they have mostly ig- nored the apparent utility of continuous word rep- resentations evident from supervised NLP appli- cations ( <ref type="bibr" target="#b24">He et al., 2017;</ref><ref type="bibr" target="#b44">Peters et al., 2018)</ref>. In this work, we focus on leveraging and explicitly representing continuous word embeddings within unsupervised models of syntactic structure. Pre-trained word embeddings from massive un- labeled corpora offer a compact way of inject- ing a prior notion of word similarity into mod- els that would otherwise treat words as discrete, isolated categories. However, the specific prop- erties of language captured by any particular em- bedding scheme can be difficult to control, and, further, may not be ideally suited to the task at hand. For example, pre-trained skip-gram em- beddings ( ) with small con- text window size are found to capture the syntac- tic properties of language well ( <ref type="bibr" target="#b8">Bansal et al., 2014;</ref>. However, if our goal is to sepa- rate syntactic categories, this embedding space is not ideal -POS categories correspond to overlap-&lt; l a t e </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x i t s h a 1 _ b a s e 6 4 = " V w Y P d o e V j X k U U 9 1 2 k w m j d y m a S + E = " &gt; A A A C C H i c Z V B L T s M w F H T 4 l v I r s G Q T U S G x q K o E k I B d B R u W R R B a q Y 0 q x 3 V a q 3 Y c 2 S 9 I J e o J E F s 4 B y v E l l t w D G 6 A k 2 Z B 2 y d Z b z x v n j W e I O Z M g + P 8 W E v L K 6 t r 6 6 W N 8 u b W 9 s 5 u Z W / / U c t E E e o R y a V q B 1 h T z i L q A Q N O 2 7 G i W A S c t o L R T T Z v P V G l m Y w e Y B x T X + B B x E J G M B j q / r l 3 1 q t U n b q T l 7 0 I 3 A J U U V H N X u W 3 2 5 c k E T Q C w r H W H d e J w U + x A k Y 4 n Z S 7 i a Y x J i M 8 o B 0 D I y y o 9 t P c 6 s Q + N k z f D q U y J w I 7 Z / 9 v p F h o g W F o l F n T M 7 O M A S m 5 r h k V D E X W s m f y u x 6 L o B a I W i Z S O t R z R i C 8 9 F M W x Q n Q i E x 9 h A m 3 Q d p Z L H a f K U q A j w 3 A R D H z F Z s M s c I E T H h l k 5 E 7 n 8 g i 8 E 7 r V 3 X n 7 r z a u C 7 C K q F D d I R O k I s u U A P d o i b y E E E D 9 I r e 0 L v 1 Y n 1 Y n 9 b X V L p k F T s H a K a s 7 z / x k Z q y &lt; / l a t e x i t &gt;</head><formula xml:id="formula_0">v p F h o g W F o l F n T M 7 O M A S m 5 r h k V D E X W s m f y u x 6 L o B a I W i Z S O t R z R i C 8 9 F M W x Q n Q i E x 9 h A m 3 Q d p Z L H a f K U q A j w 3 A R D H z F Z s M s c I E T H h l k 5 E 7 n 8 g i 8 E 7 r V 3 X n 7 r z a u C 7 C K q F D d I R O k I s u U A P d o i b y E E E D 9 I r e 0 L v 1 Y n 1 Y n 9 b X V L p k F T s H a K a s 7 z / x k Z q y &lt; / l a t e x i t &gt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Syntax Model</head><p>Figure 2: Depiction of proposed generative model. The syntax model is composed of discrete random variables, zi. Each ei is a latent continuous embeddings sampled from Gaussian distribution conditioned on zi, while xi is the observed embedding, deterministically derived from ei. The left portion depicts how the neural projector maps the simple Gaussian to a more complex distribution in the output space. The right portion depicts two instantiations of the syntax model in our approach: one is Markov-structured and the other is DMV-structured. For DMV, ztree is the latent dependency tree structure.</p><p>ping interspersed regions in the embedding space, evident in <ref type="figure" target="#fig_1">Figure 1</ref>(a).</p><p>In our approach, we propose to learn a new latent embedding space as a projection of pre- trained embeddings (depicted in <ref type="figure" target="#fig_1">Figure 1</ref>(b)), while jointly learning latent syntactic structure - for example, POS categories or syntactic depen- dencies. To this end, we introduce a new gener- ative model (shown in <ref type="figure">Figure 2</ref>) that first gener- ates a latent syntactic representation (e.g. a de- pendency parse) from a discrete structured prior (which we also call the "syntax model"), then, conditioned on this representation, generates a se- quence of latent embedding random variables cor- responding to each word, and finally produces the observed (pre-trained) word embeddings by pro- jecting these latent vectors through a parameter- ized non-linear function. The latent embeddings can be jointly learned with the structured syntax model in a completely unsupervised fashion.</p><p>By choosing an invertible neural network as our non-linear projector, and then parameterizing our model in terms of the projection's inverse, we are able to derive tractable exact inference and marginal likelihood computation procedures so long as inference is tractable in the underlying syntax model. In §3.1 we show that this derivation corresponds to an alternate view of our approach whereby we jointly learn a mapping of observed word embeddings to a new embedding space that is more suitable for the syntax model, but include an additional Jacobian regularization term to pre- vent information loss.</p><p>Recent work has sought to take advantage of word embeddings in unsupervised generative models with alternate approaches ( <ref type="bibr" target="#b28">Jiang et al., 2016;</ref><ref type="bibr" target="#b23">Han et al., 2017)</ref>.  build an HMM with Gaus- sian emissions on observed word embeddings, but they do not attempt to learn new embeddings. , <ref type="bibr" target="#b28">Jiang et al. (2016)</ref>, and <ref type="bibr" target="#b23">Han et al. (2017)</ref> extend HMM or dependency model with valence (DMV) ( <ref type="bibr" target="#b34">Klein and Manning, 2004</ref>) with multinomials that use word (or tag) embeddings in their parameterization. However, they do not represent the embeddings as latent variables.</p><p>In experiments, we instantiate our approach us- ing both a Markov-structured syntax model and a tree-structured syntax model -specifically, the DMV. We evaluate on two tasks: part-of-speech (POS) induction and unsupervised dependency parsing without gold POS tags. Experimental re- sults on the Penn Treebank ( <ref type="bibr" target="#b39">Marcus et al., 1993)</ref> demonstrate that our approach improves the ba- sic HMM and DMV by a large margin, lead- ing to the state-of-the-art results on POS induc- tion, and state-of-the-art results on unsupervised dependency parsing in the difficult training sce- nario where neither gold POS annotation nor punctuation-based constraints are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>As an illustrative example, we first present a base- line model for Markov syntactic structure (POS in- duction) that treats a sequence of pre-trained word embeddings as observations. Then, we propose our novel approach, again using Markov structure, that introduces latent word embedding variables and a neural projector. Lastly, we extend our ap- proach to more general syntactic structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Example: Gaussian HMM</head><p>We start by describing the Gaussian hidden Markov model introduced by , which is a locally normalized model with multi- nomial transitions and Gaussian emissions. Given a sentence of length , we denote the latent POS tags as z = {z i } i=1 , observed (pre-trained) word embeddings as x = {x i } i=1 , transition parame- ters as θ, and Gaussian emission parameters as η. The joint distribution of data and latent variables factors as:</p><formula xml:id="formula_1">p(z, x; θ, η) = i=1 p θ (z i |z i−1 )p η (x i |z i ), (1)</formula><p>where p θ (z i |z i−1 ) is the multinomial transition probability and p η (x i |z i ) is the multivariate Gaus- sian emission probability.</p><p>While the observed word embeddings do inform this model with a notion of word similarity -lack- ing in the basic multinomial HMM -the Gaussian emissions may not be sufficiently flexible to sepa- rate some syntactic categories in the complex pre- trained embedding space -for example the skip- gram embedding space as visualized in <ref type="figure" target="#fig_1">Figure 1</ref>(a) where different POS categories overlap. Next we introduce a new approach that adds flexibility to the emission distribution by incorporating new la- tent embedding variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Markov Structure with Neural Projector</head><p>To flexibly model observed embeddings and yield a new representation space that is more suitable for the syntax model, we propose to cascade a neu- ral network as a projection function, deterministi- cally transforming the simple space defined by the Gaussian HMM to the observed embedding space. We denote the latent embedding of the i th word in a sentence as e i ∈ R de , and the neural projection function as f , parameterized by φ. In the case of sequential Markov structure, our new model cor- responds to the following generative process:</p><p>For each time step i = 1, 2, · · · , ,</p><formula xml:id="formula_2">• Draw the latent state z i ∼ p θ (z i |z i−1 ) • Draw the latent embedding e i ∼ N (µ z i , Σ z i ) • Deterministically produce embedding x i = f φ (e i )</formula><p>The graphical model is depicted in <ref type="figure">Figure 2</ref>. The deterministic projection can also be viewed as sampling each observation from a point mass at f φ (e i ). The joint distribution of our model is:</p><formula xml:id="formula_3">p(z, e, x; θ, η, φ) = i=1 [p θ (z i |z i−1 )p η (e i |z i )p φ (x i |e i )],<label>(2)</label></formula><p>where p η (·|z i ) is a conditional Gaussian distribu- tion, and p φ (x i |e i ) is the Dirac delta function cen- tered at f φ (e i ):</p><formula xml:id="formula_4">p φ (x i |e i ) = δ(x i −f φ (e i )) = ∞ x i = f φ (e i ) 0 otherwise (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">General Structure with Neural Projector</head><p>Our approach can be applied to a broad family of structured syntax models. We denote latent em- bedding variables as e = {e i } i=1 , discrete latent variables in the syntax model as z = {z k } K k=1 (K ), where z 1 , z 2 , . . . , z are conditioned to generate e 1 , e 2 , . . . , e . The joint probability of our model factors as:</p><formula xml:id="formula_5">p(z, e, x; θ, η, φ) = i=1 p η (e i |z i )p φ (x i |e i ) · p syntax (z; θ),<label>(4)</label></formula><p>where p syntax (z; θ) represents the probability of the syntax model, and can encode any syntactic structure -though, its factorization structure will determine whether inference is tractable in our full model. As shown in <ref type="figure">Figure 2</ref>, we focus on two syntax models for syntactic analysis in this paper. The first is Markov-structured, which we use for POS induction, and the second is DMV-structured, which we use to learn dependency parses without supervision.</p><p>The marginal data likelihood of our model is:</p><formula xml:id="formula_6">p(x) = z p syntax (z; θ) · i=1 e i p η (e i |z i )p φ (x i |e i )de i p(x i |z i ) .<label>(5)</label></formula><p>While the discrete variables z can be marginal- ized out with dynamic program in many cases, it is generally intractable to marginalize out the la- tent continuous variables, e i , for an arbitrary pro- jection f in Eq. <ref type="formula" target="#formula_6">(5)</ref>, which means inference and learning may be difficult. In §3, we address this issue by constraining f to be invertible, and show that this constraint enables tractable exact infer- ence and marginal likelihood computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning &amp; Inference</head><p>In this section, we introduce an invertibility con- dition for our neural projector to tackle the op- timization challenge. Specifically, we constrain our neural projector with two requirements: <ref type="formula" target="#formula_14">(1)</ref> dim(x) = dim(e) and (2) f −1 φ exists. Invert- ible transformations have been explored before in independent components analysis <ref type="bibr" target="#b26">(Hyvärinen et al., 2004</ref>), gaussianization <ref type="bibr" target="#b14">(Chen and Gopinath, 2001)</ref>, and deep density models ( <ref type="bibr" target="#b20">Dinh et al., 2014</ref><ref type="bibr">Dinh et al., , 2016</ref><ref type="bibr" target="#b31">Kingma and Dhariwal, 2018)</ref>, for unstruc- tured data. Here, we generalize this style of ap- proach to structured learning, and augment it with discrete latent variables (z i ). Under the invertibil- ity condition, we derive a learning algorithm and give another view of our approach revealed by the objective function. Then, we present the architec- ture of a neural projector we use in experiments: a volume-preserving invertible neural network pro- posed by <ref type="bibr" target="#b20">Dinh et al. (2014)</ref> for independent com- ponents estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning with Invertibility</head><p>For ease of exposition, we explain the learning algorithm in terms of Markov structure without loss of generality. As shown in Eq. <ref type="formula" target="#formula_6">(5)</ref>, the op- timization challenge in our approach comes from the intractability of the marginalized emission fac- tor p(x i |z i ). If we can marginalize out e i and compute p(x i |z i ), then the posterior and marginal likelihood of our Markov-structured model can be computed with the forward-backward algorithm. We can apply Eq. (3) and obtain :</p><formula xml:id="formula_7">p(x i |z i ; η, φ) = e i p η (e i |z i )δ(x i − f φ (e i ))de i .</formula><p>By using the change of variable rule to the integra- tion, which allows the integration variable e i to be replaced by x i = f φ (e i ), the marginal emission factor can be computed in closed-form when the invertibility condition is satisfied:</p><formula xml:id="formula_8">p(x i |z i ; η, φ) = x i p η (f −1 φ (x i )|z i )δ(x i − x i ) det ∂f −1 φ ∂x i dx i = p η (f −1 φ (x i )|z i ) det ∂f −1 φ ∂x i ,<label>(6)</label></formula><p>where p η (·|z) is a conditional Gaussian distribu- tion,</p><formula xml:id="formula_9">∂f −1 φ ∂x i</formula><p>is the Jacobian matrix of function f −1 φ at x i , and det</p><formula xml:id="formula_10">∂f −1 φ ∂x i</formula><p>represents the absolute value of its determinant. This Jacobian term is nonzero and differentiable if and only if f −1 φ exists. Eq. <ref type="formula" target="#formula_8">(6)</ref> shows that we can directly calculate the marginal emission distribution p(x i |z i ). Denote the marginal data likelihood of Gaussian HMM as p HMM (x), then the log marginal data likelihood of our model can be directly written as:</p><formula xml:id="formula_11">log p(x) = log p HMM (f −1 φ (x)) + i=1 log det ∂f −1 φ ∂x i ,<label>(7)</label></formula><p>where f −1 φ (x) represents the new sequence of em- beddings after applying f −1 φ to each x i . Eq. <ref type="formula" target="#formula_11">(7)</ref> shows that the training objective of our model is simply the Gaussian HMM log likelihood with an additional Jacobian regularization term. From this view, our approach can be seen as equivalent to reversely projecting the data through f −1 φ to an- other manifold e that is directly modeled by the Gaussian HMM, with a regularization term. In- tuitively, we optimize the reverse projection f −1 φ to modify the e space, making it more appropri- ate for the syntax model. The Jacobian regular- ization term accounts for the volume expansion or contraction behavior of the projection. Maximiz- ing it can be thought of as preventing information loss. In the extreme case, the Jacobian determi- nant is equal to zero, which means the projection is non-invertible and thus information is being lost through the projection. Such "information pre- serving" regularization is crucial during optimiza- tion, otherwise the trivial solution of always pro- jecting data to the same single point to maximize likelihood is viable. <ref type="bibr">2</ref> More generally, for an arbitrary syntax model the data likelihood of our approach is:</p><formula xml:id="formula_12">p(x) = z p syntax (z) · i=1 p η (f −1 φ (x i )|z i ) det ∂f −1 φ ∂x i .<label>(8)</label></formula><p>If the syntax model itself allows for tractable in- ference and marginal likelihood computation, the same dynamic program can be used to marginal- ize out z. Therefore, our joint model inherits the tractability of the underlying syntax model.</p><formula xml:id="formula_13">e i,l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R K 1 3 y 4 7 C c x d R 2 g l S f 1 N v W y E I b a U = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 2 L r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z / 2 1 5 8 K &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R K 1 3 y 4 7 C c x d R 2 g l S f 1 N v W y E I b a U = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 2 L r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z / 2 1 5 8 K &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R K 1 3 y 4 7 C c x d R 2 g l S f 1 N v W y E I b a U = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 2 L r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z / 2 1 5 8 K &lt; / l a t e x i t &gt;</head><p>e i,r <ref type="figure">Figure 3</ref>: Depiction of the architecture of the inverse pro- jection f −1 φ that composes multiple volume-preserving cou- pling layers, with which we parameterize our model. On the right, we schematically depict how the inverse projection transforms the observed word embedding xi to a point ei in a new embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L R 7 g 1 A / j B s o H t Y q P 1 J E u i v g H Q 4 = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 3 r r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z 8 A u J 8 Q &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L R 7 g 1 A / j B s o H t Y q P 1 J E u i v g H Q 4 = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 3 r r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z 8 A u J 8 Q &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I L R 7 g 1 A / j B s o H t Y q P 1 J E u i v g H Q 4 = " &gt; A A A C E n i c Z V D L S g M x F M 3 4 r P V V d e k m W A Q X p U x F U H d F N y 4 r O L b Q D i W T Z t r Q Z D I m d 4 Q y z G e I W / 0 O V + L W H / A z / A M z 0 y 5 s e y H c k 3 P P D S c n i A U 3 4 L o / z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h 8 N C r R l H l U C a U 7 A T F M 8 I h 5 w E G w T q w Z k Y F g 7 W B 8 m 8 / b z 0 w b r q I H m M T M l 2 Q Y 8 Z B T A p b y e 4 F M W d Z P e Q 3 r r F + p u n W 3 K L w M G j N Q R b N q 9 S u / v Y G i i W Q R U E G M 6 T b c G P y U a O B U s K z c S w y L C R 2 T I e t a G B H J j J 8 W p j N 8 a p k B D p W 2 J w J c s P 8 3 U i K N J D C y y r y Z u V n O g F L C 1 K w K R j J v + T P F 3 U x k U A t k L R d p E 5 o F I x B e + S m P 4 g R Y R K c + w k R g U D g P C A + 4 Z h T E x A J C N b d f w X R E N K F g Y y z b j B q L i S w D 7 7 x + X X f v L 6 r N m 1 l Y J X S M T t A Z a q B L 1 E R 3 q I U 8 R N E T e k V v 6 N 1 5 c T 6 c T + d r K l 1 x Z j t H a K 6 c 7 z 8 A u J 8 Q &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Invertible Volume-Preserving Neural Net</head><p>For the projection we can use an arbitrary invert- ible function, and given the representational power of neural networks they seem a natural choice. However, calculating the inverse and Jacobian of an arbitrary neural network can be difficult, as it requires that all component functions be invert- ible and also requires storage of large Jacobian matrices, which is memory intensive. To address this issue, several recent papers propose specially designed invertible networks that are easily train- able yet still powerful ( <ref type="bibr" target="#b20">Dinh et al., 2014</ref><ref type="bibr">Dinh et al., , 2016</ref><ref type="bibr" target="#b27">Jacobsen et al., 2018)</ref>. Inspired by these works, we use the invertible transformation proposed by <ref type="bibr" target="#b20">Dinh et al. (2014)</ref>, which consists of a series of "coupling layers". This architecture is specially designed to guarantee a unit Jacobian determinant (and thus the invertibility property).</p><p>From Eq. (8) we know that only f −1 φ is re- quired for accomplishing learning and inference; we never need to explicitly construct f φ . Thus, we directly define the architecture of f −1 φ . As shown in <ref type="figure">Figure 3</ref>, the nonlinear transformation from the observed embedding x i to h <ref type="bibr">(1)</ref> i represents the first coupling layer. The input in this layer is parti- tioned into left and right halves of dimensions, x i,l and x i,r , respectively. A single coupling layer is defined as:</p><formula xml:id="formula_14">h (1) i,l = x i,l , h<label>(1)</label></formula><formula xml:id="formula_15">i,r = x i,r + g(x i,l ),<label>(9)</label></formula><p>where g : R dx/2 → R dx/2 is the coupling func- tion and can be any nonlinear form. This transfor- mation satisfies dim(h (1) ) = dim(x), and <ref type="bibr" target="#b20">Dinh et al. (2014)</ref> show that its Jacobian matrix is tri- angular with all ones on the main diagonal. Thus the Jacobian determinant is always equal to one (i.e. volume-preserving) and the invertibility con- dition is naturally satisfied.</p><p>To be sufficiently expressive, we compose mul- tiple coupling layers as suggested in <ref type="bibr" target="#b20">Dinh et al. (2014)</ref>. Specifically, we exchange the role of left and right half vectors at each layer as shown in <ref type="figure">Figure 3</ref>. For instance, from x i to h i,r remains the same. Also note that composing multiple coupling layers does not change the volume-preserving and invertibility properties. Such a sequence of invertible transfor- mations from the data space x to e is also called normalizing flow <ref type="bibr" target="#b45">(Rezende and Mohamed, 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first describe our datasets and experimental setup. We then instantiate our ap- proach with Markov and DMV-structured syntax models, and report results on POS tagging and de- pendency grammar induction respectively. Lastly, we analyze the learned latent embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>For both POS tagging and dependency parsing, we run experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank. <ref type="bibr">3</ref> To create the ob- served data embeddings, we train skip-gram word embeddings ( ) that are found to capture syntactic properties well when trained with small context window ( <ref type="bibr" target="#b8">Bansal et al., 2014;</ref>. Following , the dimensionality d x is set to 100, and the training context window size is set to 1 to encode more syntactic information. The skip-gram embeddings are trained on the one billion word language mod- eling benchmark dataset ( <ref type="bibr" target="#b13">Chelba et al., 2013</ref>) in addition to the WSJ corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Experimental Setup</head><p>For the neural projector, we employ rectified net- works as coupling function g following <ref type="bibr" target="#b20">Dinh et al. (2014)</ref>. We use a rectified network with an input layer, one hidden layer, and linear output units, the number of hidden units is set to the same as the number of input units. The number of cou- pling layers are varied as 4, 8, 16 for both tasks.</p><p>We optimize marginal data likelihood directly us- ing Adam ( <ref type="bibr" target="#b30">Kingma and Ba, 2014</ref>). For both tasks in the fully unsupervised setting, we do not tune the hyper-parameters using supervised data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Unsupervised POS tagging</head><p>For unsupervised POS tagging, we use a Markov- structured syntax model in our approach, which is a popular structure for unsupervised tagging tasks ( ).</p><p>Setup. Following existing literature, we train and test on the entire WSJ corpus (49208 sen- tences, 1M tokens). We use 45 tag clusters, the number of POS tags that appear in WSJ cor- pus. We train the discrete HMM and the Gaus- sian HMM (  as baselines. For the Gaussian HMM, mean vectors of Gaussian emis- sions are initialized with the empirical mean of all word vectors with an additive noise. We assume diagonal covariance matrix for p(e i |z i ) and initial- ize it with the empirical variance of the word vec- tors. Following , the covariance matrix is fixed during training. The multinomial probabilities are initialized as θ kv ∝ exp(u kv ), where u kv ∼ U <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. For our approach, we initialize the syntax model and Gaussian param- eters with the pre-trained Gaussian HMM. The weights of layers in the rectified network are ini- tialized from a uniform distribution with mean zero and a standard deviation of 1/n in , where n in is the input dimension. <ref type="bibr">4</ref> We evaluate the per- formance of POS tagging with both Many-to-One (M-1) accuracy <ref type="bibr" target="#b29">(Johnson, 2007)</ref> and V-Measure (VM) <ref type="bibr" target="#b46">(Rosenberg and Hirschberg, 2007</ref>). Given a model we found that the tagging performance is well-correlated with the training data likelihood, thus we use training data likelihood as a unsuper- vised criterion to select the trained model over 10 random restarts after training 50 epochs. We re- peat this process 5 times and report the mean and standard deviation of performance.</p><p>Results. We compare our approach with ba- sic HMM, Gaussian HMM, and several state- of-the-art systems, including sophisticated HMM variants and clustering techniques with hand- engineered features. The results are presented in <ref type="table" target="#tab_0">Table 1</ref>. Through the introduced latent embed- dings and additional neural projection, our ap- proach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM. Neural HMM  (NHMM) ( ) is a baseline that also learns word representation jointly. Both their ba- sic model and extended Conv version does not outperform the Gaussian HMM. Their best model incorporates another LSTM to model long dis- tance dependency and breaks the Markov assump- tion, yet our approach still achieves substantial im- provement over it without considering more con- text information. Moreover, our method outper- forms the best published result that benefits from hand-engineered features (Yatbaz et al., 2012) by 2.0 points on VM.</p><p>Confusion Matrix. We found that most tagging errors happen in noun subcategories. Therefore, we do the one-to-one mapping between gold POS tags and induced clusters and plot the normalized confusion matrix of noun subcategories in <ref type="figure" target="#fig_4">Fig- ure 4</ref>. The Gaussian HMM fails to identify "NN" and "NNS" correctly for most cases, and it often recognizes "NNPS" as "NNP". In contrast, our ap- proach corrects these errors well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Unsupervised Dependency Parsing without gold POS tags</head><p>For the task of unsupervised dependency parse in- duction, we employ the Dependency Model with Valence (DMV) ( <ref type="bibr" target="#b34">Klein and Manning, 2004</ref>) as the syntax model in our approach. DMV is a genera- tive model that defines a probability distribution over dependency parse trees and syntactic cate- gories, generating tokens and dependencies in a head-outward fashion. While, traditionally, DMV is trained using gold POS tags as observed syntac- tic categories, in our approach, we treat each tag as a latent variable, as described in §2.3. Most existing approaches to this task are not fully unsupervised since they rely on gold POS tags following the original experimental setup for DMV. This is partially because automatically pars- ing from words is difficult even when using un- supervised syntactic categories ( <ref type="bibr" target="#b47">Spitkovsky et al., 2011a</ref>). However, inducing dependencies from words alone represents a more realistic exper- imental condition since gold POS tags are of- ten unavailable in practice. Previous work that has trained from words alone often requires ad- ditional linguistic constraints (like sentence inter- nal boundaries) ( <ref type="bibr" target="#b47">Spitkovsky et al., 2011a</ref><ref type="bibr">Spitkovsky et al., ,b, 2012</ref><ref type="bibr" target="#b50">Spitkovsky et al., , 2013</ref>, acoustic cues ( <ref type="bibr" target="#b42">Pate and Goldwater, 2013)</ref>, additional training data <ref type="bibr" target="#b43">(Pate and Johnson, 2016)</ref>, or annotated data from related languages <ref type="bibr" target="#b16">(Cohen et al., 2011</ref>). Our approach is naturally designed to train on word embeddings directly, thus we at- tempt to induce dependencies without using gold POS tags or other extra linguistic information.</p><p>Setup. Like previous work we use sections 02- 21 of WSJ corpus as training data and evaluate on section 23, we remove punctuations and train the models on sentences of length 10, "head- percolation" rules <ref type="bibr" target="#b18">(Collins, 1999</ref>) are applied to obtain gold dependencies for evaluation. We train basic DMV, extended DMV (E-DMV) <ref type="bibr" target="#b25">(Headden III et al., 2009)</ref> and Gaussian DMV (which treats POS tag as unknown latent variables and generates observed word embeddings directly conditioned on them following Gaussian distri- bution) as baselines. Basic DMV and E-DMV are trained with Viterbi EM (Spitkovsky et al., 2010) on unsupervised POS tags induced from our Markov-structured model described in §4.3. Multinomial parameters of the syntax model in both Gaussian DMV and our model are initial- ized with the pre-trained DMV baseline. Other System 10 all w/o gold POS tags DMV <ref type="bibr" target="#b34">(Klein and Manning, 2004)</ref> 49.6 35.8 E-DMV <ref type="bibr" target="#b25">(Headden III et al., 2009)</ref> 52.1 38.2 UR-A E-DMV ( <ref type="bibr" target="#b54">Tu and Honavar, 2012)</ref> 58.9 46.1 CS * ( <ref type="bibr" target="#b50">Spitkovsky et al., 2013)</ref> 72.0 * 64.4 * Neural E-DMV <ref type="bibr" target="#b28">(Jiang et al., 2016)</ref> 55.3 42.7 CRFAE <ref type="bibr" target="#b12">(Cai et al., 2017)</ref> 37.2 29.5 Gaussian DMV 55.4 (1.3) 43.1 (1.2) Ours <ref type="table" target="#tab_3">(4 layers)</ref> 58.4 (1.9) 46.2 (2.3) Ours <ref type="table">(8 layers)</ref> 60.2 (1.3) 47.9 (1.2) Ours (16 layers) 54.1 (8.5) 43.9 (5.7)</p><p>w/ gold POS tags (for reference only) DMV <ref type="bibr" target="#b34">(Klein and Manning, 2004)</ref> 55.1 39.7 UR-A E-DMV ( <ref type="bibr" target="#b54">Tu and Honavar, 2012)</ref> 71.4 57.0 MaxEnc <ref type="bibr" target="#b35">(Le and Zuidema, 2015)</ref> 73.2 65.8 Neural E-DMV <ref type="bibr" target="#b28">(Jiang et al., 2016)</ref> 72.5 57.6 CRFAE <ref type="bibr" target="#b12">(Cai et al., 2017)</ref> 71.7 55.7 L-NDMV (Big training data) <ref type="bibr" target="#b23">(Han et al., 2017)</ref> 77.2 63.2 <ref type="table" target="#tab_1">Table 2</ref>: Directed dependency accuracy on section 23 of WSJ, evaluating on sentences of length 10 and all lengths. Starred entries ( * ) denote that the system benefits from ad- ditional punctuation-based constraints. Standard deviation is given in parentheses when available.</p><p>parameters are initialized in the same way as in the POS tagging experiment. The directed depen- dency accuracy (DDA) is used for evaluation and we report accuracy on sentences of length 10 and all lengths. We train the parser until training data likelihood converges, and report the mean and standard deviation over 20 random restarts.</p><p>Comparison with other related work. Our model directly observes word embeddings and does not require gold POS tags during training. Thus, results from related work trained on gold tags are not directly comparable. However, to measure how these systems might perform with- out gold tags, we run three recent state-of-the- art systems in our experimental setting: UR- A E-DMV ( <ref type="bibr" target="#b54">Tu and Honavar, 2012)</ref>, Neural E- DMV ( <ref type="bibr" target="#b28">Jiang et al., 2016)</ref>, and CRF Autoencoder (CRFAE) <ref type="bibr" target="#b12">(Cai et al., 2017)</ref>. <ref type="bibr">5</ref> We use unsupervised POS tags (induced from our Markov-structured model) in place of gold tags. <ref type="bibr">6</ref> We also train ba- sic DMV on gold tags and include several state- of-the-art results on gold tags as reference points.</p><p>Results. As shown in  lengths, which suggests the additional latent em- bedding layer and neural projector are helpful. The proposed approach yields, to the best of our knowledge, 7 state-of-the-art performance with- out gold POS annotation and without sentence- internal boundary information. DMV, UR-A E- DMV, Neural E-DMV, and CRFAE suffer a large decrease in performance when trained on unsu- pervised tags -an effect also seen in previous work ( <ref type="bibr" target="#b47">Spitkovsky et al., 2011a;</ref><ref type="bibr" target="#b16">Cohen et al., 2011</ref>). Since our approach induces latent POS tags jointly with dependency trees, it may be able to learn POS clusters that are more amenable to grammar induction than the unsupervised tags. We observe that CRFAE underperforms its gold- tag counterpart substantially. This may largely be a result of the model's reliance on prior linguistic rules that become unavailable when gold POS tag types are unknown. Many extensions to DMV can be considered orthogonal to our approach -they essentially focus on improving the syntax model. It is possible that incorporating these more sophis- ticated syntax models into our approach may lead to further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Sensitivity Analysis</head><p>Impact of Initialization. In the above experi- ments we initialize the structured syntax compo- nents with the pre-trained Gaussian or discrete baseline, which is shown as a useful technique to help train our deep models. We further study the results with fully random initialization. In the POS tagging experiment, we report the results in <ref type="table" target="#tab_2">Table 3</ref>. While the performance with 4 layers is comparable to the pre-trained Gaussian initializa- tion, deeper projections (8 or 16 layers) result in a dramatic drop in performance. This suggests that the structured syntax model with very deep projec- tions is difficult to train from scratch, and a simpler projection might be a good compromise in the ran- dom initialization setting. Different from the Markov prior in POS tag- <ref type="bibr">7</ref> We tried to be as thorough as possible in evaluation by running top performing systems using our more difficult training setup when this was feasible -but it was not possible to evaluate them all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>M-1 VM Gaussian HMM 72.0 65.0 Ours <ref type="table" target="#tab_3">(4 layers)</ref> 76.4 69.3 Ours <ref type="table">(8 layers)</ref> 76.8 69. <ref type="table" target="#tab_0">4  Ours (16 layers)</ref> 67.3 62.0  ging experiments, our parsing model seems to be quite sensitive to the initialization. For example, directed accuracy of our approach on sentences of length 10 is below 40.0 with random initializa- tion. This is consistent with previous work that has noted the importance of careful initialization for DMV-based models such as the commonly used harmonic initializer ( <ref type="bibr" target="#b34">Klein and Manning, 2004</ref>). However, it is not straightforward to apply the har- monic initializer for DMV directly in our model without using some kind of pre-training since we do not observe gold POS.</p><p>Impact of Observed Embeddings. We investi- gate the effect of the choice of pre-trained embed- ding on performance while using our approach.</p><p>To this end, we additionally include results us- ing fastText embeddings ( <ref type="bibr" target="#b11">Bojanowski et al., 2017)</ref> -which, in contrast with skip-gram embeddings, include character-level information. We set the context windows size to 1 and the dimension size to 100 as in the skip-gram training, while keep- ing other parameters set to their defaults. These results are summarized in <ref type="table" target="#tab_3">Table 4 and Table 5</ref>. While fastText embeddings lead to reduced perfor- mance with our model, our approach still yields an improvement over the Gaussian baseline with the new observed embeddings space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis of Embeddings</head><p>We perform qualitative analysis to understand how the latent embeddings help induce syntactic struc- tures. First we filter out low-frequency words and punctuations in WSJ, and visualize the rest words (10k) with t-SNE ( <ref type="bibr" target="#b38">Maaten and Hinton, 2008)</ref>    For our Markov-structured model, we have dis- played the embedding space in <ref type="figure" target="#fig_1">Figure 1(b)</ref>, where the gold POS clusters are well-formed. Further, we present five example target words and their five nearest neighbors in terms of cosine similarity. As shown in <ref type="table" target="#tab_6">Table 6</ref>, the skip-gram embedding cap- tures both semantic and syntactic aspects to some degree, yet our embeddings are able to focus es- pecially on the syntactic aspects of words, in an unsupervised fashion without using any extra mor- phological information.</p><p>In <ref type="figure" target="#fig_5">Figure 5</ref> we depict the learned latent em- beddings with the DMV-structured syntax model. Unlike the Markov structure, the DMV structure maps a large subset of singular and plural nouns to the same overlapping region. However, two clus- ters of singular and plural nouns are actually sepa- rated. We inspect the two clusters and the overlap- ping region in <ref type="figure" target="#fig_5">Figure 5</ref>, it turns out that the nouns in the separated clusters are words that can appear as subjects and, therefore, for which verb agree- ment is important to model. In contrast, the nouns in the overlapping region are typically objects. This demonstrates that the latent embeddings are focusing on aspects of language that are specifi- cally important for modeling dependency without ever having seen examples of dependency parses.</p><p>Some previous work has deliberately created embeddings to capture different notions of sim- ilarity ( <ref type="bibr" target="#b36">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b19">Cotterell and Schütze, 2015</ref>), while they use extra morphol- ogy or dependency annotations to guide the em- bedding learning, our approach provides a poten- tial alternative to create new embeddings that are guided by structured syntax model, only using un- labeled text corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Our approach is related to flow-based generative models, which are first described in NICE ( <ref type="bibr" target="#b20">Dinh et al., 2014</ref>) and have recently received more at- tention ( <ref type="bibr">Dinh et al., 2016;</ref><ref type="bibr" target="#b27">Jacobsen et al., 2018;</ref><ref type="bibr" target="#b31">Kingma and Dhariwal, 2018)</ref>. This relevant work mostly adopts simple (e.g. Gaussian) and fixed priors and does not attempt to learn inter- pretable latent structures. Another related gen- erative model class is variational auto-encoders (VAEs) <ref type="bibr" target="#b33">(Kingma and Welling, 2013</ref>) that opti- mize a lower bound on the marginal data likeli- hood, and can be extended to learn latent struc- tures ( <ref type="bibr" target="#b40">Miao and Blunsom, 2016;</ref><ref type="bibr" target="#b57">Yin et al., 2018)</ref>. Against the flow-based models, VAEs remove the invertibility constraint but sacrifice the merits of exact inference and exact log likelihood compu- tation, which potentially results in optimization challenges ( <ref type="bibr" target="#b32">Kingma et al., 2016)</ref>. Our approach can also be viewed in connection with generative adversarial networks (GANs) ( <ref type="bibr" target="#b22">Goodfellow et al., 2014</ref>) that is a likelihood-free framework to learn implicit generative models. However, it is non- trivial for a gradient-based method like GANs to propagate gradients through discrete structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we define a novel generative ap- proach to leverage continuous word representa- tions for unsupervised learning of syntactic struc- ture. Experiments on both POS induction and un- supervised dependency parsing tasks demonstrate the effectiveness of our proposed approach. Fu- ture work might explore more sophisticated in- vertible projections, or recurrent projections that jointly transform the entire input sequence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization (t-SNE) of skip-gram embeddings</figDesc><graphic url="image-1.png" coords="1,307.28,227.52,106.56,106.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Normalized Confusion matrix for POS tagging experiments, row label represents the gold tag.</figDesc><graphic url="image-3.png" coords="6,318.55,307.10,90.57,90.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization (t-SNE) of learned latent embeddings with DMV-structured syntax model. Each node represents a word and is colored according to the most likely gold POS tag in the Penn Treebank (best seen in color).</figDesc><graphic url="image-5.png" coords="9,121.01,276.63,121.90,121.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Unsupervised POS tagging results on entire WSJ, compared with other baselines and state-of-the-art systems. Standard deviation is given in parentheses when available.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 ,</head><label>2</label><figDesc></figDesc><table>our approach 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Unsupervised POS tagging results of our approach on WSJ, with random initialization of syntax model.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Unsupervised POS tagging results on WSJ, with</head><label>4</label><figDesc></figDesc><table>fastText vectors as the observed embeddings. 

System 
10 
all 
Gaussian DMV 
53.6 
41.3 
Ours (4 layers) 
56.9 
43.9 
Ours (8 layers) 
57.1 
42.3 
Ours (16 layers) 
52.9 
39.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Directed dependency accuracy on section 23 of 

WSJ, with fastText vectors as the observed embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Target words and their 5 nearest neighbors, based 

on skip-gram embeddings and our learned latent embeddings 
with Markov-structured syntax model. 

agenda 

error 

process 

timetable 

plans 

dreams 

payments 

(obj) 

smokers 

parents 
furriers 

issuers folks 
aides 

(subj) 

aide resident 
attorney 
singer 

actress 

owner 

(subj) 

</table></figure>

			<note place="foot" n="1"> Code is available at https://github.com/jxhe/structlearning-with-flow.</note>

			<note place="foot" n="2"> For example, all ei could learn to be zero vectors, leading to the trivial solution of learning zero mean and zero variance Gaussian emissions achieving infinite data likelihood.</note>

			<note place="foot">&lt; l a t e x i t s h a 1 _ b a s e 6 4 = &quot; c J N e T q 2 N h p m 5 E 1 h w B l Q I Z x 9 c 5 V c = &quot; &gt; A A A C G n i c Z V D L S g M x F M 3 4 r P U 1 2 q W b Y B E q l D I t g r o r u n F Z w d p C W 0 s m z b S h y W R I 7 g h l m E 8 R t / o d r s S t G z / D P z D T d m H b C + G e n H t u O D l + J L g B z / t x 1 t Y 3 N r e 2 c z v 5 3 b 3 9 g 0 P 3 6 P j R q F h T 1 q R K K N 3 2 i W G C h 6 w J H A R r R 5 o R 6 Q v W 8 s e 3 2 b z 1 z L T h K n y A S c R 6 k g x D H n B K w F J 9 t 9 D 1 Z T J K + w k v Y 5 E + J a X a e d p 3 i 1 7 F m x Z e B d U 5 K K J 5 N f r u b 3 e g a C x Z C F Q Q Y z p V L 4 J e Q j R w K l i a 7 8 a G R Y S O y Z B 1 L A y J Z K a X T M 2 n + M w y A x w o b U 8 I e M r + 3 0 i I N J L A y C q z Z h Z m G Q N K C V O 2 K h j J r G X P T O 9 m I v 2 y L 8 u Z S J v A L B m B 4 K q X 8 D C K g Y V 0 5 i O I B Q a F s 6 D w g G t G Q U w s I F R z + x V M R 0 Q T C j b O v M 2 o u p z I K m j W K t c V 7 / 6 i W L + Z h 5 V D J + g U l V A V X a I 6 u k M N 1 E Q U T d A r e k P v z o v z 4 X w 6 X z P p m j P f K a C F c r 7 / A H 3 Y o V M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = &quot; c J N e T q 2 N h p m 5 E 1 h w B l Q I Z x 9 c 5 V c = &quot; &gt; A A A C G n i c Z V D L S g M x F M 3 4 r P U 1 2 q W b Y B E q l D I t g r o r u n F Z w d p C W 0 s m z b S h y W R I 7 g h l m E 8 R t / o d r s S t G z / D P z D T d m H b C + G e n H t u O D l + J L g B z / t x 1 t Y 3 N r e 2 c z v 5 3 b 3 9 g 0 P 3 6 P j R q F h T 1 q R K K N 3 2 i W G C h 6 w J H A R r R 5 o R 6 Q v W 8 s e 3 2 b z 1 z L T h K n y A S c R 6 k g x D H n B K w F J 9 t 9 D 1 Z T J K + w k v Y 5 E + J a X a e d p 3 i 1 7 F m x Z e B d U 5 K K J 5 N f r u b 3 e g a C x Z C F Q Q Y z p V L 4 J e Q j R w K l i a 7 8 a G R Y S O y Z B 1 L A y J Z K a X T M 2 n + M w y A x w o b U 8 I e M r + 3 0 i I N J L A y C q z Z h Z m G Q N K C V O 2 K h j J r G X P T O 9 m I v 2 y L 8 u Z S J v A L B m B 4 K q X 8 D C K g Y V 0 5 i O I B Q a F s 6 D w g G t G Q U w s I F R z + x V M R 0 Q T C j b O v M 2 o u p z I K m j W K t c V 7 / 6 i W L + Z h 5 V D J + g U l V A V X a I 6 u k M N 1 E Q U T d A r e k P v z o v z 4 X w 6 X z P p m j P f K a C F c r 7 / A H 3 Y o V M = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = &quot; c J N e T q 2 N h p m 5 E 1 h w B l Q I Z x 9 c 5 V c = &quot; &gt; A A A C G n i c Z V D L S g M x F M 3 4 r P U 1 2 q W b Y B E q l D I t g r o r u n F Z w d p C W 0 s m z b S h y W R I 7 g h l m E 8 R t / o d r s S t G z / D P z D T d m H b C + G e n H t u O D l + J L g B z / t x 1 t Y 3 N r e 2 c z v 5 3 b 3 9 g 0 P 3 6 P j R q F h T 1 q R K K N 3 2 i W G C h 6 w J H A R r R 5 o R 6 Q v W 8 s e 3 2 b z 1 z L T h K n y A S c R 6 k g x D H n B K w F J 9 t 9 D 1 Z T J K + w k v Y 5 E + J a X a e d p 3 i 1 7 F m x Z e B d U 5 K K J 5 N f r u b 3 e g a C x Z C F Q Q Y z p V L 4 J e Q j R w K l i a 7 8 a G R Y S O y Z B 1 L A y J Z K a X T M 2 n + M w y A x w o b U 8 I e M r + 3 0 i I N J L A y C q z Z h Z m G Q N K C V O 2 K h j J r G X P T O 9 m I v 2 y L 8 u Z S J v A L B m B 4 K q X 8 D C K g Y V 0 5 i O I B Q a F s 6 D w g G t G Q U w s I F R z + x V M R 0 Q T C j b O v M 2 o u p z I K m j W K t c V 7 / 6 i W L + Z h 5 V D J + g U l V A V X a I 6 u k M N 1 E Q U T d A r e k P v z o v z 4 X w 6 X z P p m j P f K a C F c r 7 / A H 3 Y o V M = &lt; / l a t e x i t &gt;</note>

			<note place="foot" n="3"> Preprocessing is different for the two tasks, we describe the details in the following subsections.</note>

			<note place="foot" n="4"> This is the default parameter initialization in PyTorch.</note>

			<note place="foot" n="5"> For the three systems, we use implementations from the original papers (via personal correspondence with the authors), and tune their hyperparameters on section 22 of WSJ. 6 Using words directly is not practical because these systems often require a transition probability matrix between input symbols, which requires too much memory.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " Z n d H D G I B u R m t j y Q p 3 + W Y 2 6 8 R q p k = " &gt; A A A C G n i c Z V B N T w I x E J 3 1 E / A L 5 e h l I z H x Q M j i R b 0 R v X j E x B U S I N g t X W h o t 5 t 2 1 o R s + C E e j F f 9 H Z 6 M V y / + D P + B X e A g M E k z r 2 / e t D M v i A U 3 6 H k / z t r 6 x u b W d i 5 f 2 N n d 2 z 8 o H h 4 9 G J V o y n y q h N K t g B g m e M R 8 5 C h Y K 9 a M y E C w Z j C 6 y e r N J 6 Y N V 9 E 9 j m P W l W Q Q 8 Z B T g p b q F U u d Q I m + G U u b 0 g 5 y y c y k V y x 7 V W 8 a 7 i q o z U G 5 n o + f H w G g 0 S v + d v q K J p J F S A U x p l 3 z Y u y m R C O n g k 0 K n c S w m N A R G b C 2 h R G x v 3 T T 6 f A T 9 9 Q y f T d U 2 p 4 I 3 S n 7 v y M l 0 k i C Q 6 v M k l m o Z Q w q J U z F q n A o s 5 Q 9 M 7 1 n W 1 U C W c l E 2 o R m a R A M L 7 s p j + I E W U R n c 4 S J c F G 5 m V F u n 2 t G U Y w t I F R z u 4 p L h 0 Q T i t b O g v W o t u z I K v D P q 1 d V 7 8 5 6 d Q 2 z y M E x n M A Z 1 O A C 6 n A L D f C B w h h e 4 Q 3 e n R f n w / l 0 v m b S N W f e U 4 K F c L 7 / A O n O p C 4 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G A / 1 h t O 7 L I L u j m r b y f f Q U a N E F X A = " &gt; A A A C G n i c Z V B N T w I x E O 3 i F + A X y t H L R m L i g Z D F i 3 o j e v G I i S t E I K R b u t D Q b j f t r H G z 4 Y d 4 M F 7 1 5 I / w Z L x 6 8 W f 4 C 7 S 7 c B C Y p J n X N 2 / a m e e F n G l w n G 8 r t 7 K 6 t r 6 R L x Q 3 t 7 Z 3 d k t 7 + 7 d a R o p Q l 0 g u V d v D m n I W U B c Y c N o O F c X C 4 7 T l j S / T e u u e K s 1 k c A N x S H s C D w P m M 4 L B U P 1 S u e t J P t C x M C n p A h N U T / q l i l N z s r C X Q X 0 G K o 1 C + H j 3 9 v D b 7 J d + u g N J I k E D I B x r 3 a k 7 I f Q S r I A R T i f F b q R p i M k Y D 2 n H w A C b X 3 p J N v z E P j L M w P a l M i c A O 2 P / d y R Y a I F h Z J R p 0 n O 1 l A E p u a 4 a F Y x E m t J n s n u 6 V d U T 1 V S k t K 8 X B g H / r J e w I I y A B m Q 6 h x 9 x G 6 S d G m U P m K I E e G w A J o q Z V W w y w g o T M H Y W j U f 1 R U e W g X t S O 6 8 5 1 8 a r C z S N P D p A h + g Y 1 d E p a q A r 1 E Q u I i h G z + g F v V p P 1 r v 1 Y X 1 O p T l r 1 l N G c 2 F 9 / Q E A L q Z O &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G A / 1 h t O 7 L I L u j m r b y f f Q U a N E F X A = " &gt; A A A C G n i c Z V B N T w I x E O 3 i F + A X y t H L R m L i g Z D F i 3 o j e v G I i S t E I K R b u t D Q b j f t r H G z 4 Y d 4 M F 7 1 5 I / w Z L x 6 8 W f 4 C 7 S 7 c B C Y p J n X N 2 / a m e e F n G l w n G 8 r t 7 K 6 t r 6 R L x Q 3 t 7 Z 3 d k t 7 + 7 d a R o p Q l 0 g u V d v D m n I W U B c Y c N o O F c X C 4 7 T l j S / T e u u e K s 1 k c A N x S H s C D w P m M 4 L B U P 1 S u e t J P t C x M C n p A h N U T / q l i l N z s r C X Q X 0 G K o 1 C + H j 3 9 v D b 7 J d + u g N J I k E D I B x r 3 a k 7 I f Q S r I A R T i f F b q R p i M k Y D 2 n H w A C b X 3 p J N v z E P j L M w P a l M i c A O 2 P / d y R Y a I F h Z J R p 0 n O 1 l A E p u a 4 a F Y x E m t J n s n u 6 V d U T 1 V S k t K 8 X B g H / r J e w I I y A B m Q 6 h x 9 x G 6 S d G m U P m K I E e G w A J o q Z V W w y w g o T M H Y W j U f 1 R U e W g X t S O 6 8 5 1 8 a r C z S N P D p A h + g Y 1 d E p a q A r 1 E Q u I i h G z + g F v V p P 1 r v 1 Y X 1 O p T l r 1 l N G c 2 F 9 / Q E A L q Z O &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " R E k A E C s k l 6 v T P Q b 6 K r E 5 y d 3 H U v c = " &gt; A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K G U q g r o r u n F Z w b G F d i i Z N N O G J p M h u S O U o f 8 g b v U 7 X I l b v 8 H P 8 A / M T G d h 2 w v h n p x 7 b j g 5 Q S y 4 A d f 9 c U p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o y a h E U + Z R J Z T u B s Q w w S P m A Q f B u r F m R A a C d Y L J X T b v P D N t u I o e Y R o z X 5 J R x E N O C V i q 2 w 9 k y m Y D P q j W 3 I a b F 1 4 F z Q L U U F H t Q f W 3 P 1 Q 0 k S w C K o g x v a Y b g 5 8 S D Z w K N q v 0 E 8 N i Q i d k x H o W R k Q y 4 6 e 5 3 x k + s 8 w Q h 0 r b E w H O 2 f 8 b K Z F G E h h b Z d b M w i x j Q C l h 6 l Y F Y 5 m 1 7 J n 8 b q Y y q A e y n o m 0 C c 2 S E Q i v / Z R H c Q I s o n M f Y S I w K J x l g 4 d c M w p i a g G h m t u v Y D o m m l C w C V Z s R s 3 l R F a B d 9 G 4 a b g P l 7 X W b R F W G Z 2 g U 3 S O m u g K t d A 9 a i M P U S T Q K 3 p D 7 8 6 L 8 + F 8 O l 9 z a c k p d o 7 R Q j n f f 2 g o n S g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R E k A E C s k l 6 v T P Q b 6 K r E 5 y d 3 H U v c = " &gt; A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K G U q g r o r u n F Z w b G F d i i Z N N O G J p M h u S O U o f 8 g b v U 7 X I l b v 8 H P 8 A / M T G d h 2 w v h n p x 7 b j g 5 Q S y 4 A d f 9 c U p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o y a h E U + Z R J Z T u B s Q w w S P m A Q f B u r F m R A a C d Y L J X T b v P D N t u I o e Y R o z X 5 J R x E N O C V i q 2 w 9 k y m Y D P q j W 3 I a b F 1 4 F z Q L U U F H t Q f W 3 P 1 Q 0 k S w C K o g x v a Y b g 5 8 S D Z w K N q v 0 E 8 N i Q i d k x H o W R k Q y 4 6 e 5 3 x k + s 8 w Q h 0 r b E w H O 2 f 8 b K Z F G E h h b Z d b M w i x j Q C l h 6 l Y F Y 5 m 1 7 J n 8 b q Y y q A e y n o m 0 C c 2 S E Q i v / Z R H c Q I s o n M f Y S I w K J x l g 4 d c M w p i a g G h m t u v Y D o m m l C w C V Z s R s 3 l R F a B d 9 G 4 a b g P l 7 X W b R F W G Z 2 g U 3 S O m u g K t d A 9 a i M P U S T Q K 3 p D 7 8 6 L 8 + F 8 O l 9 z a c k p d o 7 R Q j n f f 2 g o n S g = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R E k A E C s k l 6 v T P Q b 6 K r E 5 y d 3 H U v c = " &gt; A A A C D X i c Z V D L S g M x F M 3 U V 6 2 v q k s 3 w S K 4 K G U q g r o r u n F Z w b G F d i i Z N N O G J p M h u S O U o f 8 g b v U 7 X I l b v 8 H P 8 A / M T G d h 2 w v h n p x 7 b j g 5 Q S y 4 A d f 9 c U p r 6 x u b W + X t y s 7 u 3 v 5 B 9 f D o y a h E U + Z R J Z T u B s Q w w S P m A Q f B u r F m R A a C d Y L J X T b v P D N t u I o e Y R o z X 5 J R x E N O C V i q 2 w 9 k y m Y D P q j W 3 I a b F 1 4 F z Q L U U F H t Q f W 3 P 1 Q 0 k S w C K o g x v a Y b g 5 8 S D Z w K N q v 0 E 8 N i Q i d k x H o W R k Q y 4 6 e 5 3 x k + s 8 w Q h 0 r b E w H O 2 f 8 b K Z F G E h h b Z d b M w i x j Q C l h 6 l Y F Y 5 m 1 7 J n 8 b q Y y q A e y n o m 0 C c 2 S E Q i v / Z R H c Q I s o n M f Y S I w K J x l g 4 d c M w p i a g G h m t u v Y D o m m l C w C V Z s R s 3 l R F a B d 9 G 4 a b g P l 7 X W b R F W G Z 2 g U 3 S O m u g K t d A 9 a i M P U S T Q K 3 p D 7 8 6 L 8 + F 8 O l 9 z a c k p d o 7 R Q j n f f 2 g o n S g = &lt; / l a t e x i t &gt;</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f F o S f H Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v w q 6 y + P o H k b N q h + f p E P Z i l a E d a o = " &gt; A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j J Z D 7 D u N X v c G X c + g N + h n 9 g B 1 g I 3 K S 5 p + e e 2 5 y e I B b c g O f 9 O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 3 z e e m L a c B X d w z h m v i S D i I e c E r C U 3 w 1 k + p z 1 U u 5 i k f X K F a / q T Q o v g 9 o M V N C s G r 3 y b 7 e v a C J Z B F Q Q Y z o 1 L w Y / J R o 4 F S w r d R P D Y k J H Z M A 6 F k Z E M u O n E 9 M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f F o S f H Q = = &lt; / l a t e x i t &gt;</head><p>x i,r </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n y k e A u I u y i v a i + i p 1 2 G c 3 T W L d k 8 = " &gt; A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j J Z D 7 D u N X v c G X c + g N + h n 9 g B 1 g I 3 K S 5 p + e e 2 5 y e I B b c g O f 9 O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 3 z e e m L a c B X d w z h m v i S D i I e c E r C U 3 w 1 k + p z 1 U u 5 i n f X K F a / q T Q o v g 9 o M V N C s G r 3 y b 7 e v a C J Z B F Q Q Y z o 1 L w Y / J R o 4 F S w r d R P D Y k J H Z M A 6 F k Z E M u O n E 9 M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f I F a f I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n y k e A u I u y i v a i + i p 1 2 G c 3 T W L d k 8 = " &gt; A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j J Z D 7 D u N X v c G X c + g N + h n 9 g B 1 g I 3 K S 5 p + e e 2 5 y e I B b c g O f 9 O I W V 1 b X 1 j e J m a W t 7 Z 3 e v v H / w Y F S i K W t S J Z R u B 8 Q w w S P W B A 6 C t W P N i A w E a w W j m 3 z e e m L a c B X d w z h m v i S D i I e c E r C U 3 w 1 k + p z 1 U u 5 i n f X K F a / q T Q o v g 9 o M V N C s G r 3 y b 7 e v a C J Z B F Q Q Y z o 1 L w Y / J R o 4 F S w r d R P D Y k J H Z M A 6 F k Z E M u O n E 9 M Z P r F M H 4 d K 2 x M B n r D / N 1 I i j S Q w t M q 8 m b l Z z o B S w r h W B U O Z t / y Z y d 2 M Z e A G 0 s 1 F 2 o R m w Q i E l 3 7 K o z g B F t G p j z A R G B T O A 8 J 9 r h k F M b a A U M 3 t V z A d E k 0 o 2 B h L N q P a Y i L L o H l W v a p 6 d + e V + v U s r C I 6 Q s f o F N X Q B a q j W 9 R A T U T R I 3 p F b + j d e X E + n E / n a y o t O L O d Q z R X z v c f I F a f I w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " n y k e A u I u y i v a i + i p 1 2 G c 3 T W L d k 8 = " &gt; A A A C E n i c Z V D L T g I x F O 3 g C / G F u n T T S E x c T M h g T N Q d 0 Y 1 L T E R I Y E I 6 p Q M N 7 X R s 7 x j</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyp-Hmm</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Blunsom and Cohn</publisher>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="67" />
		</imprint>
		<respStmt>
			<orgName>NHMM (basic</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nhmm (+ Conv ;</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="74" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;amp;</forename><surname>Nhmm (+ Conv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lstm) (tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="79" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hmm (</forename><surname>Gaussian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">75</biblScope>
		</imprint>
	</monogr>
	<note>4 (1.0) 68.5 (0.5</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hmm (</forename><surname>Feature</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="76" to="77" />
		</imprint>
	</monogr>
	<note>+ proto</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yatbaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="80" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cluster (token-based</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yatbaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References Mohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
	<note>Proceedings of HLT-NAACL</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A hierarchical pitman-yor process hmm for unsupervised part of speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crf autoencoder for unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaussianization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">A</forename><surname>Scott Saobing Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Two decades of unsupervised pos induction: How far have we come?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised structure prediction with nonparallel multilingual guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">HEAD-DRIVEN STATISTICAL MODELS FOR NATURAL LANGUAGE PARSING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density estimation using real nvp</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dependency grammar induction with neural lexicalization and big training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving unsupervised dependency parsing with richer contexts and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>William P Headden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juha</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Oyallon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07088</idno>
		<title level="m">i-revnet: Deep invertible networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Why doesnt em find good hmm pos-taggers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLPCoNLL</title>
		<meeting>the EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4743" to="4751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing: Let&apos;s use supervised parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised pos induction with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
		<meeting>the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Language as a latent variable: Discrete generative models for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing with acoustic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Pate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grammar induction from (lots of) words alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised dependency parsing without gold part-of-speech tags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Punctuation: Making a point in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Capitalization cues improve dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised part-of-speech tagging with anchor hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised neural hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Ke M Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLPCoNLL</title>
		<meeting>EMNLPCoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning syntactic categories using paradigmatic representations of word context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enis</forename><surname>Mehmet Ali Yatbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unsupervised instance-based part of speech induction using probable substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Mehmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yatbaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Enis Rıfat Sert, and Deniz Yuret</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Structvae: Tree-structured latent variable models for semi-supervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Junxian He, and Graham Neubig</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
