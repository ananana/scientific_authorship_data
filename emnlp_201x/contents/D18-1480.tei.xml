<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spherical Latent Spaces for Stable Variational Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spherical Latent Spaces for Stable Variational Autoencoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4503" to="4513"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4503</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions , typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of &quot;collapse&quot; which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution , giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work has established the effectiveness of deep generative models for a range of tasks in NLP, including text generation ( <ref type="bibr" target="#b29">Yu et al., 2017)</ref>, machine translation ( <ref type="bibr" target="#b30">Zhang et al., 2016)</ref>, and style transfer <ref type="bibr" target="#b22">(Shen et al., 2017;</ref><ref type="bibr" target="#b31">Zhao et al., 2017a)</ref>. Variational autoencoders, which have been explored in past work for text mod- eling ( <ref type="bibr" target="#b14">Miao et al., 2016;</ref><ref type="bibr" target="#b2">Bowman et al., 2016)</ref>, posit a continuous latent variable which is used to capture latent structure in the data. Typical VAE implementations assume the prior of this la- tent space is a multivariate Gaussian; during train- ing, a Kullback-Leibler (KL) divergence term in loss function encourages the variational posterior to approximate the prior. One major limitation of this approach observed by past work is that the KL term may encourage the posterior distribution of the latent variable to "collapse" to the prior, effec- tively rendering the latent structure unused <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b26">Chen et al., 2016)</ref>.</p><p>In this paper, we propose to use the von Mises- Fisher (vMF) distribution rather than Gaussian for our latent variable. vMF places a distribution over the unit hypersphere governed by a mean parame- ter µ and a concentration parameter κ. Our prior is a uniform distribution over the unit hypersphere (κ = 0) and our family of posterior distributions treats κ as a fixed model hyperparameter. Since the KL divergence only depends on κ, we can structurally prevent the KL collapse and make our model's optimization problem easier. We show that this approach is actually more robust than try- ing to flexibly learn κ, and a wide range of settings for fixed κ lead to good performance. Our model systematically achieves better log likelihoods than analogous Gaussian models while having higher KL divergence values, showing that it more suc- cessfully makes use of the latent variables at the end of training.</p><p>Past work has suggested several other tech- niques for dealing with the KL collapse in the Gaussian case. Annealing the weight of KL term <ref type="bibr" target="#b2">(Bowman et al., 2016</ref>) still leaves us with brit- tleness in the optimization process, as we show in Section 2. Other prior work ( <ref type="bibr" target="#b19">Semeniuta et al., 2017</ref>) focuses on using CNNs rather than RNNs as the decoder in order to weaken the model and encourage the use of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enc</head><p>The q (z|x)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e R n Y s h r 1 8 z 9 i d M J q H h S d 9 z f 6 Z D U = " &gt; A A A B 9 H i c b V D L T g J B E O z F F + I L 9 O h l I j H B C 9 n 1 o k c S L x 4 x k U c C G z I 7 z M K E m d l l Z h b F l e / w 4 k F j v P o N f o M 3 / 8 b h c V C w k k 4 q V d 3 p 7 g p i z r R x 3 W 8 n s 7 a + s b m V 3 c 7 t 7 O 7 t H + Q L h 3 U d J Y r Q G o l 4 p J o B 1 p Q z S W u G G U 6 b s a J Y B J w 2 g s H V 1 G + M q N I s k r d m H F N f 4 J 5 k I S P Y W M k f d t J 2 3 G e T 0 s P j / V k n X 3 T L 7 g x o l X g L U q w U 7 s Q n A F Q 7 + a 9 2 N y K J o N I Q j r V u e W 5 s / B Q r w w i n k 1 w 7 0 T T G Z I B 7 t G W p x I J q P 5 0 d P U G n V u m i M F K 2 p E E z 9 f d E i o X W Y x H Y T o F N X y 9 7 U / E / r 5 W Y 8 N J P m Y w T Q y W Z L w o T j k y E p g m g L l O U G D 6 2 B B P F 7 K 2 I 9 L H C x N i c c j Y E b / n l V V I / L 3 t u 2 b v x i p U y z J G F Y z i B E n h w A R W 4 h i r U g M A Q n u A F X p 2 R 8 + y 8 O e / z 1 o y z m D m C P 3 A + f g B z y 5 P + &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 h 8 u w G Y e s e s 6 g e t l E M E U 7 g k s / 3 s = " &gt; A A A B 9 H i c b V A 9 T 8 M w E L 3 w W c p X g Z H F o k I q S 5 S w w F i J h b F I 9 E N q o 8 p x n d a q 7 a S 2 U 1 F C f w c L A w i x 8 m P Y + D e 4 b Q Z o e d J J T + / d 6 e 5 e m H C m j e d 9 O 2 v r G 5 t b 2 4 W d 4 u 7 e / s F h 6 e i 4 o e N U E V o n M Y 9 V K 8 S a c i Z p 3 T D D a S t R F I u Q 0 2 Y 4 v J n 5 z T F V m s X y 3 k w S G g j c l y x i B B s r B a N u 1 k k G b F p 5 f H q 4 6 J b K n u v N g V a J n 5 M y 5 K h 1 S 1 + d X k x S Q a U h H G v d 9 r 3 E B B l W h h F O p 8 V O q m m C y R D 3 a d t S i Q X V Q T Y / e o r O r d J D U a x s S Y P m 6 u + J D A u t J y K 0 n Q K b g V 7 2 Z u J / X j s 1 0 X W Q M Z m k h k q y W B S l H J k Y z R J A P a Y o M X x i C S a K 2 V s R G W C F i b E 5 F W 0 I / v L L q 6 R x 6 f q e 6 9 / 5 5 a q b x 1 G A U z i D C v h w B V W 4 h R r U g c A I n u E V 3 p y x 8 + K 8 O x + L 1 j U n n z m B P 3 A + f w D P 2 J I J &lt; / l a t e x i t &gt;</head><p>q (z|x) ; we then sample z and generate the word sequence x given z. We show samples from N (0, I) and vMF(·,κ = 100); the latter samples lie on the surface of the unit sphere. While κ can be predicted from the encoder network, we find experimentally that fixing it leads to more stable optimization and better performance. latent code, but the gains are limited and chang- ing the decoder in this way requires ad hoc model engineering and careful tuning of various decoder capacity parameters. Our method is orthogonal to the choice of the decoder and can be combined with any of these approaches. Using vMF distribu- tions in VAEs also leaves us the flexibility to mod- ify the prior in other ways, such as using a product distribution with a uniform ( <ref type="bibr" target="#b7">Guu et al., 2018)</ref> or piecewise constant term ( <ref type="bibr" target="#b20">Serban et al., 2017a</ref>).</p><p>We evaluate our approach in two generative modeling paradigms. For both RNN language modeling and bag-of-words document modeling, we find that vMF is more robust than a Gaussian prior, and our model learns to rely more on the la- tent variable while achieving better held-out data likelihoods. To better understand the contrast be- tween these models, we design and conduct a se- ries of experiments to understand the properties of the Gaussian and vMF latent code spaces, which make different structural assumptions. Unsurpris- ingly, these latent code distributions capture much of the same information as in a bag of words, but we show that vMF can more readily go beyond this, capturing ordering information more effec- tively than a Gaussian code. <ref type="bibr" target="#b2">Bowman et al. (2016)</ref> propose a variational au- toencoder model for generative text modeling in- spired by <ref type="bibr" target="#b12">Kingma and Welling (2013)</ref>. Instead of modeling p(x) directly as in vanilla language models, VAEs introduce a continuous latent vari- able z and take the form p(z)p(x|z). To train a VAE, we optimize the marginal likelihood p(x) = p θ (z)p(x|z)dz. The marginal log likelihood can be written as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Variational Autoencoders for Text</head><formula xml:id="formula_0">log p θ (x) = KL(q φ (z|x)||p θ (z|x)) + L(θ, φ; x) L(θ, φ; x) = −KL(q φ (z|x)||p θ (z)) + E q φ (z|x) log p θ (x|z) (1)</formula><p>q φ (z|x), a variational approximation to the pos- terior p(z|x), can be variously interpreted as a recognition model or encoder, parameterized by a neural network to encode the sentence x into a dense code z. L(θ, φ; x) is often called the ev- idence lower bound (ELBO). The first term of ELBO is the KL divergence of the approximate posterior from prior and the second term is an ex- pected reconstruction error.</p><p>Since KL divergence is always non-negative, we can use L(θ, φ; x) as a lower bound of marginal likelihood log p θ (x).</p><p>We optimize L(θ, φ; x), jointly learning the recognition model parameters φ and generative model parameters θ.</p><p>As the choice of prior p(z), most previous work uses a centered multivariate Gaussian p θ (z) = N (z; 0, I). Since Gaussians are a location-scale family of distributions, using them for both the prior and posterior allows us to apply the reparam- eterization trick and differentiate through the sam- pling stage z ∼ E q φ (z|x) when optimizing ELBO in practice <ref type="bibr" target="#b12">(Kingma and Welling, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Case Study: NVRNN</head><p>A Neural Variational RNN (NVRNN) for lan- guage modeling is described in <ref type="bibr" target="#b2">Bowman et al. (2016)</ref> and depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. The goal of the NVRNN model is to extract a high level represen- tation of a sentence into z and reconstruct the sen- tence with a neural language model.</p><p>We denote a sequence of words as x = {x 1 , x 2 , · · · , x n }. Unlike in vanilla language mod- eling, an NVRNN conditions on the latent vari- able z at each step of the generation</p><formula xml:id="formula_1">p θ (x|z) = p θ (x 1 |z) n i=1 p(x i |x 1 , . . . , x i−1 , z)</formula><p>. This proba- bility distribution is modeled using a recurrent model like an LSTM (Hochreiter and Schmidhu- ber, 1997) as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. There is nothing unique about this choice; other recurrent sequence models like a CNN or a Transformer ( <ref type="bibr">Vaswani et al., 2017</ref>) could be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Posterior Collapse</head><p>When training a VAE, we update θ and φ si- multaneously. Optimizing Eq. 1 gives two gradi- ent terms: an update from the reconstruction loss (likelihood of the correct labels) and an update from the KL divergence. While the reconstruction loss term encourages the z to convey useful in- formation to this model, the KL term consistently tries to regularize q(z|x) towards the prior on ev- ery gradient update. This may trap the model in a bad local optimum where q φ (z|x) = p θ (z) for all x: in this case, z is simply a noise source, which is useless to the model, so the model has learned to ignore it and will not make large enough gradient updates to break q(z|x) out of this optimum. <ref type="bibr" target="#b2">Bowman et al. (2016)</ref> termed this issue KL col- lapse and proposed an annealing schedule to han- dle it, where the weight of the KL term is in- creased over the course of training. <ref type="bibr">2</ref> In this way, the model initially learns to use the latent code but is then regularized towards the prior as training progresses. However, this trick is not sufficient to avert KL collapse in all scenarios, particularly <ref type="bibr">2</ref> Reweighting the KL term is also used in methods like β- VAE ( <ref type="bibr">Higgins et al., 2017</ref>) and InfoVAE ( <ref type="bibr" target="#b32">Zhao et al., 2017b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No annealing</head><p>Sigmoid annealing 3-layer 1-layer 3-layer 1-layer  when strong decoders are used and z has a minor impact on p θ (x|z). <ref type="table" target="#tab_1">Table 1</ref> shows experiments in a similar setup to that of <ref type="bibr" target="#b2">Bowman et al. (2016)</ref>. We train an NVRNN model on the Penn Treebank with four different hyperparameter settings. We either use a 3-layer LSTM encoder or a 1-layer LSTM and use or do not use a sigmoid annealing schedule (increase the KL weight from 0 to 1 over the first 20 epochs). We observe the best performance using the 1-layer model with annealing. One might conclude from this table that the annealing trick has worked since both models achieve better performance when an- nealing is used. But in fact, a vMF-based model can do better than either (NLL of 117), and more- over, we have no way of knowing that a better an- nealing scheme might not achieve even higher per- formance after training. Furthermore, the higher- capacity 3-layer model can theoretically do any- thing the 1-layer model can, so its lower perfor- mance indicates that our training is derailed either by overfitting or getting stuck in a local optimum where the latent variable is unused. <ref type="bibr">3</ref> Getting the best performance out of a VAE is, therefore, a challenging problem that requires careful tuning of the objective function and opti- mization procedure <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b32">Zhao et al., 2017b;</ref><ref type="bibr">Higgins et al., 2017)</ref>. Beyond the well-documented problem of KL collapse, an op- timizer may simply get stuck in a local optimum during training and as a result, fail to find a model that most effectively exploits the latent variable.</p><p>The solution we advocate for in this paper is to change the distribution for the latent space and simplify the optimization problem. In the next sec- tion, we describe the von Mises-Fisher distribution and its use in VAE, where it forces the model to put the latent representations on the surface of the unit hypersphere rather than squeezing everything to the origin. Critically, this distribution lets us fix the value of the KL term by fixing the distri- bution's concentration parameter κ; this averts the KL collapse and leads to good model performance across two generative modeling paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">von Mises-Fisher VAE</head><p>The von Mises-Fisher distribution is a distribution on the (d − 1)-dimensional sphere in R d . The vMF distribution is defined by a direction vector µ with ||µ||= 1 and a concentration parameter κ ≥ 0. The PDF of the vMF distribution for the d-dimensional unit vector x is defined as:</p><formula xml:id="formula_2">f d (x; µ, κ) = C d (κ) exp(κµ T x) (2) C d (κ) = κ d/2−1 (2π) d/2 I d/2−1 (κ)<label>(3)</label></formula><p>where I v stands for the modified Bessel function of the first kind at order v. <ref type="figure" target="#fig_0">Figure 1</ref> shows samples from vMF distributions with various µ vectors (arrows), d = 3, and κ = 100. This is a high κ value, leading to samples that are tightly clustered around µ, which is the mean and mode of the distribution. When κ = 0, the distribution degenerates to a uniform distribution over the hypersphere independent of µ.</p><p>Past work has used vMF as an emission distri- bution in unsupervised clustering models <ref type="bibr" target="#b1">(Banerjee et al., 2005</ref>), VAE for other domains <ref type="bibr" target="#b5">(Davidson et al., 2018;</ref><ref type="bibr">Hasnat et al., 2017)</ref>, and a generative editing model for text ( <ref type="bibr" target="#b7">Guu et al., 2018)</ref>. We focus specifically on the empirical properties of vMF for text modeling and conduct a systematic examina- tion of how this prior affects VAE models com- pared to using a Gaussian.</p><p>VAE with vMF We will use vMF as both our prior and variational posterior in our VAE mod- els. Otherwise, the setup for our VAE remains the same as in the Gaussian case established in Section 2. Our prior is the uniform distribution vMF(·, κ = 0). Since true posterior p θ (z|x) is in- tractable, we will approximate it with a variational posterior q φ (z|x) = vMF(z; µ, κ) where the mean (a) Gaussian (b) vMF y </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 o E B U 6 / J 0 u E M h I w b K Y 5 U 1 r 5 C 6 l 0 = " &gt; A A A B 6 H i c b Z C 7 S w N B E M b n 4 i u J r 6 i l z W I Q r M K d j Z Y B G 8 s E z A O S E P b 2 5 p I 1 e 3 v H 7 p 5 w H K k t b C w U s R P / J D v / G z e P Q h M / W P j x f T P s z P i J 4 N q 4 7 r d T 2 N j c 2 t 4 p l s q 7 e / s H h 5 W j 4 7 a O U 8 W w x W I R q 6 5 P N Q o u s W W 4 E d h N F N L I F 9 j x J z e z v P O A S v N Y 3 p k s w U F E R 5 K H n F F j r W Y 2 r F T d m j s X W Q d v C d V 6 6 T H 4 A I D G s P L V D 2 K W R i g N E 1 T r n u c m Z p B T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 5 F S S P U g 3 w + 6 J S c W y c g Y a z s k 4 b M 3 d 8 d O Y 2 0 z i L f V k b U j P V q N j P / y 3 q p C a 8 H O Z d J a l C y x U d h K o i J y W x r E n C F z I j M A m W K 2 1 k J G 1 N F m b G 3 K d s j e K s r r 0 P 7 s u a 5 N a / p V e s u L F S E U z i D C / D g C u p w C w 1 o A Q O E J 3 i B V + f e e X b e n P d F a c F Z 9 p z A H z m f P 1 j 9 j r w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 X Z Z c n M r W f X V m S 7 5 j / o 6 8 e B 3 Z S k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 2 G P B i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I I / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W t 7 Z 3 d v f J + 5 e D w 6 P i k e n r W 1 X G q G H Z Y L G L V D 6 h G w S V 2 D D c C + 4 l C G g U C e 8 H s b u H 3 n l B p H s s H k y X o R 3 Q i e c g Z N V Z q Z 6 N q z a 2 7 S 5 B N 4 h W k B g V a o + r X c B y z N E J p m K B a D z w 3 M X 5 O l e F M 4 L w y T D U m l M 3 o B A e W S h q h 9 v P l o X N y Z Z U x C W N l S x q y V H 9 P 5 D T S O o s C 2 x l R M 9 X r 3 k L 8 z x u k J m z 4 O Z d J a l C y 1 a I w F c T E Z P E 1 G X O F z I j M E s o U t 7 c S N q W K M m O z q d g Q v P W X N 0 n 3 p u 6 5 d a / t 1 Z p u E U c Z L u A S r s G D W 2 j C P b S g A w w Q n u E V 3 p x H 5 8 V 5 d z 5 W r S W n m D m H P 3 A + f w D h Y 4 z o &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 o E B U 6 / J 0 u E M h I w b K Y 5 U 1 r 5 C 6 l 0 = " &gt; A A A B 6 H i c b Z C 7 S w N B E M b n 4 i u J r 6 i l z W I Q r M K d j Z Y B G 8 s E z A O S E P b 2 5 p I 1 e 3 v H 7 p 5 w H K k t b C w U s R P / J D v / G z e P Q h M / W P j x f T P s z P i J 4 N q 4 7 r d T 2 N j c 2 t 4 p l s q 7 e / s H h 5 W j 4 7 a O U 8 W w x W I R q 6 5 P N Q o u s W W 4 E d h N F N L I F 9 j x J z e z v P O A S v N Y 3 p k s w U F E R 5 K H n F F j r W Y 2 r F T d m j s X W Q d v C d V 6 6 T H 4 A I D G s P L V D 2 K W R i g N E 1 T r n u c m Z p B T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 5 F S S P U g 3 w + 6 J S c W y c g Y a z s k 4 b M 3 d 8 d O Y 2 0 z i L f V k b U j P V q N j P / y 3 q p C a 8 H O Z d J a l C y x U d h K o i J y W x r E n C F z I j M A m W K 2 1 k J G 1 N F m b G 3 K d s j e K s r r 0 P 7 s u a 5 N a / p V e s u L F S E U z i D C / D g C u p w C w 1 o A Q O E J 3 i B V + f e e X b e n P d F a c F Z 9 p z A H z m f P 1 j 9 j r w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 X Z Z c n M r W f X V m S 7 5 j / o 6 8 e B 3 Z S k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 2 G P B i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I I / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W t 7 Z 3 d v f J + 5 e D w 6 P i k e n r W 1 X G q G H Z Y L G L V D 6 h G w S V 2 D D c C + 4 l C G g U C e 8 H s b u H 3 n l B p H s s H k y X o R 3 Q i e c g Z N V Z q Z 6 N q z a 2 7 S 5 B N 4 h W k B g V a o + r X c B y z N E J p m K B a D z w 3 M X 5 O l e F M 4 L w y T D U m l M 3 o B A e W S h q h 9 v P l o X N y Z Z U x C W N l S x q y V H 9 P 5 D T S O o s C 2 x l R M 9 X r 3 k L 8 z x u k J m z 4 O Z d J a l C y 1 a I w F c T E Z P E 1 G X O F z I j M E s o U t 7 c S N q W K M m O z q d g Q v P W X N 0 n 3 p u 6 5 d a / t 1 Z p u E U c Z L u A S r s G D W 2 j C P b S g A w w Q n u E V 3 p x H 5 8 V 5 d z 5 W r S W n m D m H P 3 A + f w D h Y 4 z o &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A r Q D m P u Z h E W j z h L f w z v 7 Y l 0 y 4 N k = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l u M t 6 i l z W A Q r M K u j X Y G b C w T M B d I l j A 7 O Z u M m Z 1 d Z m b F u O Q J b C w U s f U Z f B I 7 S 9 / E y a X Q x B 8 G P v 7 / H O a c E y S C a + O 6 X 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S c K o Z 1 F o t Y t Q K q U X C J d c O N w F a i k E a B w G Y w v J r k z T t U m s f y x o w S 9 C P a l z z k j B p r 1 e 6 7 x Z J b d q c i y + D N o X T 5 8 f B d A Y B q t / j Z 6 c U s j V A a J q j W b c 9 N j J 9 R Z T g T O C 5 0 U o 0 J Z U P a x 7 Z F S S P U f j Y d d E x O r N M j Y a z s k 4 Z M 3 d 8 d G Y 2 0 H k W B r Y y o G e j F b G L + l 7 V T E 1 7 4 G Z d J a l C y 2 U d h K o i J y W R r 0 u M K m R E j C 5 Q p b m c l b E A V Z c b e p m C P 4 C 2 u v A y N s 7 L n l r 2 a V 6 q 4 M F M e j u A Y T s G D c 6 j A N V S h D g w Q H u E Z X p x b 5 8 l 5 d d 5 m p T l n 3 n M I f + S 8 / w B T 2 I 9 1 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L h V c t 7 g V O o n X 0 W k d v s p b X q 5 u S S U = " &gt; A A A B 6 H i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 9 E j i x S M k F k i g I d t l C i v b b b O 7 N Z K G X + D F g 8 Z 4 9 S d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v T A X X x n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p 6 y R T D H 2 W i E R 1 Q 6 p R c I m + 4 U Z g N 1 V I 4 1 B g J 5 z c z v 3 O I y r N E 3 l v p i k G M R 1 J H n F G j Z V a T 4 N q z a 2 7 C 5 B 1 4 h W k B g W a g + p X f 5 i w L E Z p m K B a 9 z w 3 N U F O l e F M 4 K z S z z S m l E 3 o C H u W S h q j D v L F o T N y Y Z U h i R J l S x q y U H 9 P 5 D T W e h q H t j O m Z q x X v b n 4 n 9 f L T H Q T 5 F y m m U H J l o u i T B C T</head><formula xml:id="formula_3">k P n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c W G 4 K 2 + v E 7</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>x 7 Z F S S P U f j Y d d E x O r N M j Y a z s k 4 Z M 3 d 8 d G Y 2 0 H k W B r Y y o G e j F b G L + l 7 V T E 1 7 4 G Z d J a l C y 2 U d h K o i J y W R r 0 u M K m R E j C 5 Q p b m c l b E A V Z c b e p m C P 4 C 2 u v A y N s 7 L n l r 2 a V 6 q 4 M F M e j u A Y T s G D c 6 j A N V S h D g w Q H u E Z X p x b 5 8 l 5 d d 5 m p T l n 3 n M I f + S 8 / w B T 2 I 9 1 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L h V c t 7 g V O o n X 0 W k d v s p b X q 5 u S S U = " &gt; A A A B 6 H i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 9 E j i x S M k F k i g I d t l C i v b b b O 7 N Z K G X + D F g 8 Z 4 9 S d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v T A X X x n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p 6 y R T D H 2 W i E R 1 Q 6 p R c I m + 4 U Z g N 1 V I 4 1 B g J 5 z c z v 3 O I y r N E 3 l v p i k G M R 1 J H n F G j Z V a T 4 N q z a 2 7 C 5 B 1 4 h W k B g W a g + p X f 5 i w L E Z p m K B a 9 z w 3 N U F O l e F M 4 K z S z z S m l E 3 o C H u W S h q j D v L F o T N y Y Z U h i R J l S x q y U H 9 P 5 D T W e h q H t j O m Z q x X v b n 4 n 9 f L T H Q T 5 F y m m U H J l o u i T B C T k P n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c W G 4 K 2 + v E 7 a V 3 X P r X s t r 9 Z w i z j K c A b n c A k e X E M D 7 q A J P j B A e I Z X e H M e n B f n 3 f l Y t p a c Y u Y U / s D 5 / A H f 3 4 z n &lt; / l a t e x i t &gt;</head><p>N (µ, )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V b / P R R J 9 h L R m z 1 n K Z l / X 7 P r P e e s = " &gt; A A A C A H i c b V A 9 S w N B E J 2 L X z F + n V p Y 2 C w G I Y K E O x s t A z Z W E s F 8 Q O 4 I e 5 u 9 Z M n u 3 b G 7 J 8 Q j j X / F x k I R W y t / g 4 X g v 3 E v S a G J D w Y e 7 8 0 w M y 9 I O F P a c b 6 t w t L y y u p a c b 2 0 s b m 1 v W P v 7 j V V n E p C G y T m s W w H W F H O I t r Q T H P a T i T F I u C 0 F Q w v c 7 9 1 R 6 V i c X S r R w n 1 B e 5 H L G Q E a y N 1 7 Q N P Y D 0 g m G f X 4 4 o n 0 l N P s b 7 A J 1 2 7 7 F S d C d A i c W e k X C v e f 3 0 A Q L 1 r f 3 q 9 m K S C R p p w r F T H d R L t Z 1 h q R j g d l 7 x U 0 Q S T I e 7 T j q E R F l T 5 2 e S B M T o 2 S g + F s T Q V a T R R f 0 9 k W C g 1 E o H p z M 9 V 8 1 4 u / u d 1 U h 1 e + B m L k l T T i E w X h S l H O k Z 5 G q j H J C W a j w z B R D J z K y I D L D H R J r O S C c G d f 3 m R N M + q r l N 1 b 9 x y r Q J T F O E Q j q A C L p x D D a 6 g D g 0 g M I Z H e I Y X 6 8 F 6 s l 6 t t 2 l r w Z r N 7 M M f W O 8 / c u C Y c A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m n 7 F V Z / e O d h m a y z k + 3 8 W 7 f 7 q + k w = " &gt; A A A C A H i c b V B N S 8 N A E N 3 4 W e t X 1 I M H L 8 E i V J C S e N F j w Y s n q W A / o A l l s t 2 0 S 3 c 3 Y X c j l J C L f 8 W L B 0 W 8 + j O 8 + W / c t D l o 6 4 O B x 3 s z z M w L E 0 a V d t 1 v a 2 V 1 b X 1 j s 7 J V 3 d 7 Z 3 d u 3 D w 4 7 K k 4 l J m 0 c s 1 j 2 Q l C E U U H a m m p G e o k k w E N G u u H k p v C 7 j 0 Q q G o s H P U 1 I w G E k a E Q x a C M N 7 G O f g x 5 j Y N l d X v d 5 e u E r O u J w P r B r b s O d w V k m X k l q q E R r Y H / 5 w x i n n A i N G S j V 9 9 x E B x l I T T E j e d V P F U k A T 2 B E + o Y K 4 E Q F 2 e y B 3 D k z y t C J Y m l K a G e m / p 7 I g C s 1 5 a H p L M 5 V i 1 4 h / u f 1 U x 1 d B x k V S a q J w P N F U c o c H T t F G s 6 Q S o I 1 m x o C W F J z q 4 P H I A F r k 1 n V h O A t v r x M O p c N z 2 1 4 9 1 6 t W S / j q K A T d I r q y E N X q I l u U Q u 1 E U Y 5 e k a v 6 M 1 6 s l 6 s d + t j 3 r p i l T N H 6 A + s z x 9 J k Z Y Y &lt; / l a t e x i t &gt;</head><p>N (µ, )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V b / P R R J 9 h L R m z 1 n K Z l / X 7 P r P e e s = " &gt; A A A C A H i c b V A 9 S w N B E J 2 L X z F + n V p Y 2 C w G I Y K E O x s t A z Z W E s F 8 Q O 4 I e 5 u 9 Z M n u 3 b G 7 J 8 Q j j X / F x k I R W y t / g 4 X g v 3 E v S a G J D w Y e 7 8 0 w M y 9 I O F P a c b 6 t w t L y y u p a c b 2 0 s b m 1 v W P v 7 j V V n E p C G y T m s W w H W F H O I t r Q T H P a T i T F I u C 0 F Q w v c 7 9 1 R 6 V i c X S r R w n 1 B e 5 H L G Q E a y N 1 7 Q N P Y D 0 g m G f X 4 4 o n 0 l N P s b 7 A J 1 2 7 7 F S d C d A i c W e k X C v e f 3 0 A Q L 1 r f 3 q 9 m K S C R p p w r F T H d R L t Z 1 h q R j g d l 7 x U 0 Q S T I e 7 T j q E R F l T 5 2 e S B M T o 2 S g + F s T Q V a T R R f 0 9 k W C g 1 E o H p z M 9 V 8 1 4 u / u d 1 U h 1 e + B m L k l T T i E w X h S l H O k Z 5 G q j H J C W a j w z B R D J z K y I D L D H R J r O S C c G d f 3 m R N M + q r l N 1 b 9 x y r Q J T F O E Q j q A C L p x D D a 6 g D g 0 g M I Z H e I Y X 6 8 F 6 s l 6 t t 2 l r w Z r N 7 M M f W O 8 / c u C Y c A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m n 7 F V Z / e O d h m a y z k + 3 8 W 7 f 7 q + k w = " &gt; A A A C A H i c b V B N S 8 N A E N 3 4 W e t X 1 I M H L 8 E i V J C S e N F j w Y s n q W A / o A l l s t 2 0 S 3 c 3 Y X c j l J C L f 8 W L B 0 W 8 + j O 8 + W / c t D l o 6 4 O B x 3 s z z M w L E 0 a V d t 1 v a 2 V 1 b X 1 j s 7 J V 3 d 7 Z 3 d u 3 D w 4 7 K k 4 l J m 0 c s 1 j 2 Q l C E U U H a m m p G e o k k w E N G u u H k p v C 7 j 0 Q q G o s H P U 1 I w G E k a E Q x a C M N 7 G O f g x 5 j Y N l d X v d 5 e u E r O u J w P r B r b s O d w V k m X k l q q E R r Y H / 5 w x i n n A i N G S j V 9 9 x E B x l I T T E j e d V P F U k A T 2 B E + o Y K 4 E Q F 2 e y B 3 D k z y t C J Y m l K a G e m / p 7 I g C s 1 5 a H p L M 5 V i 1 4 h / u f 1 U x 1 d B x k V S a q J w P N F U c o c H T t F G s 6 Q S o I 1 m x o C W F J z q 4 P H I A F r k 1 n V h O A t v r x M O p c N z 2 1 4 9 1 6 t W S / j q K A T d I r q y E N X q I l u U Q u 1 E U Y 5 e k a v 6 M 1 6 s l 6 s d + t j 3 r p i l T N H 6 A + s z x 9 J k Z Y Y &lt; / l a t e x i t &gt;</head><p>N (µ 0 , 0 )   In the Gaussian case, the KL term tends to pull the model to- wards the prior (moving from µ, σ to µ , σ ), whereas in the vMF case there is no such pressure towards a single distribution.</p><formula xml:id="formula_4">&lt;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " A r Q D m P u Z h E W j z h L f w z v 7 Y l 0 y 4 N k = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l u M t 6 i l z W A Q r M K u j X Y G b C w T M B d I l j A 7 O Z u M m Z 1 d Z m b F u O Q J b C w U s f U Z f B I 7 S 9 / E y a X Q x B 8 G P v 7 / H O a c E y S C a + O 6 X 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S c K o Z 1 F o t Y t Q K q U X C J d c O N w F a i k E a B w G Y w v J r k z T t U m s f y x o w S 9 C P a l z z k j B p r 1 e 6 7 x Z J b d q c i y + D N o X T 5 8 f B d A Y B q t / j Z 6 c U s j V A a J q j W b c 9 N j J 9 R Z T g T O C 5 0 U o 0 J Z U P a x 7 Z F S S P U f j Y d d E x O r N M j Y a z s k 4 Z M 3 d 8 d G Y 2 0 H k W B r Y y o G e j F b G L + l 7 V T E 1 7 4 G Z d J a l C y 2 U d h K o i J y W R r 0 u M K m R E j C 5 Q p b m c l b E A V Z c b e p m C P 4 C 2 u v A y N s 7 L n l r 2 a V 6 q 4 M F M e j u A Y T s G D c 6 j A N V S h D g w Q H u E Z X p x b 5 8 l 5 d d 5 m p T l n 3 n M I f + S 8 / w B T 2 I 9 1 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L h V c t 7 g V O o n X 0 W k d v s p b X q 5 u S S U = " &gt; A A A B 6 H i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 9 E j i x S M k F k i g I d t l C i v b b b O 7 N Z K G X + D F g 8 Z 4 9 S d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v T A X X x n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p 6 y R T D H 2 W i E R 1 Q 6 p R c I m + 4 U Z g N 1 V I 4 1 B g J 5 z c z v 3 O I y r N E 3 l v p i k G M R 1 J H n F G j Z V a T 4 N q z a 2 7 C 5 B 1 4 h W k B g W a g + p X f 5 i w L E Z p m K B a 9 z w 3 N U F O l e F M 4 K z S z z S m l E 3 o C H u W S h q j D v L F o T N y Y Z U h i R J l S x q y U H 9 P 5 D T W e h q H t j O m Z q x X v b n 4 n 9 f L T H Q T 5 F y m m U H J l o u i T B C T k P n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c W G 4 K 2 + v E 7 a V 3 X P r X s t r 9 Z w i z j K c A b n c A k e X E M D 7 q A J P j B A e I Z X e H M e n B f n 3 f l Y t p a c Y u Y U / s D 5 / A H f 3 4 z n &lt; / l a t e x i t &gt; x &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A r Q D m P u Z h E W j z h L f w z v 7 Y l 0 y 4 N k = " &gt; A A A B 6 H i c b Z C 7 S g N B F I b P x l u M t 6 i l z W A Q r M K u j X Y G b C w T M B d I l j A 7 O Z u M m Z 1 d Z m b F u O Q J b C w U s f U Z f B I 7 S 9 / E y a X Q x B 8 G P v 7 / H O a c E y S C a + O 6 X 0 5 u Z X V t f S O / W d j a 3 t n d K + 4 f N H S c K o Z 1 F o t Y t Q K q U X C J d c O N w F a i k E a B w G Y w v J r k z T t U m s f y x o w S 9 C P a l z z k j B p r 1 e 6 7 x Z J b d q c i y + D N o X T 5 8 f B d A Y B q t / j Z 6 c U s j V A a J q j W b c 9 N j J 9 R Z T g T O C 5 0 U o 0 J Z U P a x 7 Z F S S P U f j Y d d E x O r N M j Y a z s k 4 Z M 3 d 8 d G Y 2 0 H k W B r Y y o G e j F b G L + l 7 V T E 1 7 4 G Z d J a l C y 2 U d h K o i J y W R r 0 u M K m R E j C 5 Q p b m c l b E A V Z c b e p m C P 4 C 2 u v A y N s 7 L n l r 2 a V 6 q 4 M F M e j u A Y T s G D c 6 j A N V S h D g w Q H u E Z X p x b 5 8 l 5 d d 5 m p T l n 3 n M I f + S 8 / w B T 2 I 9 1 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L h V c t 7 g V O o n X 0 W k d v s p b X q 5 u S S U = " &gt; A A A B 6 H i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e 9 E j i x S M k F k i g I d t l C i v b b b O 7 N Z K G X + D F g 8 Z 4 9 S d 5 8 9 + 4 Q A 8 K v m S S l / d m M j M v T A X X x n W / n d L G 5 t b 2 T n m 3 s r d / c H h U P T 5 p 6 y R T D H 2 W i E R 1 Q 6 p R c I m + 4 U Z g N 1 V I 4 1 B g J 5 z c z v 3 O I y r N E 3 l v p i k G M R 1 J H n F G j Z V a T 4 N q z a 2 7 C 5 B 1 4 h W k B g W a g + p X f 5 i w L E Z p m K B a 9 z w 3 N U F O l e F M 4 K z S z z S m l E 3 o C H u W S h q j D v L F o T N y Y Z U h i R J l S x q y U H 9 P 5 D T W e h q H t j O m Z q x X v b n 4 n 9 f L T H Q T 5 F y m m U H J l o u i T B C T k P n X Z M g V M i O m l l C m u L 2 V s D F V l B m b T c W G 4 K 2 + v E 7 a V 3 X P r X s t r 9 Z w i z j K c A b n c A k e X E M D 7 q A J P j B A e I Z X e H M e n B f n 3 f l Y t p a c Y u Y U / s D 5 / A H f 3 4 z n &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 o E B U 6 / J 0 u E M h I w b K Y 5 U 1 r 5 C 6 l 0 = " &gt; A A A B 6 H i c b Z C 7 S w N B E M b n 4 i u J r 6 i l z W I Q r M K d j Z Y B G 8 s E z A O S E P b 2 5 p I 1 e 3 v H 7 p 5 w H K k t b C w U s R P / J D v / G z e P Q h M / W P j x f T P s z P i J 4 N q 4 7 r d T 2 N j c 2 t 4 p l s q 7 e / s H h 5 W j 4 7 a O U 8 W w x W I R q 6 5 P N Q o u s W W 4 E d h N F N L I F 9 j x J z e z v P O A S v N Y 3 p k s w U F E R 5 K H n F F j r W Y 2 r F T d m j s X W Q d v C d V 6 6 T H 4 A I D G s P L V D 2 K W R i g N E 1 T r n u c m Z p B T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 5 F S S P U g 3 w + 6 J S c W y c g Y a z s k 4 b M 3 d 8 d O Y 2 0 z i L f V k b U j P V q N j P / y 3 q p C a 8 H O Z d J a l C y x U d h K o i J y W x r E n C F z I j M A m W K 2 1 k J G 1 N F m b G 3 K d s j e K s r r 0 P 7 s u a 5 N a / p V e s u L F S E U z i D C / D g C u p w C w 1 o A Q O E J 3 i B V + f e e X b e n P d F a c F Z 9 p z A H z m f P 1 j 9 j r w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 X Z Z c n M r W f X V m S 7 5 j / o 6 8 e B 3 Z S k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 2 G P B i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I I / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W t 7 Z 3 d v f J + 5 e D w 6 P i k e n r W 1 X G q G H Z Y L G L V D 6 h G w S V 2 D D c C + 4 l C G g U C e 8 H s b u H 3 n l B p H s s H k y X o R 3 Q i e c g Z N V Z q Z 6 N q z a 2 7 S 5 B N 4 h W k B g V a o + r X c B y z N E J p m K B a D z w 3 M X 5 O l e F M 4 L w y T D U m l M 3 o B A e W S h q h 9 v P l o X N y Z Z U x C W N l S x q y V H 9 P 5 D T S O o s C 2 x l R M 9 X r 3 k L 8 z x u k J m z 4 O Z d J a l C y 1 a I w F c T E Z P E 1 G X O F z I j M E s o U t 7 c S N q W K M m O z q d g Q v P W X N 0 n 3 p u 6 5 d a / t 1 Z p u E U c Z L u A S r s G D W 2 j C P b S g A w w Q n u E V 3 p x H 5 8 V 5 d z 5 W r S W n m D m H P 3 A + f w D h Y 4 z o &lt; / l a t e x i t &gt; y &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 o E B U 6 / J 0 u E M h I w b K Y 5 U 1 r 5 C 6 l 0 = " &gt; A A A B 6 H i c b Z C 7 S w N B E M b n 4 i u J r 6 i l z W I Q r M K d j Z Y B G 8 s E z A O S E P b 2 5 p I 1 e 3 v H 7 p 5 w H K k t b C w U s R P / J D v / G z e P Q h M / W P j x f T P s z P i J 4 N q 4 7 r d T 2 N j c 2 t 4 p l s q 7 e / s H h 5 W j 4 7 a O U 8 W w x W I R q 6 5 P N Q o u s W W 4 E d h N F N L I F 9 j x J z e z v P O A S v N Y 3 p k s w U F E R 5 K H n F F j r W Y 2 r F T d m j s X W Q d v C d V 6 6 T H 4 A I D G s P L V D 2 K W R i g N E 1 T r n u c m Z p B T Z T g T O C 3 3 U 4 0 J Z R M 6 w p 5 F S S P U g 3 w + 6 J S c W y c g Y a z s k 4 b M 3 d 8 d O Y 2 0 z i L f V k b U j P V q N j P / y 3 q p C a 8 H O Z d J a l C y x U d h K o i J y W x r E n C F z I j M A m W K 2 1 k J G 1 N F m b G 3 K d s j e K s r r 0 P 7 s u a 5 N a / p V e s u L F S E U z i D C / D g C u p w C w 1 o A Q O E J 3 i B V + f e e X b e n P d F a c F Z 9 p z A H z m f P 1 j 9 j r w = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 3 X Z Z c n M r W f X V m S 7 5 j / o 6 8 e B 3 Z S k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 2 G P B i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I I / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 6 W t 7 Z 3 d v f J + 5 e D w 6 P i k e n r W 1 X G q G H Z Y L G L V D 6 h G w S V 2 D D c C + 4 l C G g U C e 8 H s b u H 3 n l B p H s s H k y X o R 3 Q i e c g Z N V Z q Z 6 N q z a 2 7 S 5 B N 4 h W k B g V a o + r X c B y z N E J p m K B a D z w 3 M X 5 O l e F M 4 L w y T D U m l M 3 o B A e W S h q h 9 v P l o X N y Z Z U x C W N l S x q y V H 9 P 5 D T S O o s C 2 x l R M 9 X r 3 k L 8 z x u k J m z 4 O Z d J a l C y 1 a I w F c T E Z P E 1 G X O F z I j M E s o U t 7 c S N q W K M m O z q d g Q v P W X N 0 n 3 p u 6 5 d a / t 1 Z p u E U c Z L u A S r s G D W 2 j C P b S g A w w Q n u E V 3 p x H 5 8 V 5 d z 5 W r S W n m D m H P 3 A + f w D h Y 4 z o &lt; / l a t e x i t &gt; µ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O w / 7 B p I 0 b l v X 5 4 d i W w t l u A n M + U Y = " &gt; A A A B 6 n i c b Z C 7 S w N B E M b n f C b x F b W 0 W Q y C V b i z 0 T J g Y x n R P C A J Y W 9 v L 1 m y u 3 f s z g n h S G 1 l Y 6 G I j Y V / k Z 3 / j Z t H o Y k f L P z 4 v h l 2 Z s J U C o u + / + 2 t r W 9 s b m 0 X i q W d 3 b 3 9 g / L h U d M m m W G 8 w R K Z m H Z I L Z d C 8 w Y K l L y d G k 5 V K H k r H F 1 P 8 9 Y D N 1 Y k + h 7 H K e 8 p O t A i F o y i s + 6 6 K u u X K 3 7 V n 4 m s Q r C A S q 3 4 G H 0 A Q L 1 f / u p G C c s U 1 8 g k t b Y T + C n 2 c m p Q M M k n p W 5 m e U r Z i A 5 4 x 6 G m i t t e P h t 1 Q s 6 c E 5 E 4 M e 5 p J D P 3 d 0 d O l b V j F b p K R X F o l 7 O p + V / W y T C + 6 u V C p x l y z e Y f x Z k k m J D p 3 i Q S h j O U Y w e U G e F m J W x I D W X o r l N y R w i W V 1 6 F 5 k U 1 8 K v B b V C p + T B X A U 7 g F M 4 h g E u o w Q 3 U o Q E M B v A E L / D q S e / Z e / P e 5 6 V r 3 q L n G P 7 I + / w B z o + P l Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 M P K d a Z C b L F n K r q B 8 v s w j p i N l k = " &gt; A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 s t A z Y W E Y 0 H 5 A c Y W 8 z l y z Z 3 T t 2 9 4 R w 5 C f Y W C h i 6 y + y 8 9 + 4 S a 7 Q x A c D j / d m m J k X p Y I b 6 / v f X m l j c 2 t 7 p 7 x b 2 d s / O D y q H p + 0 T Z J p h i 2 W i E R 3 I 2 p Q c I U t y 6 3 A b q q R y k h g J 5 r c z v 3 O E 2 r D E / V o p y m G k o 4 U j z m j 1 k k P f Z k N q j W / 7 i 9 A 1 k l Q k B o U a A 6 q X / 1 h w j K J y j J B j e k F f m r D n G r L m c B Z p Z 8 Z T C m b 0 B H 2 H F V U o g n z x a k z c u G U I Y k T 7 U p Z s l B / T + R U G j O V k e u U 1 I 7 N q j c X / / N 6 m Y 1 v w p y r N L O o 2 H J R n A l i E z L / m w y 5 R m b F 1 B H K N H e 3 E j a m m j L r 0 q m 4 E I L</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v L 1 m y u 3 f s z g n h S G 1 l Y 6 G I j Y V / k Z 3 / j Z t H o Y k f L P z 4 v h l 2 Z s J U C o u + / + 2 t r W 9 s b m 0 X i q W d 3 b 3 9 g / L h U d M m m W G 8 w R K Z m H Z I L Z d C 8 w Y K l L y d G k 5 V K H k r H F 1 P 8 9 Y D N 1 Y k + h 7 H K e 8 p O t A i F o y i s + 6 6 K u u X K 3 7 V n 4 m s Q r C A S q 3 4 G H 0 A Q L 1 f / u p G C c s U 1 8 g k t b Y T + C n 2 c m p Q M M k n p W 5 m e U r Z i A 5 4 x 6 G m i t t e P h t 1 Q s 6 c E 5 E 4 M e 5 p J D P 3 d 0 d O l b V j F b p K R X F o l 7 O p + V / W y T C + 6 u V C p x l y z e Y f x Z k k m J D p 3 i Q S h j O U Y w e U G e F m J W x I D W X o r l N y R w i W V 1 6 F 5 k U 1 8 K v B b V C p + T B X A U 7 g F M 4 h g E u o w Q 3 U o Q E M B v A E L / D q S e / Z e / P e 5 6 V r 3 q L n G P 7 I + / w B z o + P l Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M 7 M P K d a Z C b L F n K r q B 8 v s w j p i N l k = " &gt; A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o N g F e 5 s t A z Y W E Y 0 H 5 A c Y W 8 z l y z Z 3 T t 2 9 4 R w 5 C f Y W C h i 6 y + y 8 9 + 4 S a 7 Q x A c D j / d m m J k X p Y I b 6 / v f X m l j c 2 t 7 p 7 x b 2 d s / O D y q H p + 0 T Z J p h i 2 W i E R 3 I 2 p Q c I U t y 6 3 A b q q R y k h g J 5 r c z v 3 O E 2 r D E / V o p y m G k o 4 U j z m j 1 k k P f Z k N q j W / 7 i 9 A 1 k l Q k B o U a A 6 q X / 1 h w j K J y j J B j e k F f m r D n G r L m c B Z p Z 8 Z T C m b 0 B H 2 H F V U o g n z x a k z c u G U I Y k T 7 U p Z s l B / T + R U G j O V k e u U 1 I 7 N q j c X / / N 6 m Y 1 v w p y r N L O o 2 H J R n A l i E z L / m w y 5 R m b F 1 B H K N H e 3 E j a m m j L r 0 q m 4 E I L V l 9 d J + 6 o e + P X g P q g 1 /</head><formula xml:id="formula_5">C K O M p z B O V x C A N f Q g D t o Q g s Y j O A Z X u H N E 9 6 L 9 + 5 9 L F t L X j F z C n / g</formula><p>direction µ is the output of encoding neural net- works ( <ref type="figure" target="#fig_0">Figure 1</ref>, right side) and κ is treated as a constant.</p><p>Before we can implement a VAE, we need to derive an expression for KL divergence in order to optimize ELBO (Equation 1) and give a sampling algorithm that admits the reparameterization trick ( <ref type="bibr" target="#b12">Kingma and Welling, 2013)</ref>.</p><p>KL divergence With vMF(·, 0) as our prior, the KL divergence is: 4</p><formula xml:id="formula_6">KL(vMF(µ, κ)||vMF(·, 0)) = κ I d/2 (κ) I d/2−1 (κ) + d 2 − 1 log κ − d 2 log(2π) − log I d/2−1 (κ) + d 2 log π + log 2 − log Γ d 2</formula><p>Critically, this only depends on κ, not on µ. κ will be treated as a fixed hyperparameter, so this term will be constant for our model; KL collapse will therefore be rendered impossible. <ref type="figure" target="#fig_4">Figure 2</ref> shows a visualization of the learning trajectories of Gaussian and vMF VAE. For the Gaussian VAE, the KL divergence in the objec- tive function tends to pull the posterior towards the prior centered at the origin and, therefore, make the optimization difficult as mentioned before. For the vMF VAE, given fixed κ, there is no such vac- uous state and µ can vary freely.   Sampling from vMF Following the implemen- tation of <ref type="bibr" target="#b7">Guu et al. (2018)</ref>, we use the rejection sampling scheme of <ref type="bibr" target="#b25">Wood (1994)</ref> to sample a "change magnitude" w. Our sample is then given by z = wµ + v √ 1 − w 2 , where v is a randomly sampled unit vector tangent to the hypersphere at µ. Neither v nor w depends on µ, so we can now take gradients of z with respect to µ as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Language Modeling</head><p>We first evaluate our vMF approach in the NVRNN setting. We will return to this model and analyze its properties further in Sections 6 and 7 after showing experiments on document modeling.</p><p>Dataset For NVRNN, we use the Penn Treebank ( <ref type="bibr" target="#b13">Marcus et al., 1993)</ref>, also used in <ref type="bibr" target="#b2">Bowman et al. (2016)</ref>, and Yelp 2013 ( <ref type="bibr" target="#b26">Xu et al., 2016)</ref>. Examples in the Yelp dataset are much longer and more di- verse than those from PTB, requiring more under- standing of high-level semantics to generate a co- herent sequence. Yelp has a long tail of very long reviews, so we truncate the examples to a maxi- mum length of 50 words; this still gives an aver- age length over twice as long as in the PTB setting.  Statistics about all datasets used in this paper are shown in <ref type="table" target="#tab_4">Table 2</ref>.</p><p>Settings We evaluate our NVRNN as in Bow- man et al. <ref type="formula">(2016)</ref> and explore two different set- tings. In the Standard setting, the input to the RNN at each time step is the concatenation of the latent code z and the ground truth word from the last time step, while the Inputless setting does not use the prior word. The more powerful decoder of the Standard setting makes the latent represen- tations inherently less useful. In the Inputless set- ting, the decoder needs to predict the whole se- quence with only the help of given latent code. In this case, a high-quality representation of the sen- tence is badly needed and the model is driven to learn it. Our implementation of VAE uses a one layer unidirectional LSTM as both encoder and decoder. We use an embedding size of 100 and hidden units of size 400 in the LSTM. The dimension of the la- tent code is chosen from {25, 50, 100} by tuning on the development set. We use SGD to optimize all models with decayed learning rate and gradient clipping. For Yelp, the sentiment bit, which ranges from 1 to 5, is also embedded into a 50 dimension vector and input for every time step of the decod- ing phase.</p><p>Results Experimental results of the NVRNN are shown in <ref type="table" target="#tab_6">Table 3</ref>. We report negative log likeli- hood (NLL) <ref type="bibr">5</ref>    <ref type="figure">Figure 4</ref>: Comparison of Gaussian-and vMF-NVRNN with different hyper-parameters. All models are trained on PTB in the Inputless setting where the latent dimen- sion is 50. G-α indicates Gaussian VAE with KL an- nealed by the given constant α, and V-κ indicates VAE with κ set to the given value. The green bar reflects the amount of KL loss while the total height reflects the whole objective. Numbers above bars are perplexity. vMF is more highly tunable and also achieves stronger results across a wide range of κ values.</p><p>perplexities, and even when KL collapse does not appear to be the case (e.g., G-VAE on the PTB- Standard setting), a Gaussian family of distribu- tions results in lower KLs and worse log likeli- hoods, possibly due to optimization challenges.</p><p>In the Inputless setting, we see large gains: vMF VAE reduces PPL from 379 to 262 in PTB, and from 256 to 134 in Yelp compared to Gaussian VAE.</p><p>Trade-off Comparison Besides the overall per- plexity, we are also interested in the trade-off be- tween reconstruction loss and KL, and the con- tribution of KL to the whole objective. <ref type="figure">Figure 4</ref> shows the ability of our model to explicitly control the balance between the KL and the reconstruction term. First, we "permanently" anneal the Gaussian VAE by setting the weight of the KL term to a con- stant smaller than 1 (0.2 and 0.5 in our case). We find that this trick does mitigate the KL collapse, but the overall performance is worse. Therefore, this is not only a numerical game about the KL vs. NLL trade-off but a deeper challenge of how to structure models to learn effective latent repre- sentations.</p><p>For vMF VAE, when we gradually increase the value of κ, the concentration of the distribution around the mean direction µ is higher and samples from vMF are closer to µ. The model achieves the best perplexity when κ = 80. The reconstruc- tion error is bounded around 4.5 due to the dif- ficulty of the task and limited capacity of LSTM decoder. While κ is a hyperparameter that needs to be tuned, the model is overall not very sensitive to it, and we show in Section 7 that reasonable κ values transfer across similar tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Document Modeling</head><p>We also investigate how vMF VAE performs in a different setting, one less plagued by the KL col- lapse issue. Specifically, the Neural Variational Document Model (NVDM), proposed by <ref type="bibr" target="#b14">Miao et al. (2016)</ref>, is a VAE-based unsupervised doc- ument model. This model follows the VAE frame- work introduced in Section 2. Our document rep- resentation is an indicator vector x of word pres- ence or absence in the document. Since this is a fixed-size representation, we use 2-layer MLPs with 400 hidden units for both the encoder q(z|x) and decoder p(x|z); the decoder places a simple</p><formula xml:id="formula_7">Model NVRNN NVRNN-BoW Setting µ → BoW BoW → µ µ → BoW G-VAE 0.74 0.74 0.32 v-VAE</formula><p>0.77 0.57 0.23 <ref type="table">Table 6</ref>: Average cosine similarity when trying to re- construct the latent code µ from the bag of words and vice versa. In vMF, the latent code contains more infor- mation beyond the bag of words, as shown by the lower cosine similarity when predicting BoW → µ (0.57).</p><p>When the latent code is learned in a model conditioned on the bag of words (right column), it predicts the bag of words much less well, indicating that the model suc- cessfully learns orthogonal information.</p><p>similarity than G-VAE (0.23 vs. 0.32), indicating that it capturing less redundant information and using the latent space to more efficiently model other properties of the data.</p><p>Sensitivity to word order <ref type="table">Table 6</ref> shows that NVRNN with vMF encodes information beyond the bag of words; a natural hypothesis is that it is encoding word order. We can more directly in- vestigate this in the context of both NVRNN and NVRNN-BoW settings. Inspired by <ref type="bibr" target="#b31">Zhao et al. (2017a)</ref>, we propose an experiment probing the sensitivity to randomly swapping adjacent pairs of words for the encoding in the Inputless setting on PTB. We vary the probability of swapping each word pair and see how the latent code changes as the number of swaps increases. Ideally, our mod- els should capture ordering information and there- fore be sensitive to this change. <ref type="figure" target="#fig_7">Figure 5</ref> shows the results. v-VAE's representa- tions are more sensitive than those of the G-VAE: they change faster as swaps become more likely. 8 In the NVRNN-BoW setting, we see that the mod- els are even more sensitive. vMF enables us to more easily learn this kind of desirable informa- tion in our sentence encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Controlling Variance with κ</head><p>A core aspect of our approach so far has been treat- ing κ as a fixed hyperparameter. Fixing κ is ben- eficial from an optimization standpoint: it makes it more difficult for the model to get stuck in local optima. But it also reduces the model's flexibility, since we can no longer predict per-example κ val- ues, and it introduces another parameter that the <ref type="bibr">8</ref> The Gaussian VAE here makes very little use of the latent variable, hence why the representations change very little. : Sensitivity of latent codes to swapping ad- jacent words of encoding sequence. Cosine similar- ity is measured between the latent code (encoded mean vector) of the original sentence and the sentence after swaps are applied. We see that vMF is more highly sen- sitive to swaps in both the NVRNN and NVRNN-BoW settings, indicating that its latent space likely encodes more ordering information. system designer must tune.</p><p>Fortunately, a wide range of κ values appear to work well for the tasks we consider. <ref type="figure" target="#fig_8">Figure 6</ref> shows how the concentration parameter κ changes the results on PTB when the latent dimension and other hyperparameters are held fixed. We have ordered the tasks left-to-right from "hardest" to "easiest" in terms of necessity of latent represen- tation: the Inputless setting needs heavy informa- tion from the latent code to reconstruct the sen- tence, whereas the Standard-BoW setting has an extremely strong decoder to predict the next word. We see that in each of these cases, a wide range of κ values works, and moreover reasonable κ values transfer between the two Standard and between the two Inputless settings, indicating that the overall approach is not highly sensitive to these hyperpa- rameter values.</p><p>Brittleness of Learning κ Throughout this work, we have treated κ as a fixed parameter. However, we can treat κ in the same way as σ in the Gaussian case and learn it on a per-instance basis. The KL divergence of vMF is differentiable with respect to κ given gradients of the modified Bessel function of the first kind, 9 allowing us to change the concentration on a per-instance basis. However, this reintroduces the issue of KL col- lapse: the KL term will encourage κ to be as low Darker colors correspond to perplexity values closer to the best observed for that setting. For each task, we see that there is a range of κ values that work well, and these transfer between comparable tasks. as possible, potentially making the latent variable vacuous.</p><p>In practice, we observe that it is necessary to clip κ values to a certain range for numerical rea- sons. Within this range, the model gravitates to- wards the smallest κ values and performs substan- tially worse than models trained with our fixed κ approach. This indicates that even with the vMF model, the optimization problem posed by ELBO is simply a hard one and the approach of fixing KL divergence is a surprisingly good optimization technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Applications of VAE in NLP Deep generative models have achieved impressive successes in do- mains adjacent to NLP such as image genera- tion ( <ref type="bibr" target="#b6">Gregor et al., 2015;</ref><ref type="bibr" target="#b16">Oord et al., 2016a</ref>) and speech generation ( <ref type="bibr" target="#b4">Chung et al., 2015;</ref><ref type="bibr" target="#b17">Oord et al., 2016b</ref>). VAEs specifically ( <ref type="bibr" target="#b12">Kingma and Welling, 2013;</ref><ref type="bibr" target="#b18">Rezende et al., 2014)</ref> have been a popular model variant in NLP. They have been applied to tasks including document modeling ( <ref type="bibr" target="#b14">Miao et al., 2016)</ref>, language modeling ( <ref type="bibr" target="#b2">Bowman et al., 2016)</ref>, and dialogue generation ( <ref type="bibr" target="#b21">Serban et al., 2017b</ref>). VAEs can be also be applied for semi-supervised classification ( <ref type="bibr" target="#b27">Xu et al., 2017)</ref>. Recent twists on the standard VAE approach including combining VAE and holistic attribute discriminators for con- ditional generation (  and using a more flexible latent space regularized by an adver- sarial method ( <ref type="bibr" target="#b31">Zhao et al., 2017a</ref>). VAE Objective Several pieces of recent work have highlighted the issues with optimizing the VAE objective. <ref type="bibr" target="#b0">Alemi et al. (2018)</ref> shed light on the problem from the perspective of information theory. <ref type="bibr" target="#b32">Zhao et al. (2017b)</ref> and <ref type="bibr">Higgins et al. (2017)</ref> both propose various reweightings of the objective along with theoretical and empirical jus- tification.</p><p>Choices of Priors for VAE Some past work has explored various priors for VAE. <ref type="bibr" target="#b20">Serban et al. (2017a)</ref> proposed a piecewise constant distribu- tion which deals with multiple modes, but which sacrifices the property of continuous interpolation. <ref type="bibr" target="#b7">Guu et al. (2018)</ref> also applied vMF in a VAE model, but used theirs specifically in the sentence- editing case. <ref type="bibr" target="#b5">Davidson et al. (2018)</ref> explored vMF in a VAE model for MNIST and a link prediction task. <ref type="bibr">Hasnat et al. (2017)</ref> applied the vMF distri- bution for facial recognition. Other past work has used different decoders, including <ref type="bibr">CNNs (Yang et al., 2017)</ref> and CNN-RNN hybrids ( <ref type="bibr" target="#b19">Semeniuta et al., 2017)</ref>. Changing the decoder is a change largely orthogonal to changing the prior: it can alleviate the KL vanishing issue, but it does not necessarily scale to new settings and does not give explicit control over utilization of the latent code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we propose the use of a von Mises- Fisher VAE to resolve optimization issues in vari- ational autoencoders for text. This choice of dis- tribution allows us to explicitly control the bal- ance between the capacity of the decoder and the utilization of the latent representation in a princi- pled way. Experimental results demonstrate that the proposed model has better performance than a Gaussian VAE across a range of settings. Further analysis shows that vMF VAE is more sensitive to word order information and makes more effective use of the latent code space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Neural Variational RNN (NVRNN) language model based on a Gaussian prior (left) and a vMF prior (right). The encoder model first computes the parameters for the variational approximation q φ (z|x) (see dotted box); we then sample z and generate the word sequence x given z. We show samples from N (0, I) and vMF(·,κ = 100); the latter samples lie on the surface of the unit sphere. While κ can be predicted from the encoder network, we find experimentally that fixing it leads to more stable optimization and better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of optimization of how q varies over time for a single example during learning. In the Gaussian case, the KL term tends to pull the model towards the prior (moving from µ, σ to µ , σ ), whereas in the vMF case there is no such pressure towards a single distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of the interaction between κ, KL, and dimensionality in vMF. Cos represents the cosine similarity between µ and samples from vMF d (µ, κ) which reflects how disperse the distribution is. KL is defined as KL with a uniform vMF prior, KL(vMF d (µ, κ)||vMF(·, 0)). Higher κ values yield higher cosine similarities, but also higher KL costs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 shows</head><label>3</label><figDesc>Figure 3 shows the KL value and concentration of vMF(µ, κ) for two different dimensionalities. KL increases monotonically with κ, as does concentration measured by cosine similarity. To get a fixed cosine dispersion as dimensionality increases, higher κ values are needed, resulting in higher KL values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5</head><label>5</label><figDesc>Figure 5: Sensitivity of latent codes to swapping adjacent words of encoding sequence. Cosine similarity is measured between the latent code (encoded mean vector) of the original sentence and the sentence after swaps are applied. We see that vMF is more highly sensitive to swaps in both the NVRNN and NVRNN-BoW settings, indicating that its latent space likely encodes more ordering information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>9Figure 6 :</head><label>6</label><figDesc>Figure 6: Perplexity of v-VAE in different settings with different κ values when the latent dimension is 50. Darker colors correspond to perplexity values closer to the best observed for that setting. For each task, we see that there is a range of κ values that work well, and these transfer between comparable tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Development set KL and NLL values for two NVRNN models trained on the Penn</head><label>1</label><figDesc></figDesc><table>Treebank with 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the datasets used in our experi-
ments. Len stands for the average length of an example. 
Vocab is the vocabulary size; these follow prior work. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>and perplexity (PPL) on the test set. We follow the implementation reported in Bow- man et al. (2016) where the KL term weight is annealed for the Gaussian VAE; vMF VAE works well without weight annealing. The vMF distri- bution gives a performance boost in all datasets in both the Standard and Inputless settings. Even in the Standard setting, our model is able to suc- cessfully use nonzero KL values to achieve better</figDesc><table>Model 

PTB 
Yelp 
Standard 
Inputless 
Standard 
Inputless 
NLL 
PPL 
NLL 
PPL 
NLL 
PPL 
NLL 
PPL 

RNNLM (2016) 
100 ( -) 
116 
135 ( -) 
&gt;600 
-
-
-
-
G-VAE (2016) 
101 (2) 
119 
125 (15) 
380 
-
-
-
-

RNNLM (Ours) 
100 ( -) 
114 
134 ( -) 
596 
199 ( -) 
55 
300 ( -) 
432 
G-VAE (Ours) 
99 (4.4) 109 
125 (6.3) 
379 
199 (0.5) 
55 
274 (13.4) 256 
vMF-VAE (Ours) 
96 (5.7) 
98 
117 (18.6) 
262 
198 (6.4) 
54 
242 (48.5) 134 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :KLD</head><label>3</label><figDesc></figDesc><table>Experimental results of NVRNN on the test sets of PTB and Yelp. The upper RNNLM and G-VAE shows 
the result from Bowman et al. (2016). KL divergence is shown in the parenthesis, along with total NLL. Best 
results are in bold. vMF consistently uses higher KL term weights but achieves comparable or better NLL and 
perplexity values across all four settings. 

</table></figure>

			<note place="foot" n="1"> The code and dataset are available at: https:// github.com/jiacheng-xu/vmf_vae_nlp</note>

			<note place="foot" n="3"> In our experiments, we found significant variance in collapse frequency due to other hyperparameters including whether the encoder is a unidirectional or bidirectional LSTM.</note>

			<note place="foot" n="4"> Our KL divergence agrees with that of Davidson et al. (2018) (see their appendix for a derivation), and we have verified it empirically. The equation in Guu et al. (2018) gives slightly different KL values, though differences are small (&lt;5%) for most κ and dimension values we encounter.</note>

			<note place="foot" n="5"> Reported values are actually a lower bound on the true NLL, computed from ELBO by sampling z.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by NSF Grant IIS-1814522, a Bloomberg Data Science Grant, and an equipment grant from NVIDIA. The au-thors acknowledge the Texas Advanced Comput-ing Center (TACC) at The University of Texas at Austin for providing HPC resources used to con-duct this research. Thanks as well to the anony-mous reviewers for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dim   <ref type="bibr" target="#b15">Mnih and Gregor (2014)</ref>. Gaussian-based NVDM (G-NVDM) is proposed in <ref type="bibr" target="#b14">Miao et al. (2016)</ref>. Dim indicates the di- mension of the latent code. Our v-NVDM model out- performs past models by a substantial margin.</p><p>multinomial distribution over words in the vocabu- lary, and the probability of a document is the prod- uct of the probabilities of its words.</p><p>Dataset For NVDM, we use two standard news corpus, 20 News Groups (20NG) and the Reuters RCV1-v2, which were used in <ref type="bibr" target="#b14">Miao et al. (2016)</ref>. <ref type="bibr">6</ref> Results Experimental results 7 are shown in Ta- ble 4. In contrast with NVRNN, the NVDM fully relies on the power of latent code to predict the word distribution, so we never observe a KL col- lapse, yet vMF still achieves better performance than Gaussian. As shown in <ref type="figure">Figure 3</ref>, in order to keep the same amount of dispersion in samples from the variational posterior, larger latent dimen- sions need larger κ values and correspondingly larger KL term values. For 20NG, which is much smaller than RCV1, smaller dimensions therefore give better performance. For both datasets, the set- tings of κ = 100, dim = 25 and κ = 150, dim ∈ {50, 200} work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">What do our VAEs encode?</head><p>We design more probing tasks to demonstrate what is encoded in latent representations induced by vMF VAE. One additional model variant we ex- plore here is the NVRNN-BoW model. This is a variant of NVRNN where the decoder additionally <ref type="bibr">6</ref> The preprocessed version can be downloaded from https://github.com/ysmiao/nvdm <ref type="bibr">7</ref> We do not compare to results from <ref type="bibr" target="#b20">Serban et al. (2017a)</ref>. Compared to our current results, that work reports very strong performance on 20NG and very weak performance on RCV1; we believe they either used different preprocessing or made a mistake in reporting the results, but could not confirm this with the authors.   conditions on the vector BoW = 1 n n i=1 e(x i ), the average word embedding value of the sen- tence x. While an artificial setting, this lets us see how effectively the latent code can capture in- formation other than simple word choice by mak- ing a form of this information independently avail- able. <ref type="table">Table 5</ref> shows results in this setting, where once again we see the KL collapse problem for the Gaussian models and better performance from vMF on perplexity in both the Standard and Input- less settings.</p><p>Is the latent code more than a bag of words? For all of these models, one hypothesis is that the encoder may be learning to memorize the bag of words and then preferentially generate words in that bag from the decoder. To verify this, we investigate whether the BoW representation and the learned latent code can be reconstructed from each other. Specifically, given a sentence x we can compute BoW as defined above and µ = enc(x), the latent encoding of x as represented by the mean vector output by the encoder. We can use a simple multilayer perceptron to to try to map from the bag of words to the latent code: ˆ µ = M LP (BoW ), then learn the parameters of the MLP by minimizingˆµminimizingˆminimizingˆµ − µ 2 on a sample. The same process can be used to learn a mapping from µ back to the bag of words. <ref type="table">Table 6</ref> shows averaged cosine similarities of our reconstructions under both Gaussian and vMF models. For vMF, µ can reconstruct the bag-of- words more accurately than the bag-of-words can reconstruct µ, indicating that the latent code in vMF captures more information beyond the bag of words.</p><p>We repeat this experiment in a separate NVRNN model where the decoder can explicitly condition on the BoW vector described above. The results are shown in the right column of Ta- ble 6. Our model, v-VAE, achieves a lower cosine</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rif</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Fixing a Broken ELBO. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering on the unit hypersphere using von Mises-Fisher distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<title level="m">Ilya Sutskever, and Pieter Abbeel. 2016. Variational Lossy Autoencoder</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<title level="m">Hyperspherical Variational Auto-Encoders. 34th Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DRAW: A Recurrent Neural Network For Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generating Sentences by Editing Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Hasnat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Bohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Milgram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04264</idno>
		<title level="m">Stphane Gentric, and Liming Chen. 2017. von Mises-Fisher Mixture Model-based Deep learning: Application to Face Verification</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<title level="m">Shakir Mohamed, and Alexander Lerchner. 2017. β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Autoencoding Variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a Large Annotated Corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Neural Variational Inference for Text Processing. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<title level="m">Neural Variational Inference and Learning in Belief Networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Pixel Recurrent Neural Networks. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">WaveNet: A Generative Model for Raw Audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Hybrid Convolutional Variational Autoencoder for Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piecewise Latent Variables for Neural Variational Text Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Ororbia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Style Transfer from Non-Parallel Text by Cross-Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<imprint>
			<pubPlace>Llion Jones, Aidan N Gomez, ukasz</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simulation of the von Mises Fisher Distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in StatisticsSimulation and Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="164" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cached Long Short-Term Memory Neural Networks for Document-Level Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoze</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tan</surname></persName>
		</author>
		<title level="m">Variational Autoencoder for Semi-Supervised Text Classification. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved Variational Autoencoders for Text Modeling using Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelly</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Adversarially Regularized Autoencoders. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<title level="m">InfoVAE: Information Maximizing Variational Autoencoders. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
