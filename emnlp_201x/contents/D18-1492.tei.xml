<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Tree-Based Neural Sentence Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
							<email>hyshi@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">ByteDance AI Lab</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">ByteDance AI Lab</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaze</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">ByteDance AI Lab</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of EECS</orgName>
								<orgName type="department" key="dep2">ByteDance AI Lab</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing, Beijing</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On Tree-Based Neural Sentence Modeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4631" to="4641"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4631</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks with tree-based sentence en-coders have shown better results on many downstream tasks. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (i.e., binary balanced tree, left-branching tree and right-branching tree) in the encoders. Though trivial trees contain no syntactic information, those encoders get competitive or even better results on all of the ten downstream tasks we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that tree modeling gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder. Our code is open-source and available at https://github. com/ExplorerFreda/TreeEnc.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentence modeling is a crucial problem in natural language processing (NLP). Recurrent neural net- works with long short term memory <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997</ref>) or gated recurrent units ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>) are commonly used sentence modeling approaches. These models embed sen- tences into a vector space and the resulting vectors can be used for classification or sequence genera- tion in the downstream tasks.</p><p>In addition to the plain sequence of hidden units, recent work on sequence modeling proposes to impose tree structure in the encoder ( <ref type="bibr" target="#b34">Socher et al., 2013;</ref><ref type="bibr" target="#b36">Tai et al., 2015;</ref><ref type="bibr" target="#b50">Zhu et al., 2015</ref>).</p><p>These tree-based LSTMs introduce syntax tree as an intuitive structure prior for sentence modeling. They have already obtained promising results in many NLP tasks, such as natural language infer- ence ( <ref type="bibr" target="#b2">Bowman et al., 2016;</ref><ref type="bibr" target="#b5">Chen et al., 2017c</ref>) and machine translation ( <ref type="bibr" target="#b10">Eriguchi et al., 2016;</ref><ref type="bibr">Chen et al., 2017a,b;</ref><ref type="bibr" target="#b22">Zhou et al., 2017)</ref>. <ref type="bibr" target="#b19">Li et al. (2015)</ref> empirically concludes that syntax tree-based sen- tence modeling are effective for tasks requiring relative long-term context features.</p><p>On the other hand, some works propose to abandon the syntax tree but to adopt the latent tree for sentence modeling <ref type="bibr" target="#b7">(Choi et al., 2018;</ref><ref type="bibr" target="#b25">Maillard et al., 2017;</ref><ref type="bibr" target="#b40">Williams et al., 2018)</ref>. Such latent trees are di- rectly learned from the downstream task with re- inforcement learning <ref type="bibr" target="#b41">(Williams, 1992)</ref> or Gum- bel Softmax ( <ref type="bibr" target="#b13">Jang et al., 2017;</ref><ref type="bibr" target="#b24">Maddison et al., 2017</ref>). However, <ref type="bibr" target="#b40">Williams et al. (2018)</ref> empiri- cally show that, Gumbel softmax produces unsta- ble latent trees with the same hyper-parameters but different initializations, while reinforcement learning ( <ref type="bibr" target="#b40">Williams et al., 2018</ref>) even tends to gen- erate left-branching trees. Neither gives meaning- ful latent trees in syntax, but each method still ob- tains considerable improvements in performance. This indicates that syntax may not be the main contributor to the performance gains.</p><p>With the above observation, we bring up the fol- lowing questions: What does matter in tree-based sentence modeling? If tree structures are neces- sary in encoding the sentences, what mostly con- tributes to the improvement in downstream tasks? We attempt to investigate the driving force of the improvement by latent trees without syntax.</p><p>In this paper, we empirically study the effec- tiveness of tree structures in sentence modeling. We compare the performance of bi-LSTM and five tree LSTM encoders with different tree layouts, including the syntax tree, latent tree (from Gum-bel softmax) and three kinds of designed trivial trees (binary balance tree, left-branching tree and right-branching tree). Experiments are conducted on 10 different tasks, which are grouped into three categories, namely the single sentence classifica- tion (5 tasks), sentence relation classification (2 tasks), and sentence generation (3 tasks). These tasks depend on different granularities of features, and the comparison among them can help us learn more about the results. We repeat all the exper- iments 5 times and take the average to avoid the instability caused by random initialization of deep learning models.</p><p>We get the following conclusions:</p><p>• Tree structures are helpful to sentence mod- eling on classification tasks, especially for tasks which need global (long-term) context features, which is consistent with previous findings ( <ref type="bibr" target="#b19">Li et al., 2015</ref>).</p><p>• Trivial trees outperform syntactic trees, indi- cating that syntax may not be the main con- tributor to the gains of tree encoding, at least on the ten tasks we investigate.</p><p>• Further experiments shows that, given strong priors, tree based methods give better results when crucial words are closer to the final rep- resentation. If structure priors are unavail- able, balanced tree is a good choice, as it makes the path distances between word and sentence encoding to be roughly equal, and in such case, tree encoding can learn the cru- cial words itself more easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Framework</head><p>We show the applied encoder-classifier/decoder framework for each group of tasks in <ref type="figure">Figure 1</ref>. Our framework has two main components: the en- coder part and the classifier/decoder part. In gen- eral, models encode a sentence to a length-fixed vector, and then applies the vector as the feature for classification and generation. We fix the structure of the classifier/decoder, and propose to use five different types of tree structures for the encoder part including:</p><p>• Parsing tree. We apply binary constituency tree as the representative, which is widely used in natural language inference <ref type="bibr" target="#b2">(Bowman et al., 2016</ref>) and machine translation ( <ref type="bibr" target="#b10">Eriguchi et al., 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2017a</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Layer Perceptron</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax</head><p>(c) Siamese encoder-classifier framework for sentence relation classification. <ref type="figure">Figure 1</ref>: The encoder-classifier/decoder frame- work for three different groups of tasks. We ap- ply multi-layer perceptron (MLP) for classifica- tion, and left-to-right decoders for generation in all experiments.</p><p>• Binary balanced tree. To construct a binary balanced tree, we recursively divide a group of n leafs into two contiguous groups with the size of d n 2 e and b n 2 c, until each group has only one leaf node left.</p><p>• Gumbel trees, which are produced by straight-forward Gumbel softmax models ( <ref type="bibr" target="#b7">Choi et al., 2018)</ref>. Note that Gumbel trees are not stable to sentences <ref type="bibr" target="#b40">(Williams et al., 2018)</ref>, and we only draw a sample among all of them.</p><p>• Left-branching trees.</p><p>We combine two nodes from left to right, to construct a left- branching tree, which is similar to those I love my pet cat .</p><p>(a) Parsing tree.</p><p>I love my pet cat .</p><p>(b) Balanced tree.</p><p>I love my pet cat .</p><p>(c) Gumbel tree.</p><p>I love my pet cat .</p><p>(d) Left-branching tree.</p><p>I love my pet cat .  generated by the reinforce based RL-SPINN model ( <ref type="bibr" target="#b40">Williams et al., 2018</ref>).</p><p>• Right-branching trees. In contrast to left- branching ones, nodes are combined from right to left to form a right-branching tree. We show an intuitive view of the five types of tree structures in <ref type="figure" target="#fig_1">Figure 2</ref>. In addition, existing works ( <ref type="bibr" target="#b7">Choi et al., 2018;</ref><ref type="bibr" target="#b40">Williams et al., 2018)</ref> show that using hidden states of bidirectional RNNs as leaf node representations (bi-leaf-RNN) instead of word embeddings may improve the performance of tree LSTMs, as leaf RNNs help encode context information more completely. Our framework also support leaf RNNs for tree LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of Investigated Tasks</head><p>We conduct experiments on 10 different tasks, which are grouped into 3 categories, namely the single sentence classification (5 tasks), sentence relation classification (2 tasks), and sentence gen- eration (3 tasks). Each of the tasks is compat- ible to the encoder-classifier/decoder framework shown in <ref type="figure">Figure 1</ref>. These tasks cover a wide range of NLP applications, and depend on different gran- ularities of features.</p><p>Note that the datasets may use articles or para- graphs as instances, some of which consist of only one sentence. For each dataset, we only pick the subset of single-sentence instances for our experi- ments, and the detailed meta-data is in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Classification</head><p>First, we introduce four text classification datasets from , including AG's News, Amazon Review Polarity , Amazon Review Full and DBpedia. Additionally, noticing that parsing tree was shown to be effective ( <ref type="bibr" target="#b19">Li et al., 2015)</ref> on the task of word-level semantic relation classifi- cation ( <ref type="bibr" target="#b11">Hendrickx et al., 2009</ref>), we also add this dataset to our selections.</p><p>AG's News (AGN). Each sample in this dataset is an article, associated with a label indicating its topic: world, sports, business or sci/tech.</p><p>Amazon Review Polarity (ARP). The Ama- zon Review dataset is obtained from the Stanford Network Analysis Project (SNAP; <ref type="bibr" target="#b26">McAuley and Leskovec, 2013)</ref>. It collects a large amount of product reviews as paragraphs, associated with a star rate from 1 (most negative) to 5 (most posi- tive). In this dataset, 3-star reviews are dropped, while others are classified into two groups: posi- tive (4 or 5 stars) and negative (1 or 2 stars).</p><p>Amazon Review Full (ARF). Similar to the ARP dataset, the ARF dataset is also collected from Amazon product reviews. Labels in this dataset are integers from 1 to 5.</p><p>DBpedia. DBpedia is a crowd-sourced commu- nity effort to extract structured information from Wikipedia ( <ref type="bibr">Lehmann et al., 2015)</ref>.  select 14 non-overlapping classes from DBpedia 2014 to construct this dataset. Each sample is given by the title and abstract of the Wikipedia article, associated with the class label.</p><p>Word-Level Semantic Relation (WSR) <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>) is to find semantic relationships between pairs of nominals. Each sample is given by a sentence, of which two nominals are explicitly indicated, associated with manually labeled semantic rela- tion between the two nominals. For example, the sentence "My [apartment] e 1 has a pretty large [kitchen] e 2 ." has the label component-whole(e 2 , e 1 ). Different from retrieving the path between two labels ( <ref type="bibr" target="#b19">Li et al., 2015;</ref><ref type="bibr" target="#b34">Socher et al., 2013)</ref>, we feed the entire sentence together with the nominal indicators (i.e., tags of e 1 and e 2 ) as words to the framework. We also ignore the order of e 1 and e 2 in the labels given by the dataset. Thus, this task turns to be a 10-way classification one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Relation Classification</head><p>To evaluate how well a model can capture seman- tic relation between sentences, we introduce the second group of tasks: sentence relation classifi- cation.</p><p>Natural Language Inference (NLI). The Stan- ford Natural Language Inference (SNLI) <ref type="bibr">Corpus (Bowman et al., 2015</ref>) is a challenging dataset for sentence-level textual entailment. It has 550K training sentence pairs, as well as 10K for devel- opment and 10K for test. Each pair consists of two relative sentences, associated with a label which is one of entailment, contradiction and neutral.</p><p>Conjunction Prediction (Conj). Information about the coherence relation between two sen- tences is sometimes apparent in the text explicitly ( <ref type="bibr" target="#b27">Miltsakaki et al., 2004</ref>): this is the case when- ever the second sentence starts with a conjunction phrase. <ref type="bibr" target="#b14">Jernite et al. (2017)</ref> propose a method to create conjunction prediction dataset from unla- beled corpus. They create a list of phrases, which can be classified into nine types, as conjunction in- dicators. The object of this task is to recover the conjunction type of given two sentences, which can be used to evaluate how well a model captures the semantic meaning of sentences. We apply the method proposed by <ref type="bibr" target="#b14">Jernite et al. (2017)</ref> on the Wikipedia corpus to create our conj dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Generation</head><p>We also include the sentence generation tasks in our experiments, to investigate the representation ability of different encoders over global (long- term) context features. Note that our framework is based on encoding, which is different from those attention based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrasing (Para). Quora Question Pair</head><p>Dataset is a widely applied dataset to evaluate paraphrasing models ( <ref type="bibr" target="#b21">Li et al., 2017b)</ref>. <ref type="bibr">1</ref> In this work, we treat the paraphrasing task as a sequence-to-sequence one, and evaluate on it with our sentence generation framework.</p><p>Machine Translation (MT). Machine transla- tion, especially cross-language-family machine translation, is a complex task, which requires models to capture the semantic meanings of sen- tences well. We apply a large challenging English- Chinese sentence translation task for this inves- tigation, which is adopted by a variety of neural translation work ( <ref type="bibr" target="#b37">Tu et al., 2016;</ref><ref type="bibr" target="#b20">Li et al., 2017a;</ref><ref type="bibr" target="#b3">Chen et al., 2017a</ref> from the LDC corpora, 2 selecting 1.2M from them as our training set, 20K and 80K of them as our development set and test set, respectively.</p><p>Auto-Encoding (AE). We extract the English part of the machine translation dataset to form a auto-encoding task, which is also compatible with our encoder-decoder framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our experimental re- sults and analysis. Section 4.1 introduces our set- up for all the experiments. Section 4.2 shows the main results and analysis on ten downstream tasks grouped into three classes, which can cover a wide range of NLP applications. Regarding that trivial tree based LSTMs perform the best among all models, we draw two hypotheses, which are i) right-branching tree benefits a lot from strong structural priors; ii) balanced tree wins because it fairly treats all words so that crucial informa- tion could be more easily learned by the LSTM gates automatically. We test the hypotheses in  word-level for English targets and char-level for Chinese targets) for generation tasks. Large is better for both of the metrics. The best number(s) for each task are in bold. In addition, average sentence length (in words) of each dataset is attached in the last row with underline.</p><p>Section 4.3. Finally, we compare the performance of linear and tree LSTMs with three widely ap- plied pooling mechanisms in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Set-up</head><p>In experiments, we fix the structure of the clas- sifier as a two-layer MLP with ReLU activation, and the structure of decoder as GRU-based recur- rent neural networks ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>). <ref type="bibr">3</ref> The hidden-layer size of MLP is fixed to 1024, while that of GRU is adapted from the size of sentence encoding. We initialize the word embeddings with 300-dimensional GloVe (Pennington et al., 2014) vectors. <ref type="bibr">4</ref> We apply 300-dimensional bidirectional (600-dimensional in total) LSTM as leaf RNN when necessary. We use Adam ( <ref type="bibr" target="#b16">Kingma and Ba, 2015)</ref> optimizer to train all the models, with the learning rate of 1e-3 and batch size of 64. In the training stage, we drop the samples with the length of either source sentence or target sentence larger than 64. We do not apply any regularization or dropout term in all experiments except the task of WSR, on which we tune dropout term with respect to the development set. We generate the binary parsing tree for the datasets without parsing trees using ZPar (Zhang and Clark, 2011). 5 More de- tails are summarized in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>In this subsection, we aim to compare the results from different encoders. We do not include any attention ( <ref type="bibr">Wang</ref>   <ref type="table" target="#tab_3">Table 2</ref> presents the performances of different encoders on a variety of downstream tasks, which lead to the following observations:</p><p>Tree encoders are useful on some tasks. We get the same conclusion with <ref type="bibr" target="#b19">Li et al. (2015)</ref> that tree-based encoders perform better on tasks requiring long-term context features. Despit- ing the linear structured left-branching and right- branching tree encoders, we find that, tree-based encoders generally perform better than Bi-LSTMs on tasks of sentence relation and sentence genera- tion, which may require relatively more long term context features for obtaining better performances. However, the improvements of tree encoders on NLI and Para are relatively small, which may be caused by that sentences of the two tasks are shorter than others, and the tree encoder does not get enough advantages to capture long-term con- text in short sentences.</p><p>Trivial tree encoders outperform other en- coders. Surprisingly, binary balanced tree en- coder gets the best results on most tasks of clas- sification and right-branching tree encoder tends to be the best on sentence generation. Note that binary balanced tree and right-branching tree are only trivial tree structures, but outperform syntac- tic tree and latent tree encoders. The latent tree is really competitive on some tasks, as its struc- ture is directly tuned by the corresponding tasks. However, it only beats the binary balanced tree by very small margins on NLI and ARP. We will give analysis about this in Section 4.3.</p><p>Larger quantity of parameters is not the only reason of the improvements. <ref type="table" target="#tab_3">Table 2</ref> shows that tree encoders benefit a lot from adding leaf- LSTM, which brings not only sentence level in- formation to leaf nodes, but also more parame- ters than the bi-LSTM encoder. However, left- branching tree LSTM has a quite similar struc- ture with linear LSTM, and it can be viewed as a linear LSTM-on-LSTM structure. It has the same amounts of parameters as other tree-based encoders, but still falls behind the balance tree en- coder on most of the tasks. This indicates that larger quantity of parameters is at least not the only reason for binary balance tree LSTM en- coders to gain improvements against bi-LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Why Trivial Trees Work Better?</head><p>Binary balanced tree and right-branching are triv- ial ones, hardly containing syntax information. In this section, we analyze why these trees achieve high scores in deep.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Right Branching Tree Benefits from Strong Structural Prior</head><p>We argue that right-branching trees benefit from its strong structural prior. In sentence generation tasks, models generate sentences from left to right, which makes words in the left of the source sen- tence more important <ref type="bibr" target="#b35">(Sutskever et al., 2014</ref>). If the encoder fails to memorize the left words, the information about right words would not help due to the error propagation. In right-branching trees, left words of the sentence are closer to the final representation, which makes the left words are more easy to be memorized, and we call this struc- ture prior. Oppositely, in the case of left-branching trees, right words of the sentence are closer to the representation.</p><p>To validate our hypothesis, we propose to vi- sualize the Jacobian as word-level saliency <ref type="bibr" target="#b32">(Shi et al., 2018)</ref>, which can be viewed as the contri- bution of each word to the sentence encoding:</p><formula xml:id="formula_0">J(s, w) = krs(w)k 1 = X i,j | @s i @w j |</formula><p>where s = (s 1 , s 2 , · · · , s p ) T denotes the embed- ding of a sentence, and w = (w 1 , w 2 , · · · , w q ) T denotes embedding of a word. We can compute the saliency score using backward propagation. For a word in a sentence, higher saliency score means more contribution to sentence encoding. We present the visualization in <ref type="figure" target="#fig_3">Figure 3</ref> using the visualization tool from <ref type="bibr" target="#b22">Lin et al. (2017)</ref>. It shows that right-branching tree LSTM encoders tend to look at the left part of the sentence, which is very helpful to the final generation performance, as left words are more crucial. Balanced trees also have this feature and we think it is because balance tree treats these words fairly, and crucial informa- tion could be more easily learned by the LSTM gates automatically.</p><p>However, bi-LSTM and left-branching tree LSTM also pay much attention to words in the right (especially the last two words), which maybe caused by the short path from the right words to the root representation, in the two corresponding tree structures.</p><p>Additionally, <ref type="table" target="#tab_5">Table 3</ref> shows that models trained with the same hyper-parameters but different ini- tializations have strong agreement with each other. the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development -- these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform andopening up .</p><p>(a) Balanced tree, MT.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development - these are the cornerstones of lasting peace and stabi lity in the nation and an inexhaustible motive force for reform andopening up .</p><p>(b) Left-branching tree, MT.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reap the benefits of reform and development - these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform and opening up .</p><p>(c) Right-branching, MT.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development -- these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform andopening up .</p><formula xml:id="formula_1">(d) Bi-LSTM, MT.</formula><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development -- these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform andopening up .</p><p>(e) Balanced tree, AE.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development -- these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform andopening up .</p><p>(f) Left-branching tree, AE.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reap the benefits of reform and development - these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform and opening up .</p><p>(g) Right-branching, AE.</p><p>the standing committee 's training work and informati onization work has also been strengthened in varying degrees .</p><p>maintaining the overall situation of stability , taking th e improvement of people 's standard of living as the basic starting point , and allowing people to continuo usly reapthe benefits of reform and development -- these are the cornerstones of lasting peace and stab ility in the nation and an inexhaustible motive force fo r reform andopening up .    Mean average Pearson correlation (⇥100) across five models trained with same hyper-parameters. For each testing sentence, we compute the saliency scores of words. Cross- model Pearson correlation can show the agreement of two models on one sentence, and average Pear- son correlation is computed through all sentences. We report mean average Pearson correlation of the 5 ⇥ 4 model pairs.</p><p>Thus, "looking at the first words" is a stable be- havior of balanced and right-branching tree LSTM encoders in sentence generation tasks. So is "look- ing at the first and the last words" for Bi-LSTMs and left-branching tree LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Binary Balanced Tree Benefits from Shallowness</head><p>Compared to syntactic and latent trees, the only advantage of balanced tree we can hypothesize is that, it is shallower and more balanced than others. Shallowness may lead to shorter path for informa- tion propagation from leafs to the root representa- tion, and makes the representation learning more easy due to the reduction of errors in the propaga- tion process. Balance makes the tree fairly treats  all leaf nodes, which makes it more easily to au- tomatically select the crucial information over all words in a sentence.</p><p>To test our hypothesis, we conduct the follow- ing experiments. We select three tasks, on which binary balanced tree encoder wins Bi-LSTMs with a large margin (WSR, MT and AE). We gener- ate random binary trees for sentences, while con-    trolling the depth using a hyper-parameter ⇢. We start by a group with all words (nodes) in the sen- tence. At each time, we separate n nodes to two continuous groups sized (d n 2 e, b n 2 c) with proba- bility ⇢, while those sized (n 1, 1) with prob- ability 1 ⇢. Trees generated with ⇢ = 0 are exactly left-branching trees, and those generated with ⇢ = 1 are binary balanced trees. The ex- pected node depth of the tree turns smaller with ⇢ varies from 0 to 1. <ref type="figure" target="#fig_6">Figure 4</ref> shows that, in general, trees with shal- lower node depth have better performance on all of the three tasks (for binary tree, shallower also means more balanced), which validates our above hypothesis that binary balanced tree gains the re- ward from its shallow and balanced structures.</p><p>Additionally, <ref type="figure" target="#fig_10">Figure 5</ref> demonstrates that bi- nary balanced trees work especially better with relative long sentences. As desired, on short- sentence groups, the performance gap between Bi-LSTM and binary balanced tree LSTM is not obvious, while it grows with the test sentences turning longer. This explains why tree-based en- coder gives small improvements on NLI and Para, because sentences on these two tasks are much shorter than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Can Pooling Replace Tree Encoder?</head><p>Max pooling <ref type="bibr" target="#b8">(Collobert and Weston, 2008;</ref>, mean pooling ( <ref type="bibr" target="#b9">Conneau et al., 2017)</ref> and self-attentive pooling (also known as self- attention; <ref type="bibr" target="#b31">Santos et al., 2016;</ref><ref type="bibr" target="#b22">Lin et al., 2017)</ref> are three popular and efficient choices to improve sentence encoding. In this part, we will compare the performance of tree LSTMs and bi- LSTM on the tasks of WSR, MT and AE, with each pooling mechanism respectively, aiming to demonstrate the role that pooling plays in sentence  modeling, and validate whether tree encoders can be replaced by pooling.</p><p>As shown in <ref type="figure" target="#fig_11">Figure 6</ref>, for linear LSTMs, we apply pooling mechanism to all hidden states; as for tree LSTMs, pooling is applied to all hidden states and leaf states of tree LSTMs. Implementa- tion details are summarized in the supplementary materials. <ref type="table" target="#tab_7">Table 4</ref> shows that max and attentive pooling improve all the structures on the task of WSR, but all the pooling mechanisms fail on MT and AE that require the encoding to capture complete in- formation of sentences, while pooling mechanism may cause the loss of information through the pro- cedure. The result indicates that, though pooling mechanism is efficient on some tasks, it cannot totally gain the advantages brought by tree struc- tures. Additionally, we think the attention mech-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSR MT AE</head><p>Bi-LSTM 67.4 21.3 67.0 +max-pooling 71.8 " 21.6 " 48.0 # +mean-pooling 64.3 # 21.8 " 47.8 # +self-attention 72.5 " 21.2 # 60.  <ref type="table" target="#tab_7">Table 4</ref>: Performance of tree and linear-structured encoders with or without pooling, on the selected three tasks. We report accuracy (⇥100), char-level BLEU for MT and word-level BLEU for AE. All of the tree models have bidirectional leaf RNNs (BiLRNN). The best number(s) for each task are in bold. The top and down arrows indicate the increment or decrement of each pooling mecha- nism, against the baseline of pure tree based en- coder with the same structure.</p><p>anism has the benefits of the balanced tree mod- eling, which also fairly treat all words and learn the crucial parts automatically. The path from rep- resentation to words in attention are even shorter than the balanced tree. Thus the fact that attentive pooling outperforms balanced trees on WSR is not surprising to us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Balanced tree for sentence modeling has been explored by <ref type="bibr" target="#b28">Munkhdalai and Yu (2017)</ref> and <ref type="bibr" target="#b40">Williams et al. (2018)</ref> in natural language infer- ence (NLI). However, <ref type="bibr" target="#b28">Munkhdalai and Yu (2017)</ref> focus on designing inter-attention on trees, instead of comparing balanced tree with other linguistic trees in the same setting. <ref type="bibr" target="#b40">Williams et al. (2018)</ref> do compare balanced trees with latent trees, but bal- anced tree does not outperform the latent one in their experiments, which is consistent with ours. We analyze it in Section 4.2 that sentences in NLI are too short for the balanced tree to show the ad- vantage.  argue that LSTM works for the gates ability to compute an element-wise weighted sum. In such case, tree LSTM can also be regarded as a special case of attention, espe- cially for the balanced-tree modeling, which also automatically select the crucial information from all word representation. <ref type="bibr" target="#b15">Kim et al. (2017)</ref> pro- pose a tree structured attention networks, which combine the benefits of tree modeling and atten- tion, and the tree structures in their model are also learned instead of the syntax trees.</p><p>Although binary parsing trees do not produce better numbers than trivial trees on many down- stream tasks, it is still worth noting that we are not claiming the useless of parsing trees, which are intuitively reasonable for human language understanding. A recent work <ref type="bibr" target="#b0">(Blevins et al., 2018)</ref> shows that RNN sentence encodings di- rectly learned from downstream tasks can capture implicit syntax information. Their interesting re- sult may explain why explicit syntactic guidance does not work for tree LSTMs. In summary, we still believe in the potential of linguistic features to improve neural sentence modeling, and we hope our investigation could give some sense to after- wards hypothetical exploring of designing more effective tree-based encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we propose to empirically investigate what contributes mostly in the tree-based neural sentence encoding. We find that trivial trees with- out syntax surprisingly give better results, com- pared to the syntax tree and the latent tree. Fur- ther analysis indicates that the balanced tree gains from its shallow and balance properties compared to other trees, and right-branching tree benefits from its strong structural prior under the setting of left-to-right decoder.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of different tree structures for the encoder part.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Saliency visualization of words in learned MT and AE models. Darker means more important to the sentence encoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Avg. Node Depth (e) ⇢-depth line for AE. 70 74 78 0 0.1 0.2 0.5 1 í µí¼ BLEU (f) ⇢-BLEU line for AE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ⇢-depth and ⇢-performance lines for three tasks. There is a trend that the depth drops and the performance raises with the growth of ⇢.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(</head><label></label><figDesc>b) Length-BLEU lines for MT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>(</head><label></label><figDesc>c) Length-BLEU lines for AE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Length-performance lines for the further investigated tasks. We divide test instances into several groups by length, and report the performance on each group respectively. Sentences with length in [1, 8] are put to the first group, and the group i(i 2) covers the range of [4i + 1, 4i + 4] in length. ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An illustration of the investigated selfattentive pooling mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test results for different encoder architectures trained by a unified encoder-classifier/decoder 
framework. We report accuracy (⇥100) for classification tasks, and BLEU score (Papineni et al., 2002; 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>4 #</head><label>4</label><figDesc></figDesc><table>Parsing (BiLRNN) 
68.6 
22.2 
72.9 
+max-pooling 69.7 " 21.8 # 48.3 # 
+mean-pooling 58.0 # 21.2 # 50.7 # 
+self-attention 72.2 " 21.5 # 69.1# 

Balanced (BiLRNN) 
69.6 
22.3 
76.0 
+max-pooling 70.6 " 21.6 # 48.5 # 
+mean-pooling 54.1 # 21.3 # 52.7 # 
+self-attention 72.5 " 21.6 # 69.5 # 

Left (BiLRNN) 
67.7 
21.6 
72.9 
+max-pooling 71.2 " 20.5 # 47.6 # 
+mean-pooling 67.3 # 21.4 # 51.8 # 
+self-attention 72.1 " 21.6 -70.2 # 

Right (BiLRNN) 
68.7 
23.1 
80.4 
+max-pooling 71.6 " 21.6 # 48.4 # 
+mean-pooling 67.2 # 22.1 # 53.9 # 
+self-attention 72.4 " 21.6 # 68.9 # 

</table></figure>

			<note place="foot">⇤ Now at Toyota Technological Institute at Chicago, freda@ttic.edu. This work was done when HS was an intern researcher at ByteDance AI Lab.</note>

			<note place="foot" n="2"> The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06</note>

			<note place="foot" n="3"> We observe that ReLU can significantly boost the performance of Bi-LSTM on SNLI. 4 http://nlp.stanford.edu/data/glove. 840B.300d.zip</note>

			<note place="foot" n="5"> https://www.sutd.edu.sg/cmsresource/ faculty/yuezhang/zpar.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Hang Li, Yue Zhang, Lili Mou and Jiayuan Mao for their helpful comments on this work, and the anonymous reviewers for their valu-able feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep RNNs Encode Soft Hierarchical Syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terra</forename><surname>Blevins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Large Annotated Corpus for Learning Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Fast Unified Model for Parsing and Sentence Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved Neural Machine Translation with a Syntax-Aware Encoder and Decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural Machine Translation with Source Dependency Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhanced LSTM for Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Fethi Bougares, Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Compose Task-Specific Tree Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Goo</forename><surname>Kang Min Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-to-Sequence Attentional Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semeval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discourse-based Objectives for Fast Unsupervised Sentence Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00557</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structured Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<imprint>
			<pubPlace>Mohamed Morsey, Patrick Van Kleef, Sören Auer</pubPlace>
		</imprint>
	</monogr>
	<note>et al. 2015. DBpedia-A LargeScale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory as a Dynamically Computed Element-wise Weighted Sum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">When Are Tree Structures Necessary for Deep Learning of Representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling Source Syntax for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00279</idno>
		<title level="m">Paraphrase Generation with Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Structured Self-Attentive Sentence Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<title level="m">Learning Natural Language Inference using Bidirectional LSTM Model and Inner-Attention</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09189</idno>
		<title level="m">Jointly Learning Sentence embeddings and Syntax with Unsupervised Tree-LSTMs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hidden Factors and Hidden Topics: Understanding Rating Dimensions with Review Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th ACM Conference on Recommender Systems</title>
		<meeting>of the 7th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Penn Discourse Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural Tree Indexers for Text Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03609</idno>
		<title level="m">Attentive Pooling Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning Visually-Grounded Semantics from Contrastive Adversarial Samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attention-Based LSTM for AspectLevel Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilateral Multi-Perspective Matching for Natural Language Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wael</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Do Latent Tree Learning Models Identify Meaningful Structure in Sentences? Transaction of ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple Statistical GradientFollowing Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to Compose Words into Sentences with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Character-Level Convolutional Networks for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Syntactic Processing using the Generalized Perceptron and Beam Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-Adaptive Hierarchical Sentence Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chunk-Based BiScale Decoder for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic StructuredPrediction Model for Transition-based Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A SearchBased Dynamic Reranking Model for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Text Classification Improved by Integrating Bidirectional LSTM with Two-Dimensional Max Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory Over Recursive Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
