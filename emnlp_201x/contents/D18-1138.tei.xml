<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Sentiment Memories for Sentiment Modification without Parallel Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="laboratory">MOE Key Lab of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Sentiment Memories for Sentiment Modification without Parallel Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1103" to="1108"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However , aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., &quot;staff&quot;) provides strong cues for the occurrence of emotional words (e.g., &quot;friendly&quot;), we propose a novel method that automatically extracts appropriate sentiment information from the learned sentiment memories according to the specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment modification of natural language texts is a special task that connects sentiment analy- sis and natural language generation. It facili- tates many NLP applications, such as news rewrit- ing and automatic conversion of review attitude, which reduce the human effort. Sentiment mod- ification presents two requirements: one is that the sentiment or the attitude of the text needs to be transformed to the opposite; the other is that the transformed text should maintain semantic rel- evance to the input text as much as possible.</p><p>Recently, there have been some researches which focus on the work of editing a sentence to alter specific attributes, like style and sentiment <ref type="bibr" target="#b16">(Shen et al., 2017;</ref><ref type="bibr" target="#b6">Hu et al., 2017)</ref>. Typically, the parallel data with the same content but differ- ent sentiment is not available. This line of work attempts to extract the attribute-independent con- tent from a dense sentence representation by ad- versarial learning. However, it is hard to extract the attribute-independent content in such implicit ways, which makes these methods tend to generate input-irrelevant texts.</p><p>Most existing methods can not reconcile the performance of sentiment transformation and con- tent preservation. Direct replacement of emotional words can keep the context but may lead to low- quality sentences. For example, given an input "The food is cold like rock", this method probably outputs "The food is warm like rock". State-of- the-art models using neural networks struggle to generate high-quality sentences. However, these models usually lead to poor content preservation. For instance, when the source text is "This is a wonderful movie", we expect an output like "This movie is disappointing". However, the generated sentence may be "The waiters are very rude", which has little relevance to the source text. In general, it is difficult to preserve semantic content and reverse the sentiment at the same time without parallel data.</p><p>To address this problem, we propose a novel model which performs well in both sentiment transformation and content preservation. Our model first learns two kinds of sentiment mem- ories by explicitly separating emotional words. Then, according to the specific context, the model extracts appropriate sentiment information from the memory of target sentiment. The decoder takes the extracted memory and the context rep- resentation together to perform decoding. The overview of our model is shown in <ref type="figure">Figure 1</ref>. The main architecture of our model is a Sentiment- Memory based Auto-Encoder (SMAE). The pro- posed model achieves the state-of-the-art perfor- The food is…</p><formula xml:id="formula_0">Neg Mem: ⋯ ⋯ The food is delicious. (positive)</formula><p>The food is delicious.</p><p>The food is putrid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>learn memory extract memory extract memory <ref type="figure">Figure 1</ref>: Illustration of the proposed model with a pos- itive input. Solid and dashed lines indicate the training process and the testing process, respectively. The pro- cess with a negative input is in a similar way. mance, especially improves content preservation degree by a large margin.</p><p>Our contributions are concluded as follows:</p><p>• We propose a method that uses sentiment memories to accomplish sentiment modifica- tion without any help of the parallel data.</p><p>• The proposed method improves the content preservation degree by a large margin when compared with current systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recently, there has been some studies for senti- ment modification. <ref type="bibr" target="#b16">Shen et al. (2017)</ref> learn an en- coder that maps a sentence with its original style to a style-independent content representation. This is then passed to a style-dependent decoder for ren- dering. <ref type="bibr" target="#b2">Fu et al. (2017)</ref> implement a multi-decoder auto-encoder ( <ref type="bibr" target="#b0">Bengio et al., 2009;</ref><ref type="bibr" target="#b1">Dai and Le, 2015)</ref> where the encoder is used to capture the content and the sentiment-specific decoders are used to generate target sentence. <ref type="bibr" target="#b6">Hu et al. (2017)</ref> augment the unstructured variables z in vanilla VAE with a set of structured variables c each of which targets a salient and independent seman- tic feature of sentences, to control sentence sen- timent. However, all of these work attempt to im- plicitly separate the non-emotional content from the emotional information in a dense sentence rep- resentation.  explicitly filter out emotional words. They use two sentiment-specific decoders to attach sentiments to non-emotional context. The decoders bear all the burdens to generate sentiments. In our model, we use sen- timent memories to assist generating sentiments with only one decoder, which results in fewer pa- rameters.</p><p>The proposed sentiment-memory based auto- encoder ( <ref type="bibr" target="#b0">Bengio et al., 2009;</ref><ref type="bibr" target="#b14">Ma et al., 2018b</ref>) learns the idea of memory network <ref type="bibr" target="#b19">(Weston et al., 2014;</ref><ref type="bibr" target="#b17">Sukhbaatar et al., 2015</ref>) but simplifies the process. Our work is also related to the generation tasks ( <ref type="bibr" target="#b18">Wang et al., 2017;</ref><ref type="bibr" target="#b12">Liu et al., 2018;</ref><ref type="bibr" target="#b13">Ma et al., 2018a;</ref>. These tasks usually gen- erate texts that preserve main information of input texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>We first use a variant of self-attention( <ref type="bibr" target="#b11">Lin et al., 2017;</ref><ref type="bibr">Kim et al., 2017</ref>) mechanism to distinguish the emotional and non-emotional words. Then the positive words and negative words are used to up- date the corresponding memory modules. Finally, the decoder uses the target sentiment information extracted from the memory and the content repre- sentation to perform decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Emotional Words Detection Model</head><p>We first find the emotional words that have the most discriminative power for sentiment polarity. This work is done by training a sentiment classi- fier with a simple self-attention mechanism. Here the sequence of inputs {h 1 , ..., h T } are the hidden states of a LSTM, running over the words in the source sentence {x 1 , ..., x T }. The context vector can then be computed using a simple sum:</p><formula xml:id="formula_1">c = T t=1 a t · h t (1)</formula><p>where a t denotes the attention weight of the t-th word. The sentence vector c is then fed into a fully connected layer to predict the sentiment po- larity of the source text. Since the words with ob- vious emotional tendencies will be given greater weights compared to those non-emotional words during training, a t can be used to distinguish be- tween emotional and non-emotional words. The weights of standard attention mechanism sum to 1. When there are several emotional words, the sum 1 is distributed by these words. How- ever, we expect that each emotional word has a weight close to 1 to identify its sentiment attribute. Hence, following <ref type="bibr">(Kim et al., 2017)</ref>, we modify the calculation of attention weights as follows to get more distinguishable weights:</p><formula xml:id="formula_2">a t = sigmoid(v T h t )<label>(2)</label></formula><p>where v is the parameter vector. The sigmoid function follows our intention that giving each in-put word a distinguishable weight which is close to 1 or 0. However, these weights falls between 0 and 1. They still can not thoroughly distin- guish the emotional words from non-emotional words without redundant information. Following , we map attention weights to dis- crete values, 0 or 1, and we adopt their discrete method. The weights greater than the averaged at- tention value are assigned to 1 and the weights less than the averaged attention value are assigned to 0. The weight a t after discretization is denoted asâasˆasâ t . Then, ˆ a t can be regarded as the emotional word identifier. 1 − ˆ a t becomes non-emotional word identifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentiment-Memory Based Auto-Encoder</head><p>After the separation of emotional and non- emotional words, the proposed SMAE is used to process these two kinds of information. We employ the seq2seq based auto-encoder. Both the encoder and the decoder are LSTM networks <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>If x i is a context word, thenâthenˆthenâ i is 0, causing</p><formula xml:id="formula_3">(1 − ˆ a i )x i to be x i . Therefore, the sequence {(1 − ˆ a 1 )x 1 , · · · , (1 − ˆ a T )</formula><p>x T } can be regarded as non- emotional word embedding sequence. It is fed into the LSTM encoder sequentially. we select h T in the last state tuple (h T , c T ) of the encoder as the content representation of the input.</p><p>Meanwhile, the embeddings of the emotional words of the source text are used to update the sentiment-memory. Since we have two kinds of sentiments, positive and negative, we use M pos ∈ R e×γ and M neg ∈ R e×γ to denote the positive memory and the negative memory, respectively. e is the embedding size and γ is a hyper-parameter which controls the size of the memory.</p><p>We illustrate the following part by using posi- tive input as an example. We first sum the embed- ding of the emotional words to get a vector rep- resentation of the emotional information, which is denoted as s pos ∈ R e . We then use a simple at- tention mechanism to find the columns in M pos that are most closely related to the emotional in- formation. The outer product of the transposition of emotional information s pos and the attention weights w broadcasts the sentiment vector s pos to a matrix. Then, the matrix is added to the exist- ing memory M pos . Due to the attention weight w, the columns that are most closely related to the emotional information are updated more with the sentiment information s pos . Formally, we have:</p><formula xml:id="formula_4">s pos = T i=1â i=1ˆi=1â i · x i (3) w = sof tmax (s pos ) T M pos (4) M pos = M pos + s pos ⊗ w<label>(5)</label></formula><p>where ⊗ denotes the outer product.</p><p>Previous work employ two sentiment-specific decoders to generate text based on the supposed non-emotional representation. The decoders bear all the burdens to generate sentiments. In our model, we extract some sentiment information from the sentiment-memories to assist decod- ing. Intuitively, the context word "staff" is more likely to be associated with the emotional word "friendly", and "food" is more likely to be asso- ciated with "delicious". So we use the context vector s con to extract the corresponding sentiment memory that is more likely to be used in the future decoding. The context vector s con is represented as the sum of the embedding of non-emotional words. Then s con is used to compute the attention weights u over the columns of sentiment memory matrix. We sum these weighted columns as the ex- tracted memory˜mmemory˜ memory˜m and add˜madd˜ add˜m to the last cell state c T of the encoder: The negative input is processed in the same way. At the training stage, the decoder is encouraged to restore the source text. Therefore, the cross en- tropy loss function is optimized. on Yelp and is labeled as having either negative or positive sentiment. We train a CNN sentence clas- sifier <ref type="bibr" target="#b7">(Kim, 2014</ref>) to filter examples with ambigu- ous sentiment polarities (category probability &lt; 0.8). The processed dataset contains 510K, 20K, and 20K pairs for training, validation, and testing, respectively. The classifier achieves an accuracy of 94% on the processed dataset and is also used to test transformation accuracy.</p><formula xml:id="formula_5">s con = T i=1 (1 − ˆ a i ) · x i (6) u = sof tmax (s con ) T M pos<label>(7)</label></formula><formula xml:id="formula_6">˜ m = γ j=1 u j · M pos j<label>(8)</label></formula><formula xml:id="formula_7">˜ c T = c T + W ˜ m<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>We tune our hyper-parameters on the development set. The word embeddings are initialized ran- domly with a size of 128. The hidden size of the sentiment-memory based auto-encoder is 300. We use Adam optimizer <ref type="bibr" target="#b9">(Kingma and Ba, 2014</ref>) with an initial learning rate set to 0.001 to train our model and the batch size is set to 64. The hyper- parameter γ which controls the size of memory matrix is 60.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>We compare our proposed method with two state- of-the-art systems that have been used for senti- ment modification. We run the released code on our dataset. Cross-aligned Auto-Encoder (CAE): This sys- tem, proposed by <ref type="bibr" target="#b16">Shen et al. (2017)</ref>, uses a shared latent content space across different sentiments and leverages refined alignment of latent represen- tations to perform sentiment modification. Multi-decoder Auto-Encoder (MAE): This sys- tem is proposed by <ref type="bibr" target="#b2">Fu et al. (2017)</ref>. They use a multi-decoder seq2seq model ( <ref type="bibr" target="#b0">Bengio et al., 2009;</ref><ref type="bibr" target="#b1">Dai and Le, 2015)</ref> where the encoder captures con- tent information by adversarial learning <ref type="bibr" target="#b4">(Goodfellow et al., 2014</ref>) and the sentiment-specific de- coders are used to generate target sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results and Discussions</head><p>We use ACC to denote the transformation accu- racy. Following <ref type="bibr" target="#b3">Gan et al. (2017)</ref>, we also com- pute BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) between the  Input: Very helpful and informative staff! CAE: Worst service ever. MAE: Very nice here and poor! Proposed: Very rude and careless staff ! Input: I will never go here again. CAE: I love this place here! MAE: I had say this place here. Proposed: I will never go anywhere else. Input: The worst and would never recommend anyone to use them. CAE: The best place I 've been to go here! MAE: The first experience is so happy and nice. Proposed: The best and would definitely rec- ommend anyone to use them. output and the source text to evaluate the con- tent preservation degree. A high BLEU score pri- marily indicates that the system can correctly pre- serve content by retaining the same words from the source sentence.</p><p>The experimental results of our proposed model and the baselines are shown in <ref type="table">Table 1</ref>. Both base- line models have low BLEU score but high ac- curacy, which indicates that they may be trapped in a situation that they simply output a sentence with the target sentiment regardless of the content. The main reason is that these methods using adver- sarial learning attempt to implicitly separate the emotional information from the context informa- tion in a sentence vector. However, without paral- lel data, it is difficult to achieve such a goal. Our proposed SMAE model takes advantage of self- attention mechanism and explicitly removes the emotional words, leading to a significant improve- ment of content preservation and the state-of-the- art performance in terms of both metrics.</p><p>We also involve human evaluation to measure the quality of generated text. Each item contains an input and three outputs generated by different systems. Then 200 items are distributed to 2 an- notators with linguistic background. The annota-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>ACC BLEU SMEA 76.64 24.00 SMEA (w/o memories) 14.08 26.09 <ref type="table">Table 4</ref>: Ablation test of memory module.</p><p>The staff here is very rude. It really is n't worth coming here . Very pleased with this business. Been here once and loved going here. tors have no idea about which system the output is from. They are asked to score the output on three criteria on a scale from 1 to 10: the transformed sentiment degree, the content preservation degree, and the fluency. <ref type="table" target="#tab_3">Table 2</ref> shows the evaluation re- sults. Our model has obvious advantage over the baseline systems in content preservation, and also performs well in other aspects.</p><p>Several randomly selected examples generated by different models are shown in <ref type="table" target="#tab_4">Table 3</ref>. These examples clearly show our proposed model can generate sentences that are more semantically rel- evant to the input text compared to the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effectiveness of Sentiment-Memories</head><p>To verify the effectiveness of the memory mod- ule of our model, we conduct ablation study by excluding the sentiment-memory module. The re- sult is shown in <ref type="table">Table 4</ref>. According to the re- sult, the complete model achieves an improve- ment of 62.56% on transformation accuracy over the model that excludes the sentiment memories, which means the sentiment memories are key components to ensure successful sentiment mod- ification. In addition, several examples are shown in <ref type="table" target="#tab_5">Table 5</ref> to visually demonstrate the effective- ness of the memory module. we can find that the proposed model is capable of generating appro- priate emotional words (red words in <ref type="table" target="#tab_5">Table 5</ref>) to adapt different contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>To better interpret our model, we also analyze the failure examples whose sentiments are not trans- formed. We observe that in most cases, these in- puts do not have emotional tendencies. Although we have filtered the sentiment-ambiguous exam- ples in preprocessing, there are still a few am- biguous inputs such as "What can I say ?" and "Been here twice.". Since our model tries to pre- serve non-emotional content. These words are easily kept and then the decoder barely depends on sentiment-memories. Thus, it is difficult to handle the sentiment transformation with these examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a model that first learns sentiment memories without parallel data and then automatically extracts sentiment information to adapt different contexts when decoding. Exper- imental results show that our method substan- tially improves the content preservation degree and achieves the state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>9) where u j denotes the j-th value in vector u, M pos j denotes the j-th column of M pos and W is the parameter matrix. The new tuple (h T , ˜ c T ) then acts as the initial state of the decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results of human evaluation.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Examples generated by the proposed method 
and baselines. In comparison, our model changes the 
sentiment of inputs with higher semantic relevance. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>The effectiveness of the memory module with examples. The red words are absent in the input but generated with the help of sentiment memories.</figDesc><table></table></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ lancopku/SMAE</note>

			<note place="foot" n="2"> https://www.yelp.com/dataset/ challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natu-ral Science Foundation of China (No. 61673028). We thank all the reviewers for providing the con-structive suggestions. Xu Sun is the corresponding author of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Style transfer in text: Exploration and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1711.06861</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stylenet: Generating attractive visual captions with styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="955" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>abs/1703.00955</idno>
		<title level="m">Controllable text generation. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>abs/1702.00887</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global encoding for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="163" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Table-to-text generation by structure-aware seq2seq learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical end-to-end model for jointly improving text summarization and sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="4251" to="4257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoencoder as assistant supervisor: Improving text representation for chinese social media text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Houfeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="725" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>abs/1705.09655</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Affinity-preserving random walk for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="210" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>abs/1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
