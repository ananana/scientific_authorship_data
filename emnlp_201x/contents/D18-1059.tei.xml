<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizing Word Embeddings using Bag of Subwords</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinman</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidharth</forename><surname>Mudgal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizing Word Embeddings using Bag of Subwords</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="601" to="606"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>601</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional contex-tual information. We propose a subword-level word vector generation model that views words as bags of character n-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model&apos;s ability in capturing the relationship between words&apos; textual representations and their embeddings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings have been an essential part of neural-network based approaches for natural lan- guage processing tasks <ref type="bibr" target="#b6">(Goldberg, 2016)</ref>. How- ever, many popular word embeddings techniques have a fixed vocabulary ( <ref type="bibr" target="#b11">Mikolov et al., 2013;</ref><ref type="bibr" target="#b12">Pennington et al., 2014</ref>), i.e., they can only pro- vide vectors over a finite set of common words that appear frequently in a given corpus. Such methods fail to generate vectors for rare words and words not present in the training corpus, but appearing in the test corpus or downstream task texts, raising difficulty for any methods relying on word vectors to efficiently extract useful features from text. This is often referred to as the out-of- vocabulary (OOV) word problem. We aim to ad- dress this problem by inferring vectors for OOV words with only access to pre-trained vectors over a fixed vocabulary of common words and the OOV word itself without context.</p><p>The motivations come from both linguistics and natural language processing applications. First, from a linguistic view a word can be decomposed into multiple morphemes: stems, affixes, modi- fiers and etc. This is more often the case for rare words. In some field such as chemistry and ag- glutinative languages such as Turkish, there exists a systematic way of composing words from mor- phemes. Some can even be arbitrarily long.</p><p>Apart from the explicit and systematic way of making words, we can also observe the ability of a language speaker to infer the meaning of an un- seen word. For instance, one can guess that "pre- EMNLP" means "before EMNLP", even without the presence of any context, suggesting that it is part of our implicit linguistic knowledge to infer meaning of an unseen word solely from its lexi- cal form. This observation, together with the mor- pheme decomposition of many rare words, implies the feasibility of inferring their vectors from those for common words, and also raises the algorithmic question of how to compute them efficiently.</p><p>Second, there are many NLP applications where estimating word embeddings of OOV is critical. For instance, in the case of analyzing Twitter data, while there exists pre-trained word embeddings with giant vocabularies trained on massive num- ber of tweets, such as GloVe vectors <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>, this would still not cover new words coined by users everyday. In such cases, it is more prudent to extend the available pre-trained vectors trained on very large corpora, so that we can es- timate embeddings for OOV words, instead of re- training a new word / subword level embedding model on the new extended data corpus.</p><p>OOV words have always been a problem for methods that assume fixed vocabularies. A com- mon workaround is to view all OOV words as a special UNK token and use the same vector for all of them. This would restrict any downstream models from accessing distinct features of those words. Thus, we would like a method to provide vectors that capture semantic and grammatical fea-tures even for OOV words. We also would like such method to maximally rely on the word itself, instead of its context, as contextual information is already used later with sentence level models stacking over word vectors.</p><p>To achieve this, we aim to build a word embed- ding model that generalizes pre-trained word em- beddings to OOV words. First, given word embed- dings for a fixed vocabulary, our model learns the relationship between the subwords present in each word and its corresponding pre-trained word vec- tor. Then, using the learned subword information, our model can generate word embeddings for any word, regardless if it is OOV or not.</p><p>Contribution We propose a simple yet effec- tive subword-level word embedding method that can be efficiently trained given pre-trained word vectors for a limited number of words. Once trained, our embedding model takes the characters n-grams in a word as input and gives its word vec- tor as output. <ref type="bibr">1</ref> Our experiments on word similarity tasks in English and POS tagging in a variety of lan- guages suggests that the proposed word embed- der is able to mimic and generalize consistently the word vectors from in-vocabulary words to out-of-vocabulary words, and achieves state-of- the-art scores for the tasks compared to previ- ous subword-level word embedders trained under the same setting. This gives evidence that such a simple model is capable of capturing language speaker's morphological knowledge, and also pro- vides an easy way to generate word vectors for rare or unseen (OOV) words with potential application to various natural language processing tasks.</p><p>Related work There exist a large body of works that try to incorporate morphological information into word representations, e.g., <ref type="bibr" target="#b1">(Alexandrescu and Kirchhoff, 2006;</ref><ref type="bibr" target="#b9">Luong et al., 2013a;</ref><ref type="bibr" target="#b16">Qiu et al., 2014;</ref><ref type="bibr" target="#b3">Botha and Blunsom, 2014;</ref><ref type="bibr" target="#b4">Cotterell and Schütze, 2015;</ref><ref type="bibr" target="#b19">Soricut and Och, 2015)</ref>. These ap- proaches typically rely on the morphological de- composition of words. Some other approaches using subword information do not rely on mor- phological decomposition but requires context in- formation from large text corpus <ref type="bibr" target="#b18">(Schütze, 1993;</ref><ref type="bibr" target="#b17">Santos and Zadrozny, 2014;</ref><ref type="bibr" target="#b8">Ling et al., 2015;</ref><ref type="bibr" target="#b20">Wieting et al., 2016</ref>).</p><p>In particular, <ref type="bibr" target="#b2">Bojanowski et al. (2017)</ref> intro- duced fastText, a word embedding method en- hanced with subword (character n-gram) embed- dings. They are able to generate vectors for OOV words, which has been shown useful for text clas- sification ( <ref type="bibr" target="#b7">Joulin et al., 2016</ref>), but the model is to be trained over large text corpus.</p><p>Pinter et al. <ref type="formula" target="#formula_0">(2017)</ref> use a character-level bidi- rectional LSTM model called MIMICK, mapping from word strings to word vectors. The idea of using character-level recurrent neural networks (RNNs) for word vectors is not new ( <ref type="bibr" target="#b8">Ling et al., 2015;</ref><ref type="bibr" target="#b15">Plank et al., 2016</ref>), but as per authors' knowledge, they are by far the only attempt to the exact task of generalizing word vectors from only pre-trained vectors with a fixed vocabulary, i.e. with no access to contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bag-of-Substring Model</head><p>Our Bag-of-Substring (BoS) word vector gener- ation model views a word as a bag of its sub- strings, or character n-grams. Specifically, we maintain a vector lookup table for each possi- ble substrings (or character n-grams) of length between l min and l max . A word vector is then formed as the average of vectors of all its sub- strings with lengths in the range. Let Σ be the fi- nite set of characters in the language, subs b a (s) = {t is substring of s : a ≤ |t| ≤ b} for string s ∈ Σ * be the set of substrings of s whose length is be- tween a and b inclusive, and &lt;s&gt; be the concatena- tion of character &lt;, string s and character &gt; where &lt;, &gt; ∈ Σ. The BoS embedding for a string/word s can be expressed as</p><formula xml:id="formula_0">BoS(s; V ) = 1 |S &lt;s&gt; | t∈S&lt;s&gt; v t ,<label>(1)</label></formula><p>where V ∈ R d×(|Σ| l min +···+|Σ| lmax ) are the param- eters which stores the embeddings of dimension d for each possible substring of length between l min and l max , v t is the vector in V indexed by t, S &lt;s&gt; is a shorthand for subs lmax l min (&lt;s&gt;). Spe- cial characters &lt;, &gt; ∈ Σ are used to mark the start and the end of the word and thus help the model to distinguish homographic morphemes that occur at different word parts, e.g. prefixes or suffixes. An example BoS representation for word infix is subs 4 3 (&lt;infix&gt;) = {&lt;in, &lt;inf, inf, infi, nfi, nfix, fix, fix&gt;, ix&gt;}.</p><p>fastText ( <ref type="bibr" target="#b2">Bojanowski et al., 2017</ref>) uses the same idea for their word vector generation part. How-ever, unlike them, we train the model directly to- wards pre-trained vectors, instead of via context prediction over text corpora.</p><p>Training Given pre-trained vectors for a set of common words, our model views them as targets and is trained to fit these targets. Once the param- eters (the vectors v t for the substrings) are learned, the model can then be used to infer vectors for rare words. Let U ∈ R d×|W | be the target vec- tors of the same dimension d over finite vocabu- lary W ⊂ Σ * . Our model is trained by minimiz- ing the overall loss between the generated and the given vectors for each word:</p><formula xml:id="formula_1">minimize V 1 |W | w∈W l(BoS(w; V ), u w )<label>(2)</label></formula><p>where the loss function l(v, u) = 1 2 v − u 2 2 , namely the mean squared loss.</p><p>After training, one can use the learned V and Eqn (1) to compute the vector for any given word, even if it is OOV.</p><p>Hyperparameters We set the following hyper- parameters for all the experiments. For BoS model, l min = 3 and l max = 6 following <ref type="bibr" target="#b2">Bojanowski et al. (2017)</ref>. Note that under this setting, S &lt;s&gt; can never be empty for non-empty string s. For optimization, stochastic gradient descent with learning rate 1 for 100 epochs. The dimension of the word vectors is not a hyperparameters here as it needs to agree with the target vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Similarity</head><p>We run experiments to quantitatively evaluate the our model's generalizability towards OOV words.</p><p>The word similarity task asks to predict word similarity between a pair of two words. Given a set of pairs of words and gold labels for their sim- ilarities, the performance of word embeddings is measured by the correlation between the gold sim- ilarities and the similarities induced by the gen- erated embeddings. And we can thus imply how good our model is at generating word vectors. The word similarity here is computed using the cosine distance between the two word vectors, and the correlation is computed using Spearman's ρ.   <ref type="table" target="#tab_0">Table 1</ref>, along with their word similarity task scores (for in-vocabulary words only) and OOV rate over the aforementioned evaluation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Baselines We compare the scores with other subword-level models (fastText and MIMICK) and word similarity induced by non-parametric edit distance (EditDist). fastText ( <ref type="bibr" target="#b2">Bojanowski et al., 2017</ref>) uses the same subword-level character n-gram model but is to be trained via context prediction over large text corpora (here English Wikipedia dump 4 ). MIM-ICK ( <ref type="bibr" target="#b14">Pinter et al., 2017</ref>) is a character-level bidi- rectional LSTM word embedder trained against pre-trained word vectors (here Polyglot vectors 5 ).</p><p>Edit distance is defined between two strings as the smallest number of modifications: adding, deleting and changing one character, needed to turn one string into the other. It can be computed using dynamic programming in O(|s 1 | × |s 2 |) time. The word similarity between w 1 and w 2 here is the edit distance normalized by the length of the longer word:</p><formula xml:id="formula_2">s EditDist (w 1 , w 2 ) = − d edit (w 1 , w 2 ) max(|w 1 |, |w 2 |)<label>(3)</label></formula><p>where d edit is edit distance.</p><p>Results Results are summarized in <ref type="table" target="#tab_1">Table 2</ref>. When trained over Polyglot vectors, our BoS model works better than EditDist and MIMICK. When trained on Google vectors, the correlation scores are almost as good as those of fastText, the state-of-the-art subword level word embedder. However, unlike fastText, our model does not have access to word contexts in a large text corpus for training. In both cases, the significant differences of scores compared to those of EditDist, suggest that our model indeed learns to capture semantic similarities between words, rather than superficial similarities in spelling.</p><p>Comparing to MIMICK, our model is able to fill up 81% (14 to 36 against 41) and 73% (12 to 36 against 45) of the gaps in scores over RW and WS respectively. This improvement is more significant on RW with most (58%) of its words are OOV for the PloyGlot vectors, suggesting our model's power in generating consistent word vectors for OOV words. Surprisingly MIMICK performs no better than the edit distance baseline when evalu- ated on RW. Combined with the fact that it does no better for WS which has a near-zero OOV rate, it suggests MIMICK's limited power of generaliz- ing word vectors towards OOV words, or even re- produce consistent word vector for in-vocabulary words. As a sanity check, we see that all of the embedder models scores obviously better than Ed- itDist when evaluated over common words (WS), showing that all of them are able to at least remem- ber or mimic the word vectors for in-vocabulary words.</p><p>Also note that our model is fast to train. With a naive single-thread CPU-only Python implemen-Besides word similarity, we try to access our em- bedders' ability of capturing words' syntactic and semantic features by evaluating with the task of predicting part-of-speech (POS) tags and mor- phosyntactic attributes for words in a sentence. For each word in a given sentence, the task asks for a POS tag and a label for each applicable morphosyntactic category, such as gender, case or tense.</p><p>Dataset We use Universal Dependencies (UD) dataset (Petrov et al., 2012) for this task. UD is an open-community effort to build consistent an- notated treebank cross many languages. We pick the specific version 1.4 to enable a direct compar- ison with <ref type="bibr" target="#b14">Pinter et al. (2017)</ref>. Since we use Poly- Glot vectors to train our word embedders, we con- duct experiments on the 23 languages that appear in both Polyglot and UD 1.4.</p><p>Model We adopt the same sentence-level bidi- rectional LSTM model from <ref type="bibr" target="#b14">Pinter et al. (2017)</ref> for the joint prediction of both labels. Given a sentence as a sequence of words, we first embed each word using the word embedder we choose and then fed the embeddings into the LSTM. The output of LSTM is then used to predict POS and morphosyntactic tags.</p><p>We emphasize the difference in the setting that we fix the word embeddings during the training, as to better evaluate the ability and consistency of the embeddings in capturing words' semantics and syntactics, rather than LSTM's ability to memo- rize words and infer the role of words from their context.</p><p>We use the same set of hyperparameters for the LSTM model as <ref type="bibr">Sec. 5.3 in Pinter et al. (2017)</ref> and train the model for 20 epochs for each lan- guage. The BoS and MIMICK word embedders   are trained beforehand with PolyGlot dataset us- ing the same way described earlier.</p><p>Results The POS tagging accuracies and micro F1 scores for morphosyntactic attributes are re- ported in <ref type="table" target="#tab_3">Table 3</ref> with word vectors generated by different models. The BoS and MIMICK model here are trained against Polyglot vectors. As a comparison, we include the results using random word vectors of the same dimension (64). Our BoS model shows steady and significant gain compared to MIMICK embeddings for both tasks in all languages. We especially observe the greatest margins for agglutinative languages such as Turkish and Indonesian, and in Germanic lan- guages English, Swedish and Danish, suggesting that our model learns stable representations for morphemes to consistent word type signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a subword-level word embedding model and a word vector generalization method that enables extending pre-trained word embed- dings with fixed size vocabularies to estimate word embeddings for out-of-vocabulary words. Intrin- sic evaluation on word similarity tasks and extrin- sic evaluation on POS tagging task demonstrate that our model captures morphological knowledge and generates good estimates of word vectors for OOV words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Ntrain</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Target vectors statistics and word similarity task scores in Spearman's ρ × 100. In parentheses are OOV rates.</head><label>1</label><figDesc></figDesc><table>We evaluate over Stanford RareWord 
(RW) introduced by Luong et al. (2013b) and 
WordSim353 (WS) introduced by Finkelstein 
et al. (2001). RW consists of less common words 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word similarity task results measured in Spear-

man's ρ × 100. 

so we use it to access our model's ability to gen-
eralize word embeddings to OOV words. WS is 
composed of mostly common words and we use 
it to test if our subword-level models successfully 
mimic the target vectors. 

Target vectors We train our BoS model over 
the English Polyglot vectors 2 to establish a direct 
comparison with results from MIMICK (Pinter 
et al., 2017), and as well as the Google word2vec 
vectors 3 which are popularly used in NLP tasks. 
Polyglot (Al-Rfou et al., 2013) is a multilingual 
NLP dataset, which also provides pre-trained word 
vectors over each language's corpus with a vocab-
ulary of 100,000 most frequent words. For Google 
vectors, most of their vocabulary consists of non-
words such as URLs and phrases, so we normal-
ize tokens into ASCII characters by taking off all 
the diacritics and take only tokens consisting of 
a single word with all lower letters. Statistics of 
the processed vectors are summarized in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>POS tagging accuracy and morphosyntactic attributes micro F1 over 23 languages (UD 1.4). In 
parentheses are the gains to MIMICK. 

</table></figure>

			<note place="foot" n="1"> The code is available at https://github.com/ jmzhao/bag-of-substring-embedder.</note>

			<note place="foot" n="2"> http://polyglot.readthedocs.io/en/ latest/Download.html 3 https://code.google.com/archive/p/ word2vec/ 4 https://fasttext.cc/docs/en/ pretrained-vectors.html</note>

			<note place="foot" n="5"> https://github.com/yuvalpinter/Mimick tation, it can finish 100 epochs of training over English PolyGlot vectors within 352 seconds on a machine with an Intel Core i7-6700 (3.4 GHz) CPU, 32GB memory and 1TB SSD. Compared to fastText which, with a fast multithread C++ implementation, takes hours to be trained over giga bytes of text corpus, our method provides a cheap way to generalize reasonably good word vectors for OOV words. 4 Joint Prediction of Part-of-Speech Tags and Morphosyntactic Attributes</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers for helpful comments. This work was supported in part by FA9550-18-1-0166. Y. L. would also like to acknowledge that support for this research was provided by the Office of the Vice Chancellor for Research and Graduate Education at the Univer-sity of Wisconsin-Madison with funding from the Wisconsin Alumni Research Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Factored neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Alexandrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Morphological word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1287" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fermandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Marujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Luis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mimicking word embeddings using subword RNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Guthrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="102" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">412</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-learning of word representations and morpheme representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="895" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised morphology induction using word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1627" to="1637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
