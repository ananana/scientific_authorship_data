<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GhostWriter: Using an LSTM for Automatic Rap Lyric Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Potash</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Massachusetts Lowell Lowell</orgName>
								<address>
									<postCode>01854</postCode>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GhostWriter: Using an LSTM for Automatic Rap Lyric Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper demonstrates the effectiveness of a Long Short-Term Memory language model in our initial efforts to generate un-constrained rap lyrics. The goal of this model is to generate lyrics that are similar in style to that of a given rapper, but not identical to existing lyrics: this is the task of ghostwriting. Unlike previous work, which defines explicit templates for lyric generation, our model defines its own rhyme scheme, line length, and verse length. Our experiments show that a Long Short-Term Memory language model produces better &quot;ghostwritten&quot; lyrics than a baseline model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Ghostwriting defines a distinction between the performer/presenter of text, lyrics, etc, and the cre- ator of text/lyrics. The goal is to present some- thing in a style that is believable enough to be credited to the performer. In the domain of rap specifically, rappers sometimes function as ghost- writers early on before embarking on their own public careers, and there are even businesses that provide written lyrics as a service <ref type="bibr">1</ref> . The goal of GhostWriter is to produce a system that can take a given artist's lyrics and generate similar yet unique lyrics. To accomplish this, we must cre- ate a language model to produce text, while also understanding what 'style' means in a quantitative sense.</p><p>The contribution of this paper is three-fold: (1) we present the ghostwriting problem of producing similar yet different lyrics; (2) we present compu- tational, quantitative evaluation methods for these two aspects; (3) we evaluate the performance of a Long Short-Term Memory (LSTM) vs n-gram model for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent work <ref type="bibr" target="#b15">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b6">Graves, 2013)</ref> has shown the effectiveness of Recurrent Neural Networks (RNNs) for text generation. In their works, the authors use an RNN to create a language model at the character level. The results are inspiring, as the models learn various gram- matical and punctuation rules, such as opening and closing parentheses, plus learning a large vo- cabulary of English words at the character level. <ref type="bibr" target="#b6">Graves (2013)</ref> uses a variation of an RNN called LSTM architecture which creates a better lan- guage model than a regular RNN.</p><p>Text generation for artistic purposes, such as po- etry and lyrics, has also been explored, often using templates and constraints <ref type="bibr" target="#b13">(Oliveira et al., 2014;</ref><ref type="bibr" target="#b3">Barbieri et al., 2012)</ref>. In regards to rap lyrics,  present a system for rap lyric gener- ation that produces a single line of lyrics that are meant to be a response to a single line of input. However, the work that is most similar to ours is that of <ref type="bibr">Malmi et al. (2015)</ref>. The authors create fixed 16-line verses, generating the verse line-by- line using full lines from existing rap songs. The system predicts the best next line based on the pre- vious lines, using a system that records an 81.9% accuracy predicting next lines in already existing verses. The feature that provides the greatest accu- racy gain is a neural embedding of the lines, cre- ated from the character level.</p><p>Hirjee and Brown (2010b) have developed a rhyme detection tool based on a probabilistic model <ref type="bibr" target="#b7">(Hirjee and Brown, 2010a</ref>) that analyzes phoneme patterns in words. The model is trained on a set of lyrics that were manually annotated for rhyming words. The statistics generated by the rhyme detection tool will be an important part of our evaluation (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generating Lyrics</head><p>In a departure from previous work on poetry/lyric generation, our goal is to build a model that does not require templates/constraints to generate lyrics, while also being able to produce full verses, as opposed to single lines. The system must be able to model general human language in order to produce fluent lyrics, but it must also be able to model the style of a target artist, by understanding the artist's vocabulary and rhythmic style, in order to fully execute the ghostwriting task of producing similar yet new lyrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LSTM</head><p>Here we will give a very brief overview of RNNs and LSTMs. For a more detailed explanation please refer to <ref type="bibr" target="#b6">(Graves, 2013)</ref>. The foundation of an RNN (of which an LSTM is specific ar- chitecture) is a word embedding E that provides a vector representation for each of the words in our corpus. Given a history of words w k , ..., w 0 we want to determine P (w k+1 |w k , ..., w 0 ; E, Φ), where Φ is a set of parameters used by our model. In the context of an RNN we define this probability by:</p><formula xml:id="formula_0">P (w k+1 |w k , ..., w 0 ; E, Φ) = f (x, s)<label>(1)</label></formula><p>At each time-step the RNN computes f given an observation x and a previous state s. The input goes through a transformation where it passes through one or several hidden layers. The LSTM model uses a specific architecture for the hidden transformation, defined by the LSTM memory cell. The key feature to the LSTM memory cell is the presence of an input gate, out- put gate, forget gate, and cell/cell memory, which manifest themselves in the model as activation vectors. Each of these gates/cells has its own bias vector, and the hidden layer at each time-step is now a complex nonlinear combination of gate, cell, and hidden vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using LSTM for Lyrics Generation</head><p>Since previous work has shown the power of RNNs to model language, we hope that it can cap- ture the rhythmic style of an artist by learning rhyme and meter patterns. As noted in Section 2, LSTMs have performed well at sequence forecast- ing, for example at learning punctuation, such as opening and closing parentheses. We see the task of rhyme detection as something similar in na- ture. <ref type="bibr">Kaparthy et al. (2015)</ref> have also shown that LSTMs could successfully learn where to place the brackets and indentation in C++ code. In their model, certain LSTM cells activated specifically when encountering end of the line. We believe learning rhymes at the end of the line is concep- tually similar to such tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Verse Structure and Rhyme Inference</head><p>The goal of our model is to not just generate lyrics, but generate the structure for the lyrics as well. To do this, we have added "&lt;endLine&gt;" and "&lt;endVerse&gt;" tokens to the lyrics. From this, the system will generate its own line breaks, while also defining when a generated verse ends. This allows us to analyze non-rhyming features from (Hirjee and Brown, 2010a), such as number of syllables per line and number of lines per verse. We also desire that, by using the "&lt;endLine&gt;" token, the system has a better chance of un- derstanding rhyme schemes used by an artist. For example, the LSTM can capture the pat- tern of "came &lt;endLine&gt;" followed shortly by "name &lt;endLine&gt;" to understand that "came" and "name" are a rhyming pair. To do this effec- tively, the system would need sufficient training data where rhyming pairs occur frequently enough to actually dictate a pattern, similar to ( <ref type="bibr" target="#b14">Reddy and Knight, 2011;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We collected songs from 14 artists from the site The Original Hip-Hop (Rap) Lyrics Archive - OHHLA.com -Hip-Hop Since 1992 2 . In the present lyrics generation experiments, we used the lyrics from the rapper Fabolous. For training, we used 219 verses with at least 175 words in each verse. We selected Fabolous because his lyrics produced the highest accuracy in the artist recog- nition experiments in <ref type="bibr" target="#b7">(Hirjee and Brown, 2010a</ref>). We conjecture that because of this, he had the most consistent style, making him a good choice for ini- tial experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>To compare with the results of the LSTM model, we followed the work of ( <ref type="bibr" target="#b3">Barbieri et al., 2012)</ref> and created a Markov model for lyric generation. Since the goal of our work is to make an unsu- pervised system, we do not use any constraints or templates to produce the lyrics. Thus, our baseline simplifies to a basic n-gram model. Given a history of w k+n−1 ,...,w k , the system generates a new token t as follows:</p><formula xml:id="formula_1">P (w k+n = t|w k+n−1 , ..., w k ) = |w k ,...,w k+n−1 ,t| |w k ,...,w k+n−1 ,•| (2)</formula><p>where |w k ...w k+n−1 t| is the amount of times the the context w k+n−1 ,...,w 1 is followed by t in the training data, and |w k ...w k+n−1 • | is the amount of times the context appears followed by any token. There is the possibility that the context has never been encountered in the training data. When this occurs, we back off to a smaller n-gram model:</p><formula xml:id="formula_2">P (w k+n = t|w k+n−2 , ..., w k ) = |w k ,...,w k+n−2 ,•,t| |w k ,...,w k+n−2 ,•,•|<label>(3)</label></formula><p>The model may have to back-off multiple times before it encounters context it has seen in the training data. Once we back-off to the point where we compute P (w n+k = t|w k ), we are guaranteed to have at least one non-zero probability, because w k must have appeared in the vocabulary for it to have been generated previously.</p><p>Note that rather than backing off to a lower- order n-gram model, we use a skip-gram model which drops the words immediately preceding the predicted word. The main motivation for this is that it allows us to capture long-range dependen- cies, which makes it into a better baseline compar- ison for an LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Initialization</head><p>When producing lyrics with either the LSTM or baseline model, we initialize with the "&lt;startVerse&gt;" token. Once the model produces a token, it becomes part of the context for the next step of token generation. Our models are closed in the sense that they only produce tokens that appear in the training vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LSTM Implementation</head><p>We used a Python implementation of an LSTM from Jonathan Raiman 3 . The LSTM is built on top of Theano ( <ref type="bibr" target="#b4">Bastien et al., 2012;</ref><ref type="bibr" target="#b5">Bergstra et al., 2010)</ref>. Following <ref type="bibr" target="#b6">(Graves, 2013)</ref>, we set the amount of LSTM inputs/outputs to be equal to the vocabulary size. Also, to avoid the vanishing gra- dient problem when training RNNs, we clip the gradients in the range <ref type="bibr">[-1,1]</ref>. We train our LSTM model using a Tesla K40 GPU on a single work- station.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Methods</head><p>In this section, we present automated methods for evaluating the quality of generated lyrics. Ide- ally, judging system output in terms of, e.g. flu- ency, should be conducted using manual evalua- tion. However, conducting formal human evalua- tion is somewhat problematic. For a full qualita- tive evaluation of a given artist that would assess both similarity of style and novelty, the evaluator would need to know that particular artist's body of work very well. Even finding annotators who are well-versed in the general art of rap lyrics can be challenging <ref type="bibr" target="#b1">(Addanki and Wu, 2014</ref>). While this may be possible for the present experiments that focus on a single artist, it is hardly feasible for larger-scale studies that will use our full data set that contains the lyrics of 14 different artists. We therefore propose an automated evaluation method which we believe is able to capture two critical as- pects of ghostwriting, which are in fact quite tricky to capture together: being similar, yet different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Similarity to existing lyrics</head><p>In order to evaluate the novelty of generated lyrics, we compare the similarity of the generated lyrics to the lyrics in our training set. We used an algorithm proposed by <ref type="bibr" target="#b11">(Mahedero et al., 2005)</ref> for calculating the similarity between produced lyrics and all verses from the same artist. This algorithm is based on the well-known Inverse Document Frequency, using cosine on document vectors to calculate distance. First, we build the Term-Document Matrix with weights for each term in each song:</p><formula xml:id="formula_3">w ij = f ij log( N n j ) (4)</formula><p>where N is the total number of documents (verses, in our case), n j is the number of verses that contains term j and f ij is the frequency of term j in the ith verse. Using this matrix, we can calculate the cosine distance between verses and use it as a measure of similarity. When discussing similarity, we refer to the max similarity: of all verses it is most similar to, exactly how similar is it? The lower the max similarity score, the more novel the lyrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Numerical features of the lyrics</head><p>We also produced the features from (Hirjee and Brown, 2010a) for our generated verses. The statistics of these features are meant to represent how effective we are in modeling an artist's style. The point of the system is not to produce arbitrary rhymes; it is to produce rhyme types and rhyme frequency that are similar to the target artist. Fol- lowing ( <ref type="bibr">Malmi et al., 2015)</ref>, the rhyme feature we examine in this work is rhyme density. Rhyme density is defined as the total number of rhymed syllables divided by the total number of syllables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>For the lyrics generation experiments reported here, we used the rapper Fabolous as the artist whose style we tried to emulate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Example of Generated Lyrics (Warning: Explicit Content)</head><p>Below is a sample of lyrics generated by the LSTM model:</p><p>Line 1: i m old enough to know better young enough to not give a fuck Line 2: rather hold my head high and die then live and duck Line 3: i got ta fuck be up and little niggaz go in Line 4: when i m in the feds and scoped by uncle sam Line 5: dope and hunn ed grams rope and hunn ed grams Line 6: at the same time she jerking and wig- gling Line 7: smirking and giggling</p><p>While the pairs of rhyming end-lines in the gen- erated lyrics are taken from the training data (the max similarity is 0.41), no more than two lines appear from a single verse. Though the gener- ated lyrics aren't novel in a strict sense, the LSTM model is more effective than the n-gram model at using lines from multiple verses (see next Sec- tion 6.3). The rhyme density of this verse is 0.35, which is almost equal to Fabolous's average rhyme density (0.34).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Quantitative Analysis</head><p>As previously mentioned, the key to effective ghostwriting is to mirror an artist's style, but also providing original content. While vocabu- lary and lyrical content are key components for an artist's style, this is inherently satisfied by using words only from the training data. Thus, rhyme style -specifically rhyme density -will be the key performance indicator for imitating an artist's style. In terms of rap lyrics in general, a higher rhyme density is often better. Therefore for our system we would like a high rhyme density, but with a low max similarity score (a higher nov- elty). <ref type="figure" target="#fig_0">Figures 2 and 1</ref> show the graph for rhyme density and max similarity for the LSTM and n- grams models, respectively. For the LSTM model the values are graphed compared to training iter- ation number -as the model becomes more fit to the data. For the n-gram model they are graphed dependent on n-gram value. For each n-gram value, we generate 10 verses and compute the av- erage value of the two metrics. One expects that a perfectly fit LSTM model without regularization would exactly reproduce lyrics from the training data, and a high n-gram value would would also produce duplicate lyrics. This is evident in the graphs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Correlation of Rhyme Density and Max Similarity</head><p>Since exact replication would assuredly give a higher rhyme density than randomly produced lyrics, we desire a low correlation between rhyme density and max similarity. The correlation be- tween rhyme density and max similarity for the LSTM model is 0.32, and for the n-gram model it is 0.47. When examining <ref type="figure" target="#fig_0">Figures 1 and 2</ref> one may notice the anomalous points of high rhyme density (at n = 6 on the n-gram graph and 3,000 iterations for the LSTM model). After further inspection of the lyrics at these points, we see the lyrics con- tain repetitions of the exact same phrase. Since words are repeated frequently, the rhyme density of the lyrics is high (repeated words create rhymed phonemes, according to the rhyme detection tool).</p><p>These points cause the similarity-density correla- tions to be artificially lower. After removing these data points, the LSTM model still has a lower correlation than the n-gram model, but the gap is much smaller: 0.71 compared to 0.75. Ultimately however, this shows that the LSTM model is better at generating original, rhyming lyrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Style Matching</head><p>Unfortunately, the correlation numbers do not dic- tate specifically the effectiveness of the LSTM model in the ghostwriting task. Instead, we can look at that max similarity values of both systems when they generate lyrics that produce a rhyme density similar to the average rhyme density of the target rapper. Looking at 100 randomly se- lected verses, Fabolous has an average rhyme den- sity of 0.34. To do our analysis, first we create four regression lines, one for each metric (max similarity and rhyme density) in each model (we do not include the points of high rhyme density). Next we use the two rhyme density lines to deter- mine at which iteration/n value the systems gen- erate a rhyme density of 0.34. After that we plug these numbers into the two similarity lines to de- termine what similarity is needed to achieve the target rhyme density. The n-gram model line has a similarity of 1.28 at this point (above the max value of 1 for the metric), while the LSTM model has a value of 0.59. Based on these numbers, the LSTM model clearly outperforms the n-gram model when it comes to making original lyrics that are similar in style to our target rapper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we have shown the effectiveness of an LSTM model for generating novel lyrics that are similar in style to a target artist. We com- pare the performance of the LSTM model to a much simpler system: an n-gram model. The re- sults of our experiments show that, as an unsu- pervised, non-template model, the LSTM model is better able to produce novel lyrics that also re- flect the rhyming style of the target artist. In fu- ture work, we plan to use more data to train our model, making it easier for our system to actually identify rhyming pairs and use them in new con- texts. We also plan to encode phoneme features of words to improve rhyme discovery. Furthermore, we plan to generate lyrics from artists with a vary- ing vocabulary size to see if it is easier to generate lyrics for an artist with a smaller vocabulary. In terms of evaluation, we hope to incorporate some method to evaluate the fluency of generated lyrics <ref type="bibr" target="#b1">(Addanki and Wu, 2014)</ref>. Lastly, to further avoid over-fitting to the training data and reproducing lyrics with a high similarity, we plan to use weight noise ( <ref type="bibr" target="#b9">Jim et al., 1996</ref>) to regularize our model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Values of rhyme density and max similarity versus n-gram value for the n-gram model.</figDesc><graphic url="image-1.png" coords="4,307.28,567.27,218.27,141.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Values of rhyme density and max similarity versus iteration number when training the LSTM model.</figDesc><graphic url="image-2.png" coords="5,72.00,62.81,218.27,141.72" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.rap-rebirth.com/, http://www.precisionwrittens.com/ rap-ghostwriters-for-hire/</note>

			<note place="foot" n="2"> http://www.ohhla.com/</note>

			<note place="foot" n="3"> https://github.com/JonathanRaiman/ theano_lstm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised rhyme scheme identification in hip hop lyrics using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Evaluating improvised hip hop lyrics-challenges and observa1923</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Proceedings of The Ninth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>The Ninth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Markov constraints for generating lyrics with style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Pachet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko Degli</forename><surname>Esposti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5590</idno>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Theano: a cpu and gpu math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for scientific computing conference (SciPy)</title>
		<meeting>the Python for scientific computing conference (SciPy)<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using automated rhyme detection to characterize rhyming style in rap music</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><surname>Hirjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rhyme analyzer: An analysis tool for rap lyrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussein</forename><surname>Hirjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel G Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Society for Music Information Retrieval Conference</title>
		<meeting>the 11th International Society for Music Information Retrieval Conference</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of noise in recurrent neural networks: convergence and generalization. Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Chuen</forename><surname>Jim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clyde</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Horne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1424" to="1438" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1506.02078</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language processing of lyrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Mahedero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Mart´inezmart´mart´inez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Koppenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gouyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th annual ACM international conference on Multimedia</title>
		<meeting>the 13th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="475" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pyry</forename><surname>Takala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannu</forename><surname>Toivonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04771</idno>
		<title level="m">Tapani Raiko, and Aristides Gionis. 2015. Dopelearning: A computational approach to rap lyrics generation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adapting a generic platform for poetry generation to produce spanish poems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo Gonçalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Hervás</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Díaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gervás</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Computational Creativity</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICCC</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of rhyme schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sravana</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to freestyle: Hip hop challenge-response induction via transduction rule segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Saers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meriem</forename><surname>Beloucif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
