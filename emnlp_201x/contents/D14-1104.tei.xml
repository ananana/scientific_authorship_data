<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Importance weighting and unsupervised domain adaptation of POS taggers: a negative result</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
							<email>bplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Language Technology</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<addrLine>Denmark Njalsgade 140</addrLine>
									<postCode>DK-2300</postCode>
									<settlement>Copenhagen S</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Importance weighting and unsupervised domain adaptation of POS taggers: a negative result</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="968" to="973"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Importance weighting is a generalization of various statistical bias correction techniques. While our labeled data in NLP is heavily biased, importance weighting has seen only few applications in NLP, most of them relying on a small amount of labeled target data. The publication bias toward reporting positive results makes it hard to say whether researchers have tried. This paper presents a negative result on unsu-pervised domain adaptation for POS tagging. In this setup, we only have unlabeled data and thus only indirect access to the bias in emission and transition probabilities. Moreover, most errors in POS tagging are due to unseen words, and there, importance weighting cannot help. We present experiments with a wide variety of weight functions, quantilizations, as well as with randomly generated weights, to support these claims.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many NLP tasks rely on the availability of anno- tated data. The majority of annotated data, how- ever, is sampled from newswire corpora. The performance of NLP systems, e.g., part-of-speech (POS) tagger, parsers, relation extraction sys- tems, etc., drops significantly when they are ap- plied to data that departs from newswire conven- tions. So while we can extract information, trans- late and summarize newswire in major languages with some success, we are much less successful processing microblogs, chat, weblogs, answers, emails or literature in a robust way. The main rea- sons for the drops in accuracy have been attributed to factors such as previously unseen words and bi- grams, missing punctuation and capitalization, as well as differences in the marginal distribution of data ( <ref type="bibr" target="#b0">Blitzer et al., 2006;</ref><ref type="bibr" target="#b7">McClosky et al., 2008;</ref><ref type="bibr" target="#b16">Søgaard and Haulrich, 2011)</ref>.</p><p>The move from one domain to another (from a source to a new target domain), say from newspa- per articles to weblogs, results in a sample selec- tion bias. Our training data is now biased, since it is sampled from a related, but nevertheless dif- ferent distribution. The problem of automatically adjusting the model induced from source to a dif- ferent target is referred to as domain adaptation.</p><p>Some researchers have studied domain adap- tation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic as- sumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsuper- vised domain adaptation, assuming only unlabeled target data is available. <ref type="bibr" target="#b6">Jiang and Zhai (2007)</ref>, <ref type="bibr" target="#b4">Foster et al. (2010;</ref><ref type="bibr" target="#b10">Plank and Moschitti (2013)</ref> and <ref type="bibr" target="#b16">Søgaard and Haulrich (2011)</ref> have previously tried to use importance weighting to correct sample bias in NLP. Im- portance weighting means assigning a weight to each training instance, reflecting its impor- tance for modeling the target distribution. Im- portance weighting is a generalization over post- stratification <ref type="bibr" target="#b15">(Smith, 1991)</ref> and importance sam- pling ( <ref type="bibr" target="#b14">Smith et al., 1997</ref>) and can be used to cor- rect bias in the labeled data.</p><p>Out of the four papers mentioned, only Søgaard and Haulrich (2011) and <ref type="bibr" target="#b10">Plank and Moschitti (2013)</ref> considered an unsupervised domain adap- tation scenario, obtaining mixed results. These two papers assume covariate shift <ref type="bibr" target="#b13">(Shimodaira, 2000</ref>), i.e., that there is only a bias in the marginal distribution of the training data. Under this as- sumption, we can correct the bias by applying a weight function <ref type="bibr">Pt(x)</ref> Ps(x) to our training data points (labeled sentences) and learn from the weighted data. Of course this weight function cannot be computed in general, but we can approximate it in different ways.</p><p>In POS tagging, we typically factorize se- quences into emission and transition probabilities. Importance weighting can change emission prob- abilities and transition probabilities by assigning weights to sentences. For instance, if our corpus consisted of three sequences: 1) a/A b/A, 2) a/A b/B, and 3) a/A b/B, then P (B|A) = 2/3. If se- quences two and three were down-weighted to 0.5, then P (B|A) = 1/2.</p><p>However, this paper argues that importance weighting cannot help adapting POS taggers to new domains using only unlabeled target data. We present three sources of evidence: (a) negative results with the most obvious weight functions across various English datasets, (b) negative re- sults with randomly sampled weights, as well as (c) an analysis of annotated data indicating that there is little variation in emission and transition probabilities across the various domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discrimi- nate between source and target instances ( <ref type="bibr" target="#b16">Søgaard and Haulrich, 2011;</ref><ref type="bibr" target="#b10">Plank and Moschitti, 2013)</ref> (y ∈ {s, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based clas- sifier on relation extraction instances. In these studies, ˆ P (t|x) is used as an approximation of</p><formula xml:id="formula_0">Pt(x)</formula><p>Ps(x) , following <ref type="bibr" target="#b20">Zadrozny (2004)</ref>. In §3, we fol- low the approach of Søgaard and Haulrich (2011), but consider a wider range of weight functions. Others have proposed to use kernel mean match- ing ( <ref type="bibr" target="#b5">Huang et al., 2007)</ref> or minimizing KL- divergence ( <ref type="bibr" target="#b18">Sugiyama et al., 2007)</ref>. <ref type="bibr" target="#b6">Jiang and Zhai (2007)</ref> use importance weight- ing to select a subsample of the source data by subsequently setting the weight of all selected data points to 1, and 0 otherwise. However, they do so by relying on a sequential model trained on labeled target data. Our results indicate that the covariate shift assumption fails to hold for cross- domain POS tagging. While the marginal distri- butions obviously do differ (since we can tell do- mains apart without POS analysis), this is most likely not the only difference. This might explain the positive results obtained by <ref type="bibr" target="#b6">Jiang and Zhai (2007)</ref>. We will come back to this in §4. <ref type="bibr" target="#b3">Cortes et al. (2010)</ref> show that importance weighting potentially leads to over-fitting, but pro- pose to use quantiles to obtain more robust weight functions. The idea is to rank all weights and ob- tain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i ≤ q), x is weighted by the average weight of data points in the ith quantile.</p><p>The weighted structured perceptron ( §3) used in the experiments below was recently used for a dif- ferent problem, namely for correcting for bias in annotations ( <ref type="bibr" target="#b11">Plank et al., 2014</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data</head><p>We use the data made available in the SANCL 2012 Shared Task (  For each target domain, we have both development and test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model</head><p>In the weighted perceptron ( <ref type="bibr" target="#b1">Cavallanti et al., 2006</ref>), we make the learning rate dependent on the current instance x n , using the following update:</p><formula xml:id="formula_1">w i+1 ← w i + β n α(y n − sign(w i · x n ))x n (1)</formula><p>where β n is the weight associated with x n . See <ref type="bibr" target="#b5">Huang et al. (2007)</ref> for similar notation.</p><p>We extend this idea straightforwardly to the structured perceptron <ref type="bibr" target="#b2">(Collins, 2002)</ref>  <ref type="table">Table 1</ref>: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).</p><p>we use an in-house implementation. We use commonly used features, i.e., w,w −1 , w −2 , w +1 , w +2 , digit, hyphen, capitalization, pre- /suffix features, and Brown word clusters. The model seems robust with respect to number of training epochs, cf. <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore we fix the number of epochs to five and use this setting in all our experiments. Our code is available at: https://bitbucket.org/ bplank/importance-weighting-exp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Importance weighting</head><p>In our first set of experiments, we follow Søgaard and Haulrich (2011) in using document classifiers to obtain weights for the source instances. We train a text classifier that discriminates the two domains (source and target). For each sentence in the source and target domain (the unlabeled text that comes with the SANCL data), we mark whether it comes from the source or target do- main and train a binary classifier (logistic regres- sion) to discriminate between the two. For ev- ery sentence in the source we obtain its probabil- ity for the target domain by doing 5-fold cross- validation. While Søgaard and Haulrich (2011) use only token-based features (word n-grams ≤ 3), we here exploit a variety of features: word token n-grams, and two generalizations: using Brown clusters (estimated from the union of the 5 target domains), and Wiktionary tags (if a word has multiple tags, we assign it the union of tags as single tag; OOV words are marked as such).</p><p>The distributions of weights can be seen in the upper half of <ref type="figure" target="#fig_2">Figure 2</ref>. <ref type="table">Table 1</ref> shows that our baseline model achieves state-of-the-art performance compared to SANCL   <ref type="bibr">1</ref> and FLORS ( <ref type="bibr" target="#b12">Schnabel and Schütze, 2014</ref>). Our results align well with the second best POS tagger in the SANCL 2012 Shared Task. Note  that the best tagger in the shared task explicitly used normalization and various other heuristics to achieve better performance. In the rest of the paper, we use the universal tag set part of the SANCL data ( . <ref type="figure" target="#fig_3">Figure 3</ref> presents our results on development data for different importance weighting setups. None of the above weight functions lead to signifi- cant improvements on any of the datasets. We also tried scaling and binning the weights, as suggested by <ref type="bibr" target="#b3">Cortes et al. (2010)</ref>, but results kept fluctuating around baseline performance, with no significant improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Random weighting</head><p>Obviously, weight functions based on document classifiers may simply not characterize the rele- vant properties of the instances and hence lead to bad re-weighting of the data. We consider three random sampling strategies, namely sampling ran- dom uniforms, random exponentials, and random Zipfians and ran 500 samples for each. For these experiments, we estimate significance cut-off lev- els of tagging accuracies using the approximate randomization test. To find the cut-off levels, we randomly replace labels with gold labels until the achieved accuracy significantly improves over the baseline for more than 50% of the samples. For each accuracy level, 50 random samples were taken.   <ref type="figure">Figure 4</ref>: Random weight functions (500 runs each) on test sets. Solid line is the baseline per- formance, while the dashed line is the p-value cut- off. From top: random, exponential and Zipfian weighting. All runs fall below the cut-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Results</head><p>The dashed lines in <ref type="figure">Figure 4</ref> show the p-value cut- offs for positive results. We see that most random weightings of data lead to slight drops in perfor- mance or are around baseline performance, and no weightings lead to significant improvements. Ran- dom uniforms seem slightly better than exponen- tials and Zipfians.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis</head><p>Some differences between the gold-annotated source domain data and the gold-annotated tar- get data used for evaluation are presented in <ref type="table" target="#tab_5">Ta- ble 2</ref>. One important observation is the low ambi- guity of word forms in the data. This makes the room for improvement with importance weight- ing smaller. Moreover, the KL divergencies over POS bigrams are also very low. This tells us that transition probabilities are also relatively constant across domains, again suggesting limited room for improvement for importance weighting. Compared to this, we see much bigger differ- ences in OOV rates. OOV rates do seem to explain most of the performance drop across domains. In order to verify this, we implemented a ver- sion of our structured perceptron tagger with type- constrained inference <ref type="bibr">(Täckström et al., 2013)</ref>. This technique only improves performance on un- seen words, but nevertheless we saw significant improvements across all five domains (cf. Ta- ble 3). This suggests that unseen words are a more important problem than the marginal distri- bution of data for unsupervised domain adaptation of POS taggers.   <ref type="table">Table 3</ref>: Results on the test sets by adding Wik- tionary type constraints. †=p-value &lt; 0.001.</p><p>We also tried Jiang and Zhai's subset selection technique ( §3.1 in <ref type="bibr" target="#b6">Jiang and Zhai (2007)</ref>), which assumes labeled training material for the target domain. However, we did not see any improve- ments. A possible explanation for these different findings might be the following. <ref type="bibr" target="#b6">Jiang and Zhai (2007)</ref> use labeled target data to learn their weight- ing model, i.e., in a supervised domain adaptation scenario. This potentially leads to very different weight functions. For example, let the source do- main be 100 instances of a/A b/B and 100 in- stances of b/B b/B, and the target domain be 100 instances of a/B a/B. Note that a domain classi- fier would favor the first 100 sentences, but in an HMM model induced from the labeled target data, things look very different. If we apply Laplace smoothing, the probability of a/A b/B accord- ing to the target domain HMM model would be ∼ 8.9e −7 , and the probability of b/B b/B would be ∼ 9e −5 . Note also that this set-up does not as- sume covariate shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Importance weighting, a generalization of various statistical bias correction techniques, can poten- tially correct bias in our labeled training data, but this paper presented a negative result about impor- tance weighting for unsupervised domain adapta- tion of POS taggers. We first presented exper- iments with a wide variety of weight functions, quantilizations, as well as with randomly gener- ated weights, none of which lead to significant im- provements. Our analysis indicates that most er- rors in POS tagging are due to unseen words, and what remains seem to not be captured adequately by unsupervised weight functions.</p><p>For future work we plan to extend this work to further weight functions, data sets and NLP tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training epochs vs tagging accuracy for the baseline model on the dev data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>https://sites.google.com/site/sancl2012/home/ shared-task/results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histogram of different weight functions.</figDesc><graphic url="image-1.png" coords="3,307.28,166.85,218.26,261.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results on development data for different weight functions, i.e., document classifiers trained on a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids. The weight was the raw p t (y|x) value, no scaling, no quantiles. Replacing only open-class tokens for b) and c) gave similar or lower performance.</figDesc><graphic url="image-2.png" coords="4,72.00,62.81,453.54,100.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>ans</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Relevant statistics for our analysis ( §4) 
on the test sets: average tag ambiguity, out-of-
vocabulary rate, and KL-divergence and Pearson 
correlation coefficient (ρ) on POS bigrams. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is funded by the ERC Starting Grant LOWLANDS No. 313695.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain adaptation with structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tracking the best hyperplane with a simple budget perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Cavallanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Nicoì O Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning bounds for importance weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative instance weighting for domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Correcting sample bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instance weighting for domain adaptation in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">When is self-training effective for parsing? In COLING</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of the 2012 Shared Task on Parsing the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning part-of-speech taggers with inter-annotator agreement loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flors: Fast and simple domain adaptation for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="16" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the loglikelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quick simulation: A review of importance sampling techniques in communications systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansoor</forename><surname>Shafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="597" to="613" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Post-stratification. The Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M F</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haulrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sentence-level instance-weighting for graph-based and transition-based dependency parsing</title>
	</analytic>
	<monogr>
		<title level="m">IWPT</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoaki</forename><surname>Paul Von Bünau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Token and type constraints for cross-lingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
