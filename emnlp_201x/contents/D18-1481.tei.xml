<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Universal Sentence Representations with Mean-Max Attention Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghua</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Universal Sentence Representations with Mean-Max Attention Autoencoder</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4514" to="4523"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4514</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In order to learn universal sentence representations , previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outper-forms the state-of-the-art unsupervised single methods, including the classical skip-thoughts (Kiros et al., 2015) and the advanced skip-thoughts+LN model (Ba et al., 2016). Furthermore , compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To automatically get the distributed representa- tions of texts (words, phrases and sentences) is a fundamental task for natural language process- ing (NLP). There have been efficient learning al- gorithms to acquire the representations of word- s ( <ref type="bibr" target="#b19">Mikolov et al., 2013a)</ref>, which have shown to provide useful features for various tasks. Inter- estingly, the acquired word representations reflect some observed aspects of human conceptual orga- * Corresponding author. <ref type="bibr">1</ref> Our code is publicly available at https://github. com/Zminghua/SentEncoding. nization ( <ref type="bibr" target="#b10">Hill et al., 2015)</ref>. In recent years, learn- ing sentence representations has attracted much at- tention, which is to encode sentences into fixed- length vectors that could capture the semantic and syntactic properties of sentences and can then be transferred to a variety of other NLP tasks.</p><p>The most widely used method is to employ an encoder-decoder architecture with recurrent neu- ral networks (RNN) to predict the original input sentence or surrounding sentences given an in- put sentence <ref type="bibr" target="#b3">Ba et al., 2016;</ref><ref type="bibr" target="#b9">Hill et al., 2016;</ref><ref type="bibr" target="#b8">Gan et al., 2017)</ref>. However, the RNN becomes time consuming when the se- quence is long. The problem becomes more se- rious when learning general sentence representa- tions that needs training on a large amount of data. For example, it took two weeks to train the skip- thought ( . Moreover, the tra- ditional RNN autoencoder generates words in se- quence conditioning on the previous ground-truth words, i.e., teacher forcing training <ref type="bibr" target="#b28">(Williams and Zipser, 1989)</ref>. This teacher forcing strategy has been proven important because it forces the output of the RNN to stay close to the ground-truth se- quence. However, at each time step, allowing the decoder solely to access the previous ground-truth words weakens the encoder's ability to learn the global information of the input sequence.</p><p>Some other approaches ( <ref type="bibr" target="#b6">Conneau et al., 2017;</ref><ref type="bibr" target="#b5">Cer et al., 2018;</ref><ref type="bibr" target="#b25">Subramanian et al., 2018</ref>) attempt to use the labelled data to build a generic sentence encoder, such as the Stanford Natural Language Inference (SNLI) dataset <ref type="bibr" target="#b4">(Bowman et al., 2015</ref>), but such large-scale high-quality labelled data ap- propriate for training sentence representations is generally not available in other languages.</p><p>In this paper, we are interested in learning u- niversal sentence representations based on a large amount of naturally occurring corpus, without us- ing any labelled data. We propose a mean-max attention autoencoder (mean-max AAE) to model sentence representations. Specifically, an encoder performs the MultiHead self-attention on an input sentence, and then the combined mean-max pool- ing operation is employed to produce the laten- t representation of the sentence. The representa- tion is then fed into a decoder to reconstruct the input sequence, which also depends entirely on the MultiHead self-attention. At each time step, the decoder performs attention operations over the mean-max encoding, which on the one hand, en- ables the decoder to utilize the global information of the input sequence rather than generating word- s solely conditioning on the previous ground-truth words, and on the other hand, allows the decoder to attend to different representation subspaces dy- namically.</p><p>We train our autoencoder on a large collection of unlabelled data, and evaluate the sentence em- beddings across a diverse set of 10 transfer tasks. The experimental results show that our model out- performs the state-of-the-art unsupervised single models, and obtains comparable results with the combined models. Our mean-max representation- s yield consistent performance gain over the indi- vidual mean and max representations. At the same time, our model can be efficiently parallelized and so achieves significant improvement in computa- tional efficiency.</p><p>In summary, our contributions are as follows:</p><p>• We apply the MultiHead self-attention mech- anism to train autoencoder for learning uni- versal sentence representations, which allows our model to do processing parallelization and thus greatly reduce the training time in large unlabelled data.</p><p>• we adopt a mean-max representation strate- gy in the encoding and then the decoder con- ducts attention over the latent representation- s, which can well capture the global informa- tion of the input from different views.</p><p>• After training only on naturally occurring un- ordered sentences, we obtain a simple and fast sentence encoder, which is an unsuper- vised single model and achieves state-of-the- art performance on various transfer tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>With the flourishing of deep learning in NLP re- search, a variety of approaches have been de- veloped for mapping word embeddings to fixed- length sentence representations. The methods generally fall into the following categories.</p><p>Unsupervised training with unordered sen- tences. This kind of methods depends only on naturally occurring individual sentences. <ref type="bibr" target="#b14">Le and Mikolov (2014)</ref> propose the paragraph vec- tor model, which incorporates a global contex- t vector into the log-linear neural language mod- el ( <ref type="bibr" target="#b20">Mikolov et al., 2013b</ref>), but at test time, infer- ence needs to be performed to compute a new vec- tor. <ref type="bibr" target="#b2">Arora et al. (2017)</ref> propose a simple but ef- fective Smooth Inverse Frequency (SIF) method, which represents sentence by a weighted average of word embeddings. <ref type="bibr" target="#b9">Hill et al. (2016)</ref> intro- duce sequential denoising autoencoders (SDAE), which employ the denoising objective to predict the original source sentence given a corrupted ver- sion. They also implement bag-of-words model- s such as word2vec-SkipGram, word2vec-CBOW. Our model belongs to this group, which has no re- striction on the required training data and can be trained on sets of sentences in arbitrary order.</p><p>Unsupervised training with ordered sen- tences. This kind of method is trained to predic- t the surrounding sentences of an input sentence, based on the naturally occurring coherent texts.  propose the skip-thoughts mod- el, which uses an encoder RNN to encode a sen- tence and two decoder RNN to predict the sur- rounding sentences. The skip-thought vectors per- form well on several tasks, but training this mod- el is very slow, requiring several days to produce meaningful results. <ref type="bibr" target="#b3">Ba et al. (2016)</ref> further obtain better results by adding layer-norm regularization on the skip-thoughts model. <ref type="bibr" target="#b8">Gan et al. (2017)</ref> ex- plore a hierarchical model to predict multiple fu- ture sentences, using a convolutional neural net- work (CNN) encoder and a long-short term mem- ory (LSTM) decoder. <ref type="bibr" target="#b17">Logeswaran and Lee (2018)</ref> reformulate the problem of predicting the contex- t in which a sentence appears as a classification task. Given a sentence and its context, a classifi- er distinguishes context sentences from other con- trastive sentences based on their vector represen- tations.</p><p>Supervised learning of sentence representa- tions. <ref type="bibr" target="#b9">Hill et al. (2016)</ref> implement models trained on supervised data, including dictionary defini- tions, image captions from the COCO dataset ( <ref type="bibr" target="#b16">Lin et al., 2014</ref>) and sentence-aligned translat-ed texts. <ref type="bibr" target="#b6">Conneau et al. (2017)</ref> attempt to ex- ploit the SNLI dataset for building generic sen- tence encoders. Through examining 7 different model schemes, they show that a bi-directional L- STM network with the max pooling yields excel- lent performance. <ref type="bibr" target="#b5">Cer et al. (2018)</ref> apply multi- task learning to train sentence encoders, including a skip-thought like task, a conversational input- response task and classification tasks from the SNLI dataset. They also explore combining the sentence and word level transfer models. <ref type="bibr" target="#b25">Subramanian et al. (2018)</ref> also present a multi-task learning framework for sentence representation- s, and train their model on several data resources with multiple training objectives on over 100 mil- lion sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Description</head><p>Our model follows the encoder-decoder architec- ture, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The input sequence is compressed into a latent mean-max representa- tion via an encoder network, which is then used to reconstruct the input via a decoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>In our model, we treat the input sentence as one sequence of tokens. Let S denote the in- put, which is comprised of a sequence of token- s {w 1 , w 2 , . . . , w N }, where N denotes the length of the sequence. An additional "&lt;/S&gt;" token is appended to each sequence. Each word w t in S is embedded into a k-dimensional vector</p><formula xml:id="formula_0">e t = W e [w t ],</formula><p>where W e ∈ R dw×V is a word em- bedding matrix, w t indexes one element in a V - dimensional set (vocabulary), and W e [v] denotes the v-th column of matrix W e .</p><p>In order for the model to take account of the se- quence order, we also add "positional encodings" ( <ref type="bibr" target="#b26">Vaswani et al., 2017</ref>) to the input embeddings:</p><formula xml:id="formula_1">p t [2i] = sin( t 10000 2i/dw ) (1) p t [2i + 1] = cos( t 10000 2i/dw ) (2)</formula><p>where t is the position and i is the dimension. Each dimension of the positional encoding corre- sponds to a sinusoid. Therefore, the input of our model can be represented as x t = e t + p t .</p><p>In the following description, we use h e t and h d t to denote the hidden vectors of the encoder and decoder respectively, the subscripts of which indi- cate timestep t, and the superscripts indicate oper- ations at the encoding or decoding stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MultiHead Self-Attention</head><p>In this subsection, we give a quick overview of MultiHead Self-Attention mechanism ( <ref type="bibr" target="#b26">Vaswani et al., 2017</ref>). The attention is to map a query q and a set of key-value pairs (K, V ) to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed based on the query and the correspond- ing key. The MultiHead mechanism applies mul- tiple attention operations in parallel. Given q and (K, V ), we can obtain the attention vector a by:</p><formula xml:id="formula_2">a = M ultiHead(q, K, V ) (3) = concat(head 1 , . . . , head l )<label>(4)</label></formula><formula xml:id="formula_3">head i = attention(q, K, V ) (5) = sof tmax( qK T √ d k )V<label>(6)</label></formula><p>where</p><formula xml:id="formula_4">q, K, V = qW q i , KW K i , V W V i<label>(7)</label></formula><p>W q i , W K i and W V i are parameter matrices; q ∈ R d k , K ∈ R n k ×d k and V ∈ R n k ×dv ; d k and d v are the dimensions of K and V respectively; n k is the number of key-value pairs.</p><p>The MultiHead self-attention allows the mod- el to jointly attend to information from different positions. Due to the reduced dimension of each head and parallel operations, the total computa- tional cost is similar to that of a single-head at- tention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Encoder</head><p>The encoder has two sub-layers. The first is a Mul- tiHead self-attention mechanism, and the second is a position-wise fully connected feed-forward network which consists of two linear transforma- tions with a ReLU activation in between. Different from Vaswani et al. <ref type="formula" target="#formula_4">(2017)</ref>, we remove the residual connections in the MultiHead self-attention layer and only employ a residual connection in the fully connected layer, allowing the model to expand the dimension of hidden vectors to incorporate more information.</p><p>Given the input x = (x 1 , . . . , x N ), the hidden vector h e t at time-step t is computed by:</p><formula xml:id="formula_5">a e t = M ultiHead(x t , x, x) (8) a e t = LN (a e t )<label>(9)</label></formula><formula xml:id="formula_6">h e t = max(0, a e t W e 1 + b e 1 )W e 2 + b e 2<label>(10)</label></formula><formula xml:id="formula_7">h e t = LN (h e t + a e t )<label>(11)</label></formula><p>where W e 1 ∈ R dm×d f and W e 2 ∈ R d f ×dm are pa- rameter matrices; b e 1 ∈ R d f and b e 2 ∈ R dm are bias vectors; d m and d f are the dimensions of hidden vector and fully connected inner layer respective- ly; LN denotes layer normalization.</p><p>Our model can be efficiently parallelized over the whole input. We can obtain all hidden vec- tors (h e 1 , . . . , h e N ) simultaneously for an input se- quence, thus greatly reducing the computational complexity compared with the sequential process- ing of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mean-Max Representation</head><p>Given the varying number of hidden vectors {h e t } t= <ref type="bibr">[1,...,N ]</ref> , we need to transform these local hidden vectors into a global sentence representa- tion. We would like to apply the pooling strategy, which makes the extracted representation indepen- dent of the length of the input sequence and ob- tains a fixed-length vector. <ref type="bibr" target="#b6">Conneau et al. (2017)</ref> examine BiLSTM with mean and max pooling for fixed-size sentence representation, and they con- clude that the max pooling operation performs bet- ter on transfer tasks.</p><p>In this work, we propose to apply mean and max pooling simultaneously. The max pooling takes the maximum value over the sequence, which tries to capture the most salient property while filter- ing out less informative local values. On the oth- er hand, the mean pooling does not make sharp choices on which part of the sequence is more im- portant than others, and so it captures general in- formation while not focusing too much on specific features. Obviously, the two pooling strategies can complement each other. The mean-max represen- tation is obtained by:</p><formula xml:id="formula_8">z max [i] = max t h e ti (12) z mean = 1 N t h e t (13) z = [z max , z mean ]<label>(14)</label></formula><p>Through combining two different pooling s- trategies, our model enjoys the following advan- tages. First, in the encoder, we can summarize the hidden vectors from different perspectives and so capture more diverse features of the input se- quence, which will bring robustness on differen- t transfer tasks. Second, in the decoder (as de- scribed in the next subsection), we can perform attention over the mean-max representation rather than over the local hidden vectors step by step, which would potentially make the autoencoder ob- jective trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Attention Decoder</head><p>As with the encoder, the decoder also applies the MultiHead self-attention to reconstruct the in- put sequence. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the en- coder and decoder are connected through a mean- max attention layer, which performs attention over the mean-max representation generated by the en- coder.</p><p>To facilitate expansion of the hidden size, we employ residual connections in the mean-max at- tention layer and the fully connected layer, but not in the MultiHead self-attention layer. Given y = (x 1 , . . . , x t−1 ) and z as the decoder input, the hidden vector h d t at time step t is obtained by:</p><formula xml:id="formula_9">a d t = M ultiHead(y t , y, y)<label>(15)</label></formula><formula xml:id="formula_10">a d t = LN (a d t )<label>(16)</label></formula><formula xml:id="formula_11">a z t = M ultiHead(a d t , z, z)<label>(17)</label></formula><formula xml:id="formula_12">a z t = LN (a z t + a d t )<label>(18)</label></formula><formula xml:id="formula_13">h d t = max(0, a z t W d 1 + b d 1 )W d 2 + b d 2<label>(19)</label></formula><formula xml:id="formula_14">h d t = LN (h d t + a z t )<label>(20)</label></formula><p>where <ref type="formula" target="#formula_4">(17)</ref> is the mean-max representation generated by Equation <ref type="bibr">(14)</ref>.</p><note type="other">W d 1 ∈ R dm×d f and W d 2 ∈ R d f ×dm are pa- rameter matrices; b d 1 ∈ R d f and b d 2 ∈ R dm are bias vectors. z in Equation</note><p>Given the hidden vectors (h d 1 , . . . , h d N ), the probability of generating a sequence S with length-N is defined as:</p><formula xml:id="formula_15">P (w t |w &lt;t , z) ∝ exp(W d 3 h d t + b d 3 )<label>(21)</label></formula><formula xml:id="formula_16">J(θ) = t logP (w t |w &lt;t , z)<label>(22)</label></formula><p>The model learns to reconstruct the input sequence by optimizing the objective in Equation (22).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating Sentence Representations</head><p>In the previous work, researchers evaluated the distributed representations of sentences by adding them as features in transfer tasks ( <ref type="bibr" target="#b8">Gan et al., 2017;</ref><ref type="bibr" target="#b6">Conneau et al., 2017</ref>). We use the same benchmarks and follow the same pro- cedure to evaluate the capability of sentence em- beddings produced by our generic encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transfer Tasks</head><p>We conduct extensive experiments on 10 trans- fer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (M- R, SST) (Pang and <ref type="bibr" target="#b22">Lee, 2005;</ref><ref type="bibr" target="#b23">Socher et al., 2013)</ref>, customer product reviews (CR) ( <ref type="bibr" target="#b11">Hu and Liu, 2004</ref>), subjectivity/objectivity classification (SUBJ) ( <ref type="bibr" target="#b21">Pang and Lee, 2004</ref>), opinion polarity (MPQA) ( <ref type="bibr" target="#b27">Wiebe et al., 2005)</ref> and question type classification (TREC) ( <ref type="bibr" target="#b15">Li and Roth, 2002</ref>). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) ( <ref type="bibr" target="#b7">Dolan et al., 2004</ref>), where the evaluation metrics are accuracy and F1 score.</p><p>We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evalua- tion metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (A- girre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations. The processing on each task is as follows: 1) Employ the pre-trained attention autoencoder to encode all sentences into the latent mean-max rep- resentations. 2) Using the representations as fea- tures, apply the open SentEval with a logistic re- gression classifier ( <ref type="bibr" target="#b6">Conneau et al., 2017</ref>) to au- tomatically evaluate on all the tasks. For a fair comparison of the plain sentence embeddings, we adopt all the default settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Setup</head><p>We train our model on the open Toronto Books Corpus ( , which was also used to train the skip-thoughts ( ) and skip-thoughts+LN ( <ref type="bibr" target="#b3">Ba et al., 2016)</ref>. The Toron- to Book Corpus consists of 70 million sentences from over 7, 000 books, which is not biased to- wards any particular domain or application.</p><p>The dimensions of hidden vectors and fully con- nected inner layer are set to 2, 048 and 4, 096 re- spectively. Hence, our mean-max AAE represents sentences with 4, 096 dimensional vectors. We set l = 8 parallel attention heads according to the de- velopment data.</p><p>We use the Adam algorithm ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) with learning rate 2×10 −4 for optimization. Gradient clipping is adopted by scaling gradients when the norm of the parameter vector exceeds a threshold of 5. We perform dropout ( <ref type="bibr" target="#b24">Srivastava et al., 2014</ref>) and set the dropout rate to 0.5. Mini- batches of size 64 are used. Our model learns un- til the reconstruction accuracy in the development data stops improving.</p><p>Our aim is to learn a generic sentence encoder that could encode a large number of words. There always are some words that haven't been seen dur- ing training, and so we use the publicly available GloVe vectors 2 to expand our encoder's vocab- ulary. We set the word vectors in our models as the corresponding word vectors in GloVe, and do not update the word embeddings during training. Thus, any word vectors from GloVe can be nat- urally used to encode sentences. Our models are trained with a vocabulary of 21, 583 top frequent words in the Toronto Book corpus. After vocab- ulary expansion, we can now successfully cover 2, 196, 017 words.</p><p>All experiments are implemented in Tensorflow ( <ref type="bibr" target="#b0">Abadi et al., 2016)</ref>, using a NVIDIA GeForce GTX 1080 GPU with 8GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Results</head><p>A summary of our experimental results on 10 tasks is given in <ref type="table">Table 1</ref>, in which the evaluation metric of the first 8 tasks is accuracy. To make a clear comparison of the overall performance, we com- pute the "macro" and "micro" average of accuracy <ref type="bibr">Method</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R STS14</head><p>Unsupervised training of single model  in <ref type="table" target="#tab_1">Table 2</ref>, where the micro average is weighted by the number of test samples in each task. In the previous work, different approaches conduct ex- periments on different benchmarks. Therefore we report the average scores on 6 tasks, 7 tasks and 8 tasks, respectively.</p><p>We divide related models into three groups. The first group contains unsupervised single model- s, including the Paragraph Vector model ( <ref type="bibr" target="#b14">Le and Mikolov, 2014</ref>), the SDAE method ( <ref type="bibr" target="#b9">Hill et al., 2016</ref>), the SIF model ( <ref type="bibr" target="#b2">Arora et al., 2017)</ref>, the Fast- Sent ( <ref type="bibr" target="#b9">Hill et al., 2016)</ref>, the skip-thoughts (uni- skip and bi-skip) ( , the CNN encoder (hierarchical-CNN and composite-CNN) ( <ref type="bibr" target="#b8">Gan et al., 2017</ref>) and skip-thoughts+LN ( <ref type="bibr" target="#b3">Ba et al., 2016)</ref>. Our mean-max attention autoencoder sits in this group. The second group consists of un- supervised combined models, including combine- skip ( ) and combine-CNN ( <ref type="bibr" target="#b8">Gan et al., 2017</ref>). In the third group, we list the results from the work of <ref type="bibr" target="#b6">Conneau et al. (2017)</ref> only for reference, since it is trained on labelled data.</p><p>Comparison with skip-thoughts+LN. The skip-thoughts+LN is the best model among the existing single models. Compared with the skip- thoughts+LN, our method obtains better results on 4 datasets (SST, TREC, SICK-E, STS14) and comparable results on 3 datastes (SUBJ, MPQA, SICK-R). Looking at the STS14 results, we ob- serve that the cosine metrics in our representation space is much more semantically informative than in skip-thoughts+LN representation space (pear- son score of 0.58 compared to 0.44). Consid- ering the overall performance shown in <ref type="table" target="#tab_1">Table 2</ref>, our model obtains better results both in the macro and micro average accuracy across 7 considered tasks. In view of the required training data, the skip-thoughts+LN needs coherent texts while our model needs only individual sentences. Moreover, we train our model in less than 5 hours on a sin- gle GPU compared to the best skip-thoughts+LN network trained for a month.</p><p>Unsupervised combined models. The result- s of the individual models ( <ref type="bibr" target="#b8">Gan et al., 2017)</ref> are not promising. To get bet- ter performance, they train two separate models on the same corpus and then combine the laten- t representations together. As shown in <ref type="table" target="#tab_1">Table 2</ref>, our mean-max attention autoencoder outperform- s the classical combine-skip model by 1.3 points in the average performance across 8 considered tasks. Specially, the pearson correlation of our model is 2 times over the combine-skip model on the STS14 task. Looking at the overall perfor- mance of 6 tasks, our model gets comparable re- sults with the combine-CNN, which combines the hierarchical and composite approaches to exploit the intra-sentence and inter-sentence information. Obviously, our model is simple and fast to imple- ment compared with the combined methods.</p><p>Supervised representation training. It is un- fair to directly compare our totally unsupervised model with the supervised representation learning method. <ref type="bibr" target="#b6">Conneau et al. (2017)</ref> train the BiLSTM- Max (on ALLNLI) on the high-quality natural lan- guage inference data. Our model even perform- s better than the BiLSTM-Max (on ALLNLI) on the SUBJ and TREC tasks. More importantly, our model can be easily adapted to other low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis</head><p>Our model contains three main modules: the mean-max attention layer, the combined pooling strategy and the encoder-decoder network. We make a further study on these components. The experimental results are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>In our model, the mean-max attention layer al- lows the decoder to pay attention to the encod- ing representation of the full sentence at each time step dynamically. To summarize the contribu- tion of the mean-max attention layer, we compare with traditional baselines, including the sequential denoising autoencoder (SDAE) with LSTM net- works ( <ref type="bibr" target="#b9">Hill et al., 2016</ref>) and the CNN-LSTM au- toencoder ( <ref type="bibr" target="#b8">Gan et al., 2017)</ref>, both of which only use the encoding representation to set the initial s- tate of the decoder and follow the teacher forcing strategy.</p><p>We employ both the mean and max pooling op- erations over the local hidden vectors to obtain sentence embeddings. To validate the effective- ness of our mean-max representations, we train two additional models: (i) an attention autoen- coder only with max pooling (max AAE) and (ii) an attention autoencoder only with mean pooling (mean AAE). The dimension of hidden vectors is also set to 2, 048.</p><p>Our encoder-decoder network depends on the MultiHead self-attention mechanism to recon- struct the input sequence. To test the effect of the MultiHead self-attention mechanism, we replace it with RNN and implement a mean-max RNN au- toencoder (mean-max RAE) training on the same Toronto Books Corpus. A bidirectional LSTM computes a set of hidden vectors on an input sen- tence, and then the mean and max pooling oper- ations are employed to generate the latent mean- max representation. The representation is then fed to a LSTM decoder to reconstruct the input sequence through attention operation over the la- tent representation. The parameter configurations are consistent with our other models. Moreover, we also train two additional models with different pooling strategies: mean RAE and max RAE.</p><p>Analysis on the mean-max attention layer. Our mean-max attention layer brings significan- t performance gain over the previous autoen- coders. Compared the mean RAE with LSTM- SDAE, both of which use the RNN-RNN encoder- decoder network to reconstruct the input sequence, our mean RAE consistently obtains better perfor- mance than LSTM-SDAE across all considered tasks. In particular, it yields a performance gain of 10.4 on the TREC dataset and 29 on the STS14 dataset. Compared with another CNN-LSTM au- toencoder, our mean RAE also gets better perfor- mance for all but one task. It demonstrates that the mean-max attention layer enables the decoder to attend to the global information of the input se- quence, thus go beyond the "teacher forcing train- ing".</p><p>Analysis on the pooling strategy. Considering the overall performance, our mean-max represen- tations outperform the individual mean and max representations both in the attention and RNN net- works. In our attention autoencoder, the macro av- erage score of the mean-max AAE is more than 0.6 over the individual pooling strategy. In the RNN autoencoder, the combined pooling strategy Method Macro MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R STS14 mean AAE 84.1 78.6 81. <ref type="bibr">6</ref>   yields a performance gain of 0.5 over the mean pooling and 0.6 over the max pooling. The re- sults indicate that our mean-max pooling captures more diverse information of the input sequence, which is robust and effective in dealing with vari- ous transfer tasks.</p><p>Comparison with RNN-based autoencoder. As shown in <ref type="table" target="#tab_3">Table 3</ref>, our MultiHead self-attention network obtains obvious improvement over the RNN network in different sets of pooling strate- gies, and it yields a performance gain of 1.1 when applying the best combined mean-max pooling operation. The results demonstrate that the Mul- tiHead self-attention mechanism enables the sen- tence representations to capture more useful infor- mation about the input sequence.</p><p>Analysis on computational complexity. A self-attention layer connects all positions with a constant number of sequentially executed opera- tions, whereas a recurrent layer requires O(n) se- quential operations. Therefore, our model greatly reduces the computational complexity. Excluding the number of parameters used in the word embed- dings, the skip-thought model (  contains 40 million parameters, while our mean- max AAE has approximately 39 million parame- ters. It took nearly 50.4 and 25.4 minutes to train the skip-thought model ( ) and the skip-thoughts+LN ( <ref type="bibr" target="#b3">Ba et al., 2016)</ref> per 1000 mini- batches respectively. Both the skip-thought and skip-thought+LN are implemented in Theano. A recent implementation of the skip-thoughts model was released by Google 3 , which took nearly 25.9 minutes to train 1000 mini-batches on a GTX 1080 GPU. In our experiment, it took 3.3 minutes to train the mean-max AAE model every 1000 mini- batches.</p><p>3 https://github.com/tensorflow/models/ tree/master/research/skip_thoughts mean max mean max <ref type="figure">Figure 2</ref>: Two examples illustrate that our mean-max attention layer could attend to the two different repre- sentations dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Attention Visualization</head><p>The above experimental results have proven the ef- fectiveness of the mean-max attention mechanism in the decoding. We further inspect the attention distributions captured by the mean-max attention layer, as shown in <ref type="figure">Figure 2</ref>. The side-by-side heat illustrates how much the decoder pay attention to the mean representation and max representation respectively at each decoding step. We can see that the attention layer learns to selectively retrieve the mean or max representations dynamically, which relieve the decoder from the burden of generating words solely conditioning on the previous ground- truth words. Also, the two different representa- tions can complement each other, and the mean representation plays a greater role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we present a mean-max AAE to learn universal sentence representations from un- labelled data. Our model applies the MultiHead self-attention mechanism both in the encoder and decoder, and employs a mean-max pooling strat- egy to capture more diverse information of the input. To avoid the impact of "teacher forcing training", our decoder performs attention over the encoding representations dynamically. To eval- uate the effectiveness of sentence representation- s, we conduct extensive experiments on 10 trans- fer tasks. The experimental results show that our model obtains state-of-the-art performance among the unsupervised single models. Furthermore, it is fast to train a high-quality generic encoder due to the paralleling operation. In the future, we will adapt our mean-max AAE to other low-resource languages for learning universal sentence repre- sentations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the mean-max attention autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The macro and micro average accuracy across different tasks. The bold is the highest score among all unsupervised models.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of different pooling strategies and different encoder-decoder networks on 10 transfer tasks. 
Macro is the macro average over the first 8 tasks whose metric is accuracy. 

</table></figure>

			<note place="foot" n="2"> https://nlp.stanford.edu/projects/ glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work is support-ed by the National Key Basic Research Program of China (2014CB340504) and National Natural Sci-ence Foundation of China (61773026, 61572245). The corresponding author of this paper is Yunfang Wu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>arX- iv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Universal sentence encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno>arX- iv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">350</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning generic sentence representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchen</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2380" to="2390" />
		</imprint>
	</monogr>
	<note>Xiaodong He, and Lawrence Carin</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT</title>
		<meeting>NAACLHLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1367" to="1377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="665" to="695" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Seattle, Washington, Usa, August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Coling</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">696</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>arX- iv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning general purpose distributed sentence representations via large scale multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher J</forename><surname>Pal</surname></persName>
		</author>
		<idno>arX- iv:1804.00079</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno>arX- iv:1506.06724</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
