<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Source-side Monolingual Data in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">‡ CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<orgName type="institution">CASIA</orgName>
								<address>
									<settlement>Beijing, Shanghai</settlement>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Source-side Monolingual Data in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1535" to="1545"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural Machine Translation (NMT) based on the encoder-decoder architecture has recently become a new paradigm. Researchers have proven that the target-side monolingual data can greatly enhance the decoder model of NMT. However, the source-side monolingual data is not fully explored although it should be useful to strengthen the encoder model of NMT, especially when the parallel corpus is far from sufficient. In this paper, we propose two approaches to make full use of the source-side monolingual data in NMT. The first approach employs the self-learning algorithm to generate the synthetic large-scale parallel data for NMT training. The second approach applies the multi-task learning framework using two NMTs to predict the translation and the reordered source-side monolingual sentences simultaneously. The extensive experiments demonstrate that the proposed methods obtain significant improvements over the strong attention-based NMT.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) following the encoder-decoder architecture proposed by <ref type="bibr" target="#b24">(Kalchbrenner and Blunsom, 2013;</ref>) has become the novel paradigm and obtained state-of- the-art translation quality for several language pairs, such as English-to-French and English-to-German ( <ref type="bibr" target="#b38">Sutskever et al., 2014;</ref><ref type="bibr" target="#b29">Luong et al., 2015b;</ref><ref type="bibr" target="#b36">Sennrich et al., 2015)</ref>. This end- to-end NMT typically consists of two recurrent neu- ral networks. The encoder network maps the source sentence of variable length into the context vector representation; and the decoder network generates the target translation word by word starting from the context vector.</p><p>Currently, most NMT methods utilize only the sentence aligned parallel corpus for model train- ing, which limits the capacity of the model. Re- cently, inspired by the successful application of tar- get monolingual data in conventional statistical ma- chine translation (SMT) ( <ref type="bibr" target="#b14">Chiang, 2007</ref>), <ref type="bibr">Gulcehre et al. (2015)</ref> and <ref type="bibr" target="#b36">Sennrich et al. (2015)</ref> attempt to enhance the decoder net- work model of NMT by incorporating the target- side monolingual data so as to boost the transla- tion fluency. They report promising improvements by using the target-side monolingual data. In con- trast, the source-side monolingual data is not fully explored. <ref type="bibr" target="#b28">Luong et al. (2015a)</ref> adopt a simple autoencoder or skip-thought method ( <ref type="bibr" target="#b25">Kiros et al., 2015)</ref> to exploit the source-side monolingual data, but no significant BLEU gains are reported. Note that, in parallel to our efforts, <ref type="bibr" target="#b13">Cheng et al. (2016b)</ref> have explored the usage of both source and target monolingual data using a similar semi-supervised reconstruction method, in which two NMTs are em- ployed. One translates the source-side monolingual data into target translations, and the other recon- structs the source-side monolingual data from the target translations.</p><p>In this work, we investigate the usage of the source-side large-scale monolingual data in NMT and aim at greatly enhancing its encoder network so that we can obtain high quality context vector rep- resentations. To achieve this goal, we propose two approaches. Inspired by <ref type="bibr" target="#b40">(Ueffing et al., 2007;</ref><ref type="bibr" target="#b44">Wu et al., 2008)</ref> handling source-side monolingual cor- pus in SMT and ( <ref type="bibr" target="#b36">Sennrich et al., 2015</ref>) exploiting target-side monolingual data in NMT, the first ap- proach adopts the self-learning algorithm to gener- ate adequate synthetic parallel data for NMT train- ing. In this method, we first build the baseline ma- chine translation system with the available aligned sentence pairs, and then obtain more synthetic par- allel data by translating the source-side monolingual sentences with the baseline system.</p><p>The proposed second approach applies the multi- task learning framework to predict the target trans- lation and the reordered source-side sentences at the same time. The main idea behind is that we build two NMTs: one is trained on the aligned sen- tence pairs to predict the target sentence from the source sentence, while the other is trained on the source-side monolingual corpus to predict the re- orderd source sentence from original source sen- tences <ref type="bibr">1</ref> . It should be noted that the two NMTs share the same encoder network so that they can help each other to strengthen the encoder model. In this paper, we make the following contribu- tions:</p><p>• To fully investigate the source-side monolin- gual data in NMT, we propose and compare two methods. One attempts to enhance the en- coder network of NMT by producing rich syn- thetic parallel corpus using a self-learning algo- rithm, and the other tries to perform machine translation and source sentence reordering si- multaneously with a multi-task learning archi- tecture.</p><p>• The extensive experiments on Chinese-to- English translation show that our proposed methods significantly outperform the strong NMT baseline augmented with the attention mechanism. We also find that the usage of the source-side monolingual data in NMT is more effective than that in SMT. Furthermore, we find that more monolingual data does not al- ways improve the translation quality and only relevant monolingual data helps.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>Our approach on using source-side monolingual cor- pora can be applied in any neural machine trans- lation as long as it employs the encoder-decoder framework. Without loss of generality, we use the attention-based NMT proposed by ( ), which utilizes recurrent neural networks for both encoder and decoder as illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. The encoder-decoder NMT first encodes the source sentence X = (x 1 , x 2 , · · · , x Tx ) into a se- quence of context vectors C = (h 1 , h 2 , · · · , h Tx ) whose size varies with respect to the source sen- tence length. Then, the encoder-decoder NMT de- codes from the context vectors C and generates tar- get translation Y = (y 1 , y 2 , · · · , y Ty ) one word each time by maximizing the probability of p(y i |y &lt;i , C). Note that x j (y i ) is word embedding corresponding to the j th (i th ) word in the source (target) sentence. Next, we briefly review the encoder introducing how to obtain C and the decoder addressing how to cal- culate p(y i |y &lt;i , C).</p><p>Encoder: The context vectors C are generated by the encoder using a pair of recurrent neural net- works (RNN) which consists of a forward RNN and a backward RNN. The forward RNN operates left-to-right over the source sentence from the first word, resulting in the forward context vectors</p><formula xml:id="formula_0">C f = ( − → h 1 , − → h 2 , · · · , − → h Tx ), in which − → h j = RN N ( − → h j−1 , x j )<label>(1)</label></formula><p>← − h j can be calculated similarly.</p><p>RNN can be a Gated Recurrent Unit (GRU) ) or a Long Short-Term Memory Unit (LSTM) <ref type="bibr" target="#b22">(Hochreiter and Schmidhuber, 1997)</ref>. At each position j of the source sentence, the context vector h j is defined as the concatenation of the for- ward and backward context vectors.</p><p>Decoder:</p><p>The conditional probability p(y i |y &lt;i , C) is computed in different ways ac- cording to the choice of the context C at time i. In ( ), the authors choose C = h Tx , while  use different context c i at different time step and the conditional probability will become:</p><formula xml:id="formula_1">p(y i |y &lt;i , C) = p(y i |y &lt;i , c i ) = g(y i−1 , z i , c i ) (2)</formula><p>where z i is the i th hidden state of the decoder and is calculated conditioning on the previous hidden state z i−1 , previous output y i−1 and and the source context vector c i at time i:</p><formula xml:id="formula_2">z i = RN N (z i−1 , y i−1 , c i )<label>(3)</label></formula><p>In attention-based NMT, c i is computed as the weighted sum of the source-side context vectors, just as illustrated in the top half of <ref type="figure" target="#fig_1">Fig. 1</ref>.</p><p>All the parameters of the encoder-decoder NMT are optimized to maximize the following condi- tional log-likelihood of the sentence aligned bilin- gual data:</p><formula xml:id="formula_3">L(θ) = 1 N N n=1 Ty i=1 logp(y (n) i |y (n) &lt;i , X (n) , θ) (4)</formula><p>3 Incorporating Source-side Monolingual Data in NMT</p><p>We can see from the above objective function that all the network parameters are only optimized on the sentence aligned parallel corpus. It is well known that more related data of high quality leads to better and more robust network models. However, bilin- gual data is scarce in many languages (or domains). It becomes a key issue how to improve the encoder and decoder networks using other data besides the parallel sentence pairs. <ref type="bibr">Gulcehre et al. (2015)</ref> and <ref type="bibr" target="#b36">Sennrich et al. (2015)</ref> have tried to fine-tune the decoder neural network with target-side large-scale monolingual data and they report remarkable perfor- mance improvement with the enhanced decoder. In contrast, we believe that the encoder part of NMT can also be greatly strengthened with the source-side monolingual data.</p><p>To investigate fully the source-side monolingual data in improving the encoder network of NMT, we propose two approaches: the first one employs the self-learning algorithm to provide synthetic parallel data in which the target part is obtained through au- tomatically translating the source-side monolingual data, which we refer to as self-learning method. The second one applies the multi-task learning frame- work that consists of two NMTs sharing the same encoder network to simultaneously train one NMT model on bilingual data and the other sentence reordering NMT model 2 on source-side monolin- gual data, which we refer to as sentence reordering method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-learning Method</head><p>Given the sentence aligned bitext</p><formula xml:id="formula_4">D b = {(X (n) b , Y (n) b )} N n=1</formula><p>in which N is not big enough, we have the source-side large-scale monolingual data D sm = {X m sm } M m=1 which is related to the bitext and M N .</p><p>Our goal is to generate much more bilingual data using D b and D sm . From the view of machine learn- ing, we are equipped with some labelled data D b and plenty of unlabelled data D sm , and we aim to obtain more labelled data for training better models. Self- learning is a simple but effective algorithm to tackle this issue. It first establishes a baseline with labelled data and then adopts the baseline to predict the la- bels of the unlabelled data. Finally, the unlabelled data together with the predicted labels become new labelled data.</p><p>In our scenario, the self-learning algorithm per- form the following three steps . First, a baseline ma- chine translation (MT) system (can use any transla- tion model, SMT or NMT) is built with the given bilingual data D b . Second, the baseline MT sys- In principle, we can apply any MT system as the baseline to generate the synthetic bilingual data. In accordance with the translation model we focus on in this work, we employ NMT as the baseline MT system. Note that the synthetic target parts may neg- atively influence the decoder model of NMT. To ad- dress this problem, we can distinguish original bitext from the synthetic bilingual sentences during NMT training by freezing the parameters of the decoder network for the synthetic data.</p><p>It is worthy to discuss why self-learning algo- rithm can improve the encoder model of NMT. Even though we require D sm to share the same source lan- guage vocabulary as D b and no new word transla- tions can be generated, the source-side monolingual data provides much more permutations of words in the vocabulary. Our RNN encoder network model will be optimized to well explain all of the word per- mutations. Thus, the encoder model of NMT can be enhanced for better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence Reordering Method</head><p>The self-learning algorithm needs to translate first the large-scale source-side monolingual data. A nat- ural question arises that whether can we improve the encoder model of NMT using just source-side monolingual corpora rather than the synthetic par- allel data. <ref type="bibr" target="#b28">Luong et al. (2015a)</ref> attempt to lever- age source-side monolingual data in NMT using a simple autoencoder and skip-thought vectors. How- ever, no promising results are reported. We believe that the reason lies in two aspects: 1) the large-scale monolingual data is not carefully selected; and 2) the adopted model is relatively simple. In this work, we propose to apply the multi-task learning method which designs a parameter sharing neural network framework to perform two tasks: machine transla- tion and source sentence reordering. <ref type="figure">Fig.2</ref> illus- trates the overview of our framework for source-side monolingual data usage.</p><p>As shown in <ref type="figure">Fig. 2</ref>, our framework consists of two neural networks that shares the same encoder model but employs two different decoder models for machine translation and sentence reordering respec- tively. For the machine translation task trained on the sentence aligned parallel data D b , the network parameters are optimized to maximize the condi- tional probability of the target sentence</p><formula xml:id="formula_5">Y (n) b given a source sentence X (n) b , namely argmaxp(Y (n) b |X (n) b ).</formula><p>As for the sentence reordering task trained on source-side monolingual data D sm , we regard it as a special machine translation task in which the target output is just the reordered source sentence, Y (m)</p><formula xml:id="formula_6">sm = X (m) sm . X (m) sm is obtained from X (m)</formula><p>sm by using the pre-ordering rules proposed by ( <ref type="bibr" target="#b41">Wang et al., 2007)</ref>, which can permutate the words of the source sen- tence so as to approximate the target language word order <ref type="bibr">3</ref> . In this way, the sentence reordering NMT is more powerful than an autoencoder. Using the NMT paradigm, the shared encoder network is leveraged to learn the deep representation C (n) sm of the source sentence X (n) sm , and the decoder network is employed to predict the reordered source sentence from the deep representation C (n)</p><formula xml:id="formula_7">sm (here X (n) sm ∈ D sm ) by maximizing p(X (n) sm |X (n)</formula><p>sm ). Note that the above two tasks share the same encoder model to obtain the en- coding of the source sentences. Accordingly, the overall objective function of this multi-task learn- ing is the summation of log probabilities of machine translation and sentence reordering:</p><formula xml:id="formula_8">L(θ) = 1 N N n=1 Ty i=1 logp(y (n) i |y (n) &lt;i , X (n) , θ) + 1 M M m=1 T X i=1 logp(X (m) i |X (m) &lt;i , X (m) , θ)<label>(5)</label></formula><p>where (θ = θ enc , θ dec T , θ dec R ). θ enc is the param- eter collection of source language encoder network, θdec T denotes the parameter set of the decoder net- work for translation, and θ dec R represents the param- eters of the decoder network for sentence reordering.</p><p>Intuitively, the sentence reordering task is easier than the translation task. Furthermore, in this paper, we pay much more attention on the translation task compared to the sentence reordering task. Consider- ing these, we distinguish these two tasks during the parameter optimization process. It is performed us- ing an alternate iteration strategy. For each iteration, we first optimize the encoder-decoder network pa- rameters in the reordering task for one epoch. The learnt encoder network parameters are employed to initialize the encoder model for the translation task. Then, we learn the encoder-decoder network param- eters in the translation task for several epochs <ref type="bibr">4</ref> . The new encoder parameters are then used to initialize the encoder model for the reordering task. We con- tinue the iteration until the constraint (e.g. iteration number or no parameter change) is satisfied. The weakness is that this method is less efficient than the self-learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head><p>In this section we describe the data set used in our experiments, data preprocessing, the training and evaluation details, and all the translation methods we compare in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We perform two tasks on Chinese-to-English trans- lation: one for small data set and the other for large-scale data set. Our small training data in- cludes 0.63M sentence pairs (after data cleaning) extracted from LDC corpora <ref type="bibr">5</ref> . The large-scale data set contains about 2.1M sentence pairs including the small training data. For validation, we choose NIST 2003 (MT03) dataset. For testing, we use NIST 2004 (MT04), NIST 2005 (MT05) and NIST 2006 (MT06) datasets. As for the source-side monolin- gual data, we collect about 20M Chinese sentences from LDC and we retain the sentences in which more than 50% words should appear in the source- side portion of the bilingual training data, resulting in 6.5M monolingual sentences for small training data set (12M for large-scale training data set) or- dered by the word hit rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Preprocessing</head><p>We apply word-level translation in experiments. The Chinese sentences are word segmented using Stan- ford Word Segmenter <ref type="bibr">6</ref> . To pre-order the Chinese sentences using the syntax-based reordering method proposed by ( <ref type="bibr" target="#b41">Wang et al., 2007)</ref>, we utilize the Berkeley parser ( <ref type="bibr" target="#b35">Petrov et al., 2006</ref>). The English sentences are tokenized using the tokenizer script from the Moses decoder <ref type="bibr">7</ref> . To speed up the training procedure, we clean the training data and remove all the sentences of length over 50 words. We limit the vocabulary in both Chinese and English to the most 40K words and all the out-of-vocabulary words are replaced with UNK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Evaluation Details</head><p>Each NMT model is trained on GPU K40 us- ing stochastic gradient decent algorithm AdaGrad ( <ref type="bibr" target="#b18">Duchi et al., 2011</ref>). We use mini batch size of 32. The word embedding dimension of source and tar- get language is 500 and the size of hidden layer is set to 1024. The training time for each model ranges from 5 days to 10 days for small training data set and ranges from 8 days to 15 days for large training data <ref type="bibr">5</ref>   set 8 . We use case-insensitive 4-gram BLEU score as the evaluation metric ( <ref type="bibr" target="#b34">Papineni et al., 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Translation Methods</head><p>In the experiments, we compare our method with conventional SMT model and a strong NMT model. We list all the translation methods as follows:</p><p>• Moses: It is the state-of-the-art phrase-based SMT system ( . We use its default configuration and train a 4-gram lan- guage model on the target portion of the bilin- gual training data.</p><p>• RNNSearch: It is an attention-based NMT sys- tem ( ).</p><p>• RNNSearch-Mono-SL: It is our NMT system which makes use of the source-side large-scale monolingual data by applying the self-learning algorithm.</p><p>• RNNSearch-Mono-MTL: It is our NMT sys- tem that exploits the source-side monolingual data by using our multi-task learning frame- work which performs machine translation and sentence reordering at the same time.</p><p>• RNNSearch-Mono-Autoencoder: It also ap- plies the multi-task learning framework in which a simple autoencoder is adopted on source-side monolingual data ( <ref type="bibr" target="#b28">Luong et al., 2015a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Translation Results on Small Data</head><p>For translation quality evaluation, we attempt to fig- ure out four questions: 1) Can the source-side mono- lingual data improve the neural machine translation? 2) Could the improved NMT outperform the state- of-the-art phrase-based SMT? 3) Whether it is true that the more the source-side monolingual data the better the translation quality? 4) Which MT model is more suitable to incorporate source-side monolin- gual data: SMT or NMT? The focus of this work is to figure out whether the encoder model of NMT can be improved using source-side monolingual data and further boost the translation quality. The four lines (3-6 in <ref type="table" target="#tab_3">Table 1)</ref> show the BLEU scores when applying self-learning algorithm to incorporate the source-side monolin- gual data. Clearly, RNNSearch-Mono-SL outper- forms RNNSearch in most cases. The best perfor- mance is obtained if the top 50% monolingual data is used. The biggest improvement is up to 4.05 BLEU points (32.43 vs. 28.38 on MT03) and it also signif- icantly outperforms Moses. When employing our multi-task learning frame- work to incorporate source-side monolingual data, the translation quality can be further improved (Lines 7-10 in <ref type="table" target="#tab_3">Table 1</ref>). For example, RNNSearch- Mono-MTL using the top 50% monolingual data can remarkably outperform the baseline RNNSearch, with an improvement up to 5.0 BLEU points (33.38 vs. 28.38 on MT03). Moreover, it also performs significantly better than the state-of-the-art phrase- based SMT Moses by the largest gains of 3.38 BLEU points (31.57 vs. 28.19 on MT05). The promis- ing results demonstrate that source-side monolin- gual data can improve neural machine translation and our multi-task learning is more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effects of Source-side Monolingual Data in NMT</head><p>From the last two lines in <ref type="table" target="#tab_3">Table 1</ref>, we can see that RNNSearch-Mono-Autoencoder can also im- prove the translation quality by more than 1.0 BLEU points when using the most related monolingual data. However, it underperforms RNNSearch-Mono- MTL by a large gap. It indicates that sentence re- ordering model is better than sentence reconstruc- tion model for exploiting the source-side monolin- gual data.</p><p>Note that we sort the source-side monolingual data according to the word coverage 9 in the bilin- gual training data. Sentences in the front have more shared words with the source-side vocabulary of bilingual training data. We can clearly see from <ref type="table" target="#tab_3">Table 1</ref> that monolingual data cannot always im- prove NMT. By adding closely related corpus (25% to 50%), the methods can achieve better and bet- ter performance. However, when adding more unre- <ref type="bibr">9</ref> In current work, the simple word coverage is applied to indicate the similarity. In the future, we plan to use phrase em- bedding ( <ref type="bibr" target="#b45">Zhang et al., 2014</ref>) or sentence embedding ( <ref type="bibr" target="#b46">Zhang et al., 2015;</ref>) to select the relevant monolingual data. lated monolingual data (75% to 100%) which shares fewer and fewer words in common with the bilin- gual data, the translation quality becomes worse and worse, and even worse than the baseline RNNSearch.</p><p>Both self-learning algorithm RNNSearch-Mono-SL and multi-task learning framework RNNSearch- Mono-MTL have the same trend. This indicates that only closely related source-side monolingual data can lead to performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NMT vs. SMT on Using Source-side Monolingual Data</head><p>Although the proposed multi-task learning frame- work cannot fit SMT because of no shared deep information between the two tasks in SMT, self- learning algorithm can also be applied in SMT as done by <ref type="bibr" target="#b40">(Ueffing et al., 2007;</ref><ref type="bibr" target="#b44">Wu et al., 2008)</ref>. We may want to know whether NMT is more effective in using source-side monolingual data than SMT. We apply the self-learning algorithm in SMT by incorporating top 25%, 50%, 75% and 100% syn- thetic sentence pairs to retrain baseline Moses. <ref type="figure" target="#fig_2">Fig.  3</ref> shows the effect of source-side monolingual data in different methods on test set MT04. The fig- ure reveals three similar phenomena. First, related monolingual data can boost the translation quality no matter whether NMT or SMT is used, but mixing more unrelated monolingual corpus will decrease the performance. Second, integrating closely related source-side monolingual data in NMT (RNNSearch- SL and RNNSearch-MTL) is much more effective than that in SMT (e.g. results for top 50%  learnt from the bilingual training data and the syn- thetic parallel data is obtained by these rules, and thus the synthetic parallel data cannot generate much more information. In contrast, NMT provides a encoder-decoder mechanism and depends heavily on the source language semantic vector representa- tions which facilitate the information sharing. Third, the translation quality changes much more dramati- cally in NMT methods than that in SMT. It indicates that the neural network models incline to be more affected by the quality of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Translation Results on Large-scale Data</head><p>A natural question arises that is the source-side monolingual data still very helpful when we have much more bilingual training data. We conduct the large-scale experiments using our proposed multi- task framework RNNSearch-Mono-MTL. <ref type="table" target="#tab_6">Table 2</ref> re- ports the results.</p><p>We can see from the table that closely related source-side monolingual data (the top 50%) can also boost the translation quality on all of the test sets. The performance improvement can be more than 1.0 BLEU points. Compared to the results on small training data, the gains from source-side monolingual data are much smaller. It is reasonable since large-scale training data can make the param- eters of the encoder-decoder parameters much sta- ble. We can also observe the similar phenomenon that adding more unrelated monolingual data leads to decreased translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>As a new paradigm for machine translation, the encoder-decoder based NMT has drawn more and more attention. Most of the existing methods mainly focus on designing better alignment mechanisms (attention model) for the decoder network ( <ref type="bibr" target="#b12">Cheng et al., 2016a;</ref><ref type="bibr" target="#b29">Luong et al., 2015b;</ref><ref type="bibr" target="#b16">Cohn et al., 2016;</ref><ref type="bibr" target="#b19">Feng et al., 2016;</ref><ref type="bibr" target="#b39">Tu et al., 2016;</ref>), better objective functions for BLEU evaluation ) and better strategies for handling unknown words ( <ref type="bibr" target="#b30">Luong et al., 2015c;</ref><ref type="bibr" target="#b36">Sennrich et al., 2015;</ref>) or large vocabularies ( <ref type="bibr" target="#b23">Jean et al., 2015;</ref><ref type="bibr" target="#b33">Mi et al., 2016c)</ref>.</p><p>Our focus in this work is aiming to make full use of the source-side large-scale monolingual data in NMT, which is not fully explored before. The most related works lie in three aspects: 1) apply- ing target-side monolingual data in NMT, 2) target- ing knowledge sharing with multi-task NMT, and 3) using source-side monolingual data in conventional SMT and NMT. <ref type="bibr">Gulcehre et al. (2015)</ref> first investigate the target- side monolingual data in NMT. They propose shal- low and deep fusion methods to enhance the decoder network by training a big language model on target- side large-scale monolingual data. <ref type="bibr" target="#b36">Sennrich et al. (2015)</ref> further propose a new approach to use target- side monolingual data. They generate the synthetic bilingual data by translating the target monolingual sentences to source language sentences and retrain NMT with the mixture of original bilingual data and the synthetic parallel data. It is similar to our self- learning algorithm in which we concern the source- side monolingual data. Furthermore, their method requires to train an additional NMT from target lan- guage to source language, which may negatively in- fluence the attention model in the decoder network. <ref type="bibr" target="#b17">Dong et al. (2015)</ref> propose a multi-task learn- ing method for translating one source language into multiple target languages in NMT so that the en- coder network can be shared when dealing with sev- eral sets of bilingual data. ,  and <ref type="bibr" target="#b20">Firat et al. (2016)</ref> further deal with more complicated cases (e.g. multi-source lan- guages). Note that all these methods require bilin- gual training corpus. Instead, we adapt the multi- task learning framework to better accommodate the source-side monolingual data. <ref type="bibr" target="#b40">Ueffing et al. (2007)</ref> and <ref type="bibr" target="#b44">Wu et al. (2008)</ref> explore the usage of source-side monolingual data in con- ventional SMT with a self-learning algorithm. Al- though we apply self-learning in this work, we use it to enhance the encoder network in NMT rather than generating more translation rules in SMT and we also adapt a multi-task learning framework to take full advantage of the source-side monolingual data. <ref type="bibr" target="#b28">Luong et al. (2015a)</ref> also investigate the source-side monolingual data in the multi-task learning frame- work, in which a simple autoencoder or skip-thought vectors are employed to model the monolingual data. Our sentence reordering model is more pow- erful than simple autoencoder in encoder enhance- ment. Furthermore, they do not carefully prepare the monolingual data for which we show that only related monolingual data leads to big improvements. In parallel to our work, <ref type="bibr" target="#b13">Cheng et al. (2016b)</ref> pro- pose a similar semi-supervised framework to handle both source and target language monolingual data. If source-side monolingual data is considered, a re- construction framework including two NMTs is em- ployed. One NMT translates the source-side mono- lingual data into target language translations, from which the other NMT attempts to reconstruct the original source-side monolingual data. In contrast to their approach, we propose a sentence reorder- ing model rather than the sentence reconstruction model. Furthermore, we carefully investigate the re- lationship between the monolingual data quality and the translation performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>In this paper, we propose a self-learning algo- rithm and a new multi-task learning framework to use source-side monolingual data so as to improve the encoder network of the encoder-decoder based NMT. The self-learning algorithm generates the syn- thetic parallel corpus and enlarge the bilingual train- ing data to enhance the encoder model of NMT. The multi-task learning framework performs ma- chine translation on bilingual data and sentence re- ordering on source-side monolingual data by shar- ing the same encoder network. The experiments show that our method can significantly outperform the strong attention-based NMT baseline, and the proposed multi-task learning framework performs better than the self-learning algorithm at the expense of low efficiency. Furthermore, the experiments also demonstrate that NMT is more effective for incor- porating the source-side monolingual data than con- ventional SMT. We also observe that more mono- lingual data does not always improve the translation quality and only relevant data does help.</p><p>In the future, we would like to design smarter mechanisms to distinguish real data from synthetic data in self-learning algorithm, and attempt to pro- pose better models for handling source-side mono- lingual data. We also plan to apply our methods in other languages, especially for low-resource lan- guages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The encoder-decoder NMT with attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effects of source-side monolingual data on MT04.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Translation results (BLEU score) for different translation methods. For our methods exploring the source-side monolingual data, we investigate the performance change as we choose different scales of monolingual data (e.g. from top 25% to 100% according to the word coverage of the monolingual sentence in source language vocabulary of bilingual training corpus).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 reports</head><label>1</label><figDesc></figDesc><table>the translation quality for different 
methods. Comparing the first two lines in Table 
1, it is obvious that the NMT method RNNSearch 
performs much worse than the SMT model Moses 
on Chinese-to-English translation. The gap is as 
large as approximately 2.0 BLEU points (28.38 vs. 
30.30). We speculate that the encoder-decoder net-
work models of NMT are not well optimized due to 
insufficient bilingual training data. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>). It is because that SMT relies on the translation rules</figDesc><table>Method 

MT03 MT04 MT05 MT06 
RNNSearch 
35.18 36.20 33.21 32.86 
RNNSearch-Mono-MTL (50%) 
36.32 37.51 35.08 34.26 
RNNSearch-Mono-MTL (100%) 35.75 36.74 34.23 33.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 : Translation results (BLEU score) for different translation methods in large-scale training data.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We reorder all the source-side monolingual sentences so as to make them close to target language in word order.</note>

			<note place="foot" n="2"> NMT is essentially a sequence-to-sequence prediction model. In most cases, the input sequence is different from the output sequence. In the sentence reordering NMT, we require that output sequence to be the reordered input sentences which are close to English word order.</note>

			<note place="foot" n="3"> The pre-ordering rules are obtained from the parsed source trees which heavily depend on the accuracy and efficiency of the parser. In fact, it takes us lots of time (even longer than synthetic parallel data generation) to parse all the source-side monolingual data. In the future, we attempt to design a more efficient pre-ordering method relying only on the bilingual training data.</note>

			<note place="foot" n="4"> We rune four epochs for the translation task in each iteration.</note>

			<note place="foot" n="8"> It needs another 5 to 10 days when adding millions of monolingual data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their valuable com-ments and suggestions. This research work has been partially funded by the Natural Science Foun-dation of China under Grant No. 91520204 and No. 61303181, and supported by the Strate-gic Priority Research Program of the CAS (Grant XDB02070007).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Method MT03 MT04 MT05 MT06 Moses</title>
		<idno>30.30 31.04 28.19 30.04</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">RNNSearch 28.38 30.85 26.78 29</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rnnsearch-Mono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl</surname></persName>
		</author>
		<idno>25%) 29.65 31.92 28.65</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rnnsearch-Mono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl</surname></persName>
		</author>
		<idno>50%) 32.43 33.16 30.43 32.35</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Rnnsearch-Mono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sl</surname></persName>
		</author>
		<idno>100%) 29.97 30.78 26.45 28.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnnsearch-Mono-Mtl</forename></persName>
		</author>
		<idno>25%) 31.68 32.51 29.8 31.29</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnnsearch-Mono-Mtl</forename></persName>
		</author>
		<idno>50%) 33.38 34.30 31.57 33.40</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnnsearch-Mono-Mtl</forename></persName>
		</author>
		<idno>75%) 31.69 32.83 28.17 30.26</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnnsearch-Mono-Mtl</forename></persName>
		</author>
		<idno>100%) 30.31 30.62 27.23 28.85</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rnnsearch-Mono-Autoencoder</surname></persName>
		</author>
		<idno>50%) 31.55 32.07 28.19 30.85</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rnnsearch-Mono-Autoencoder</surname></persName>
		</author>
		<idno>100%) 27.81 30.32 25.84 27.73</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>References Dzmitry Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Agreementbased joint training for bidirectional attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Implicit distortion and fertility models for attentionbased encoder-decoder nmt model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Shi Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03317</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.01073</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03535</idno>
		<title level="m">Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2015. On using monolingual corpora in neural machine translation</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2007</title>
		<meeting>ACL 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards zero unknown word in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A coverage embedding model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Supervised attentions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vocabulary manipulation for large vocabulary neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning accurate, compact, and interpretable tree annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLINGACL</title>
		<meeting>COLINGACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coverage-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transductive learning for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chinese syntactic reordering for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised clustering for short text via deep representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07019</idno>
		<title level="m">Sentence similarity learning by lexical decomposition and composition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2008</title>
		<meeting>COLING 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilingually-constrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Local translation prediction with global sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dakun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Transfer learning for low-resource neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keven</forename><surname>Knight</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02201v1</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
