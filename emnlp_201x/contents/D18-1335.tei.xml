<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parnia</forename><surname>Bahar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>D-52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Brix</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>D-52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Human Language Technology and Pattern Recognition Group Computer Science Department</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<postCode>D-52056</postCode>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3009" to="3015"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3009</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modeling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German↔English.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The widely used state-of-the-art neural machine translation (NMT) systems are based on an encoder-decoder architecture equipped with at- tention layer(s).</p><p>The encoder and the de- coder can be constructed using recurrent neu- ral networks (RNNs), especially long-short term memory (LSTM) ( <ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b26">Wu et al., 2016)</ref>, convolutional neural networks (CNNs) <ref type="bibr" target="#b4">(Gehring et al., 2017)</ref>, self-attention units ( <ref type="bibr" target="#b24">Vaswani et al., 2017)</ref>, or a combination of them ( <ref type="bibr" target="#b2">Chen et al., 2018)</ref>. In all these architectures, source and target sentences are handled separately as a one-dimensional sequence over time. Then, an attention mechanism (additive, multiplicative or multihead) is incorporated into the decoder to selectively focus on individual parts of the source sentence.</p><p>One of the weaknesses of such models is that the encoder states are computed only once at the beginning and are left untouched with respect to the target histories. In this case, at every decod- ing step, the same set of vectors are read repeat- edly. Hence, the attention mechanism is limited in its ability to effectively model the coverage of the source sentence. By providing the encoder states with the greater capacity to remember what has been generated and what needs to be translated, we believe that we can alleviate the coverage prob- lems such as over-and under-translation.</p><p>One solution is to assimilate the context from both source and target sentences jointly and to align them in a two-dimensional grid. Two- dimensional LSTM (2DLSTM) is able to pro- cess data with complex interdependencies in a 2D space <ref type="bibr" target="#b6">(Graves, 2012)</ref>.</p><p>To incorporate the solution, in this work, we propose a novel architecture based on the 2DLSTM unit, which enables the computation of the encoding of the source sentence as a function of the previously generated target words. We treat translation as a 2D mapping. One dimension pro- cesses the source sentence, and the other dimen- sion generates the target words. Each time a tar- get word is generated, its representation is used to compute a hidden state sequence that models the source sentence encoding. In principle, by updat- ing the encoder states across the second dimension using the target history, the 2DLSTM captures the coverage concepts internally by its cell states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>MDLSTM <ref type="bibr" target="#b5">(Graves, 2008</ref><ref type="bibr" target="#b6">(Graves, , 2012</ref>) has been suc- cessfully used in handwriting recognition (HWR) to automatically extract features from raw images which are inherently two-dimensional ( <ref type="bibr" target="#b7">Graves and Schmidhuber, 2008;</ref><ref type="bibr" target="#b13">Leifert et al., 2016a;</ref><ref type="bibr" target="#b25">Voigtlaender et al., 2016)</ref>. <ref type="bibr" target="#b25">Voigtlaender et al. (2016)</ref> explore a larger MDLSTM for deeper and wider architectures using an implementation for the graphical processing unit (GPU). It has also been applied to automatic speech recognition (ASR) where a 2DLSTM scans the input over both time and frequency jointly ( <ref type="bibr" target="#b19">Sainath and Li, 2016)</ref>. As an alternative architecture to the concept of MDLSTM, <ref type="bibr" target="#b10">Kalchbrenner et al. (2015)</ref> propose a grid LSTM that is a network of LSTM cells arranged in a multidimensional grid, in which the cells are communicating between layers as well as time recurrences. <ref type="bibr" target="#b15">Li et al. (2017)</ref> also ap- ply the grid LSTM architecture for the endpoint detection task in ASR.</p><p>This work, for the first time, presents an end-to- end 2D neural model where we process the source and the target words jointly by a 2DLSTM layer.</p><formula xml:id="formula_0">3 Two-Dimensional LSTM c j,i Cell × s j,i × ˜ c j,i × λ j,i Lambda Gate f j,i Forget Gate i j,i Input Gate o j,i Output Gate x j,i s j,i−1 s j−1,i x j,i s j,i−1 s j−1,i x j,i s j,i−1 s j−1,i x j,i s j,i−1 s j−1,i x j,i s j,i−1 s j−1,i</formula><p>Figure 1: 2DLSTM unit. The additional links vs. standard LSTM are marked in blue.</p><p>The 2DLSTM has been introduced by <ref type="bibr" target="#b5">(Graves, 2008)</ref> as a generalization of standard LSTM. <ref type="figure">Fig- ure 1</ref> illustrates one of the stable variants proposed by <ref type="bibr" target="#b14">(Leifert et al., 2016b)</ref>. A 2DLSTM unit pro- cesses a 2D sequential data x ∈ R J×I of arbitrary lengths, J and I. At time step (j, i), the computa- tion of its cell depends on both vertical s j,i−1 and horizontal hidden states s j−1,i (see Equations (1)- (5)). Similar to the LSTM cell, it maintains some state information in an internal cell state c j,i . Be- sides the input i j,i , the forget f j,i and the output o j,i gates that all control information flows, 2DL- STM employs an extra lambda gate λ j,i . As written in Equ. 5, its activation is computed anal- ogously to the other gates. The lambda gate is used to weight the two predecessor cells c j−1,i and c j,i−1 before passing them through the forget gate (Equation 6). g and σ are the tanh and the sig- moid functions. V s, W s and U s are the weight matrices.</p><p>In order to train a 2DLSTM unit, back- propagation through time (BPTT) is performed over two dimensions <ref type="bibr" target="#b5">(Graves, 2008</ref><ref type="bibr" target="#b6">(Graves, , 2012</ref>). Thus, the gradient is passed backwards from the time step (J, I) to (1, 1), the origin. More details, as well as the derivations of the gradients, can be found in <ref type="bibr" target="#b5">(Graves, 2008)</ref>.</p><formula xml:id="formula_1">i j,i = σ W 1 x j,i + U 1 ls j−1,i + V 1 s j,i−1 (1) f j,i = σ W 2 x j,i + U 2 s j−1,i + V 2 s j,i−1 (2) o j,i = σ W 3 x j,i + U 3 s j−1,i + V 3 s j,i−1 (3) ˜ c j,i = g W 4 x j,i + U 4 s j−1,i + V 4 s j,i−1 (4) λ j,i = σ W 5 x j,i + U 5 s j−1,i + V 5 s j,i−1 (5) c j,i = f j,i • lλ j,i • c j−1,i + (1 − λ j,i ) • c j,i−1 + ˜ c j,i • i j,i (6) s j,i = g (c j,i ) • o j,i<label>(7)</label></formula><p>4 Two-Dimensional Sequence to Sequence Model</p><p>We aim to apply a 2DLSTM to map the source and the target sequences into a 2D space as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We call this architec- ture, the two-dimensional sequence to sequence (2D-seq2seq) model. Given a source sequence x J 1 = x 1 , . . . , x J and a target sequence y I 1 = y 1 , . . . , y I , we scan the source sequence from left to right and the target sequence from bottom to top as shown in <ref type="figure" target="#fig_0">Figure  2</ref>. In the 2D-seq2seq model, one dimension of the 2DLSTM (horizontal-axis in the figure) serves as the encoder and another (vertical axis) plays the role of the decoder. As a pre-step before the 2DLSTM, in order to have the whole source con- text, a bidirectional LSTM scans the input words once from left to right and once from right to left to compute a sequence of encoder states h J 1 = h 1 , . . . , h J . At time step (j, i), the 2DLSTM re- ceives both encoder state, h j , and the last target embedding vector, y i−1 , as an input. It repeatedly updates the source information, h J 1 , while generat- ing new target word, y i . The state of the 2DLSTM is computed as follows.</p><formula xml:id="formula_2">s j,i = ψ W h j ; y i−1 , U s j−1,i , V s j,i−1<label>(8)</label></formula><p>where ψ stands for the 2DLSTM as a function. At each decoder step, once the whole source se- quence is processed from 1 to J, the last hidden state of the 2DLSTM, s J,i , is used as the context vector. It means, at time step i, t i = s J,i . In order to generate the next target word, y i , a transforma- tion followed by a softmax operation is applied. Therefore:</p><formula xml:id="formula_3">p i (y i = w|y i−1 1 , x J 1 ) = exp(W o t iw ) |Vt| v=1 exp(W o t iv )<label>(9)</label></formula><p>where W o and |V t | are the weight matrix and the target vocabulary respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training versus Decoding</head><p>One practical concern that should be noticed is the difference between the training and the decoding. Since the whole target sequence is known during training, all states of the 2DLSTM can be com- puted once at the beginning. Slices of it can then be used during the forward and backward train- ing passes. In theory, the complexity of training is O(JI). But, in practice, the training computation can be optimally parallelized to take linear time <ref type="bibr" target="#b25">(Voigtlaender et al., 2016)</ref>. During the decoding, only the already generated target words are avail- able. Thus, either all 2DLSTM states have to be recomputed, or it has to be extended by an addi- tional row at every time step i that cause higher complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We have done the experiments on the WMT 2017 German→English and English→German news tasks consisting of 4.6M training sam- ples collected from the well-known data sets Europarl-v7, News-Commentary-v10</p><p>and Common-Crawl. We use newstest2015 as our development set and newstest2016 and -2017 as our test sets, which contain 2169, 2999 and 3004 sentences respectively. No synthetic data and no additional features are used. Our goal is to keep the baseline model simple and standard to compare methods rather that advancing the state-of-the-art systems.</p><p>After tokenization and true-casing using Moses toolkit ( <ref type="bibr" target="#b12">Koehn et al., 2007)</ref>, byte pair encoding (BPE) ( ) is used jointly with 20k merge operations. We remove sentences longer than 50 subwords and batch them together with a batch size of 50. All models are trained from scratch by the Adam optimizer ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>), dropout of 30% ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) and the norm of the gradient is clipped with the threshold of 1. The final models are the average of the 4 best checkpoints of a single run based on the perplexity on the development set ( <ref type="bibr" target="#b9">Junczys-Dowmunt et al., 2016)</ref>. Decoding is performed using beam search of size 12, without ensemble of various networks.</p><p>We have used our in-house implementation of the NMT system which relies on Theano ( <ref type="bibr" target="#b1">Bastien et al., 2012</ref>) and <ref type="bibr">Blocks (Merriënboer et al., 2015)</ref>. Our implementation of 2DLSTM is based on CUDA code adapted from <ref type="bibr" target="#b25">(Voigtlaender et al., 2016;</ref><ref type="bibr" target="#b27">Zeyer et al., 2018)</ref>, leveraging some speedup.</p><p>The models are evaluated using case-sensitive BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>) computed by mteval-v13a 1 and case-sensitive TER <ref type="bibr" target="#b21">(Snover et al., 2006</ref>) using tercom 2 . We also report per- plexities on the development set.</p><p>Attention Model: the attention based sequence to sequence model ( <ref type="bibr" target="#b0">Bahdanau et al., 2014</ref>) is se- lected as our baseline that performs quite well. The model consists of one layer bidirectional en- coder and a unidirectional decoder with an addi- tive attention mechanism. All words are projected into a 500-dimensional embedding on both sides. To explore the performance of the models with re- spect to hidden size, we try LSTMs (Hochreiter and Schmidhuber, 1997) with both 500 and 1000 nodes.</p><p>2D-Seq2Seq Model: we apply the same em- bedding size of that of the attention model. The 2DLSTM, as well as the bidirectional LSTM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Hidden <ref type="table">Size   De→En  En→De  devset newstest2016 newstest2017 devset newstest2016 newstest2017  PPL  BLEU TER BLEU TER  PPL  BLEU TER BLEU TER  1</ref>  layer, are structured using the same number of nodes (500 or 1000). The 2D-seq2seq model is trained with the learning rate of 0.0005 vs. 0.001 for the attention model. Translation Performance: in the first set of experiments, we compare the 2D-seq2seq model with the attention sequence to sequence model. The results are shown in <ref type="table">Table 1</ref> in the rows 1 and 2. As it is seen, for size n = 500, the 2D- seq2seq model outperforms the standard attention model on average by 0.7% BLEU and 0.6% TER on De→En, 0.4% BLEU and no improvements in TER on En→De. The model is also superior for larger hidden size (n = 1000) on average by 0.5% BLEU and 0.3% TER on De→En, 0.9% BLEU and 1.0% TER on En→De. In both cases, the perplex- ity of the 2D-seq2seq model is lower compared to that of the attention model. The 2D-seq2seq topology is analogous to the bidirectional encoder-decoder model without at- tention. To examine whether the 2DLSTM re- duces the need of attention, in the second set of experiments, we equip our model with a weighted sum of 2DLSTM states, t i , over j positions to dy- namically select the most relevant information. In other words:</p><formula xml:id="formula_4">γ j,i = sof tmax j v T tanh W s j,i<label>(10)</label></formula><formula xml:id="formula_5">t i = J j=1 γ j,i s j,i<label>(11)</label></formula><p>In these equations, γ j,i is the normalized weight over source positions, s j,i is the 2DLSTM states and W and v are weight matrices. As the results shown in the <ref type="table">Table 1</ref> in the rows 2 and 3, adding an additional weighting layer on top of the 2DLSTM layer does not help in terms of BLEU and rarely helps in TER.</p><p>By updating the encoder states across the sec- ond dimension with respect to the target his- tory, the 2D-seq2seq model can internally indi- cate which source words have already been trans- lated and where it should focus next. Therefore, it reduces the risk of over-and under-translation. To examine our assumption, we compare the 2D- seq2seq model with two NMT models where the concepts such as fertility and coverage have been addressed ( <ref type="bibr" target="#b23">Tu et al., 2016;</ref><ref type="bibr" target="#b3">Cohn et al., 2016)</ref>.</p><p>Coverage Model: in the coverage model, we feed back the last alignments from the time step i − 1 to compute the attention weight at time step i. Therefore, in the coverage model, we redefine the attention weight, α i,j , as:</p><formula xml:id="formula_6">α i,j = a s i−1 , h j , α i−1,j (12)</formula><p>where a is an attention function followed by the softmax. h j and s i−1 are the the encoder and the previous decoder states respectively. In our exper- iments, we use additive attention similar to <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>). Fertility Model: in the fertility model, we feed back the sum of the alignments over the past de- coder steps to indicate how much attention has been given to the source position j up to step i and divide it over the fertility of source word at posi- tion j. This term depends on the encoder states and it varies if the word is used in a different con- text ( <ref type="bibr" target="#b23">Tu et al., 2016)</ref>.</p><formula xml:id="formula_7">β i,j = 1 N · σ(υ φ · h j ) i−1 k=1 α i,j (13) α i,j = a s i−1 , h j , β i,j (14)</formula><p>where N specifies the maximum value for the fer- tility which set to 2 in our experiments. υ φ is a weight vector. source HP beschäftigte zum Ende des Geschäftsjahres 2013/14 noch rund 302.000 Mitarbeiter. reference</p><p>At the end of the 2013/14 business year HP still employed around 302,000 staff. attention</p><p>At the end of the financial year, HP employed some 302,000 employees at the end of the financial year of 2013/14. 2D-seq2seq HP still employs about 302,000 people at the end of the financial year 2013/14. coverage HP employed around 302,000 employees at the end of the fiscal year 2013/14. fertility HP employed some 302,000 people at the end of the fiscal year 2013/14. <ref type="table">Table 2</ref>: An example of over-translation.</p><p>As it is seen in <ref type="table">Table 1</ref>, rows 2, 4 and 5, our proposed model is 0.3% BLEU ahead and 0.3% TER worse compared to the fertility approach and slightly better compared to the coverage one. We note, the fertility and coverage models were trained using embedding size of 620.</p><p>We have also qualitatively verified the coverage issue in <ref type="table">Table 2</ref> by showing an example from the test set. Without the knowledge of which source words have already been translated, the attention layer is at risk of attending to the same positions multiple times. This could lead to over-translation. Similarly, under-translation could be occur when the attention model rarely focusing at the corre- sponding source positions. As shown in the ex- ample, the 2DLSTM can internally track which source positions have already contributed to the target generation.</p><p>Speed: we have also compared the models in terms of speed on a single GPU training. In gen- eral, the training and decoding speed of the 2D- seq2seq model is 791 and 0.7 words/s respectively compared to those of standard attention model which is 2944 and 48 words/s. The computation of the added weighting mechanism is negligible in this case. This is still an initial architecture which indicates the necessity of multi-GPU usage. We also expect to speedup the decoding phase by avoiding the unnecessary recomputation of previ- ous 2DLSTM states. In the current implementa- tion, at each target step, we re-compute the 2DL- STM states from time step 0 to i − 1, while we only need to store the states from the last step i−1. This does not influence our results, as it is purely an implementation issue, not algorithm. However, decoding will still be slower than the training. One suggestion for further speedup of training phase is applying truncated BPTT on both directions to re- duce the number of updates.</p><p>The 2DLSTM can be simply combined with self-attention layers ( <ref type="bibr" target="#b24">Vaswani et al., 2017</ref>) in the encoder and the decoder for better context repre- sentation as well as RNMT+ ( <ref type="bibr" target="#b2">Chen et al., 2018)</ref> that is composed of standard LSTMs. We believe that 2D-seq2seq model can be potentially applied to the other applications where sequence to se- quence modeling is helpful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Works</head><p>We have introduced a novel 2D sequence to se- quence model (2D-seq2seq), a network that ap- plies a 2DLSTM unit to read both the source and the target sentences jointly. Hence, in each decod- ing step, the network implicitly updates the source representation conditioned on the generated target words so far. The experimental results show that we outperform the attention model on two WMT 2017 translation tasks. We have also shown that our model implicitly handles the coverage issue.</p><p>As future work, we aim to develop a bidi- rectional 2DLSTM and consider stacking up 2DLSTMs for a deeper model. We consider the results promising and try more language pairs and fine-tune the hyperparameters.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two-dimensional sequence to sequence model (2D-seq2seq).</figDesc></figure>

			<note place="foot" n="1"> ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 2 http://www.cs.umd.edu/ snover/tercom/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has received funding from the Euro-pean Research Council (ERC) (under the Euro-pean Union's Horizon 2020 research and inno-vation programme, grant agreement No 694537, project "SEQCLAS") and the Deutsche Forschungsgemeinschaft (DFG; grant agreement NE 572/8-1, project "CoreTec"). The GPU computing cluster was supported by DFG (Deutsche Forschungsgemeinschaft) under grant INST 222/1168-1 FUGG. The work reflects only the authors' views and none of the funding agencies is responsible for any use that may be made of the information it contains.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Mia Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughes</surname></persName>
		</author>
		<idno>abs/1804.09849</idno>
		<title level="m">The best of both worlds: Combining recent advances in neural machine translation. CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incorporating structural alignment biases into an attentional neural translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Duy Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vymolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1217" />
			<biblScope unit="page" from="876" to="885" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11" />
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Technical University Munich</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-08" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="319" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Grid long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1507.01526</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Citlab ARGUS for historical handwritten documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundram</forename><surname>Leifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Strauß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Labahn</surname></persName>
		</author>
		<idno>abs/1605.08412</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cells in multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gundram</forename><surname>Leifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Strauß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Grüning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welf</forename><surname>Wustlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Labahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3313" to="3349" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Endpoint detection using grid long short-term memory networks for streaming speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Parada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Simko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shuo Yiin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring multidimensional lstms for large vocabulary ASR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03-20" />
			<biblScope unit="page" from="4940" to="4944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<title level="m">Chorowski, and Yoshua Bengio. 2015. Blocks and fuel: Frameworks for deep learning</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling timefrequency patterns with LSTM vs. convolutional architectures for LVCSR tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2016, 17th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09-08" />
			<biblScope unit="page" from="813" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Study of Translation Edit Rate with Targeted Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas</title>
		<meeting>the 7th Conference of the Association for Machine Translation in the Americas<address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handwriting recognition with large multidimensional long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Frontiers in Handwriting Recognition, ICFHR 2016</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-23" />
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">RETURNN as a generic flexible neural toolkit with application to translation and speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, Melbourne, Australia</title>
		<meeting>ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<date type="published" when="2018-07-15" />
			<biblScope unit="page" from="128" to="133" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
