<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ReVal: A Simple and Effective Machine Translation Evaluation Metric Based on Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Group in Computational Linguistics</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Or˘ Asan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Research Group in Computational Linguistics</orgName>
								<orgName type="institution">University of Wolverhampton</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
							<email>josef.van genabith@dfki.de</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Saarland University and German Research Center for Artificial Intelligence (DFKI)</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ReVal: A Simple and Effective Machine Translation Evaluation Metric Based on Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many state-of-the-art Machine Translation (MT) evaluation metrics are complex, involve extensive external resources (e.g. for paraphrasing) and require tuning to achieve best results. We present a simple alternative approach based on dense vector spaces and recurrent neural networks (RNNs), in particular Long Short Term Memory (LSTM) networks. For WMT-14, our new metric scores best for two out of five language pairs, and overall best and second best on all language pairs, using Spearman and Pearson correlation, respectively. We also show how training data is computed automatically from WMT ranks data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning approaches have turned out to be successful in many NLP applications such as para- phrasing ( <ref type="bibr" target="#b18">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b23">Socher et al., 2011</ref>), sentiment analysis ( <ref type="bibr" target="#b25">Socher et al., 2013b</ref>), parsing <ref type="bibr" target="#b24">(Socher et al., 2013a</ref>) and machine trans- lation ( <ref type="bibr" target="#b17">Mikolov et al., 2013a</ref>). While dense vec- tor space representations such as those obtained through Deep Neural Networks (DNNs) or RNNs are able to capture semantic similarity for words ( <ref type="bibr" target="#b18">Mikolov et al., 2013b</ref>), segments <ref type="bibr" target="#b23">(Socher et al., 2011</ref>) and documents ( <ref type="bibr" target="#b14">Le and Mikolov, 2014</ref>) naturally, traditional MT evaluation metrics can only achieve this using resources like WordNet and paraphrase databases. This paper presents a novel, efficient and compact MT evaluation mea- sure based on RNNs. Our metric is simple in the sense that it does not require much machinery and resources apart from the dense word vectors. This cannot be said of most of the state-of-the-art MT evaluation metrics, which tend to be complex and require extensive feature engineering. Our metric is based on RNNs and particularly on Tree Long Short Term Memory (Tree-LSTM) networks <ref type="bibr" target="#b26">(Tai et al., 2015)</ref>. LSTM (Hochreiter and <ref type="bibr" target="#b10">Schmidhuber, 1997</ref>) is a sequence learning technique which uses a memory cell to preserve a state over a long period of time. This enables distributed represen- tations of sentences using distributed representa- tions of words. Tree-LSTM is a recent approach, which is an extension of the simple LSTM frame- work ( <ref type="bibr" target="#b27">Zaremba and Sutskever, 2014)</ref>. To provide the required training data, we also show how to automatically convert the WMT-13 ( <ref type="bibr" target="#b2">Bojar et al., 2013</ref>) human evaluation rankings into similarity scores between the reference and the translation. Our metric including training data is available at https://github.com/rohitguptacs/ReVal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many metrics have been proposed for MT eval- uation. Earlier popular metrics are based on n- gram counts (e.g. BLEU ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) and NIST <ref type="bibr" target="#b6">(Doddington, 2002)</ref>) or word error rate. Other popular metrics like METEOR <ref type="bibr" target="#b5">(Denkowski and Lavie, 2014</ref>) and <ref type="bibr">TERp (Snover et al., 2008)</ref> also use external resources like WordNet and para- phrase databases. However, system-level cor- relation with human judgements for these met- rics remains below 0.90 Pearson correlation co- efficient (as per WMT-14 results, BLEU-0.888, NIST-0.867, METEOR-0.829, TER-0.826, WER- 0.821).</p><p>Recent best-performing metrics in the WMT-14 metric shared task <ref type="bibr" target="#b15">(Machácek and Bojar, 2014)</ref> used a combination of different metrics. The top performing system DISKOTK-PARTY-TUNED ( <ref type="bibr" target="#b12">Joty et al., 2014</ref>) in the WMT-14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit ( <ref type="bibr" target="#b8">Giménez and M` arquez, 2010)</ref>. The metric com- putes the number of common sub-trees between a reference and a translation using a convolution tree kernel <ref type="bibr" target="#b3">(Collins and Duffy, 2001</ref>). The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT-14 metric shared task. Another top performing metric LAYERED ( <ref type="bibr" target="#b7">Gautam and Bhattacharyya, 2014</ref>), uses linear interpolation of dif- ferent metrics. LAYERED uses BLEU and TER to capture lexical similarity, Hamming score and Kendall Tau Distance ( <ref type="bibr" target="#b1">Birch and Osborne, 2011)</ref> to identify syntactic similarity, and dependency parsing <ref type="bibr" target="#b4">(De Marneffe et al., 2006</ref>) and the Univer- sal Networking Language 1 for semantic similarity. Recently, <ref type="bibr" target="#b9">Guzmán et al. (2015)</ref> presented a metric based on word embeddings and neural networks. However, this metric is limited to ranking the available systems and does not provide an absolute score.</p><p>In this paper we propose a compact MT eval- uation metric. We hypothesize that our model learns different notions of similarity (which other metrics tend to capture using different metrics) using input, output and forget gates of an LSTM architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSTMs and Tree-LSTMs</head><p>Recurrent Neural Networks allow processing of arbitrary length sequences, but early RNNs had the problem of vanishing and exploding gradi- ents ( <ref type="bibr" target="#b0">Bengio et al., 1994)</ref>. RNNs with LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) tackle this problem by introducing a memory cell composed of a unit called constant error carousel (CEC) with multiplicative input and output gate units. Input gates protect against irrelevant inputs and output gates against current irrelevant memory contents. This architecture is capable of capturing important pieces of information seen in a bigger context. Tree-LSTM is an extension of simple LSTM. A typical LSTM processes the information sequen- tially whereas Tree-LSTM architectures enable sentence representation through a syntactic struc- ture. Equation (1) represents the composition of a hidden state vector for an LSTM architecture. For a simple LSTM, c t represents the memory cell and o t the output gate at time step t in a sequence. For Tree-LSTM, c t represents the memory cell and o t represents the output gate corresponding to node t in a tree. The structural processing of Tree-LSTM makes it better suited to representing 1 http://www.undl.org/unlsys/unl/unl2005/UW.htm sentences. For example, dependency tree structure captures syntactic features and model parameters the importance of words (content vs. function words).</p><p>h t = o t ⊙ tanh c t (1) <ref type="figure" target="#fig_0">Figure 1</ref> shows simple LSTM and Tree-LSTM architectures.</p><p>. .</p><formula xml:id="formula_0">x 2 .</formula><p>.</p><formula xml:id="formula_1">y 2 .</formula><p>.</p><formula xml:id="formula_2">y 1 . x 1 . . y 3 . . y 4 . x 4</formula><p>. . y 5</p><p>.</p><p>x 5</p><p>. .</p><formula xml:id="formula_3">x 1 . y 1 . . x 2 . y 2 . . x 3 . y 3</formula><p>. . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Metric</head><p>We represent both the reference (h ref ) and the translation (h tra ) using an LSTM and predict the similarity scorê y based on a neural network which considers both distance and angle between h ref and h tra :</p><formula xml:id="formula_4">h × = h ref ⊙ h tra h + = |h ref − h tra | h s = σ ( W (×) h × + W (+) h + + b (h) ) ˆ p θ = softmax ( W (p) h s + b (p) ) ˆ y = r T ˆ p θ (2)</formula><p>where, σ is a sigmoid function, ˆ p θ is the estimated probability distribution vector and</p><formula xml:id="formula_5">r T = [1 2...K].</formula><p>The cost function J(θ) is defined over probability distributions p andˆpandˆ andˆp θ using regularised Kullback- Leibler (KL) divergence.</p><formula xml:id="formula_6">J(θ) = 1 n n ∑ i=1 KL ( p (i) ˆ p (i) θ ) + λ 2 ||θ|| 2 2 (3)</formula><p>In Equation 3, i represents the index of each train- ing pair, n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows:</p><formula xml:id="formula_7">p j =      y − ⌊y⌋, j = ⌊y⌋ + 1 ⌊y⌋ − y + 1, j = ⌊y⌋ 0 otherwise for 1 ≤ j ≤ K, where, y ∈ [1, K]</formula><p>is the similarity score of a training pair. For example, for y = 2.7, p T = [0 0.3 0.7 0 0]. In our case, the similarity score y is a value between 1 and 5.</p><p>For our work, we use glove word vectors <ref type="bibr" target="#b20">(Pennington et al., 2014</ref>) and the simple LSTM, the dependency Tree-LSTM and neural network im- plementations by <ref type="bibr" target="#b26">Tai et al. (2015)</ref>. <ref type="bibr">2</ref> The system uses the scientific computing framework Torch <ref type="bibr">3</ref> . Training is performed on the data computed in Section 5. The system uses a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001. The compositional parameters for our Tree-LSTM systems with memory di- mensions 150 and 300 are 203,400 and 541,800, respectively. The training is performed for 10 epochs. System-level scores are computed by ag- gregating and normalising segment-level scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Computing Similarity Scores from WMT Rankings</head><p>As we do not have access to any dataset which provides scores to segments on the basis of trans- lation quality, we used the WMT-13 ranks corpus to automatically derive training data. This corpus is a by-product of the manual systems evaluation carried out in the WMT-13 evaluation. In the eval- uation, the annotators are presented with a source segment, the output of five systems and a reference translation. The annotators are given the following instructions: "You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed)". Using the WMT-13 ranked corpus, we derived a corpus where the reference and corresponding translations are assigned simi- larity scores. The fact that ties are allowed makes it more suitable to generate similarity scores. If all translations are bad, annotators can mark all as rank 5 and if all translations are accurate, an- notators can mark all as rank 1. The selection of the WMT-13 corpus over other WMT workshops is motivated by the fact that it is the largest among them. It contains ten times more ranks than WMT- 12 and three to four times more than WMT-14. This also makes it possible to obtain enough refer- ence translation pairs which are evaluated several times.</p><p>Our hypothesis is that if a translation is given a certain rank many times, this reflects its simi- larity score with the reference. A better ranked translation among many systems will be close to the reference whereas a worse ranked translation among many systems will be dissimilar from the reference. To remove noisy pairs, we collect ref- erence translation pairs below a certain variance only. We determined appropriate variance values using Algorithm 1 below for n = 3, 4, 5, 6, 7 and ≥ 8, separately. The computed variance values are given in <ref type="table" target="#tab_0">Table 1</ref>.  </p><formula xml:id="formula_8">V, v ← −1, 0.25 ▷ Initialise N 3:</formula><p>for v ≤ max do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>prs ← pairs with variance below v</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>score ← kendall(prs, judgements)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>if score ≥ 0.78 then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>V ← v</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>v ← v + 0.05</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>else 10:</p><formula xml:id="formula_9">break 11: Return V ▷ Return variance</formula><p>In Algorithm 1, the kendall function calculates Kendall tau correlation using the WMT-13 hu- man judgements. We select a set for which the correlation coefficient is greater than 0.78. <ref type="bibr">4</ref> The correlation is computed using the annotations for which scores are available in the corpus (prs). In other words, the corpus acts as a scoring function for the available reference translation pairs, which gives a similarity score between a reference and a translation. We selected pairs below the variance values obtained for n = 4, 5, 6, 7 and ≥ 8. Finally, all the pairs are merged to obtain a set (L). Apart from this set, we created three other sets for our experiments. The last two also use the SICK data ( <ref type="bibr" target="#b16">Marelli et al., 2014</ref>) which was developed for evaluating semantic similarity. All four sets are described below:</p><p>L: contains the set generated by selecting the pairs ranked four or more times and filtering the segments based on the variance LNF: contains the set generated by selecting the pairs ranked four or more times without any filtering depending on the variance L+Sick: Added 4500 sentence pairs from the SICK training set to Set L in the training set and 500 pairs in the development set.</p><p>XL+Sick: Added also the pairs ranked three times to Set L+Sick.   <ref type="table" target="#tab_2">Table 2</ref> shows the number of pairs extracted for each set to train our LSTM based models. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We evaluate our approach trained on the four dif- ferent datasets obtained from WMT-13 (as given in <ref type="table" target="#tab_2">Table 2</ref>) on WMT-14. <ref type="table" target="#tab_3">Table 3</ref> shows system- level Pearson correlation obtained on different lan- guage pairs as well as average Pearson correlation (PAvg) over all language pairs. The last column of the table also shows average Spearman corre- lation (SAvg). The 95% confidence level scores are obtained using bootstrap resampling as used in the WMT-2014 metric task evaluation. The scores in bold show best scores overall and the scores in bold italic show best scores in our variants.</p><p>In <ref type="table" target="#tab_3">Table 3 and Table 4</ref>, the first section (L+Sick(lstm)) shows the results obtained us- ing simple LSTM (layer 1, hidden dimension 50, memory dimension 150, compositional pa- rameters 203400). The second section shows the scores of our Tree-LSTM metric trained on different training sets and dimensions. Di- mensions are shown in brackets, e.g L(50,150) shows the results on set 'L' with the hidden dimension 50 and the memory dimension 150. <ref type="bibr">L+Sick(mix)</ref> shows results of combining the two systems: L+Sick(50,150) and L+Sick(100,150). For the sentences longer than 20 words, the sys- tem uses scores of L+Sick(100,150) and scores of L+Sick(50,150) for the rest. The third sec- tion shows the best three overall systems from the WMT-14 metric task. The fourth section in <ref type="table" target="#tab_3">Table  3</ref> shows the systems from the WMT-14 task which obtained best results for certain languages but do not preform well overall. The last section in <ref type="table" target="#tab_3">Tables  3 and 4</ref> shows systems implementing BLEU (or variants for the segment level) and METEOR in the WMT-14 metric task. <ref type="table" target="#tab_3">Tables 3 and 4</ref> contain a deluge of evaluation data, mainly to explore the effect of different training data and model parameter settings for our models. The main messages can be summarised as follows: 1. Tree LSTM models significantly outperform the LSTM model (L+Sick(lstm) and <ref type="bibr">L+Sick(50,150)</ref> have the same data and parameter settings). 2. For Tree-LSTM models different parameter settings have only a minor impact on performance (in fact only for a few language pairs (e.g. hi-en at system-level, L+Sick(100, 300) and L+Sick <ref type="figure" target="#fig_0">(100,150)</ref>) results are statistically signifi- cantly different). This is reassuring as it indicates that the metric is not overly sensitive to exten- sive and delicate parameter tuning. 3. For the system level evaluation Tree-LSTM models are fully competitive with the best of the current com- plex models that combine many different metrics, substantial external resources and may require a significant amount of feature engineering and tun- ing. 4. For the segment level evaluation our met- ric outperforms BLEU based approaches and the other three systems 6 but lags behind some other approaches. We investigate this further below. <ref type="table" target="#tab_3">Tables 3 and 4</ref> show that set L is able to obtain similar results compared to set LNF even though we filter out almost half of the pairs. <ref type="table" target="#tab_3">Table 3</ref> shows that for L+Sick(50, 150) and L+Sick(mix), we ob- tained an average second best Pearson correlation and best Spearman correlation coefficient. We also obtained better results for the Russian-English and Czech-English language pairs compared to any other systems in the WMT-14 task.</p><p>We also evaluate our setting L-Sick(50,150) on the WMT-12 task dataset. Our metric performs best for two out of four language pairs and best overall at the system level with 0.950 and 0.926 Pearson and Spearman correlation coefficient, re- spectively. At the segment level, we obtained 0.222 Kendall tau correlation which was better than seven out of the total ten metrics in the WMT- 12 task.</p><p>One of the reasons for the difference in segment-level and system-level correlations is that Kendall Tau segment-level correlation is calcu-  <ref type="bibr">L+Sick(lstm)</ref> .922 ± .051 .882 ± .028 .974 ± .009 .898 ± .011 .863 ± .023 .908 ± .024 .872 ± .060 LNF <ref type="bibr">(50,</ref><ref type="bibr">150)</ref> .972 ± .032 .900 ± .026 .974 ± .009 .900 ± .011 .882 ± .021 .925 ± .020 .913 ± .045 L <ref type="bibr">(50,</ref><ref type="bibr">150)</ref> .988 ± .022 .897 ± .027 .978 ± .008 .905 ± .010 .875 ± .022 .929 ± .018 . <ref type="bibr">904 ± .042 L+Sick(50,150)</ref> .993 ± .017 .904 ± .025 .978 ± .008 .908 ± .010 .881 ± .022 .933 ± .016 . <ref type="bibr">915 ± .042 L+Sick(100,300)</ref> .993 ± .018 .907 ± .025 .973 ± .009 .866 ± .012 .890 ± .020 .926 ± .017 . <ref type="bibr">902 ± .050 XL+Sick(100,300)</ref> .913 ± .054 .917 ± .024 .978 ± .008 .904 ± .010 .884 ± .022 .919 ± .024 . <ref type="bibr">889 ± .055 L+Sick(100,150)</ref> .994 ± .016 .911 ± .025 .975 ± .009 .923 ± .010 .870 ± .022 .935 ± .016 .904 ± .049 <ref type="bibr">L+Sick(mix)</ref> .994 ± .017 .906 ± .025 .979 ± .008 .918 ± .010 .881 ± .022 .935 ± .016 .919 ± .045 DISCOTK-PARTY-TUNED .975 ± .031 .943 ± .020 .977 ± .009 .956 ± .007 .870 ± .022 .944 ± .018 .912 ± .043 LAYERED .941 ± .045 .893 ± .026 .973 ± .009 .976 ± .006 .854 ± .023 .927 ± .022 .894 ± .047 DISCOTK-PARTY .983 ± .025 .921 ± .024 .970 ± .010 .862 ± .015 .856 ± .023 .918 ± .019 .856 ± .046 REDSYS .989 ± .021 .898 ± .026 .981 ± .008 .676 ± .022 .814 ± .026 .872 ± .021 .786 ± .047 REDSYSSENT .993 ± .018 .910 ± .024 .980 ± .008 .644 ± .023 .807 ± .027 .867 ± .020 .771 ± .043 BLEU .909 ± 0.54 .832 ± .034 .952 ± .012 .956 ± .007 .789 ± .027 .888 ± .027 .833 ± .058 METEOR .980 ± .029 .927 ± .022 .975 ± .009 .457 ± .027 .805 ± .026 .829 ± .023 .788 ± .046   <ref type="bibr">, 2013)</ref>. Suppose four systems produce the translations T0, T1, T2 and T3. Suppose we have two metrics M1 and M2 and they produce scores and rankings as follows. GS represents the correct ranking and scores; Scores are in a scale <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> with a higher score indicating a better translation: Certainly, M1 produces better scores and rank- ing than M2. But, Kendall Tau segment-level correlation is higher for M2. (There are four concordant pairs in the M1 rank and five in the M2 rank.) Therefore, if a metric does not scale well as per the quality of translations, it may still obtain a good Kendall Tau segment-level corre- lation and a better metric may end up getting a low correlation. Another reason for the discrep- ancy between segment and system-level scores may be a low agreement on annotations. For the WMT-14 dataset, inter-annotator and intra- annotator agreement were 0.367 and 0.522. These problems should not occur with Pearson corre- lation at the system level because system-level scores are calculated using more sophisticated ap- proaches <ref type="bibr" target="#b13">(Koehn, 2012;</ref><ref type="bibr" target="#b11">Hopkins and May, 2013;</ref><ref type="bibr" target="#b21">Sakaguchi et al., 2014</ref>). For example, <ref type="bibr" target="#b11">Hopkins and May (2013)</ref> model the differences among annota- tors by adding random Gaussian noise.</p><note type="other">-en de-en fr-en hi-en ru-en PAvg SAvg</note><formula xml:id="formula_10">M1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We conclude that our dense-vector-space-based ReVal metric is simple, elegant and effective with state-of-the-art results. ReVal is fully competitive with the best of the current complex alternative approaches that involve system combination, ex- tensive external resources, feature engineering and tuning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Tree-LSTM (left) and simple LSTM (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>: T0 (0.10), T3 (0.71), T1 (0.72), T2 (0.73) M2: T1 (0.71), T0 (0.72), T2 (0.73), T3 (0.74) GS: T0 (0.10), T1 (0.71), T2 (0.72), T3 (0.73)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Variances computed using Algorithm 1 

Algorithm 1 Variance Computation 

1: procedure GETVARIANCE(judgements) 
2: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Derived Corpus statistics</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results: System-Level Correlations on WMT-14 

Test 
cs-en 
de-en 
fr-en 
hi-en 
ru-en 
Average 
Avg wmt12 
L+Sick(lstm) 
.204 ± .015 
.232 ± .014 
.289 ± .013 
.319 ± .013 
.236 ± .012 
.256 ± .013 
.254 ± .013 
NFL(50,150) 
.228 ± .015 
.288 ± .014 
.318 ± .014 
.341 ± .014 
.271 ± .012 
.289 ± .014 
.287 ± .014 
L(50,150) 
.225 ± .015 
.272 ± .014 
.328 ± .013 
.346 ± .013 
.280 ± .011 
.290 ± .013 
.287 ± .013 
L+Sick(50,150) 
.243 ± .016 
.274 ± .013 
.333 ± .013 
.360 ± .014 
.278 ± .011 
.298 ± .013 
.295 ± .014 
L+Sick(100,300) 
.233 ± .014 
.286 ± .014 
.343 ± .014 
.358 ± .013 
.281 ± .011 
.300 ± .013 
.297 ± .013 
XL+Sick(100,300) 
.252 ± .014 
.279 ± .014 
.347 ± .013 
.367 ± .013 
.274 ± .011 
.304 ± .013 
.301 ± .013 
L+Sick(100,150) 
.243 ± .016 
.274 ± .014 
.329 ± .013 
.368 ± .012 
.276 ± .011 
.298 ± .013 
.295 ± .013 
L+Sick(mix) 
.243 ± .016 
.276 ± .013 
.338 ± .013 
.358 ± .013 
.273 ± .011 
.298 ± .013 
.295 ± .013 
DISCOTK-PARTY-TUNED 
.328 ± .014 
.380 ± .014 
.433 ± .013 
.434 ± .013 
.355 ± .010 
.386 ± .013 
.386 ± .013 
BEER 
.284 ± .015 
.337 ± .014 
.417 ± .013 
.438 ± .014 
.333 ± .011 
.362 ± .013 
.358 ± .013 
REDCOMBSENT 
.284 ± .015 
.338 ± .013 
.406 ± .012 
.417 ± .014 
.336 ± .011 
.356 ± .013 
.346 ± .013 
METEOR 
.282 ± .015 
.334 ± .014 
.406 ± .012 
..420 ± .013 
.329 ± .010 
.354 ± .013 
.341 ± .013 
BLEU NRC 
.226 ± .014 
.272 ± .014 
.382 ± .013 
.322 ± .013 
.269 ± .011 
.294 ± .013 
.267 ± .013 

SENTBLEU 

.213 ± .016 
.271 ± .014 
.378 ± .013 
.300 ± .013 
.263 ± .011 
.285 ± .013 
.258 ± .014 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results: Segment-Level Correlations on WMT-14 

lated based on rankings and does not consider 
the amount of difference between scores. Here is 
an example similar to that given in (Hopkins and 
May</table></figure>

			<note place="foot" n="2"> The adapted code for MT evaluation scenarios is available at https://github.com/rohitguptacs/ReVal. 3 http://torch.ch</note>

			<note place="foot" n="4"> The score was decided so that we obtain around 10K pairs which are annotated at least four times.</note>

			<note place="foot" n="5"> For testing our approach we use WMT-12 and WMT-14 rankings instead of the test sets in this table.</note>

			<note place="foot" n="6"> These three systems are not given in this paper. Please refer (Machácek and Bojar, 2014) for results of these systems.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research leading to these results has received funding from the People Programme <ref type="table">(</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reordering metrics for MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolution kernels for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Duffy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
		<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Layered: Metric for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Gautam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Linguistic measures for automatic machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Giménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="209" to="240" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pairwise neural machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="805" to="814" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Models of translation competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DiscoTK: Using Discourse Structure for Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simulating human judgment in machine translation evaluation campaigns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Spoken Language Translation</title>
		<meeting>the Ninth International Workshop on Spoken Language Translation<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Results of the WMT-14 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matouš</forename><surname>Machácek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient elicitation of annotations for human evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TERp system description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MetricsMATR workshop at AMTA. Citeseer</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eh</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing With Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jy</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
