<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Distillation for Bilingual Dictionary Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
							<email>nnakashole@eng.ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Flauger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Distillation for Bilingual Dictionary Induction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2497" to="2506"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Leveraging zero-shot learning to learn mapping functions between vector spaces of different languages is a promising approach to bilingual dictionary induction. However, methods using this approach have not yet achieved high accuracy on the task. In this paper, we propose a bridging approach, where our main contribution is a knowledge distillation training objective. As teachers, rich resource translation paths are exploited in this role. And as learners, translation paths involving low resource languages learn from the teachers. Our training objective allows seamless addition of teacher translation paths for any given low resource pair. Since our approach relies on the quality of monolin-gual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic information. Our experiments on various languages show large performance gains from our distillation training objective, obtaining as high as 17% accuracy improvements .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In traditional supervised learning, a classifier is trained on a labeled dataset of the form <ref type="bibr">(X, Y)</ref>. Each x i ∈ X is a feature vector representing a single training instance and y i ∈ Y is the label as- sociated with x i . In zero-shot learning ( <ref type="bibr" target="#b19">Mitchell et al., 2008)</ref>, at test time we can encounter a test instance x j whose corresponding label was not seen at training time. This setting occurs in do- mains where Y can take on many values, and ob- taining labeled examples for all possible Y values is expensive. Computer vision is one such do- main, where there are thousands of objects a sys- tem needs to recognize yet at training time we may only see examples of some of the objects. In zero- shot learning, instead of learning parameters as- sociated with each possible label in Y , the learn- ing task is cast as a problem of learning a single mapping function from the vector space of input instances to the vector space of the output labels. The resulting induced function can then be applied to test instances x j whose labels may not have been seen at training time, producing a projected vector, ˆ y j , in the label space. The nearest neigh- bor of the mapped vector in the label space is then considered to be the label of x j .</p><p>In this paper, we study zero-shot learning in the context of bilingual dictionary induction, which is the problem of mapping words from a source language to equivalent words in a target language. The label space is the full vocabulary of the target language which can be on the order of millions of tokens. First, word embeddings are learned separately for each language, and second, using a given seed dictionary, we train a mapping func- tion to connect the two monolingual vector spaces, thereby facilitating bilingual dictionary induction. The advantage of zero-shot learning is that it can help reduce the amount of labeled data for applica- tions with many possible labels, such as the appli- cation we study in this paper, bilingual dictionary induction. However, the state-of-the-art accuracy on zero-shot bilingual dictionary induction is still low. On the task of English to Italian (en → it), top-1 and top-10 accuracies are around 40% and 60%, respectively ( <ref type="bibr" target="#b15">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b6">Dinu et al., 2014</ref>).</p><p>An important aspect of zero-shot learning for bilingual dictionary induction is that, it relies on availability of a large seed dictionary <ref type="bibr">1</ref> . Such large es en nl en sv en pt af da <ref type="figure">Figure 1</ref>: Trilingual paths for Portuguese(pt) to English(en) via Spanish (es), Afrikaans(af ) to (en) via Dutch (nl), and Danish(da) to (en) via Swedish(sv).</p><p>training dictionaries might not be available for all languages. However, for a given language with only a small seed dictionary, there could be a highly related language with a much larger seed dictionary. For example, we might have a small seed dictionary for translating Portuguese to English (pt → en), but a large seed dictio- nary for translating Spanish to English language (es → en). At training time, we can train the (pt → en) mapping function not only using the small seed dictionary, but also make use of the trilingual path going through Spanish, (pt → es → en). Since pt and es are highly re- lated, a small amount of data may be sufficient to learn the projection (pt → es). This is the idea of using a bridge or pivot language in machine trans- lation ( <ref type="bibr" target="#b27">Utiyama and Isahara, 2007)</ref>. Our contri- bution is a knowledge distillation training objec- tive function that encourages the mapping func- tion ( pt → en) to predict the true English target words as well as to match the predictions of the trilingual path ( pt → es → en) within a margin. This is approach allows seamless Example trilin- gual paths are shown in <ref type="figure">Figure 1</ref>. By setting up our objective function in this way, we are distilling knowledge ( <ref type="bibr" target="#b4">Bucilu et al., 2006;</ref><ref type="bibr" target="#b11">Hinton et al., 2015</ref>) from the trilingual paths to train a single mapping function for ( pt → en). In our experiments, we show performance gains for several language pairs, 17% for top-10 precision for ( pt → en). We also show that, for a given lan- guage pair, our objective seamlessly allows us to distill from several related languages. Moreover, we learn weights for each of the distillation paths, thereby automatically learning indicative weights of how useful each distillation path is. Finally, we show that even when we only use unlabeled data to distill knowledge from trilingual paths, we still obtain performance gains over a model trained on a small seed dictionary.</p><p>Since our approach relies on the quality of monolingual word embeddings, we also propose to enhance vector representations of both the source and target language with linguistic infor- mation. In particular, we augment word vec- tors with additional dimensions capturing corpus statistics of part-of-speech tags of words. Second, we model sub-word information in the vector rep- resentations of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Cross Vector Space Mapping with Seed Dictio- naries. Our work is most related to models that do zero-shot learning for bilingual dictionary induc- tion, using maps between vector spaces with seed dictionaries as training data. Examples include the models of ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Dinu et al., 2014;</ref><ref type="bibr" target="#b15">Lazaridou et al., 2015;</ref><ref type="bibr" target="#b28">Vulic and Korhonen, 2016)</ref>. Like these approaches, we first learn word embed- dings for each language, then use a seed dictionary to train a mapping function between the two vec- tor spaces. In a departure from these prior meth- ods, we propose to distill knowledge from trilin- gual paths of nearby languages for languages with small seed dictionaries using a distillation train- ing objective. Additionally, we model linguistic information in the vector space of the source and target languages. Another line of research in this vein is the work of ( <ref type="bibr" target="#b28">Vulic and Korhonen, 2016)</ref>, who analyze how properties of the seed dictio- nary affect bilingual dictionary induction across different dimensions (i.e., lexicon source, lexicon size, translation method, translation pair reliabil- ity). However, methodologically, their approach is based on prior work ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Dinu et al., 2014)</ref>.</p><p>Bilingual word embeddings. There is a rich body of work on bilingual embeddings. Bilin- gual word embedding learning methods produce a shared bilingual word embedding space where words from two languages are represented in the new space so that similar words, which may be in different languages, have similar representations. Such bilingual word embeddings have been used in a number of tasks including semantic word sim- ilarity <ref type="bibr" target="#b7">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b0">Ammar et al., 2016</ref>) learning bilingual word lexicons ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b8">Gouws et al., 2015;</ref><ref type="bibr" target="#b28">Vulic and Korhonen, 2016</ref>), parsing ( <ref type="bibr" target="#b9">Guo et al., 2015;</ref><ref type="bibr" target="#b26">Täckström et al., 2012)</ref>, information retrieval ( <ref type="bibr" target="#b29">Vulic and Moens, 2015)</ref>, and cross-lingual document clas- sification ( <ref type="bibr" target="#b13">Klementiev et al., 2012;</ref><ref type="bibr" target="#b14">Kočisk`Kočisk`y et al., 2014</ref>).</p><p>Some bilingual word embedding methods such as <ref type="bibr" target="#b8">Gouws et al., 2015</ref>) require sentence or word aligned data, which our approach does not require. We com- pare our approach to the bilingual embeddings produced by the recent method of <ref type="bibr" target="#b0">(Ammar et al., 2016)</ref>. Like our approach, this work does not re- quire availability of parallel corpora but only a seed dictionary.</p><p>On the aspect of enriching word embeddings with linguistic knowledge for the purpose of ma- chine translation, Sennrich and Barry <ref type="bibr" target="#b25">(Sennrich and Haddow, 2016</ref>) introduce linguistic features in sequence to sequence neural machine translation. Like our work, they also represent such features in the embedding layer. In addition to part-of- speech tags and morphological features, they also use syntactic dependency labels which are not ap- plicable to our model since we work at the word level while their model is at the sentence level.</p><p>Knowledge Distillation. Knowledge distilla- tion was introduced for model compression to learn small models from larger models ( <ref type="bibr" target="#b4">Bucilu et al., 2006;</ref><ref type="bibr" target="#b11">Hinton et al., 2015</ref>). For exam- ple, from a large neural network model a smaller model can be distilled such that it generalizes in the same way as the large model ( <ref type="bibr" target="#b24">Romero et al., 2014</ref>). Knowledge distillation was also used by ( <ref type="bibr">Hu et al., 2016</ref>) to distill knowledge from logical rules in the tasks of named entity recognition and sentiment analysis, thereby enforcing constraints on the trained model. Our approach is different from this prior work on knowledge distillation in that we distill knowledge from mapping functions of related languages into mapping functions of languages with only small seed dictionaries.</p><p>Domain adaptation, for which there is a long history, is also related to our work ( <ref type="bibr" target="#b1">Ben-David et al., 2007;</ref><ref type="bibr" target="#b5">Daumé III, 2007;</ref><ref type="bibr" target="#b22">Pan et al., 2010;</ref><ref type="bibr" target="#b16">Long and Wang, 2015)</ref>. <ref type="bibr" target="#b5">(Daumé III, 2007</ref>) pro- posed feature augmentation, suggesting that a model should have features that are general across domains, as well as features that are domain- specific. Thus the model learns from all do- mains while preserving domain-specific informa- tion. These kinds of models have to be retrained when a new domain is added. Our work however only has to train mapping functions that involve a new language, all others can be distilled without retraining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Embedding Linguistic Information</head><p>Since our approach relies on the quality of mono- lingual word embeddings, we would like to work with high quality word embeddings. We therefore, first seek to enhance the vector representations of words in the source and target languages so that they can capture useful linguistic information. The intuition is that such information can help narrow down the words in the target language that are con- sidered valid translations for a given source lan- guage word. To that end, we model both part of speech (POS) tag distributions of words and sub- word information in the vector representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Part of Speech Distributions</head><p>The idea behind modeling POS tags is that words should have the same part of speech tag in dif- ferent languages. For example, if we are trans- lating the noun Katze from German to English, in English we expect the singular noun cat and not the plural cats. While this information may be monolithically represented in word vectors gen- erated by embedding methods such as Skip-gram and CBOW, here we seek to explicitly model POS tags. Since each word can have multiple POS tags, we model a word's part of speech information as a distribution over all the possible POS tags that it can take on. We learn POS tag statistics by first tagging a large corpus of each language, we then use tag counts to generate distributions. For exam- ple, if the English word, bark appears tagged as a verb 30 times in our corpus, and tagged as a noun 10 times, we generate a vector which puts 2/3 in the verb direction, and 1/3 in the noun direction, and 0 in the directions of all other POS tags. While these statistics can be noisy, we hope they can still provide useful signals. We use the universal POS tags, there are 12 tags in the universal POS tags <ref type="bibr" target="#b23">(Petrov et al., 2011</ref>).</p><p>For a given word w, we compute a vector rep- resentation w i ∈ R d using a word embedding method. For now, let's assume we use the Skip- gram model. In the next section, we describe an enhanced word embedding method. We compute a POS corpus statistics vector v i ∈ R 12 for the word using the 12 universal POS tags. With this new information, the representation for word i is given by</p><formula xml:id="formula_0">x i = (w i , v i ) ∈ R d+12 .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Internal Structure</head><p>Morphology carries information that is useful for capturing the identity of a word. It represents in- formation such as tense. When doing cross-lingual zero-shot projection of a word in a source lan- guage, we wish to translate to words that have the same linguistic properties. For example, the Ger- man word gewinnen should be translated to the present tense win, not the past tense won which in German is gewonnen. We approximate morphol- ogy by incorporating sub-word information into the vector representations. There are several ways of doing this, one approach is to work on the level of characters. We go for the middle-ground, in which a word is represented as a combination of a vector for the word itself with vectors of sub-word units that comprise it. In particular, for a given a word we learn a vector representation for the word itself, and also for each n-gram of &gt;= 3 and &lt; 6 in the word ( <ref type="bibr" target="#b3">Bojanowski et al., 2017)</ref>. Each word is thus represented by the sum of the vector rep- resentations of its n-grams, including the word it- self. This representation is then used to replace w i in equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training Objective</head><p>A common objective function used in prior work ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Dinu et al., 2014</ref>) for learn- ing cross vector space mapping functions is the regularized least squares error:</p><formula xml:id="formula_1">ˆ W = arg min W∈R s×t ||XW − Y|| F + λ||W|| (2)</formula><p>where matrixˆWmatrixˆ matrixˆW is the learned mapping function, X and Y represent the matrices containing the vectors for the source language words and vectors for the target language words, respectively. In- stead of the least squares loss shown in equation 2, we use a ranking loss, as in ( <ref type="bibr" target="#b15">Lazaridou et al., 2015)</ref>, which aims to rank correct training data pairs (x i , y i ) higher than incorrect pairs (x i , y j ) with a margin of at least γ. The margin γ is a hyper-parameter which is application specific, and the incorrect labels, y j can be selected randomly such that j = i or in a more application specific manner 2 . <ref type="bibr">2</ref> In our experiments, we explored several application spe- cific approaches for choosing negative examples, including one that picks negative examples among words whose part of speech class is different from the positive example. However, these approaches did produce significant improvement, and we resorted back to randomly selected negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Given a seed dictionary training data of the form</head><formula xml:id="formula_2">D tr = {x i , y i } m i=1</formula><p>, the margin-based ranking loss is defined as:</p><formula xml:id="formula_3">J single = m i=1 k j =i max 0, γ+d(y i , ˆ y i )−d(y j , ˆ y i )<label>(3)</label></formula><p>wherê y i = Wx i is the prediction, k is the number of incorrect examples per training instance, and d(x, y) = (x − y) 2 is the distance measure.</p><p>For a given correct pair and incorrect pair, sub- stitutingˆystitutingˆ stitutingˆy i = Wx i . The loss is given by:</p><formula xml:id="formula_4">max 0, γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 : j = i.<label>(4)</label></formula><p>To evaluate the derivative analytically, we can write:</p><formula xml:id="formula_5">max 0, γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 = θ γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 × γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2<label>(5)</label></formula><p>where θ(x) denotes the Heaviside θ-function. The derivative with respect to the elements of the ma- trix W is then approximated by, after neglecting a term that would only contribute if the difference</p><formula xml:id="formula_6">(y j − ˆ y i ) 2 − (y i − ˆ y i ) 2 were exactly γ ∂ ∂W ab (θ γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 × γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 2θ γ + (y i − ˆ y i ) 2 − (y j − ˆ y i ) 2 × x i b (y j a − y i a ) (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Distillation</head><p>In zero-shot learning for bilingual dictionary in- duction a large seed dictionary is used to train a mapping function. Such large training dictionar- ies might not be available for all languages. How- ever, for a given language with only a small seed dictionary, there could be a highly related lan- guage with a much larger seed dictionary. We propose a method for leveraging mapping func- tions of nearby languages to train mapping func- tions for languages where large seed dictionaries may not be available. Our method is related to no- tion of having a bridge or pivot language as done in sentence level translation <ref type="bibr" target="#b27">(Utiyama and Isahara, 2007)</ref>. We develop a distillation training objec- tive that allows us to seamlessly leverage several bridge languages for word level translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Trilingual Paths for distillation</head><p>Let us consider the problem of translating from a given source language to English. As a running example, we use Portuguese(pt) as the source lan- guage. We wish to learn a mapping function from word vectors in Portuguese to word vectors in En- glish. We can set up a learning task, using a train- ing dataset D = {x i , y i } m i=1 and the loss defined in Equation 3. This gives us the projection function in the form of a matrix: W (pt→en) . We can thus translate Portuguese words to English as follows:</p><formula xml:id="formula_7">ˆ y i (en)←(pt) = W (pt→en) x (pt) i<label>(7)</label></formula><p>If the seed dictionary for Portuguese to English is small, W (pt→en) might generalize poorly, pro- ducing many wrong translations when using Equa- tion 7. Suppose, a related language, for example, Spanish has a lot of training data available, and we have independently trained its mapping func- tion, which can make predictions from Spanish to English as follows:</p><formula xml:id="formula_8">ˆ y i (en)←(es) = W (es→en) x (es) i<label>(8)</label></formula><p>Since W (es→en) is trained with a lot of data, we expect it to generalize better and make more accu- rate predictions than W (pt→en) . One insight here is that since the languages es and pt are highly re- lated, we need much less data to train an accurate mapping matrix W (pt→es) than we to need to learn an accurate W (pt→en) . Therefore we train a map- ping function from Portuguese to Spanish, which makes predictions as follows.</p><formula xml:id="formula_9">ˆ y i (es)←(pt) = W (pt→es) x (pt) i<label>(9)</label></formula><p>We now have a second path that goes from Por- tuguese to English much like Equation 7 but this path goes via Spanish as follows: <ref type="figure" target="#fig_2">Figure 2</ref> illustrates the two paths from Por- tuguese to English. Our main insight is to use knowledge distillation, to improve the accu- racy of the mapping matrix W (pt→en) throughˆy throughˆ throughˆy i . This distillation is done by mod- ifying our learning objective.  </p><formula xml:id="formula_10">ˆ y i (es)←(pt) = W (pt→es) x (pt) i ˆ y i (en)←(es)←(pt) = W (es→en) ˆ y i (es)←(pt)<label>(10)</label></formula><formula xml:id="formula_11">ˆ y i (en) pt x (pt) i W (es→en) W (pt→es) W (pt→en) (a) ˆ y i (es)←(pt) = W (pt→es) x (pt) i ˆ y i (en)←(es)←(pt) = W (es→en) ˆ y i (es)←(pt) (b) ˆ y i (en)←(pt) = W (pt→en) x (pt) i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Distillation Objective</head><p>For a given Portuguese word x which involves three languages. We would like to improve predictions made by Equation 7 by im- proving the mapping matrix W (en→pt) . There- fore when training using the Portuguese to English training data, we want our objective to both min- imize the loss defined in Equation 3 and simulta- neously to let W (en→pt) mimic predictions made through the pathˆypathˆ pathˆy i (en)←(es)←(pt)</p><p>as "soft targets" within a margin. The distillation objective is as follows:</p><formula xml:id="formula_12">J d = m i=1 max 0, ˆ y i (en)←(pt) − ˆ y i (en)←(es)←(pt) 2 − φ ,(11)</formula><p>where φ is the margin. We combine J single and J d through a weighted average of the two different objective functions. Notice that J d can be com- puted without having labeled training data. In our experiments, we show that even in this case of un- labeled data, which gets rid of J single since it re- quires labeled data, J d outperforms models trained using only J single when the training data is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multiple Trilingual Paths</head><p>We are not restricted to distilling Portuguese through Spanish only. Our model can, in addition, for example distill through German, French, and other languages. We can modify the distillation loss as follows:</p><formula xml:id="formula_13">J d−multi = m i=1 n j=1 ψ j max 0, ˆ y i (en)←(pt) − ˆ y i (en)←(j)←(pt) 2 − φ ,<label>(12)</label></formula><p>where j labels the distillation language. With the objective J d−multi combined with J single , we are training a mapping function which mimics the be- havior of many trilingual paths, as "soft targets" within a margin ψ. We keep φ the same in our experiments across all trilingual paths. The ψ i are weights that reflect how much we penalize our model if it diverges from the predictions of a par- ticular trilingual path. Intuitively, if a language is similar to our source language, (pt) in this case, its corresponding ψ value should be high. For ex- ample, if Spanish is considered more related to Portuguese than any other language in the trilin- gual paths in Equation 12, than we expect ∀i = 1, ψ 1 &gt; φ i . This is assuming that the second parts of the trilingual paths have similar accuracies, ie.</p><formula xml:id="formula_14">W (es→en) , W (f r→en)</formula><p>, and W (...→en) have sim- ilar projection accuracies. The most similar lan- guage is expected to be the easiest to project into from Portuguese. For example we might expect W (pt→es) to be more accurate than W (pt→de) , if we have similar amounts of training data for learn- ing both of these. We next present how we learn the ψ i values for the multiple paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Weighted Trilingual Paths</head><p>Going back to the example, we first learn the weights using the Portuguese to English training set,</p><formula xml:id="formula_15">D = {x pt i , y en i } m i=1</formula><p>, and then input the weights into the model before training with J d−multi and J single . Suppose we want to compute ψ 1 which corresponds to Spanish in Equation 12. For a given Portuguese word x pt i ∈ D, whose English translation is y en i , we can compute:</p><formula xml:id="formula_16">ψ dot 1i = (y en i ) T ˆ y i (en)←(es)←(pt)<label>(13)</label></formula><p>We also experimented with a bilinear term:</p><formula xml:id="formula_17">ψ bilinear 1i = (y en i ) T H ˆ y i (en)←(es)←(pt)<label>(14)</label></formula><p>We found a better performing approach to be:</p><formula xml:id="formula_18">V = ˆ y i (en)←(pt) − ˆ y i (en)←(es)←(pt) 2 ψ euclid 1i = exp( 1 V ).<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P@1 P@5 P@10</head><p>Italian (en → it) THIS 51.0 66.6 72.4 THIS w/pos 51.6 68.5 73.4 <ref type="table">Table 1</ref>: Translation accuracy on the English to Italian dataset of ( <ref type="bibr" target="#b6">Dinu et al., 2014</ref>).</p><note type="other">Ridge 29.7 44.2 49.1 Lazaridou et. al 40.2 54.2 60.4 MultiCluster 2.40 7.30 11.0 MultiCCA 0 0.1 0.3</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Evaluation</head><p>In this section, we study the following questions:</p><p>• What is the effect of modeling linguistic in- formation in the vector representations of the source and target languages on accuracy of bilingual dictionary induction?</p><p>• Can our knowledge distillation objective from trilingual paths involving related lan- guages improve accuracy of mapping func- tions of languages with small seed dictionar- ies?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data and Experimental Setup</head><p>In most of our experiments, we use the train- ing data that was used to train the multi-lingual embeddings in ( <ref type="bibr" target="#b0">Ammar et al., 2016</ref>). We indi- cate when this is not the training data used. This data was obtained automatically by using Google Translate. For test data, we use manual transla- tions either from prior work or from searching the Web, including genealogical word lists 3 .</p><p>For word vector representations, we use Wikipedia to train 300 dimensional vectors for all languages we evaluate on. Based on a validation set, we set the margin γ in Equation 3 through Equation 6 to be γ = 0.4, φ in Equations 11, 12, and 15 to be φ = 0.01. We estimate model param- eters using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methods Under Comparison</head><p>In our experiments, we compare performance of the following methods.</p><p>• The method THIS refers to our model which uses a max-margin loss function as defined in Equation 3. It uses sub-word informa- tion in the vector representations of words. One variation of our method is, THIS w/pos, which includes POS tag statistics as addi- tional dimensions. The distilled variations of our method explicitly indicate the languages involved, for example (pt → es → en).</p><p>• The method Ridge is used in a number of prior work ( <ref type="bibr" target="#b17">Mikolov et al., 2013;</ref><ref type="bibr" target="#b6">Dinu et al., 2014;</ref><ref type="bibr" target="#b28">Vulic and Korhonen, 2016)</ref>. These ap- proaches use an L2-regularized least-squares error objective as shown in Equation 2.</p><p>• The method Lazaridou et al. was proposed by ( <ref type="bibr" target="#b15">Lazaridou et al., 2015)</ref>. It uses a max- margin ranking function and introduces a way of picking negative examples in comput- ing the loss.</p><p>• The methods MultiCluster and MultiCCA refer to the multilingual word embeddings introduced by <ref type="bibr" target="#b0">(Ammar et al., 2016</ref>). They extend canonical correlation analysis (CCA) based methods ( <ref type="bibr" target="#b10">Haghighi et al., 2008;</ref><ref type="bibr" target="#b7">Faruqui and Dyer, 2014</ref>) to a multi-lingual setting where they treat English as the com- mon vector space. For these methods, we use their pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Linguistic Information Evaluation</head><p>To address the first of our evaluation questions, we performed experiments on the dataset introduced by ( <ref type="bibr" target="#b6">Dinu et al., 2014)</ref>, where the state-of-the art is the work of ( <ref type="bibr" target="#b15">Lazaridou et al., 2015)</ref>. This is an Italian to English dataset, which consists of 5K translation pairs as training data, and 1.5K pairs as test data. In both ( <ref type="bibr" target="#b6">Dinu et al., 2014</ref>) and (Lazari- dou et al., 2015), the embeddings were trained on Wikipedia and additional corpora, we only train on Wikipedia.</p><p>The results for this experiment are shown in Ta- ble 1. Our method, THIS, performs well above the previous state of the art ( <ref type="bibr" target="#b15">Lazaridou et al., 2015</ref>). For top-1 precision, as can been seen in <ref type="table">Table 1</ref>   <ref type="table">Table 3</ref>: Top-10 precision for eight languages translated to English. The high accuracy on Ital- ian can be explained by the fact that, unlike other language pairs, for Italian we do not use Google Translate training data, but the data of ( <ref type="bibr" target="#b6">Dinu et al., 2014</ref>), as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>in <ref type="table">Table 3</ref>, and the corresponding data is shown in <ref type="table" target="#tab_1">Table 2</ref>. For these language pairs, we do not show results for our method, THIS w/pos, since POS taggers are not available for some of the lan- guages. We also do not show ( <ref type="bibr" target="#b15">Lazaridou et al., 2015</ref>), as they did not do experiments on these data sets, and we did not have an implementation of their approach. Additionally, ( <ref type="bibr" target="#b0">Ammar et al., 2016)</ref> did not have trained embeddings for Dutch (nl).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Trilingual Paths for Distillation</head><p>To address our second evaluation question, we car- ried out experiments with languages for which we only had small seed dictionaries. The training and test datasets for this setup are shown in <ref type="table">Table 4</ref>. We gathered these datasets by searching for man- ually created datasets. In the cases were we could not find any, we used Google Translate, which, however produces some noisy translations. This is partly due to the fact that the translations are done out of context. We begin with thorough experiments on the  <ref type="table" target="#tab_1">pt  pt  da  da  af  af  ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓  en  es  f r  de  en  sv  en  nl  Train 573 701 1,808 465 3,000 1,980 3,744 2,000  Test  296 0  0  0  262  0  459  0   Table 4</ref>: Training and test datasets used in the trilingual path distillation experiments. We evalu- ated sub-parts of trilingual paths such as pt → es, and pt → f r using cross validation hence the test sets for those languages are zero.</p><formula xml:id="formula_19">P@10 Portuguese (pt → en) 1 THIS (pt → en) 65.2 2 (pt → en) +(pt → es → en) [unlabeled data] 74.0 3 (pt → en) + (pt → es → en) 82.1 4 (pt → en) + (pt →   de es f r   → en) [Weighted] 81.8 5 (pt → en) + (pt →   de es f r   → en)[Unweighted] 78.4</formula><p>6 Ridge 60.8 <ref type="table">Table 5</ref>: Trilingual path distillation results for Por- tuguese to English.</p><p>Portuguese-English language pair. The results are shown in <ref type="table">Table 5</ref>. First, we see that if we distill through the Spanish trilingual path (pt → es → en), without using any labeled data from pt → en, we already obtain a 9% gain in accuracy, line 2 in <ref type="table">Table 5</ref>. If, in addition to distilling through Span- ish, we use the available training data pt → en, 573 translation pairs, line 3 in <ref type="table">Table 5</ref>, we ob- tain a 17% gain in accuracy. We see however that adding the distillation paths via French, and Ger- man did not improve performance, line 4 in Ta- ble 5. This can be attributed to the fact that with multiple distillation paths, the model has to opti- mize a more difficult function. On the other hand, we see that our trilingual weighting mechanism is effective. Without path weights, top-10 accu- racy is 78.4% vs 81.8% with weights, lines 4 and 5 in <ref type="table">Table 5</ref>. The learned weights for the three languages involved in the trilingual paths for Por- tuguese are shown in <ref type="figure">Figure 3</ref>. Spanish is the high- est weighted, followed by French, and German has the lowest weight. By definition, the learned weights add up to 1. In <ref type="figure">Figure 4</ref>, we show accu- racy while varying the size of the seed dictionary. We can see that, given the small size of the training data, distillation provides a strong advantage. Finally, we applied our distillation method to Afrikaans and Danish. Afrikaans distills from Dutch, and Danish distills from Swedish. As shown in <ref type="table" target="#tab_2">Table 6</ref>, in both cases, we obtained per- formance gains. However, in both of these cases, performance gains are modest. Unlike Portuguese to English, the seed dictionaries involved in train- ing these language pairs were obtained automat- ically using Google Translate and contain noisy translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented a knowledge distillation train- ing objective that leverages trilingual paths of re- lated languages to improve mapping functions of languages with small seed dictionaries. The model produces substantial gains in accuracy for several language pairs.</p><p>There are several future directions. First, due</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P@10</head><p>Afrikaans (af → en) THIS (af → en) 46.4 (af → en)+ (af → nl → en) [unlabeled data] 49.9 (af → en)+(af → nl → en) 51.0 Ridge 38.6 Danish (da → en) THIS (da → en)</p><p>44.4 (da → en) + (da → sv → en) [unlabeled data] 45.2 (da → en) + (da → sv → en)</p><p>47.2 Ridge 37.1 to advances in methods for extracting general pur- pose knowledge ( <ref type="bibr" target="#b21">Nakashole et al., 2013;</ref><ref type="bibr" target="#b30">Wijaya et al., 2014)</ref>, the use of se- mantic knowledge has been explored for several natural language tasks ( <ref type="bibr" target="#b20">Nakashole and Mitchell, 2015;</ref><ref type="bibr" target="#b31">Yang and Mitchell, 2017)</ref>. However, for bilingual dictionary induction, and more generally, machine translation, the role of semantic knowl- edge has not been fully explored. We consider this to be a promising line of future work. Second, although we focus on bilingual dictionary induc- tion, our knowledge distillation training objective that enables seamless use of paths of rich resource languages as teachers of low resource languages is general and can be applied to problems such as multilingual tagging and parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>en)←(es)←(pt)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>es enˆy enˆ enˆy i (es)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Translating with both a trilingual path (dotted lines, and equation (a)) , and a bilingual path (solid line, and equation (b))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>i</head><label></label><figDesc>, Equation 7 makes the predictionˆypredictionˆ predictionˆy i (en)←(pt) and Equation 10 makes the trilingual predictionˆypredictionˆ predictionˆy i (en)←(es)←(pt)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Learned weights for languages involved in trilingual paths for translating Portuguese to English. Spanish is the highest weighted and German is the lowest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Training and test sets for various lan-
guage pairs. The training datasets marked with (*) 
are from (Ammar et al., 2016) obtained through 
Google Translate. Italian to English is from (Dinu 
et al., 2014). The Dutch to English training dataset 
is introduced in this paper. With the exception of 
Italian to English, all test datasets are introduced 
in this paper. 

de 
es 
f r 
it 
nl 
sv 
↓ 
↓ 
↓ 
↓ 
↓ 
↓ 
en 
en 
en 
en 
en 
en 
P@10 
THIS 
57.8 59.5 67.4 70.0 60.8 54.6 
Ridge 
32.8 54.2 59.9 66.4 58.8 44.6 
MultiCluster 12.2 8.1 
4.6 
6.9 
-
9.0 
MultiCCA 
6.7 
4.3 
2.9 
5.6 
-
10.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Trilingual path distillation results for 
Afrikaans and Danish. 

</table></figure>

			<note place="foot" n="1"> 5000 seed pairs for the (en → it) dataset.</note>

			<note place="foot" n="3"> For example: https://familysearch.org/wiki/en/Afrikaans Word List</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Massively multilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>abs/1602.01925</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multilingual distributed representations without word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">256</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6568</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-lingual dependency parsing based on distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bilingual lexicons from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Eduard Hovy, and Eric Xing. 2016. Harnessing deep neural networks with logic rules</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">`</forename><surname>Kočisk`y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0947</idno>
		<title level="m">Learning bilingual word representations by marginalizing alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into cross-space mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02117</idno>
		<title level="m">Learning multiple tasks with deep relationship networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thahir Mohamed, Ndapandula Nakashole, Emmanouil Antonios Platanios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estevam</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Partha Pratim Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><forename type="middle">Dalvi</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazaitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, (AAAI)</title>
		<editor>Alan Ritter, Mehdi Samadi, Burr Settles, Richard C. Wang, Derry Tanti Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling</editor>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2302" to="2310" />
		</imprint>
	</monogr>
	<note>Never-ending learning</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting human brain activity associated with the meanings of nouns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><forename type="middle">V</forename><surname>Tom M Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Shinkareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Min</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicente</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">Adam</forename><surname>Robert A Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Just</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">5880</biblScope>
			<biblScope unit="page" from="1191" to="1195" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A knowledge-intensive model for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics, (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="365" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering semantic relations from the web and organizing them with patty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochuan</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1104.2086</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparison of pivot methods for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On the role of seed lexicons in learning bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ctps: Contextual temporal profiles for time scoping facts using state change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Derry Tanti Wijaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in lstms for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, (ACL)</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, (ACL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
