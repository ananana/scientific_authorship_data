<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prune-and-Score: Learning for Greedy Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Prashanth</roleName><forename type="first">J</forename><forename type="middle">Walker</forename><surname>Orr</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannem</forename><forename type="middle">Xiaoli</forename><surname>Fern</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="department" key="dep2">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution" key="instit1">Oregon State University</orgName>
								<orgName type="institution" key="instit2">Washington State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prune-and-Score: Learning for Greedy Coreference Resolution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2115" to="2126"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel search-based approach for greedy coreference resolution, where the mentions are processed in order and added to previous coreference clusters. Our method is distinguished by the use of two functions to make each corefer-ence decision: a pruning function that prunes bad coreference decisions from further consideration, and a scoring function that then selects the best among the remaining decisions. Our framework reduces learning of these functions to rank learning, which helps leverage powerful off-the-shelf rank-learners. We show that our Prune-and-Score approach is superior to using a single scoring function to make both decisions and outperforms several state-of-the-art approaches on multiple benchmark corpora including OntoNotes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution is the task of clustering a set of mentions in the text such that all mentions in the same cluster refer to the same entity. It is one of the first stages in deep language understanding and has a big potential impact on the rest of the stages. Several of the state-of-the-art approaches learn a scoring function defined over mention pair, cluster-mention or cluster-cluster pair to guide the coreference decision-making process <ref type="bibr" target="#b9">(Daumé III, 2006</ref>; <ref type="bibr" target="#b1">Bengtson and Roth, 2008;</ref><ref type="bibr" target="#b37">Rahman and Ng, 2011b;</ref><ref type="bibr" target="#b41">Stoyanov and Eisner, 2012;</ref>. One common and persistent problem with these approaches is that the scoring function has to make all the coreference decisions, which leads to a highly non-realizable learning problem. Inspired by the recent success of the HC-Search Framework ( <ref type="bibr" target="#b11">Doppa et al., 2014a</ref>) for studying a variety of structured prediction problems ( <ref type="bibr" target="#b25">Lam et al., 2013;</ref><ref type="bibr" target="#b13">Doppa et al., 2014c</ref>), we study a novel approach for search-based coreference resolution called Prune-and-Score. HC-Search is a divide- and-conquer solution that learns multiple compo- nents with pre-defined roles, and each of them contribute towards the overall goal by making the role of the other components easier. The HC- Search framework operates in the space of com- plete outputs, and relies on the loss function which is only defined on the complete outputs to drive it- s learning. Unfortunately, this method does not work for incremental coreference resolution since the search space for coreference resolution con- sists of partial outputs, i.e., a set of mentions only some of which have been clustered so far.</p><p>We develop an alternative framework to HC- Search that allows us to effectively learn from par- tial output spaces and apply it to greedy corefer- ence resolution. The key idea of our work is to address the problem of non-realizability of the s- coring function by learning two different function- s: 1) a pruning function to prune most of the bad decisions, and 2) a scoring function to pick the best decision among those that are remaining. Our Prune-and-Score approach is a particular instanti- ation of the general idea of learning nearly-sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic function- s for guiding the search. The pruning constraints can take different forms (e.g., classifiers, decision- list, or ranking functions) depending on the search architecture. Therefore, other coreference resolu- tion systems ( <ref type="bibr" target="#b2">Björkelund and Kuhn, 2014</ref>) can also benefit from this idea. While our basic idea of two- level selection might appear similar to the coarse- to-fine inference architectures <ref type="bibr" target="#b16">(Felzenszwalb and McAllester, 2007;</ref><ref type="bibr" target="#b44">Weiss and Taskar, 2010)</ref>, the details differ significantly. Importantly, our prun- ing and scoring functions operate sequentially at each greedy search step, whereas in the cascades approach, the second level function makes its pre- diction only when the first level decision-making is done.</p><p>Summary of Contributions. The main contribu- tions of our work are as follows. First, we moti- vate and introduce the Prune-and-Score approach to search-based coreference resolution. Second, we identify a decomposition of the overall loss of the Prune-and-Score approach into the pruning loss and the scoring loss, and reduce the problem of learning these two functions to rank learning, which allows us to leverage powerful and efficien- t off-the-shelf rank learners. Third, we evaluate our approach on OntoNotes, ACE, and MUC da- ta, and show that it compares favorably to sever- al state-of-the-art approaches as well as a greedy search-based approach that uses a single scoring function.</p><p>The remainder of the paper proceeds as follows. In Section 2, we dicuss the related work. We intro- duce our problem setup in Section 3 and then de- scribe our Prune-and-Score approach in Section 4. We explain our approaches for learning the prun- ing and scoring functions in Section 5. Section 6 presents our experimental results followed by the conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The work on learning-based coreference resolu- tion can be broadly classified into three types. First, the pair-wise classifier approaches learn a classifier on mention pairs (edges) ( <ref type="bibr">Soon et al., 2001;</ref><ref type="bibr" target="#b32">Ng and Cardie, 2002;</ref><ref type="bibr" target="#b1">Bengtson and Roth, 2008)</ref>, and perform some form of approximate de- coding or post-processing using the pair-wise s- cores to make predictions. However, the pair-wise classifier approach suffers from several drawback- s including class imbalance (fewer positive edges compared to negative edges) and not being able to leverage the global structure (instead making in- dependent local decisions).</p><p>Second, the global approaches such as Struc- tured SVMs and Conditional Random Fields (CRFs) learn a cost function to score a potential clustering output for a given input set of men- tions <ref type="bibr" target="#b30">(Mccallum and Wellner, 2003;</ref><ref type="bibr" target="#b18">Finley and Joachims, 2005;</ref><ref type="bibr" target="#b8">Culotta et al., 2007;</ref><ref type="bibr" target="#b48">Yu and Joachims, 2009;</ref><ref type="bibr" target="#b20">Haghighi and Klein, 2010;</ref><ref type="bibr" target="#b45">Wick et al., 2011;</ref><ref type="bibr" target="#b46">Wick et al., 2012;</ref><ref type="bibr">Fernandes et al., 2012</ref>). These methods address some of the prob- lems with pair-wise classifiers, however, they suf- fer from the intractability of "Argmin" inference (finding the least cost clustering output among ex- ponential possibilities) that is encountered during both training and testing. As a result, they resort to approximate inference algorithms (e.g., MCMC, loopy belief propagation), which can suffer from local optima.</p><p>Third, the incremental approaches construct the clustering output incrementally by processing the mentions in some order <ref type="bibr" target="#b9">(Daumé III, 2006;</ref><ref type="bibr" target="#b10">Denis and Baldridge, 2008;</ref><ref type="bibr" target="#b37">Rahman and Ng, 2011b;</ref><ref type="bibr" target="#b41">Stoyanov and Eisner, 2012;</ref>. These methods learn a scoring function to guide the decision-making process and differ in the form of the scoring function (e.g., mention pair, cluster- mention or cluster-cluster pair) and how it is being learned. They have shown great success and are very efficient. Indeed, several of the approach- es that have achieved state-of-the-art results on OntoNotes fall under this category ( <ref type="bibr" target="#b2">Björkelund and Kuhn, 2014</ref>). However, their efficiency requirement leads to a highly non- realizable learning problem. Our Prune-and-Score approach is complementary to these methods, as we show that having a pruning function (or a set of learned pruning rules) makes the learning prob- lem easier and can improve over the performance of scoring-only approaches. Also, the models in ( ) try to leverage cluster-level information implicitly (vi- a latent antecedents) from mention-pair features, whereas our model explicitly leverages the cluster level information.</p><p>Coreference resolution systems can benefit by incorporating the world knowledge including rules, constraints, and additional information from external knowledge bases ( <ref type="bibr" target="#b26">Lee et al., 2013;</ref><ref type="bibr" target="#b36">Rahman and Ng, 2011a;</ref><ref type="bibr" target="#b38">Ratinov and Roth, 2012;</ref><ref type="bibr" target="#b49">Zheng et al., 2013;</ref><ref type="bibr" target="#b21">Hajishirzi et al., 2013)</ref>. Our work is orthogonal to this line of work, but domain constraints and rules can be incorporated into our model as done in ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setup</head><p>Coreference resolution is a structured pre- diction problem where the set of mentions m 1 , m 2 , · · · , m D extracted from a document cor-reponds to a structured input x and the structured output y corresponds to a partition of the men- tions into a set of clusters C 1 , C 2 , · · · , C k . Each mention m i belongs to exactly one of the clusters C j . We are provided with a training set of input- output pairs drawn from an unknown distribution D, and the goal is to return a function/predictor from inputs to outputs. The learned predictor is evaluated against a non-negative loss function L : X × Y × Y → + , L(x, y , y) is the loss asso- ciated with predicting incorrect output y for input x when the true output is y (e.g., B-Cubed Score).</p><p>In this work, we formulate the coreference resolution problem in a search-based framework. There are three key elements in this framework: 1) the Search space S p whose states correspond to partial clustering outputs; 2) the Action prun- ing function F prune that is used to prune irrelevant actions at each state; and 3) the Action scoring function F score that is used to construct a com- plete clustering output by selecting actions from those that are left after pruning. S p is a 3-tuple I, A, T , where I is the initial state function, A gives the set of possible actions in a given state, and T is a predicate which is true for terminal s- tates. In our case, s 0 = I(x) corresponds to a s- tate where every mention is unresolved, and A(s i ) consists of actions to place the next mention m i+1 in each cluster in s i or a NEW action which creates a new cluster for it. Terminal nodes correspond to states with all mentions resolved.</p><p>We focus on greedy search. The decision pro- cess for constructing an output corresponds to s- electing a sequence of actions leading from the initial state to a terminal state using both F prune and F score , which are parameterized functions over state-action pairs (F prune (φ 1 (s, a)) ∈ and F score (φ 2 (s, a)) ∈ ), where φ 1 and φ 2 stand for feature functions. We want to learn the parameters of both F prune and F score such that the predicted outputs on unseen inputs have low expected loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Greedy Prune-and-Score Approach</head><p>Our greedy Prune-and-Score approach for coref- erence resolution is parameterized by a pruning function F prune : S × A → , a scoring func- tion F score : S × A → , and a pruning param- eter b ∈ [1, A max ], where A max is the maximum number of actions at any state s ∈ S. Given a set of input mentions m 1 , m 2 , · · · , m D extracted from a document (input x), and a pruning param- Algorithm 1 Greedy Prune-and-Score Resolver</p><formula xml:id="formula_0">Input: x = set of mentions m 1 , m 2 , · · · , m D from a document D, I, A, T = Search space defini- tion, F prune = learned pruning function, b = prun- ing parameter, F score = learned scoring function 1: s ← I(x) // initial state 2: while not T (s) do 3: A ← Top b actions from A(s) according to F prune // prune 4: a p ← arg max a∈A F score (s, a) // score 5:</formula><p>s ← Apply a p on s 6: end while 7: return coreference output corresponding to s eter b, our Prune-and-Score approach makes pre- dictions as follows. The search starts at the ini- tial state s 0 = I(x) (see Algorithm 1). At each non-terminal state s, the pruning function F prune retains only the top b actions (A ) from A(s) (Step 3), and the scoring function F score picks the best scoring action a p ∈ A (Step 4) to reach the next state. When a terminal state is reached its con- tents are returned as the prediction. <ref type="figure" target="#fig_1">Figure 1</ref> illus- trates the decision-making process of our Prune- and-Score approach for an example state.</p><p>We now formalize the learning objective of our Prune-and-Score approach. LetˆyLetˆ Letˆy be the predicted coreference output for a coreference input-output pair (x, y * ). The expected loss of the greedy Prune-and-Score approach E(F prune , F score ) for a given pruning function F prune and scoring func- tion F score can be defined as follows.</p><formula xml:id="formula_1">E(F prune , F score ) = E (x,y * )∼D L (x, ˆ y, y * )</formula><p>Our goal is to learn an optimal pair of pruning and scoring functions F o prune , F o score that min- imizes the expected loss of the Prune-and-Score approach. The behavior of our Prune-and-Score approach depends on the pruning parameter b, which dictates the workload of pruning and scor- ing functions. For small values of b (aggressive pruning), pruning function learning may be harder, but scoring function learning will be easier. Simi- larly, for large values of b (conservative pruning), scoring function learning becomes hard, but prun- ing function learning is easy. Therefore, we would expect beneficial behavior if pruning function can aggressively prune (small values of b) with little loss in accuracy. It is interesting to note that our Prune-and-Score approach degenerates to existing incremental approaches that use only the scoring function for search <ref type="bibr" target="#b9">(Daumé III, 2006</ref>; Rahman and (a) Text with input set of mentions Ramallah ( West Bank 2 ) 1 10-15 ( AFP 3 ) -Eyewitnesses 4 reported that Palestinians 5 demonstrated today Sunday in the West Bank 6 against the Sharm el-Sheikh 7 summit to be held in Egypt 8 tomorrow Monday. In Ramallah 9 , around 500 people 10 took to the town 11 's streets chanting slogans denouncing the summit ... State:  Analysis of Representational Power. The fol- lowing proposition formalizes the intuition that t- wo functions are strictly better than one in expres- sive power. See Appendix for the proof.</p><formula xml:id="formula_2">s = {C 1 , C 2 , C 3 , C 4 , C 5 , C 6 } Actions: A(s) = {a 1 , a 2 , a</formula><p>Proposition 1. Let F prune and F score be func- tions from the same function space. Then for all learning problems,</p><formula xml:id="formula_3">min Fscore E(F score , F score ) ≥ min (Fprune,Fscore) E(F prune , F score ).</formula><p>More- over there exist learning problems for which min Fscore E(F score , F score ) can be arbitrarily worse than min (Fprune,Fscore) E(F prune , F score ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Algorithms</head><p>In general, learning the optimal F o prune , F o score pair can be intractable due to their potential inter- dependence. Specifically, when learning F prune in the worst case there can be ambiguity about which of the non-optimal actions to retain, and for only some of those an effective F score can be found. However, we observe a loss decomposi- tion in terms of the individual losses due to F prune and F score , and develop a stage-wise learning ap- proach that first learns F prune and then learns a corresponding F score .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Loss Decomposition</head><p>The overall loss of the Prune-and-Score approach E (F prune , F score ) can be decomposed into prun- ing loss prune , the loss due to F prune not be- ing able to retain the optimal terminal state in the search space; and scoring loss score|Fprune , the additional loss due to F score not guiding the greedy search to the best terminal state after prun- ing using F prune . Below, we will define these losses more formally.</p><p>Pruning Loss is defined as the expected loss of the Prune-and-Score approach when we perform greedy search with F prune and F * score , the opti- mal scoring function. A scoring function is said to be optimal if at every state s in the search space S p , and for any set of remaining actions A(s), it can score each action a ∈ A(s) such that greedy search can reach the best terminal state (as eval- uated by task loss function L) that is reachable from s through A(s). Unfortunately, computing the optimal scoring function is highly intractable for the non-decomposable loss functions that are employed in coreference resolution (e.g., B-Cubed F1). The main difficulty is that the decision at any one state has interdependencies with future deci- sions (see Section 5.5 in <ref type="bibr" target="#b9">(Daumé III, 2006</ref>) for more details). So we need to resort to some form of approximate optimal scoring function that ex- hibits the intended behavior. This is very similar to the dynamic oracle concept developed for de- pendency parsing <ref type="bibr" target="#b19">(Goldberg and Nivre, 2013)</ref>.</p><p>Let y * prune be the coreference output corre- sponding to the terminal state reached from input x by Prune-and-Score approach when performing search using F prune and F * score . Then the pruning loss can be expressed as follows.</p><formula xml:id="formula_4">prune = E (x,y * )∼D L x, y * prune , y *</formula><p>Scoring Loss is defined as the additional loss due to F score not guiding the greedy search to the best terminal state reachable via the pruning function F score (i.e., y * prune ). LetˆyLetˆ Letˆy be the coreference out- put corresponding to the terminal state reached by Prune-and-Score approach by performing search with F prune and F score for an input x. Then the scoring loss can be expressed as follows:</p><formula xml:id="formula_5">score|Fprune = E (x,y * )∼D L (x, ˆ y, y * ) − L x, y * prune , y *</formula><p>The overall loss decomposition of our Prune-and- Score approach can be expressed as follows.</p><formula xml:id="formula_6">E (F prune , F score ) = E (x,y * )∼D L x, y * prune , y * prune + E (x,y * )∼D L (x, ˆ y, y * ) − L x, y * prune , y * score|Fprune</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Stage-wise Learning</head><p>The loss decomposition motivates a learning ap- proach that targets minimizing the errors of prun- ing and scoring functions independently. In par- ticular, we optimize the overall loss of the Prune- and-Score approach in a stage-wise manner. We first train a pruning functionˆFfunctionˆ functionˆF prune to optimize the pruning loss component prune and then train a scoring functionˆFfunctionˆ functionˆF score to optimize the scoring loss score|ˆFprunescore|ˆ score|ˆFprune conditioned onˆFonˆ onˆF prune .</p><p>ˆ F prune ≈ arg min Fprune∈Fp prunêprunê F score ≈ arg min Fscore∈Fs score|ˆFprunescore|ˆ score|ˆFprune Note that this approach is myopic in the sense thatˆF thatˆ thatˆF prune is learned without considering the impli- cations for learningˆFlearningˆ learningˆF score . Below, we first de- scribe our approach for pruning function learning, and then explain our scoring function learning al- gorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pruning Function Learning</head><p>In our greedy Prune-and-Score approach, the role of the pruning function F prune is to prune away irrelevant actions (as specified by the pruning pa- rameter b) at each search step. More specifically, we want F prune to score actions A(s) at each s- tate s such that the optimal action a * ∈ A(s) is ranked within the top b actions to minimize prune . For this, we assume that for any training input- output pair (x, y * ) there exists a unique action se- quence, or solution path (initial state to terminal state), for producing y * from x. More formally, let While we can employ an online-LaSO style ap- proach <ref type="bibr" target="#b23">(III and Marcu, 2005;</ref><ref type="bibr" target="#b47">Xu et al., 2009</ref>) to learn the parameters of the pruning function, it is quite inefficient, as it must regenerate the same search trajectory again and again until it learn- s to make the right decision. Additionally, this approach limits applicability of the off-the-shelf learners to learn the parameters of F prune . To overcome these drawbacks, we apply offline train- ing.</p><formula xml:id="formula_7">(s * 0 , a * 0 ), (s * 1 , a * 1 ), · · · , (s * D , ∅) correspond</formula><p>Reduction to Rank Learning. We reduce the pruning function learning to a rank learning prob- lem. This allows us to leverage powerful and effi- cient off-the-shelf rank-learners ( <ref type="bibr" target="#b28">Liu, 2009</ref>). The reduction is as follows. At each state s * i on the so- lution path of a training example (x, y * ), we create an example by labeling optimal action a * i ∈ A(s * i ) as the only relevant action, and then try to learn a ranking function that can rank actions such that the relevant action a * i is in the top b actions, where b is the input pruning paramter. In other word- s, we have a rank learning problem, where the learner's goal is to optimize the Precision at Top- b. The training approach creates such an exam- ple for each state s in the solution path. The set of aggregate imitation examples collected over al- l the training data is then given to a rank learner (e.g., <ref type="bibr">LambdaMART (Burges, 2010)</ref>) to learn the parameters of F prune by optimizing the Precision at Top-b loss. See appendix for the pseudocode.</p><p>If we can learn a function F prune that is con- sistent with these imitation examples, then the learned pruning function is guaranteed to keep the solution path within the pruned space for al- l the training examples. We can also employ more advanced imitation learning algorithms in- cluding DAgger <ref type="bibr" target="#b39">(Ross et al., 2011</ref>) and SEARN <ref type="bibr" target="#b22">(Hal Daumé III et al., 2009</ref>) if we are provid- ed with an (approximate) optimal scoring function F * score that can pick optimal actions at states that are not in the solution path (i.e., off-trajectory s- tates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Scoring Function Learning</head><p>Given a learned pruning function F prune , we want to learn a scoring function that can pick the best action from the b actions that remain after prun- ing at each state. We formulate this problem in the framework of imitation learning <ref type="bibr" target="#b24">(Khardon, 1999)</ref>.</p><formula xml:id="formula_8">More formally, let (ˆ s 0 , a * 0 ), (ˆ s 1 , a * 1 ), · · · , (ˆ s * D , ∅)</formula><p>correspond to the sequence of state-action pairs along the greedy trajectory obtained by running the Prune-and-Score approach with F prune and F * score , the optimal scoring function, on a train- ing example (x, y * ), wherê s * D is the best terminal state in the pruned space. The goal of our imita- tion training approach is to learn the parameters of F score such that at each statê s i , a * i ∈ A is ranked higher than all other actions in A , where A ⊆ A(ˆ s i ) is the set of b actions that remain after pruning.</p><p>It is important to note that the distribution of states in the pruned space due to F prune on the testing data may be somewhat different from those on training data. Therefore, we train our scoring function via cross-validation by training the scor- ing function on heldout data that was not used to train the pruning function. This methodology is commonly employed in Re-Ranking and Stacking approaches <ref type="bibr">(Collins, 2000;</ref><ref type="bibr" target="#b7">Cohen and de Carvalho, 2005</ref>).</p><p>Our scoring function learning procedure uses cross validation and consists of the following four steps. First, we divide the training data D in- to k folds. Second, we learn k different pruners, where each pruning function F i prune is learned us- ing the data from all the folds excluding the i th fold. Third, we generate ranking examples for scoring function learning as described above us- ing each pruning function F i prune on the data it was not trained on. Finally, we give the aggregate set of ranking examples R to a rank learner (e.g., SVM-Rank or LambdaMART) to learn the scoring function F score . See appendix for the pseudocode.</p><p>Approximate Optimal Scoring Function. If the learned pruning function is not consistent with the training data, we will encounter statesˆsstatesˆ statesˆs i that are not on the target path, and we will need some su- pervision for learning in those cases. As discussed before in Section 5.1, computing an optimal scor- ing function F * score is intractable for combinatorial loss functions that are used for coreference resolu- tion. So we employ an approximate function from existing work that is amenable to evaluate partial outputs <ref type="bibr" target="#b9">(Daumé III, 2006</ref>). It is a variant of the ACE scoring function that removes the bipartite matching step from the ACE metric. Moreover this score is computed only on the partial coref- erence output corresponding to the "after state" s resulting from taking action a in state s, i.e., F * score (s, a) = F * score (s ). To further simplify the computation, we give uniform weight to the three types of costs: 1) Credit for correct linking, 2) Penalty for incorrect linking, and 3) Penalty for missing links. Intuitively, this is similar to the correct-link count computed only on a subgraph. We direct the reader to <ref type="bibr" target="#b9">(Daumé III, 2006</ref>) for more details (see Section 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Results</head><p>In this section, we evaluate our greedy Prune- and-Score approach on three benchmark corpora -OntoNotes 5.0 (Pradhan et al., 2012), ACE 2004 <ref type="bibr" target="#b33">(NIST, 2004)</ref>, and MUC6 (MUC6, 1995) -and compare it against the state-of-the-art approaches for coreference resolution. For OntoNotes data, we report the results on both gold mentions and predicted mentions. We also report the results on gold mentions for ACE 2004 and MUC6 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Datasets. For OntoNotes corpus, we employ the official split for training, validation, and testing. There are 2802 documents in the training set; 343 documents in the validation set; and 345 docu- ments in the testing set. The ACE 2004 corpus contains 443 documents. We follow the <ref type="bibr" target="#b8">(Culotta et al., 2007;</ref><ref type="bibr" target="#b1">Bengtson and Roth, 2008</ref>) split in our experiments by employing 268 documents for training, 68 documents for validation, and 107 documents (ACE2004-CULOTTA-TEST) for test- ing. We also evaluate our system on the 128 newswire documents in ACE 2004 corpus for a fair comparison with the state-of-the-art. The MUC6 corpus containts 255 documents. We em- ploy the official test set of 30 documents (MUC6- TEST) for testing purposes. From the remaining 225 documents, which includes 195 official train- ing documents and 30 dry-run test documents, we randomly pick 30 documents for validation, and use the remaining ones for training.</p><p>Evaluation Metrics. We compute three most pop- ular performance metrics for coreference resolu- tion: MUC ( <ref type="bibr" target="#b43">Vilain et al., 1995)</ref>, B-Cubed ( <ref type="bibr" target="#b0">Bagga and Baldwin, 1998)</ref>, and Entity-based CEAF (CEAF φ4 ) ( <ref type="bibr" target="#b29">Luo, 2005)</ref>. As it is commonly done in CoNLL shared tasks <ref type="bibr" target="#b35">(Pradhan et al., 2012</ref>), we employ the average F1 score (CoNLL F1) of these three metrics for comparison purposes. We evalu- ate all the results using the updated version 1 (7.0) of the coreference scorer.</p><p>Features. We built 2 our coreference resolver based on the Easy-first coreference system <ref type="bibr" target="#b41">(Stoyanov and Eisner, 2012)</ref>, which is derived from the Reconcile system ( <ref type="bibr" target="#b42">Stoyanov et al., 2010)</ref>. We es- sentially employ the same features as in the Easy- first system. However, we provide some high- level details that are necessary for subsequent dis- cussion. Recall that our features φ(s, a) for both pruning and scoring functions are defined over state-action pairs, where each state s consists of a set of clusters and an action a corresponds to merging an unprocessed mention m with a clus- ter C in state s or create one for itself. Therefore, φ(s, a) defines features over cluster-mention pairs (C, m). Our feature vector consists of three part- s: a) mention pair features; b) entity pair features; and c) a single indicator feature to represent NEW action (i.e., mention m starts its own cluster). For mention pair features, we average the pair-wise features over all links between m and every men- tion m c in cluster C (often referred to as average- link). Note that, we cannot employ the best-link feature representation because we perform offline training and do not have weights for scoring the links. For entity pair features, we treat mention m as a singleton entity and compute features by pairing it with the entity represented by cluster C (exactly as in the Easy-first system). The indica- tor feature will be 1 for the NEW action and 0 for all other actions.We have a total of 140 features: 90 mention pair features; 49 entity pair features; and one NEW indicator feature. We believe that our approach can benefit from employing features of the mention for the NEW action <ref type="bibr" target="#b37">(Rahman and Ng, 2011b;</ref>. However, we were constrained by the Reconcile system and could not leverage these features for the NEW ac- tion.</p><p>Base Rank-Learner. Our pruning and scoring function learning algorithms need a base rank- learner. We employ LambdaMART <ref type="bibr" target="#b3">(Burges, 2010)</ref>, a state-of-the art rank learner from the RankLib 3 library. LambdaMART is a variant of boosted regression trees. We use a learning rate of 0.1, specify the maximum number of boost- ing iterations (or trees) as 1000 noting that its ac- tual value is automatically decided based on the validation set, and tune the number of leaves per tree based on the validation data. Once we fix the hyper-parameters of LambdaMART, we train the final model on all of the training data. Lamb- daMART uses an internal train/validation split of the input ranking examples to decide when to stop the boosting iterations. We fixed this ratio to 0.8 noting that the performance is not sensitive to this parameter. For scoring function learning, we used 5 folds for the cross-validation training.</p><p>Pruning Parameter b. The hyper-parameter b controls the amount of pruning in our Prune-and- Score approach. We perform experiments with d- ifferent values of b and pick the best value based on the performance on the validation set.</p><p>Singleton Mention Filter for OntoNotes Cor- pus. We employ the Illinois-Coref system (Chang et al., 2012) to extract system mentions for our OntoNotes experiments, and observe that the num-ber of predicted mentions is thrice the number of gold mentions. Since the training data provides the clustering supervision for only gold mentions, it is not clear how to train with the system mention- s that are not part of gold mentions. A common way of dealing with this problem is to treat all the extra system mentions as singleton clusters . Howev- er, this solution most likely will not work with our current feature representation (i.e., NEW action is represented as a single indicator feature). Recall that to predict these extra system mentions as s- ingleton clusters with our incremental clustering approach, the learned model should first predic- t a NEW action while processing these mention- s to form a temporary singleton cluster, and then refrain from merging any of the subsequent men- tions with that cluster so that it becomes a single- ton cluster in the final clustering output. Howev- er, in OntoNotes corpus, the training data does not include singleton clusters for the gold mentions. Therefore, only the large number (57%) of system mentions that are not part of gold mentions will constitute the set of singleton clusters. This leads to a highly imbalanced learning problem because our model needs to learn (the weight of the sin- gle indicator feature) to predict NEW as the best action for a large set of mentions, which will bias our model to predict large number of NEW actions during testing. As a result, we will generate many singleton clusters, which will hurt the recall of the mention detection after post-processing. There- fore, we aim to learn a singleton mention filter that will be used as a pre-processor before training and testing to overcome this problem. We would like to point out that our filter is complementary to other solutions (e.g., employing features that can discriminate a given mention to be anaphoric or not in place of our single indicator feature, or us- ing a customized loss to weight our ranking exam- ples for cost-sensitive training) .</p><p>Filter Learning. The singleton mention filter is a classifier that will label a given mention as "s- ingleton" or not. We represent each mention m in a document by averaging the mention-pair fea- tures φ(m, m ) of the k-most similar mentions (obtained by ranking all other mentions m in the document with a learned ranking function R given m) and then learn a decision-tree classifier by opti- mizing the F1 loss. We learn the mention-ranking function R by optimizing the recall of positive pairs for a given k, and employ LambdaMART as our base ranker. The hyper-parameters are tuned based on the performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We first describe the results of the learned single- ton mention filter, and then the performance of our Prune-and-Score approach with and without the filter. Next, we compare the results of our ap- proach with several state-of-the-art approaches for coreference resolution.</p><p>Singleton Mention Filter Results. <ref type="table" target="#tab_3">Table 1</ref> shows the performance of the learned singleton mention filter with k = 2 noting that the results are ro- bust for all values of k ≥ 2. As we can see, the learned filter improves the precision of the men- tion detection with only small loss in the recall of gold mentions.  Prune-and-Score Results. <ref type="table">Table 2</ref> shows the per- formance of Prune-and-Score approach with and without the singleton mention filter. We can see that the results with filter are much better than the corresponding results without the filter. These re- sults show that our approach can benefit from hav- ing a good singleton mention filter.  <ref type="table">Table 2</ref>: Performance of Prune-and-Score approach with and without the singleton mention filter, and Only- Score approach without the filter. <ref type="table">Table 3</ref> shows the performance of different con- figurations of our Prune-and-Score approach. As we can see, Prune-and-Score gives better results than the configuration where we employ only the scoring function (b = ∞) for small values of b.   <ref type="table">Table 3</ref>: Performance of Prune-and-Score approach with different values of the pruning parameter b. For b = ∞, Prune-and-Score becomes an Only-Scoring al- gorithm.</p><formula xml:id="formula_9">2122 MUC B 3 CEAF φ4 CoNLL P R F1 P R F1 P R F1 Avg-F1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a. Results on OntoNotes 5.0 Test Set with Predicted Mentions</head><p>Comparison to State-of-the-Art. <ref type="table" target="#tab_6">Table 4</ref> shows the results of our Prune-and-Score ap- proach compared with the following state-of-the- art coreference resolution approaches: HOTCoref system (Björkelund and Kuhn, 2014); Berkeley system with the FINAL feature set (Durrett and K- lein, 2013); CPL 3 M system (Chang et al., 2013); Stanford system ( <ref type="bibr" target="#b26">Lee et al., 2013)</ref>; Easy-first sys- tem <ref type="bibr" target="#b41">(Stoyanov and Eisner, 2012);</ref><ref type="bibr">and</ref><ref type="bibr">Fernandes et al., 2012 (Fernandes et al., 2012</ref>). On- ly Scoring is the special case of our Prune-and- Score approach where we employ only the scoring function. This corresponds to existing incremen- tal approaches <ref type="bibr" target="#b9">(Daumé III, 2006;</ref><ref type="bibr" target="#b37">Rahman and Ng, 2011b</ref>). We report the best published results for CPL 3 M system, Easy-first, and <ref type="bibr">Fernandes et al., 2012</ref>. We ran the publicly available software to generate the results for Berkeley and Stanford sys- tems with the updated CoNLL scorer. We include the results of Prune-and-Score for best b on the de- velopment set with singleton mention filter for the comparison. In <ref type="table" target="#tab_6">Table 4</ref>, '-' indicates that we could not find published results for those cases. We see that results of the Prune-and-Score approach are comparable to or better than the state-of-the-art in- cluding Only-Scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>We introduced the Prune-and-Score approach for greedy coreference resolution whose main idea is to learn a pruning function along with a scor- ing function to effectively guide the search. We showed that our approach improves over the meth- ods that only learn a scoring function, and gives comparable or better results than several state-of- the-art coreference resolution systems.</p><p>Our Prune-and-Score approach is a particular instantiation of the general idea of learning nearly- sound constraints for pruning, and leveraging the learned constraints to learn improved heuristic functions for guiding the search (See ( <ref type="bibr" target="#b6">Chen et al., 2014</ref>) for another instantiation of this idea for multi-object tracking in videos). Therefore, oth- er coreference resolution systems ( <ref type="bibr" target="#b2">Björkelund and Kuhn, 2014</ref>) can also benefit from this idea. One way to further improve the peformance of our approach is to perform a search in the Limited Discrepancy Search (LDS) space ( <ref type="bibr" target="#b12">Doppa et al., 2014b</ref>) using the learned functions.</p><p>Future work should apply this general idea to other natural language processing tasks including dependency parsing ( <ref type="bibr" target="#b34">Nivre et al., 2007)</ref> and in- formation extraction ( <ref type="bibr" target="#b27">Li et al., 2013)</ref>. We would expect more beneficial behavior with the prun- ing constraints for problems with large action sets (e.g., labeled dependency parsing). It would be in- teresting and useful to generalize this approach to search spaces where there are multiple target paths from the initial state to the terminal state, e.g., as in the Easy-first framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of Prune-and-Score approach. (a) Text with input set of mentions. Mentions are highlighted and numbered. (b) Illustration of decision-making process for mention m 11. The partial clustering output corresponding to the current state s consists of six clusters denoted by C 1 , C 2 , · · · , C 6. Highlighted circles correspond to the clusters. Edges from mention m 11 to each of the six clusters and to itself stand for the set of possible actions A(s) in state s, and are denoted by a 1 , a 2 , · · · , a 7. The pruning function F prune scores all the actions in A(s) and only keeps the top 3 actions A = {a 2 , a 1 , a 7 } as specified by the pruning parameter b. The scoring function picks the best scoring action a 1 ∈ A as the final decision, and mention m 11 is merged with cluster C 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 a 7 a 5 a 6 a 3 a 4 a 1 a 2 a 7 a A</head><label></label><figDesc></figDesc><table>3 , a 4 , a 5 , a 6 , a 7 } 

Pruning step: 

Scoring step: 

2.5 
2.2 
1.9 
1.5 
1.4 
0.7 
0.4 

4.5 
3.1 
2.6 

2 

a 

(s) = {a 2 , a 1 , a 7 } 
b = 3 

Decision: a 1 is the best action for state s 

F prune values 

F score values 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance of the singleton mention filter on 
the OntoNotes 5.0 development set. The numerators of 
the fractions in the brackets show the exact numbers of 
mentions that are matched with the gold mentions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Prune-and-Score 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56 Only-Scoring 75.95 61.53 67.98 63.94 47.37 54.42 58.54 49.76 53.</figDesc><table>79 
58.73 
HOTCoref 67.46 
74.3 
70.72 54.96 62.71 58.58 52.27 
59.4 
55.61 
61.63 
CPL 3 M 
-
-
69.48 
-
-
57.44 
-
-
53.07 
60.00 
Berkeley 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 
61.41 
Fernandes et al., 2012 75.91 65.83 70.51 65.19 51.55 57.58 57.28 50.82 53.86 
60.65 
Stanford 65.31 64.11 64.71 56.54 48.58 52.26 46.67 52.29 49.32 
55.43 

b. Results on OntoNotes 5.0 Test Set with Gold Mentions 
Prune-and-Score 88.10 85.85 86.96 76.82 76.16 76.49 80.90 74.06 77.33 
80.26 
Only-Scoring 86.96 84.52 85.73 74.51 74.25 74.38 79.04 70.67 74.62 
78.24 
CPL 3 M 
-
-
84.80 
-
-
78.74 
-
-
68.75 
77.43 
Berkeley 85.73 89.26 87.46 78.23 75.11 76.63 82.89 70.86 76.40 
80.16 
Stanford 89.94 78.17 83.64 81.75 68.95 74.81 73.97 61.20 66.98 
75.14 

c. Results on ACE2004 Culotta Test Set with Gold Mentions 
Prune-and-Score 85.57 72.68 78.60 90.09 77.02 83.04 74.64 86.02 79.42 
80.35 
Only-Scoring 82.75 69.25 75.40 88.54 74.22 80.75 73.69 85.22 78.58 
78.24 
CPL 3 M 
-
-
78.29 
-
-
82.20 
-
-
79.26 
79.91 
Stanford 82.91 69.90 75.85 89.14 74.05 80.90 75.67 77.45 76.55 
77.77 

d. Results on ACE2004 Newswire with Gold Mentions 
Prune-and-Score 89.72 75.72 82.13 90.89 76.15 82.87 72.43 86.83 78.69 
81.23 
Only-Scoring 86.92 76.49 81.37 88.10 75.83 81.51 73.15 84.31 78.05 
80.31 
Easy-first 
-
-
80.1 
-
-
81.8 
-
-
-
-
Stanford 84.75 75.34 79.77 87.50 74.59 80.53 73.32 81.49 77.19 
79.16 

e. Results on MUC6 Test Set with Gold Mentions 
Prune-and-Score 89.53 82.75 86.01 86.48 76.18 81.00 60.74 80.33 68.68 
78.56 
Only-Scoring 86.77 80.96 83.76 81.72 72.99 77.11 57.56 75.38 64.91 
75.26 
Easy-first 
-
-
88.2 
-
-
77.5 
-
-
-
-
Stanford 91.19 69.54 78.91 91.07 63.39 74.75 62.43 69.62 65.83 
73.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of Prune-and-Score with state-of-the-art approaches. Metric values reflect version 7 of 
CoNLL scorer. 

The performance is clearly better than the degen-
erate case (b = ∞) over a wide range of b values, 
suggesting that it is not necessary to carefully tune 
the parameter b. 

Pruning param. b MUC B 3 
CEAF φ4 CoNLL 

OntoNotes 5.0 Dev Set w. Predict Ment. 
2 
69.12 56.80 56.30 
60.74 
3 
70.50 57.89 57.24 
61.88 
4 
71.00 58.65 57.41 
62.35 
5 
71.18 58.87 57.88 
62.64 
6 
70.93 58.66 57.85 
62.48 
8 
70.12 58.13 57.37 
61.87 
10 
70.24 58.34 56.27 
61.61 
20 
67.97 57.73 56.63 
60.78 
∞ 
67.03 56.31 55.56 
59.63 

</table></figure>

			<note place="foot" n="1"> http://code.google.com/p/reference-coreference-scorers/ 2 See http://research.engr.oregonstate.edu/dral/ for our software.</note>

			<note place="foot" n="3"> http://sourceforge.net/p/lemur/wiki/RankLib/</note>

			<note place="foot">Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of International Conference on Machine Learning (ICML), pages 175-182.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Authors would like to thank Veselin Stoyanov (JHU) for answering several questions related to the Easy-first and Reconcile systems; Van Dang (UMass, Amherst) for technical discussions relat-ed to the RankLib library; Kai-Wei Chang (UIUC) for the help related to the Illinois-Coref mention extractor; and Greg Durrett (UC Berkeley) for his help with the Berkeley system. This work was supported in part by NSF grants IIS 1219258, I-IS 1018490 and in part by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under Con-tract No. FA8750-13-2-0033. Any opinions, findings and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect the views of the NS-F, the DARPA, the Air Force Research Laboratory (AFRL), or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding the value of features for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bengtson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning structured perceptrons for coreference resolution with latent antecedents and non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">From RankNet to LambdaRank to LambdaMART: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burges</surname></persName>
		</author>
		<idno>MSR-TR-2010</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">Microsoft Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IllinoisCoref: The UI system in the CoNLL-2012 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Conference on EMNLP and CoNLL-Shared Task</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="113" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A constrained latent variable model for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="601" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-object tracking via constrained sequential labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To appear in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stacked sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocha De Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="671" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">First-order probabilistic models for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</title>
		<meeting>Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Practical Structured Learning Techniques for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>University of Southern California, Los Angeles, CA</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Specialized models and ranking for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="660" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">HC-Search: A learning framework for search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="369" to="407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured prediction via output space search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1317" to="1350" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HC-Search for multilabel prediction: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decentralized entity-level modeling for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Leo Wright</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguistics (ACL) Conference</title>
		<meeting>Association of Computational Linguistics (ACL) Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="114" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The generalized A* architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="153" to="190" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eraldo Rezende Fernandes</surname></persName>
		</author>
		<title level="m">Cícero Nogueira dos Santos, and Ruy Luiz Milidiú. 2012. Latent structure perceptron with feature induction for unrestricted coreference resolution. International Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coreference resolution in a modular, entity-centered model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</title>
		<meeting>Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint coreference resolution and named-entity linking with multipass sieves</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="289" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal (MLJ)</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="325" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to take actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roni</forename><surname>Khardon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal (MLJ)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="90" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to detect basal tubules of nematocysts in sem images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>Reft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marymegan</forename><surname>Daly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop on Computer Vision for Accelerated Biosciences (CVAB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deterministic coreference resolution based on entity-centric, precision-ranked rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Peirsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="885" to="916" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Toward conditional models of identity uncertainty with application to proper noun coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wellner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems (NIPS)</title>
		<meeting>Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coreference task definition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muc6</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Message Understanding Conference (MUC-6)</title>
		<meeting>the Sixth Message Understanding Conference (MUC-6)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving machine learning approaches to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguistics (ACL) Conference</title>
		<meeting>Association of Computational Linguistics (ACL) Conference</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The ACE evaluation plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Maltparser: A language-independent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task</title>
		<meeting>the Joint Conference on EMNLP and CoNLL: Shared Task</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coreference resolution with world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguistics (ACL) Conference</title>
		<meeting>Association of Computational Linguistics (ACL) Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="814" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Narrowing the modeling gap: A cluster-ranking approach to coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research (JAIR)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="469" to="521" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learningbased multi-sieve co-reference resolution with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP) Conference</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP) Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research-Proceedings Track</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A machine learning approach to coreference resolution of noun phrases</title>
		<editor>Wee Meng Soon, Daniel Chung, Daniel Chung Yong Lim, Yong Lim, and Hwee Tou Ng</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Easy-first coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics (COLING)</title>
		<meeting>International Conference on Computational Linguistics (COLING)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2519" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coreference resolution with reconcile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hysom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguistics (ACL) Conference</title>
		<meeting>Association of Computational Linguistics (ACL) Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="156" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A model-theoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">B</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Aberdeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynette</forename><surname>Hirschman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MUC</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured prediction cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research-Proceedings Track</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="916" to="923" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SampleRank: Training factor graphs with atomic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Rohanimanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Bellare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A discriminative hierarchical model for fast coreference at large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association of Computational Linguistics (ACL) Conference</title>
		<meeting>Association of Computational Linguistics (ACL) Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="379" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning linear ranking functions for beam search with application to planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Wook</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1571" to="1610" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning structural SVMs with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic knowledge-base alignment for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
