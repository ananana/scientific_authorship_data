<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-Resource Translation with Multi-Lingual Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
							<email>orhan.firat@ceng.metu.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">IBM T.J. Watson Research Center</orgName>
								<orgName type="laboratory" key="lab2">IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Middle East Technical University</orgName>
								<orgName type="institution" key="instit2">Middle East Technical University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">IBM T.J. Watson Research Center</orgName>
								<orgName type="laboratory" key="lab2">IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Middle East Technical University</orgName>
								<orgName type="institution" key="instit2">Middle East Technical University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">IBM T.J. Watson Research Center</orgName>
								<orgName type="laboratory" key="lab2">IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Middle East Technical University</orgName>
								<orgName type="institution" key="instit2">Middle East Technical University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatos</forename><forename type="middle">T</forename><surname>Yarman Vural</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">IBM T.J. Watson Research Center</orgName>
								<orgName type="laboratory" key="lab2">IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Middle East Technical University</orgName>
								<orgName type="institution" key="instit2">Middle East Technical University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">IBM T.J. Watson Research Center</orgName>
								<orgName type="laboratory" key="lab2">IBM T.J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Middle East Technical University</orgName>
								<orgName type="institution" key="instit2">Middle East Technical University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-Resource Translation with Multi-Lingual Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="268" to="277"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, multilingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A recently introduced neural machine transla- tion ( <ref type="bibr">Forcada and˜Necoand˜ and˜Neco, 1997;</ref><ref type="bibr" target="#b9">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Cho et al., 2014</ref>) has proven to be a platform for new opportu- nities in machine translation research. Rather than word-level translation with language-specific pre- processing, neural machine translation has found to work well with statistically segmented subword se- quences as well as sequences of characters ( <ref type="bibr" target="#b3">Chung et al., 2016;</ref><ref type="bibr" target="#b13">Luong and Manning, 2016;</ref><ref type="bibr" target="#b22">Sennrich et al., 2015b;</ref><ref type="bibr" target="#b12">Ling et al., 2015)</ref>. Also, recent works show that neural machine translation provides a seamless way to incorporate multiple modalities Work carried out while the author was at IBM Research.</p><p>other than natural language text in translation ( <ref type="bibr" target="#b14">Luong et al., 2015a;</ref><ref type="bibr" target="#b1">Caglayan et al., 2016)</ref>. Further- more, neural machine translation has been found to translate between multiple languages, achieving better translation quality by exploiting positive lan- guage transfer ( <ref type="bibr" target="#b4">Dong et al., 2015;</ref><ref type="bibr" target="#b27">Zoph and Knight, 2016)</ref>.</p><p>In this paper, we conduct in-depth investiga- tion into the recently proposed multi-way, multilin- gual neural machine translation . Specifically, we are interested in its potential for zero-resource machine translation, in which there does not exist any direct parallel examples between a target language pair. Zero-resource translation has been addressed by pivot-based translation in tradi- tional machine translation research ( <ref type="bibr" target="#b25">Wu and Wang, 2007;</ref><ref type="bibr" target="#b24">Utiyama and Isahara, 2007;</ref><ref type="bibr">Habash and Hu, 2009</ref>), but we explore a way to use the multi-way, multilingual neural model to translate directly from a source to target language.</p><p>In doing so, we begin by studying different trans- lation strategies available in the multi-way, multi- lingual model in Sec. 3-4. The strategies include a usual one-to-one translation as well as variants of many-to-one translation for multi-source transla- tion ( <ref type="bibr" target="#b27">Zoph and Knight, 2016)</ref>. We empirically show that the many-to-one strategies significantly outper- form the one-to-one strategy.</p><p>We move on to zero-resource translation by first evaluating a vanilla multi-way, multilingual model on a zero-resource language pair, which revealed that the vanilla model cannot do zero-resource trans- lation in Sec. 6.1. Based on the many-to-one strate- gies we proposed earlier, we design a novel finetun-ing strategy that does not require any direct paral- lel corpus between a target, zero-resource language pair in Sec. 5.2, which uses the idea of generating a pseudo-parallel corpus <ref type="bibr" target="#b21">(Sennrich et al., 2015a</ref>). This strategy makes an additional copy of the attention mechanism and finetunes only this small set of pa- rameters.</p><p>Large-scale experiments with Spanish, French and English show that the proposed finetuning strat- egy allows the multi-way, multilingual neural trans- lation model to perform zero-resource translation as well as a single-pair neural translation model trained with up to 1M true parallel sentences. This result re-confirms the potential of the multi-way, multilin- gual model for low/zero-resource language transla- tion, which was earlier argued by .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-Way, Multilingual Neural Machine Translation</head><p>Recently  proposed an extension of attention-based neural machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015</ref>) that can handle multi-way, mul- tilingual translation with a shared attention mech- anism. This model was designed to handle multi- ple source and target languages. In this section, we briefly overview this multi-way, multilingual model. For more detailed exposition, we refer the reader to ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model Description</head><p>The goal of multi-way, multilingual model is to build a neural translation model that can translate a source sentence given in one of N languages into one of M target languages. Thus to handle those N source and M target languages, the model con- sists of N encoders and M decoders. Unlike these language-specific encoders and decoders, only a sin- gle attention mechanism is shared across all M × N language pairs.</p><p>Encoder An encoder for the n-th source language reads a source sentence X = (x 1 , . . . , x Tx ) as a sequence of linguistic symbols and returns a set of context vectors</p><formula xml:id="formula_0">C n = h n 1 , . . . , h n Tx .</formula><p>The encoder is usually implemented as a bidirectional recurrent network <ref type="bibr" target="#b20">(Schuster and Paliwal, 1997)</ref>, and each con- text vector h n t is a concatenation of the forward and reverse recurrent networks' hidden states at time t.</p><p>Without loss of generality, we assume that the di- mensionalities of the context vector for all source languages are all same.</p><p>Decoder and Attention Mechanism A decoder for the m-th target language is a conditional recur- rent language model ( <ref type="bibr">Mikolov et al., 2010)</ref>. At each time step t , it updates its hidden state by</p><formula xml:id="formula_1">z m t = ϕ m (z m t −1 , ˜ y m t −1 , c m t ),</formula><p>based on the previous hidden state z m t −1 , previous target symbol˜ysymbol˜ symbol˜y m t −1 and the time-dependent context vector c m t . ϕ m is a gated recurrent unit (GRU, ( <ref type="bibr" target="#b2">Cho et al., 2014)</ref>).</p><p>The time-dependent context vector is computed by the shared attention mechanism as a weighted sum of the context vectors from the encoder C n :</p><formula xml:id="formula_2">c m t = U Tx t=1 α m,n t,t h n t + b,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">α m,n t,t ∝ exp f score (W n h n t , W m z m t −1 , ˜ y m t −1 ) .<label>(2)</label></formula><p>The scoring function f score returns a scalar and is im- plemented as a feedforward neural network with a single hidden layer. For more variants of the atten- tion mechanism for machine translation, see <ref type="bibr" target="#b15">(Luong et al., 2015b</ref>). The initial hidden state of the decoder is initial- ized as</p><formula xml:id="formula_4">z m 0 = φ m init (W n h n t ).<label>(3)</label></formula><p>With the new hidden state z m t , the probability dis- tribution over the next symbol is computed by</p><formula xml:id="formula_5">p(y t = w|˜yw|˜y &lt;t , X n ) ∝ exp(g m w (z m t , c m t , E m y [˜ y t−1 ]),<label>(4)</label></formula><p>where g m w is a decoder specific parametric func- tion that returns the unnormalized probability for the next target symbol being w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning</head><p>Training this multi-way, multilingual model does not require multi-way parallel corpora but only a set of bilingual corpora. For each bilingual pair, the conditional log-probability of a ground-truth transla- tion given a source sentence is maximize by adjust- ing the relevant parameters following the gradient of the log-probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Translation Strategies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">One-to-One Translation</head><p>In the original paper by , only one translation strategy was evaluated, that is, one-to- one translation. This one-to-one strategy works on a source sentence given in one language by taking the encoder of that source language, the decoder of a target language and the shared attention mecha- nism. These three components are glued together as if they form a single-pair neural translation model and translates the source sentence into a target lan- guage.</p><p>We however notice that this is not the only transla- tion strategy available with the multi-way, multilin- gual model. As we end up with multiple encoders, multiple decoders and a shared attention mecha- nism, this model naturally enables us to exploit a source sentence given in multiple languages, lead- ing to a many-to-one translation strategy which was proposed recently by <ref type="bibr" target="#b27">Zoph and Knight (2016)</ref> in the context of neural machine translation.</p><p>Unlike <ref type="bibr" target="#b27">(Zoph and Knight, 2016)</ref>, the multi-way, multilingual model is not trained with multi-way parallel corpora. This however does not necessar- ily imply that the model cannot be used in this way. In the remainder of this section, we propose two alternatives for doing multi-source translation with the multi-way, multilingual model, which eventually pave the way towards zero-resource translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Many-to-One Translation</head><p>In this section, we consider a case where a source sentence is given in two languages, X 1 and X 2 . However, any of the approaches described below ap- plies to more than two source languages trivially.</p><p>In this multi-way, multilingual model, multi- source translation can be thought of as averaging two separate translation paths. For instance, in the case of Es+Fr to En, we want to combine Es→En and Fr→En so as to get a better English translation. We notice that there are two points in the multi-way, multilingual model where this averaging may hap- pen.</p><p>Early Average The first candidate is to averag- ing two translation paths when computing the time- dependent context vector (see Eq. (1).) At each time t in the decoder, we compute a time-dependent con- text vector for each source language, c 1 t and c 2 t re- spectively for the two source languages. In this early averaging strategy, we simply take the average of these two context vectors:</p><formula xml:id="formula_6">c t = c 1 t + c 2 t 2 .<label>(5)</label></formula><p>Similarly, we initialize the decoder's hidden state to be the average of the initializers of the two encoders:</p><formula xml:id="formula_7">z 0 = 1 2 φ init (φ 1 init (h 1 Tx 1 )) + φ init (φ 2 init (h 2 Tx 1 )) ,<label>(6)</label></formula><p>where φ init is the decoder's initializer (see Eq. (3).)</p><p>Late Average Alternatively, we can average those two translation paths (e.g., Es→En and Fr→En) at the output level. At each time t, each translation path computes the distribution over the target vocabulary, i.e., p(y t = w|y &lt;t , X 1 ) and p(y t = w|y &lt;t , X 2 ). We then average them to get the multi-source output dis- tribution:</p><formula xml:id="formula_8">p(y t = w|y &lt;t , X 1 , X 2 ) = (7) 1 2 (p(y t = w|y &lt;t , X 1 ) + p(y t = w|y &lt;t )).</formula><p>An advantage of this late averaging strategy over the early averaging one is that this can work even when those two translation paths were not from a single multilingual model. They can be two separately trained single-pair models. In fact, if X 1 and X 2 are same and the two translation paths are simply two different models trained on the same language pair- direction, this is equivalent to constructing an en- semble, which was found to greatly improve transla- tion quality (Sutskever et al., 2014; Jean et al., 2015)</p><p>Early+Late Average The two strategies above can be further combined by late-averaging the out- put distributions from the early averaged model and the late averaged one. We empirically evaluate this early+late average strategy as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>270</head><p>Before continuing on with zero-resource machine translation, we first evaluate the translation strate- gies described in the previous section on multi- source translation, as these translation strategies form a basic foundation on which we extend the multi-way, multilingual model for zero-resource machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Settings</head><p>When evaluating the multi-source translation strate- gies, we use English, Spanish and French, and focus on a scenario where only En-Es and En-Fr parallel corpora are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Corpora</head><p>En-Es We combine the following corpora to form 34.71m parallel Es-En sentence pairs: UN (8.8m), Europarl-v7 (1.8m), news-commentary-v7 (150k), LDC2011T07-T12 (2.9m) and internal technical- domain data (21.7m).</p><p>En-Fr We combine the following corpora to form 65.77m parallel En-Fr sentence pairs: UN (9.7m), Europarl-v7 (1.9m), news-commentary-v7 (1.2m), LDC2011T07-T10 (1.6m), ReutersUN (4.5m), in- ternal technical-domain data (23.5m) and Gigaword R2 (20.66m).</p><p>Evaluation Sets We use newstest-2012 and newstest-2013 from WMT as development and test sets, respectively.</p><p>Monolingual Corpora We do not use any addi- tional monolingual corpus.</p><p>Preprocessing All the sentences are tokenized us- ing the tokenizer script from Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007)</ref>. We then replace special tokens, such as numbers, dates and URL's with predefined markers, which will be replaced back with the original to- kens after decoding. After using byte pair encoding <ref type="bibr">(BPE, (Sennrich et al., 2015b)</ref>) to get subword sym- bols, we end up with 37k, 43k and 45k unique tokens for English, Spanish and French, respectively. For training, we only use sentence pairs in which both sentences are only up to 50 symbols long. See <ref type="table">Table 1</ref> for the detailed statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models and Training</head><p>We start from the code made publicly available as a part of (Firat et al., 2016) 1 . We made two changes to the original code. First, we replaced the decoder with the conditional gated recurrent network with the attention mechanism as outlines in . Second, we feed a binary indicator vec- tor of which encoder(s) the source sentence was pro- cessed by to the output layer of each decoder (g m w in Eq. <ref type="formula" target="#formula_5">(4)</ref>). Each dimension of the indicator vector cor- responds to one source language, and in the case of multi-source translation, there may be more than one dimensions set to 1.</p><p>We train the following models: four single-pair models (Es↔En and Fr↔En) and one multi-way, multilingual model (Es,Fr,En↔Es,Fr,En). As pro- posed by , we share one attention mechanism for the latter case.</p><p>Training We closely follow the setup from . Each symbol is represented as a 620- dimensional vector. Any recurrent layer, be it in the encoder or decoder, consists of 1000 gated recurrent units (GRU, ( <ref type="bibr" target="#b2">Cho et al., 2014)</ref>), and the attention mechanism has a hidden layer of 1200 tanh units (f score in Eq. <ref type="formula" target="#formula_3">(2)</ref>). We use Adam ( <ref type="bibr" target="#b10">Kingma and Ba, 2015</ref>) to train a model, and the gradient at each up- date is computed using a minibatch of at most 80 sentence pairs. The gradient is clipped to have the norm of at most 1 ( <ref type="bibr" target="#b18">Pascanu et al., 2012</ref>). We early- stop any training using the T-B score on a develop- ment set 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">One-to-One Translation</head><p>We first confirm that the multi-way, multilingual translation model indeed works as well as single- pair models on the translation paths that were con- sidered during training, which was the major claim    in . In <ref type="table" target="#tab_2">Table 2</ref>, we present the re- sults on four language pair-directions (Es↔En and Fr↔En). It is clear that the multi-way, multilingual model indeed performs comparably on all the four cases with less parameters (due to the shared attention mechanism.) As observed earlier in , we also see that the multilingual model per- forms better when a target language is English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Many-to-One Translation</head><p>We consider translating from a pair of source sen- tences in Spanish (Es) and French (Fr) to English (En). It is important to note that the multilingual model was not trained with any multi-way parallel corpus. Despite this, we observe that the early aver- aging strategy improves the translation quality (mea- sured in BLEU) by 3 points in the case of the test set (compare <ref type="table" target="#tab_2">Table 2</ref> (a-b) and <ref type="table" target="#tab_3">Table 3</ref> (a).) We con- jecture that this happens as training the multilingual model has implicitly encouraged the model to find a common context vector space across multiple source languages.</p><p>The late averaging strategy however outperforms the early averaging in both cases of multilingual model and a pair of single-pair models (see Ta- ble 3 (b)) albeit marginally. The best quality was observed when the early and late averaging strate- gies were combined at the output level, achieving up to +3.5 BLEU (compare <ref type="table" target="#tab_2">Table 2 (a) and Table 3</ref> (c).)</p><p>We emphasize again that there was no multi-way parallel corpus consisting of Spanish, French and English during training 3 .</p><p>The result presented in this section shows that the multi-way, multilin- gual model can exploit multiple sources effectively without requiring any multi-way parallel corpus, and we will rely on this property together with the pro- posed many-to-one translation strategies in the later sections where we propose and investigate zero- resource translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Zero-Resource Translation Strategies</head><p>The network architecture of multi-way, multilingual model suggests the potential for translating between two languages without any direct parallel corpus available. In the setting considered in this paper (see Sec. 4.1,) these translation paths correspond to Es↔Fr, as only parallel corpora used for training were Es↔En and Fr↔En.</p><p>The most naive approach for translating along a zero-resource path is to simply treat it as any other path that was included as a part of training. This corresponds to the one-to-one strategy from Sec. 3.1.</p><p>In our experiments, it however turned out that this naive approach does not work at all, as can be seen in <ref type="table" target="#tab_5">Table 4</ref> (a).</p><p>In this section, we investigate this potential of zero-resource translation with the multi-way, mul- tilingual model in depth. More specifically, we propose a number of approaches that enable zero- resource translation without requiring any additional bilingual or multi-way corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pivot-based Translation</head><p>The first set of approaches exploits the fact that the target zero-resource translation path can be decom- posed into a sequence of high-resource translation paths ( <ref type="bibr" target="#b25">Wu and Wang, 2007;</ref><ref type="bibr" target="#b24">Utiyama and Isahara, 2007;</ref><ref type="bibr">Habash and Hu, 2009)</ref>. For instance, in our case, Es→Fr can be decomposed into a sequence of Es→En and En→Fr. In other words, we translate a source sentence (Es) into a pivot language (En) and then translate the English translation into a target language (Fr), all within the same multi-way, mul- tilingual model trained by using bilingual corpora.</p><p>One-to-One Translation The most basic ap- proach here is to perform each translation path in the decomposed sequence independently from each other. This one-to-one approach introduces only a minimal computational complexity (the multiplica- tive factor of two.) We can further improve this one- to-one pivot-based translation by maintaining a set of k-best translations from the first stage (Es→En), but this increase the overall computational complex- ity by the factor of k, making it impractical in prac- tice. We therefore focus only on the former approach of keeping the best pivot translation in this paper.</p><p>Many-to-One Translation With the multi-way, multilingual model considered in this paper, we can extend the naive one-to-one pivot-based strategy by replacing the second stage (En→Fr) to be many-to- one translation from Sec. 4.4 using both the origi- nal source language and the pivot language as a pair of source languages. We first translate the source sentence (Es) into English, and use both the original source sentence and the English translation (Es+En) to translate into the final target language (Fr).</p><p>Both approaches described and proposed above do not require any additional action on an already- trained multilingual model. They are simply differ- ent translation strategies specifically aimed at zero- resource translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Finetuning with Pseudo Parallel Corpus</head><p>The failure of the naive zero-resource translation earlier (see <ref type="table" target="#tab_5">Table 4</ref> (a)) suggests that the context vec- tors returned by the encoder are not compatible with the decoder, when the combination was not included during training. The good translation qualities of the translation paths included in training however im- ply that the representations learned by the encoders and decoders are good. Based on these two obser- vations, we conjecture that all that is needed for a zero-resource translation path is a simple adjustment that makes the context vectors from the encoder to be compatible with the target decoder. Thus, we propose to adjust this zero-resource translation path however without any additional parallel corpus.</p><p>First, we generate a small set of pseudo bilin- gual pairs of sentences for the zero-resource lan- guage pair (Es→Fr) in interest. We randomly select N sentences pairs from a parallel corpus between the target language (Fr) and a pivot language (En) and translate the pivot side (En) into the source lan- guage (Es). Then, the pivot side is discarded, and we construct a pseudo parallel corpus consisting of sen- tence pairs of the source and target languages (Es- Fr).</p><p>We make a copy of the existing attention mech- anism, to which we refer as target-specific atten- tion mechanism. We then finetune only this target- specific attention mechanism while keeping all the other parameters of the encoder and decoder intact, using the generated pseudo parallel corpus. We do not update any other parameters in the encoder and decoder, because they are already well-trained (evi- denced by high translation qualities in <ref type="table" target="#tab_2">Table 2</ref>) and we want to avoid disrupting the well-captured struc- tures underlying each language.</p><p>Once the model has been finetuned with the pseudo parallel corpus, we can use any of the trans- lation strategies described earlier in Sec. 3 for the finetuned zero-resource translation path. We ex- pect a similar gain by using many-to-one translation, which we empirically confirm in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments:</head><p>Zero-Resource Translation 6.1 Without Finetuning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Settings</head><p>We use the same multi-way, multilingual model trained earlier in Sec. 4.2 to evaluate the zero- resource translation strategies. We emphasize here that this model was trained only using Es-En and Fr-En bilingual parallel corpora without any Es-Fr parallel corpus.</p><p>We evaluate the proposed approaches to zero- resource translation with the same multi-way, multi- lingual model from Sec. 4.1. We specifically select the path from Spanish to French (Es→Fr) as a target zero-resource translation path.  </p><formula xml:id="formula_9">Pivot Many-to-1 Dev Test (a) &lt; 1 &lt; 1 (b) √ 20.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Result and Analysis</head><p>As mentioned earlier, we observed that the multi- way, multilingual model cannot directly translate between two languages when the translation path between those two languages was not included in training <ref type="table" target="#tab_5">(Table 4</ref> (a).) On the other hand, the model was able to translate decently with the pivot- based one-to-one translation strategy, as can be seen in <ref type="table" target="#tab_5">Table 4</ref> (b). Unsurprisingly, all the many-to- one strategies resulted in worse translation quality, which is due to the inclusion of the useless transla- tion path (direct path between the zero-resource pair, Es-Fr). Another interesting trend we observe is the Early+Late averaging <ref type="table" target="#tab_5">(Table 4</ref> (e)) seems to per- form worse than Late averaging <ref type="figure">(Table 4 (d)</ref>) alone, opposite of the results in <ref type="table" target="#tab_3">Table 3</ref> (b-c). We conjec- ture that, by simply averaging two model outputs (as in E+L), when one of them is drastically worse than the other, has the effect of pulling down the perfor- mance of final results. But early averaging can still recover from this deficiency, upto some extent, since the decoder output probability function g m w (Eq. (4).) is a smooth function not only using the averaged context vectors <ref type="bibr">(Eq. (5)</ref>.).</p><p>These results clearly indicate that the multi-way, multilingual model trained with only bilingual par- allel corpora is not capable of direct zero-resource translation as it is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Finetuning with a Pseudo Parallel Corpus</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Settings</head><p>The proposed finetuning strategy raises a number of questions. First, it is unclear how many pseudo sentence pairs are needed to achieve a decent trans- lation quality. Because the purpose of this finetuning stage is simply to adjust the shared attention mecha- nism so that it can properly bridge from the source- side encoder to the target-side decoder, we expect it to work with only a small amount of pseudo pairs. We validate this by creating pseudo corpora of dif- ferent sizes-1k, 10k, 100k and 1m.</p><p>Second, we want to know how detrimental it is to use the generated pseudo sentence pairs compared to using true sentence pairs between the target language pair.</p><p>In order to answer this question, we compiled a true multi-way par- allel corpus by combining the subsets of UN (7.8m), Europarl-v7 (1.8m), <ref type="bibr">OpenSubtitles-2013 (1m)</ref>, news-commentary-v7 (174k), LDC2011T07 (335k) and news-crawl (310k), and use it to finetune the model 4 . This allows us to evaluate the effect of the pseudo and true parallel corpora on finetuning for zero-resource translation.</p><p>Lastly, we train single-pair models translating di- rectly from Spanish to French by using the true par- allel corpora. These models work as a baseline against which we compare the multi-way, multilin- gual models.</p><p>Training Unlike the usual training procedure de- scribed in Sec. 4.2, we compute the gradient for each update using 60 sentence pairs only, when finetuning the model with the multi-way parallel corpus (either pseudo or true.) <ref type="table" target="#tab_7">Table 5</ref> summarizes all the result. The most im- portant observation is that the proposed finetuning strategy with pseudo-parallel sentence pairs outper- forms the pivot-based approach (using the early av- eraging strategy from Sec. 4.4) even when we used only 10k such pairs (compare (b) and (d).) As we in- crease the size of the pseudo-parallel corpus, we ob- serve a clear improvement. Furthermore, these mod- els perform comparably to or better than the single- pair model trained with 1M true parallel sentence pairs, although they never saw a single true bilin- gual sentence pair of Spanish and French (compare (a) and (d).)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Result and Analysis</head><p>Another interesting finding is that it is only ben- eficial to use true parallel pairs for finetuning the multi-way, mulitilingual models when there are enough of them (1m or more). When there are only a small number of true parallel sentence pairs, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudo Parallel Corpus</head><p>True Parallel Corpus Pivot <ref type="table" target="#tab_2">Many-to-1  1k  10k  100k  1m  1k  10k  100k  1m   (a) Single-Pair Models  Dev  - - - - - - 11.25 21.32  Test  - - - - - - 10.43</ref>   even found using pseudo pairs to be more benefi- cial than true ones. This effective as more apparent, when the direct one-to-one translation of the zero- resource pair was considered (see (c) in <ref type="table" target="#tab_7">Table 5</ref>.) This applies that the misalignment between the en- coder and decoder can be largely fixed by using pseudo-parallel pairs only, and we conjecture that it is easier to learn from pseudo-parallel pairs as they better reflect the inductive bias of the trained model and as the pseudo-parallel corpus is expected to be more noisy, this may be an implicit regularization effect. When there is a large amount of true parallel sentence pairs available, however, our results indi- cate that it is better to exploit them.</p><p>Unlike we observed with the multi-source trans- lation in Sec. 3.2, we were not able to see any im- provement by further averaging the early-averaged and late-average decoding schemes (compare (d) and (e).) This may be explained by the fact that the context vectors computed when creating a pseudo source (e.g., En from Es when Es→Fr) already con- tains all the information about the pseudo source. It is simply enough to take those context vectors into account via the early averaging scheme.</p><p>These results clearly indicate and verify the po- tential of the multi-way, multilingual neural trans- lation model in performing zero-resource machine translation. More specifically, it has been shown that the translation quality can be improved even without any direct parallel corpus available, and if there is a small amount of direct parallel pairs available, the quality may improve even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion: Implications and Limitations</head><p>Implications There are two main results in this paper. First, we showed that the multi-way, multilin- gual neural translation model by  is able to exploit common, underlying structures across many languages in order to better translate when a source sentence is given in multiple lan- guages. This confirms the usefulness of positive lan- guage transfer, which has been believed to be an im- portant factor in human language learning <ref type="bibr" target="#b17">(Odlin, 1989;</ref><ref type="bibr" target="#b19">Ringbom, 2007)</ref>, in machine translation. Fur- thermore, our result significantly expands the ap- plicability of multi-source translation <ref type="bibr" target="#b27">(Zoph and Knight, 2016)</ref>, as it does not assume the availability of multi-way parallel corpora for training and relies only on bilingual parallel corpora.</p><p>Second, the experiments on zero-resource trans- lation revealed that it is not necessary to have a di- rect parallel corpus, or deep linguistic knowledge, between two languages in order to build a machine translation system. Importantly we observed that the proposed approach of zero-resource translation is better both in terms of translation quality and data efficiency than a more traditional pivot-based translation ( <ref type="bibr" target="#b25">Wu and Wang, 2007;</ref><ref type="bibr" target="#b24">Utiyama and Isahara, 2007)</ref>. Considering that this is the first attempt at such zero-resource, or extremely low-resource, translation using neural machine translation, we ex- pect a large progress in near future.</p><p>Limitations Despite the promising empirical re- sults presented in this paper, there are a number of shortcomings that needs to addressed in follow-up research. First, our experiments have been done only with three European languages-Spanish, French and English. More investigation with a diverse set of lan- guages needs to be done in order to make a more solid conclusion, such as was done in <ref type="bibr" target="#b3">Chung et al., 2016)</ref>. Furthermore, the effect of varying sizes of available parallel corpora on the per- formance of zero-resource translation must be stud- ied more in the future.</p><p>Second, although the proposed many-to-one translation is indeed generally applicable to any number of source languages, we have only tested a source sentence in two languages. We expect even higher improvement with more languages, but it must be tested thoroughly in the future.</p><p>Lastly, the proposed finetuning strategy requires the model to have an additional set of parameters rel- evant to the attention mechanism for a target, zero- resource pair. This implies that the number of pa- rameters may grow linearly with respect to the num- ber of target language pairs. We expect future re- search to address this issue by, for instance, mix- ing in the parallel corpora of high-resource language pairs during finetuning as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Multi</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>One-to-one translation qualities using the multi-way, 

multilingual model and four separate single-pair models. 

Multi 
Single 
Dev 
Test 
Dev 
Test 

(a) Early 31.89 31.35 
-
-
(b) Late 
32.04 31.57 32.00 31.46 
(c) E+L 
32.61 31.88 
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Many-to-one quality (Es+Fr→En) using three transla-

tion strategies. Compared to Table 2 (a-b) we observe a signif-

icant improvement (up to 3+ BLEU), although the model was 

never trained in these many-to-one settings. The second column 

shows the quality by the ensemble of two separate single-pair 

models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Zero-resource translation from Spanish (Es) to French 

(Fr) without finetuning, using multi-way, multilingual model. 

When pivot is 
√ 
, English is used as a pivot language. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Zero-resource translation from Spanish (Es) to French (Fr) with finetuning. When pivot is 
√ 
, English is used as a pivot 

language. Row (b) is from Table 4 (b). 

</table></figure>

			<note place="foot" n="1"> https://github.com/nyu-dl/dl4mt-multi 2 T-B score is defined as TER−BLEU 2 which we found to be more stable than either TER or BLEU alone for the purpose of early-stopping (Zhao and Chen, 2009).</note>

			<note place="foot" n="3"> We do not assume the availability of annotation on multiway parallel sentence pairs. It is likely that there will be some sentence (or a set of very close variants of a single sentence) translated into multiple languages (eg. Europarl). One may decide to introduce a mechanism for exploiting these (Zoph and Knight, 2016), or as we present here, it may not be necessary at all to do so.</note>

			<note place="foot" n="4"> See the last row of Table 1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>OF thanks Iulian Vlad Serban and Georgiana Dinu for insightful discussions. KC thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of <ref type="bibr">Excellence 2015</ref><ref type="bibr">Excellence-2016</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09186</idno>
		<title level="m">Does multimodality help human and machine for translation and image captioning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A character-level decoder without explicit segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DL4MTTutorial: Conditional gated recurrent unit with attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-way, multilingual neural machine translation with a shared attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving arabicchinese statistical machine translation using english as pivot language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramón P ˜</forename><surname>Forcada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09</title>
		<meeting>the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997-06" />
			<biblScope unit="page" from="173" to="181" />
		</imprint>
	</monogr>
	<note>Biological and Artificial Computation: From Neuroscience to Technology. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Montreal neural machine translation systems for wmt&apos;15</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="134" to="140" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04586</idno>
		<title level="m">Character-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<title level="m">Cernock`nock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Language Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Odlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Cambridge Books Online</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cross-linguistic similarity in foreign language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Håkan</forename><surname>Ringbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparison of pivot methods for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="484" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pivot language approach for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
