<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
							<email>gdurrett@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">S</forename><surname>Portnoff</surname></persName>
							<email>rsportnoff@cs.berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mccoy</surname></persName>
							<email>mccoy@nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Levchenko</surname></persName>
							<email>klevchen@cs.ucsd.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vern</forename><surname>Paxson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Sadia Afroz ICSI, UC Berkeley</orgName>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">UC Berkeley</orgName>
								<address>
									<settlement>Austin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">NYU</orgName>
								<orgName type="institution" key="instit2">UC San Diego</orgName>
								<orgName type="institution" key="instit3">ICSI</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<country>UC</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2598" to="2607"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in on-line cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own &quot;fine-grained domain&quot; in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NLP can be extremely useful for enabling scien- tific inquiry, helping us to quickly and efficiently understand large corpora, gather evidence, and test hypotheses ( <ref type="bibr" target="#b0">Bamman et al., 2013</ref> (b) File 0-initiator10815</p><p>Figure 1: Example posts and annotations from Darkode, with annotated product tokens under- lined. The second example exhibits jargon (fud means "fully undetectable"), nouns that could be a product in other contexts (Exploit), and multiple lexically-distinct descriptions of a single service. Note that these posts are much shorter than the av- erage Darkode post (61.5 words).</p><p>2013). One domain for which automated analy- sis is particularly useful is Internet security: re- searchers obtain large amounts of text data perti- nent to active threats or ongoing cybercriminal ac- tivity, for which the ability to rapidly characterize that text and draw conclusions can reap major ben- efits <ref type="bibr">(Krebs, 2013a,b)</ref>. However, conducting auto- matic analysis is difficult because this data is out- of-domain for conventional NLP models, which harms the performance of both discrete models ( <ref type="bibr" target="#b17">McClosky et al., 2010</ref>) and deep models ( <ref type="bibr" target="#b28">Zhang et al., 2017)</ref>. Not only that, we show that data from one cybercrime forum is even out of domain with respect to another cybercrime forum, making this data especially challenging.</p><p>In this work, we present the task of identify- ing products being bought and sold in the market- place sections of these online cybercrime forums. We define a token-level annotation task where, for each post, we annotate references to the product or products being bought or sold in that post. Hav- ing the ability to automatically tag posts in this way lets us characterize the composition of a fo- rum in terms of what products it deals with, iden- tify trends over time, associate users with particu- lar activity profiles, and connect to price informa- tion to better understand the marketplace. Some of these analyses only require post-level informa- tion (what is the product being bought or sold in this post?) whereas other analyses might require token-level references; we annotate at the token level to make our annotation as general as possi- ble. Our dataset has already proven enabling for case studies on these particular forums ( <ref type="bibr" target="#b20">Portnoff et al., 2017)</ref>, including a study of marketplace ac- tivity on bulk hacked accounts versus users selling their own accounts. Our task has similarities to both slot-filling in- formation extraction (with provenance informa- tion) as well as standard named-entity recogni- tion (NER). Compared to NER, our task features a higher dependence on context: we only care about the specific product being bought or sold in a post, not other products that might be men- tioned. Moreover, because we are operating over forums, the data is substantially messier than clas- sical NER corpora like CoNLL <ref type="bibr" target="#b24">(Tjong Kim Sang and De Meulder, 2003)</ref>. While prior work has dealt with these messy characteristics for syntax ( <ref type="bibr" target="#b8">Kaljahi et al., 2015</ref>) and for discourse <ref type="bibr" target="#b15">(Lui and Baldwin, 2010;</ref><ref type="bibr" target="#b9">Kim et al., 2010;</ref><ref type="bibr" target="#b26">Wang et al., 2011</ref>), our work is the first to tackle forum data (and marketplace forums specifically) from an in- formation extraction perspective.</p><p>Having annotated a dataset, we examine super- vised and semi-supervised learning approaches to the product extraction problem. Binary or CRF classification of tokens as products is effective, but performance drops off precipitously when a sys- tem trained on one forum is applied to a differ- ent forum: in this sense, even two different cy- bercrime forums seem to represent different "fine- grained domains." Since we want to avoid having to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation , token-level annotation <ref type="bibr" target="#b2">(Daume III, 2007)</ref>, and semi-supervised approaches <ref type="bibr" target="#b25">(Turian et al., 2010;</ref><ref type="bibr" target="#b12">Kshirsagar et al., 2015)</ref>. We find little improve- ment from these methods and discuss why they fail to have a larger impact.</p><p>Overall, our results characterize the challenges of our fine-grained domain adaptation problem in online marketplace data. We believe that this new dataset provides a useful testbed for additional inquiry and investigation into modeling of fine- grained domain differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset and Annotation</head><p>We consider several forums that vary in the nature of products being traded:</p><p>• Darkode: Cybercriminal wares, including ex- ploit kits, spam services, ransomware pro- grams, and stealthy botnets.</p><p>• Hack Forums: A mixture of cyber-security and computer gaming blackhat and non- cybercrime products.</p><p>• Blackhat: Blackhat Search Engine Optimiza- tion techniques.</p><p>• Nulled: Data stealing tools and services. <ref type="table">Table 1</ref> gives some statistics of these forums. These are the same forums used to study prod- uct activity in <ref type="bibr" target="#b20">Portnoff et al. (2017)</ref>. We collected all available posts and annotated a subset of them. In total, we annotated 130,336 tokens; accounting for multiple annotators, our annotators considered 478,176 tokens in the process of labeling the data. <ref type="figure">Figure 1</ref> shows two examples of posts from Darkode. In addition to aspects of the annotation, which we describe below, we see that the text ex- hibits common features of web text: abbreviations, ungrammaticality, spelling errors, and visual for- matting, particularly in thread titles. Also, note how some words that are not products here might be in other contexts (e.g., Exploits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Process</head><p>We developed our annotation guidelines through six preliminary rounds of annotation, covering 560 posts. Each round was followed by discussion and resolution of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the develop- ment process trained annotators who were not se- curity experts. The data annotated during this pro- cess is not included in <ref type="table">Table 1</ref>.</p><p>Once we had defined the annotation standard, we annotated datasets from Darkode, Hack Fo- rums, Blackhat, and Nulled as described in Ta- ble 1. <ref type="bibr">2</ref> Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Fo- rums), and then every disagreement was discussed and resolved to produce a final annotation. The au- thors, who are researchers in either NLP or com- puter security, did all of the annotation.</p><p>We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit ( ). Note that many sentences in the data are already delimited by line breaks, making the sentence-splitting task much easier. We performed annotation on the tokenized data so that annotations would be consistent with surrounding punctuation and hyphenated words.</p><p>Our full annotation guide is available with our data release. <ref type="bibr">3</ref> Our basic annotation principle is to annotate tokens when they are either the prod- uct that will be delivered or are an integral part of the method leading to the delivery of that product. <ref type="figure">Figure 1</ref> shows examples of this for a deliverable product (bot) as well as a service (cleaning). Both a product and service may be annotated in a sin- gle example: for a post asking to hack an account, hack is the method and the deliverable is the ac- count, so both are annotated. In general, methods expressed as verbs may be annotated in addition to nominal references.</p><p>When the product is a multiword expression (e.g., Backconnect bot), it is almost exclusively a noun phrase, in which case we annotate the head word of the noun phrase (bot). Annotating single tokens instead of spans meant that we avoided hav- ing to agree on an exact parse of each post, since even the boundaries of base noun phrases can be quite difficult to agree on in ungrammatical text.</p><p>If multiple different products are being bought or sold, we annotate them all. We do not annotate:</p><p>• Features of products</p><p>• Generic product references, e.g., this, them</p><p>• Product mentions inside "vouches" (reviews from other users)</p><p>• Product mentions outside of the first and last 10 lines of each post 4 <ref type="table">Table 1</ref> shows inter-annotator agreement ac- cording to our annotation scheme. We use the Fleiss' Kappa measurement <ref type="bibr" target="#b5">(Fleiss, 1971)</ref>, treat- ing our task as a token-level annotation where every token is annotated as either a product or not. We chose this measure as we are inter- ested in agreement between more than two annota- tors (ruling out Cohen's kappa), have a binary as- signment (ruling out correlation coefficients) and have datasets large enough that the biases Krip- pendorff's Alpha addresses are not a concern. The values indicate reasonable agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discussion</head><p>Because we annotate entities in a context-sensitive way (i.e., only annotating those in product con- text), our task resembles a post-level information extraction task. The product information in a post can be thought of as a list-valued slot to be filled in the style of TAC KBP <ref type="bibr" target="#b22">(Surdeanu, 2013;</ref><ref type="bibr" target="#b23">Surdeanu and Ji, 2014</ref>), with the token-level annota- tions constituting provenance information. How- ever, we chose to anchor the task fully at the to- ken level to simplify the annotation task: at the post level, we would have to decide whether two distinct product mentions were actually distinct products or not, which requires heavier domain knowledge. Our approach also resembles the fully token-level annotations of entity and event infor- mation in the ACE dataset <ref type="bibr" target="#b18">(NIST, 2005</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Metrics</head><p>In light of the various views on this task and its dif- ferent requirements for different potential applica- tions, we describe and motivate a few distinct eval- uation metrics below. The choice of metric will impact system design, as we discuss in the follow- ing sections.</p><p>Token-level accuracy We can follow the ap- proach used in token-level tasks like NER and compute precision, recall, and F 1 over the set of tokens labeled as products. This most closely mimics our annotation process.</p><p>Type-level product extraction (per post) For many applications, the primary goal of the extrac- tion task is more in line with KBP-style slot filling, where we care about the set of products extracted from a particular post. Without a domain-specific lexicon containing full synsets of products (e.g., something that could recognize that hack and ac- cess are synonymous), it is difficult to evaluate this in a fully satisfying way. However, we approxi- mate this evaluation by comparing the set of prod- uct types 5 in a post with the set of product types predicted by the system. Again, we consider preci- sion, recall, and F 1 over these two sets. This met- ric favors systems that consistently make correct post-level predictions even if they do not retrieve every token-level occurrence of the product.</p><p>Post-level accuracy Most posts contain only one product, but our type-level extraction will nat- urally be a conservative estimate of performance simply because there may seem to be multiple <ref type="bibr">5</ref> Two product tokens are considered the same type if after lowercasing and stemming they have a sufficiently small edit distance: 0 if the tokens are length 4 or less, 1 if the lengths are between 5 and 7, and 2 for lengths of 8 or more "products" that are actually just different ways of referring to one core product. Roughly 60% of posts in the two forums contain multiple an- notated tokens that are distinct beyond stemming and lowercasing. However, we analyzed 100 of these multiple product posts across Darkode and Hack Forums, and found that only 6 of them were actually selling multiple products, indicating that posts selling multiple types of products are actu- ally quite rare (roughly 3% of cases overall). In the rest of the cases, the variations were due to slightly different ways of describing the same product.</p><p>In light of this, we also might consider ask- ing the system to extract some product reference from the post, rather than all of them. Specifically, we compute accuracy on a post-level by checking whether the first product type extracted by the sys- tem is contained in the annotated set of product types. <ref type="bibr">6</ref> Because most posts feature one product, this metric is sufficient to evaluate whether we un- derstood what the core product of the post was.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phrase-level Evaluation</head><p>Another axis of variation in metrics comes from whether we consider token-level or phrase-level outputs. As noted in the previous section, we did not annotate noun phrases, but we may actually be interested in identifying them. In <ref type="figure">Figure 1</ref>, for ex- ample, extracting Backconnect bot is more useful than extracting bot in isolation, since bot is a less specific characterization of the product.</p><p>We can convert our token-level annotations to phrase-level annotations by projecting our annotations to the noun phrase level based on the output of an automatic parser. We used the parser of <ref type="bibr" target="#b1">Chen and Manning (2014)</ref> to parse all sentences of each post. For each annotated token that was given a nominal tag (N*), we projected that token to the largest NP containing it of length less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis found that it typically reflects a mistake. In <ref type="figure">Figure 1</ref>, the entire noun phrase Backconnect bot would be labeled as a product. For products realized as verbs (e.g., hack), we leave the annotation as the single token.</p><p>Throughout the rest of this work, we will evalu- ate sometimes at the token-level and sometimes at the NP-level 7 (including for the product type eval- uation and post-level accuracy); we will specify which evaluation is used where.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We consider several baselines for product ex- traction, two supervised learning-based methods (here), and semi-supervised methods (Section 5).</p><p>Baselines One approach takes the most fre- quent noun or verb in a post and classifies all oc- currences of that word type as products. A more sophisticated lexical baseline is based on a prod- uct dictionary extracted from our training data: we tag the most frequent noun or verb in a post that also appears in this dictionary. This method fails primarily in that it prefers to extract common words like account and website even when they do not occur as products. The most relevant off-the- shelf system is an NER tagging model; we retrain the Stanford NER system on our data ( <ref type="bibr" target="#b4">Finkel et al., 2005</ref>). Finally, we can tag the first noun phrase of the post as a product, which will often capture the product if it is mentioned in the title of the post. <ref type="bibr">8</ref> We also include human performance results. We averaged the results for annotators compared with the consensus annotations. For the phrase level evaluation, we apply the projection method described in Section 3.1.</p><p>Binary classifier/CRF One learning-based ap- proach to this task is to employ a binary SVM classifier for each token in isolation. We also ex- perimented with a token-level CRF with a binary tagset, and found identical performance, so we de- scribe the binary classifier version. <ref type="bibr">9</ref> Our features look at both the token under consideration as well as neighboring tokens, as described in the next paragraph. A vector of "base features" is extracted for each of these target tokens: these include 1) sentence position in the document and word posi- tion in the current sentence as bucketed indices; 2) word identity (for common words), POS tag, and dependency relation to parent for each word in a window of size 3 surrounding the current word; 3) character 3-grams of the current word. The same base feature set is used for every token.</p><p>Our token-classifying SVM extracts base fea- tures on the token under consideration as well as its syntactic parent. Before inclusion in the final classifier, these features are conjoined with an in- dicator of their source (i.e., the current token or the parent token). Our NP-classifying SVM ex- tracts base features on first, last, head, and syntac- tic parent tokens of the noun phrase, again with each feature conjoined with its token source.</p><p>We weight false positives and false negatives differently to adjust the precision/recall curve (tuned on development data for each forum), and we also empirically found better performance by upweighting the contribution to the objective of singleton products (product types that occur only once in the training set).</p><p>Post-level classifier As discussed in Section 3, one metric we are interested in is whether we can find any occurrence of a product in a post. This task is easier than the general tagging problem: if we can effectively identify the product in, e.g., the title of a post, then we do not need to identify additional references to that product in the body of the post. Therefore, we also consider a post- level model, which directly tries to select one to- ken (or NP) out of a post as the most likely prod- uct. Structuring the prediction problem in this way naturally lets the model be more conservative in its extractions, since highly ambiguous product men- tions can be ignored if a clear product mention is present. Put another way, it supplies a useful form of prior knowledge, namely that each post has ex- actly one product in almost all cases.</p><p>Our post-level system is formulated as an in- stance of a latent SVM ( <ref type="bibr" target="#b27">Yu and Joachims, 2009)</ref>. The output space is the set of all tokens (or noun phrases, in the NP case) in the post. The latent variable is the choice of token/NP to select, since there may be multiple correct choices of product tokens. The features used on each token/NP are the same as in the token classifier.</p><p>We trained all of the learned models by subgra- dient descent on the primal form of the objective ( <ref type="bibr" target="#b21">Ratliff et al., 2007;</ref><ref type="bibr" target="#b13">Kummerfeld et al., 2015</ref>). We use AdaGrad ( <ref type="bibr" target="#b3">Duchi et al., 2011</ref>) to speed conver- gence in the presence of a large weight vector with heterogeneous feature types. All product extrac- tors in this section are trained for 5 iterations with 1 -regularization tuned on the development set.  <ref type="table">Table 2</ref>: Development set results on Darkode. Bolded F 1 values represent statistically-significant improvements over all other system values in the column with p &lt; 0.05 according to a bootstrap re- sampling test. Our post-level system outperforms our binary classifier at whole-post accuracy and on type-level product extraction, even though it is less good on the token-level metric. All systems consistently identify product NPs better than they identify product tokens. However, there is a sub- stantial gap between our systems and human per- formance. <ref type="table">Table 2</ref> shows development set results on Dark- ode for each of the four systems for each metric described in Section 3. Our learning-based sys- tems substantially outperform the baselines on the metrics they are optimized for. The post-level sys- tem underperforms the binary classifier on the to- ken evaluation, but is superior at not only post- level accuracy but also product type F 1 . This lends credence to our hypothesis that picking one product suffices to characterize a large fraction of posts. Comparing the automatic systems with hu- man annotator performance we see a substantial gap. Note that our best annotator's token F 1 was 89.8, and NP post accuracy was 100%; a careful, well-trained annotator can achieve very high per- formance, indicating a high skyline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Results</head><p>The noun phrase metric appears to be generally more forgiving, since token distinctions within noun phrases are erased. The post-level NP system achieves an F-score of 78 on product type identi- fication, and post-level accuracy is around 88%. While there is room for improvement, this system is accurate enough to enable analysis of Darkode with automatic annotation.</p><p>Throughout the rest of this work, we focus on NP-level evaluation and post-level NP accuracy. <ref type="table">Table 2</ref> only showed results for training and evalu- ating within the same forum (Darkode). However, we wish to apply our system to extract product oc- currences from a wide variety of forums, so we are interested in how well the system will general- ize to a new forum. <ref type="table" target="#tab_3">Tables 3 and 4</ref> show full re- sults of several systems in within-forum and cross- forum evaluation settings. Performance is severely degraded in the cross-forum setting compared to the within-forum setting, e.g., on NP-level F 1 , a Hack Forums-trained model is 14.6 F 1 worse at the Darkode task than a Darkode-trained model (61.2 vs. 75.8). Differences in how the systems adapt between different forums will be explored more thoroughly in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Domain Adaptation</head><p>In the next few sections, we explore several pos- sible methods for improving results in the cross- forum settings and attempting to build a more domain-general system. These techniques gen- erally reflect two possible hypotheses about the source of the cross-domain challenges:</p><p>Hypothesis 1: Product inventories are the pri- mary difference across domains; context-based features will transfer, but the main challenge is not being able to recognize unknown products.</p><p>Hypothesis 2: Product inventories and stylistic conventions both differ across domains; we need to capture both to adapt models successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Brown Clusters</head><p>To test Hypothesis 1, we investigate whether addi- tional lexical information helps identify product- like words in new domains. A classic semi- supervised technique for exploiting unlabeled tar- get data is to fire features over word clusters or word vectors ( <ref type="bibr" target="#b25">Turian et al., 2010)</ref>. These fea- tures should generalize well across domains that the clusters are formed on: if product nouns occur in similar contexts across domains and therefore wind up in the same cluster, then a model trained on domain-limited data should be able to learn that that cluster identity is indicative of products.</p><p>We form Brown clusters on our unlabeled data from both Darkode and Hack Forums (see <ref type="table">Table 1</ref> System Eval data <ref type="table" target="#tab_5">Darkode  Hack Forums  Blackhat  Nulled  Avg  P  R  F1  P  R  F1  P  R  F1  P  R  F1  F1  Trained on Darkode  Dict  55.9 54.2</ref> 55.0 42.1 39.8 40.9 37.1 36.6 36.8 52.6 43.2 47.4 45.0 Binary 73.3 78.6 75.8 51.1 50.2 50.6 55.2 58.3 56.7 55.2 64.0 59.3 60.6 Binary + Brown Clusters 75.5 76.4</p><p>76.0 52.1 55.9 48.1 59.7 57.1 58.4 60.0 61.1 60.5 60.8 Binary + Gazetteers 73.1 75.6 74.3 52.6 51.1 51.8</p><formula xml:id="formula_0">− − − − − − − Trained</formula><note type="other">on Hack Forums Dict 57.3 44.8 50.3 50.0 52.7 51.3 45.0 44.7 44.8 51.1 43.6 47.1 48.4 Binary 67.0 56.4 61.2 58.0 64.2 61.0 62.4 60.8 61.6 71.0 68.9 69.9 63.4 Binary + Brown Clusters 67.2 52.5</note><p>58.9 59.3 64.7 61.9 61.9 59.6 60.7 73.1 67.4 70.2 62.9 Binary + Gazetteers 67.8 64.1 †65.9 59.9 61.3 60.6 for sizes). We use <ref type="bibr" target="#b14">Liang (2005)</ref>'s implementation to learn 50 clusters. <ref type="bibr">10</ref> Upon inspection, these clus- ters do indeed capture some of the semantics rele- vant to the problem: for example, the cluster 110 has as its most frequent members service, account, price, time, crypter, and server, many of which are product-associated nouns. We incorporate these as features into our model by characterizing each to- ken with prefixes of the Brown cluster ID; we used prefixes of length 2, 4, and 6. <ref type="table" target="#tab_3">Tables 3 and 4</ref> show the results of incorporat- ing Brown cluster features into our trained mod- els. These features do not lead to statistically- significant gains in either NP-level F 1 or post-level accuracy, despite small improvements in some cases. This indicates that Brown clusters might be a useful feature sometimes, but do not solve the domain adaptation problem in this context. <ref type="bibr">11</ref> </p><formula xml:id="formula_1">− − − − − − −</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Type-level Annotation</head><p>Another approach following Hypothesis 1 is to use small amounts of supervised data, One cheap approach for annotating data in a new domain is to exploit type-level annotation ( . Our token- level annotation standard is relatively complex to learn, but a researcher could quite easily provide a few exemplar products for a new forum based on just a few minutes of reading posts and analyzing the forum.</p><p>Given the data that we've already annotated, we can simulate this process by iterating through  our labeled data and collecting annotated prod- uct names that are sufficiently common. Specifi- cally, we take all (lowercased, stemmed) product tokens and keep those occurring at least 4 times in the training dataset (recall that these datasets are ≈ 700 posts). This gives us a list of 121 products in Darkode and 105 products in Hack Forums.</p><p>To incorporate this information into our system, we add a new feature on each token indicating whether or not it occurs in the gazetteer. At train- ing time, we use the gazetteer scraped from the training set. At test time, we use the gazetteer from the target domain as a form of partial type- level supervision. <ref type="table" target="#tab_3">Tables 3 and 4</ref> shows the re- sults of incorporating the gazetteer into the sys- tem. Gazetteers seem to provide somewhat consis- tent gains in cross-domain settings, though many of these individual improvements are not statisti- cally significant, and the gazetteers can sometimes hurt performance when testing on the same do- main the system was trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test</head><p>Darkode Hack Forums Blackhat Nulled % OOV Rseen Roov % OOV Rseen Roov % OOV Rseen Roov % OOV Rseen Roov Binary <ref type="table" target="#tab_3">(Darkode)  20  78  62  41  64  47  42  69  46  30  72  45  Binary (HF)  50  76  40  35  75  42  51  70  38  33  83  32   Table 5</ref>: Product token out-of-vocabulary rates on development sets (test set for Blackhat and Nulled) of various forums with respect to training on Darkode and Hack Forums. We also show the recall of an NP- level system on seen (R seen ) and OOV (R oov ) tokens. Darkode seems to be more "general" than Hack Forums: the Darkode system generally has lower OOV rates and provides more consistent performance on OOV tokens than the Hack Forums system. gives little benefit over na¨ıvena¨ıve use of the new data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Token-level Annotation</head><p>We now turn our attention to methods that might address Hypothesis 2. If we assume the domain transfer problem is more complex, we really want to leverage labeled data in the target domain rather than attempting to transfer features based only on type-level information. Specifically, we are in- terested in cases where a relatively small num- ber of labeled posts (less than 100) might provide substantial benefit to the adaptation; a researcher could plausibly do this annotation in a few hours.</p><p>We consider two ways of exploiting labeled target-domain data. The first is to simply take these posts as additional training data. The sec- ond is to also employ the "frustratingly easy" do- main adaptation method of Daume III <ref type="bibr">(2007)</ref>. In this framework, each feature fired in our model is actually fired twice: one copy is domain- general and one is conjoined with the domain la- bel (here, the name of the forum). <ref type="bibr">12</ref> In doing so, the model should gain some ability to separate domain-general from domain-specific feature val- ues, with regularization encouraging the domain- general feature to explain as much of the phe- nomenon as possible. For both training methods, we upweight the contribution of the target-domain posts in the objective by a factor of 5. <ref type="figure" target="#fig_0">Figure 2</ref> shows learning curves for both of these methods in two adaptation settings as we vary the amount of labeled target-domain data. The system trained on Hack Forums is able to make good use of labeled data from Darkode: having access to 20 labeled posts leads to gains of roughly 7 F 1 . In- terestingly, the system trained on Darkode is not able to make good use of labeled data from Hack Forums, and the domain-specific features actually cause a drop in performance until we include a substantial amount of data from Hack Forums (at least 80 posts). We are likely overfitting the small Hack Forums training set with the domain-specific features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>In order to understand the variable performance and shortcomings of the domain adaptation ap- proaches we explored, it is useful to examine our two initial hypotheses and characterize the datasets a bit further. To do so, we break down system performance on products seen in the train- ing set versus novel products. Because our sys- tems depend on lexical and character n-gram fea- tures, we expect that they will do better at predict- ing products we have seen before. <ref type="table">Table 5</ref> confirms this intuition: it shows product out-of-vocabulary rates in each of the four forums relative to training on both Darkode and Hack Fo- rums, along with recall of an NP-level system on both previously seen and OOV products. As ex- pected, performance is substantially higher on in-vocabulary products. OOV rates of a Darkode- trained system are generally lower on new forums, indicating that that forum has better all-around product coverage. A system trained on Darkode is therefore in some sense more domain-general than one trained on Hack Forums.</p><p>This would seem to support Hypothesis 1. Moreover, <ref type="table" target="#tab_3">Table 3</ref> shows that the Hack Forums- trained system achieves a 21% error reduction on Hack Forums compared to a Darkode-trained system, while a Darkode-trained system obtains a 38% error reduction on Darkode relative to a Hack Forums-trained system; this greater error reduction means that Darkode has better cover- age of Hack Forums than vice versa. Darkode's better product coverage also helps explain why Section 5.3 showed better performance of adapt- ing Hack Forums to Darkode than the other way around: augmenting Hack Forums data with a few posts from Darkode can give critical knowl- edge about new products, but this is less true if the forums are reversed. Duplicating features and adding parameters to the learner also has less of a clear benefit when adapting from Darkode, when the types of knowledge that need to be added are less concrete.</p><p>Note, however, that these results do not tell the full story. <ref type="table">Table 5</ref> reports recall values, but not all systems have the same precision/recall trade- off: although they were tuned to balance precision and recall on their respective development sets, the Hack Forums-trained system is slightly more precision-oriented on Nulled than the Darkode- trained system. 13 In fact, <ref type="table" target="#tab_3">Table 3</ref> shows that the Hack Forums-trained system actually performs better on Nulled, largely due to better performance on previously-seen products. This indicates that there is some truth to Hypothesis 2: product cov- erage is not the only important factor determining performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a new dataset of posts from cybercrime marketplaces annotated with product references, a task which blends IE and NER. Learning-based methods degrade in performance when applied to new forums, and while we explore methods for fine-grained domain adaption in this data, effective methods for this task are still an open question.</p><p>Our datasets used in this work are avail- able at https://evidencebasedsecurity.org/ forums/ Code for the product extractor can be found at https://github.com/ccied/ugforum- analysis/tree/master/extract-product</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Token-supervised domain adaptation results for two settings. As our system is trained on an increasing amount of target-domain data (xaxis), its performance generally improves. However, adaptation from Hack Forums to Darkode is much more effective than the other way around, and using domain features as in Daume III (2007) gives little benefit over na¨ıvena¨ıve use of the new data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>2602</head><label>2602</label><figDesc></figDesc><table>Token Prediction 
Tokens 
Products 
Posts 
P 
R 
F1 
P 
R 
F1 
Acc. 
Freq 41.9 42.5 42.2 48.4 33.5 39.6 45.3 
Dict 57.9 51.1 54.3 65.6 44.0 52.7 60.8 
NER 59.7 62.2 60.9 60.8 62.6 61.7 72.2 
Binary 62.4 76.0 68.5 58.1 77.6 66.4 75.2 
Post 82.4 36.1 50.3 83.5 56.6 67.5 82.4 
Human  *  86.9 80.4 83.5 87.7 77.6 82.2 89.2 
NP Prediction 
NPs 
Products 
Posts 
P 
R 
F1 
P 
R 
F1 
Acc. 
Freq 61.8 28.9 39.4 61.8 50.0 55.2 61.8 
Dict 57.9 61.8 59.8 71.8 57.5 63.8 68.0 
First 73.1 34.2 46.7 73.1 59.1 65.4 73.1 
NER 63.6 63.3 63.4 69.7 70.3 70.0 76.3 
Binary 67.0 74.8 70.7 65.5 82.5 73.0 82.4 
Post 87.6 41.0 55.9 87.6 70.8 78.3 87.6 
Human  *  87.6 83.2 85.3 91.6 84.9 88.1 93.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test set results at the NP level in within-forum and cross-forum settings for a variety of different 
systems. Using either Brown clusters or gazetteers gives mixed results on cross-forum performance: only 
one of the improvements ( †) is statistically significant with p &lt; 0.05 according to a bootstrap resampling 
test. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Test set results at the whole-post level 
in within-forum and cross-forum settings for a va-
riety of different systems. Brown clusters and 
gazetteers give similarly mixed results as in the 
token-level evaluation;  † indicates statistically sig-
nificant gains over the post-level system with p &lt; 
0.05 according to a bootstrap resampling test. 

</table></figure>

			<note place="foot" n="1"> Dataset and code to train models available at https://evidencebasedsecurity.org/forums/</note>

			<note place="foot" n="2"> The table does not include additional posts that were labeled by all annotators in order to check agreement. 3 https://evidencebasedsecurity.org/ forums/annotation-guide.pdf</note>

			<note place="foot" n="4"> In preliminary annotation we found that content in the middle of the post typically described features or gave instructions without explicitly mentioning the product. Most posts are unaffected by this rule: 96% of Darkode, 77% of Hack Forums, 84% of Blackhat, and 93% of Nulled posts are less than 20 lines. However, the cutoff still substantially reduced annotator effort on the tail of very long posts.</note>

			<note place="foot" n="6"> For this metric we exclude posts containing no products. These are usually posts that have had their content deleted or are about forum administration.</note>

			<note place="foot" n="7"> Where NP-level means &quot;noun phrases and verbs&quot; as described in Section 3.1. 8 Since this baseline fundamentally relies on noun phrases, we only evaluate it in the noun phrase setting. 9 We further experimented with a bidirectional LSTM tagger and found similar performance as well.</note>

			<note place="foot" n="10"> This value was chosen based on dev set experiments. 11 We could also use vector representations of words here, but in initial experiments, these did not outperform Brown clusters. That is consistent with the results of Turian et al. (2010) who showed similar performance between Brown clusters and word vectors for chunking and NER.</note>

			<note place="foot" n="12"> If we are training on data from k domains, this gives rise to up to k + 1 total versions of each feature.</note>

			<note place="foot" n="13"> While a hyperparameter controlling the precision/recall tradeoff could theoretically be tuned on the target domain, it is hard to do this in a robust, principled way without having access to a sizable annotated dataset from that domain. This limitation further complicates the evaluation and makes it difficult to set up apples-to-apples comparisons across domains.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the National Science Foundation under grants CNS-1237265 and CNS-1619620, by the Office of Naval Re-search under MURI grant N000140911081, by the Center for Long-Term Cybersecurity and by gifts from Google. We thank all the people that pro-vided us with forum data for our analysis; in par-ticular Scraping Hub and SRI for their assistance in collecting data for this study. Any opinions, findings, and conclusions expressed in this mate-rial are those of the authors and do not necessarily reflect the views of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Latent Personas of Film Characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measuring nominal scale agreement among many raters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="378" to="382" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a Part-of-Speech Tagger from Two Hours of Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Mielens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Foreebank: Syntactic Analysis of Customer Support Forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasoul</forename><surname>Kaljahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Roturier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Ribeyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teresa</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tagging and Linking Web Forum Posts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Krebs</surname></persName>
		</author>
		<title level="m">Cards Stolen in Target Breach Flood Underground Markets</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Who&apos;s Selling Credit Cards from Target?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Krebs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Frame-Semantic Role Labeling with Heterogeneous Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghana</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An Empirical Analysis of Optimization for Max-Margin NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-Supervised Learning for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s Thesis</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Classifying User Forum Participants: Separating the Gurus from the Hacks, and Other Tales of the Internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
			<affiliation>
				<orgName type="collaboration">ALTA</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
			<affiliation>
				<orgName type="collaboration">ALTA</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Language Technology Association Workshop</title>
		<meeting>the Australasian Language Technology Association Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP Natural Language Processing Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)</title>
		<meeting>52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic domain adaptation for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ACE 2005 Evaluation Plan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to Extract International Relations from Political Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tools for Automated Analysis of Cybercriminal Markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">S</forename><surname>Portnoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadia</forename><surname>Afroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damon</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vern</forename><surname>Paxson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online) Subgradient Methods for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">J</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Overview of the TAC2013 Knowledge Base Population Evaluation: English Slot Filling and Temporal Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC-KBP 2013 Workshop</title>
		<meeting>the TAC-KBP 2013 Workshop</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAC-KBP 2014 Workshop</title>
		<meeting>the TAC-KBP 2014 Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Natural Language Learning (CoNLL)</title>
		<meeting>the Conference on Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Arie</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting Thread Discourse Structure over Technical Web Forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Structural SVMs with Latent Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Nam John</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aspect-augmented Adversarial Networks for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
