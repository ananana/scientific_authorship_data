<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2540</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">iFlytek Co</orgName>
								<address>
									<region>Ltd</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2540" to="2549"/>
							<date type="published">October 31-November 4, 2018. 2018. 2540</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Part-of-Speech (POS) tagging for Twitter has received considerable attention in recent years. Because most POS tagging methods are based on supervised models, they usually require a large amount of labeled data for training. However, the existing labeled datasets for Twitter are much smaller than those for newswire text. Hence, to help POS tagging for Twitter, most domain adaptation methods try to leverage newswire datasets by learning the shared features between the two domains. However, from a linguistic perspective, Twit-ter users not only tend to mimic the formal expressions of traditional media, like news, but they also appear to be developing linguistically informal styles. Therefore, POS tagging for the formal Twitter context can be learned together with the newswire dataset, while POS tagging for the informal Twitter context should be learned separately. To achieve this task, in this work, we propose a hypernetwork-based method to generate different parameters to separately model contexts with different expression styles. Experimental results on three different datasets show that our approach achieves better performance than state-of-the-art methods in most cases.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the continuous growth of online communica- tion, hundreds of millions of online conversational messages have become important resources for various applications such as real-time event detec- tion ( <ref type="bibr" target="#b21">Sakaki et al., 2010)</ref>, stock prediction <ref type="bibr" target="#b1">(Bollen et al., 2011</ref>) and public health analysis <ref type="bibr" target="#b29">(Wilson and Brownstein, 2009)</ref>. Because these appli- cations need to process natural language text, POS tagging, which is one of the fundamental natural language processing tasks, has become one of the basic pre-processing components of such applications. The performance of POS RT @jamstik : Lol :) there is more than one way to start living a greener life.</p><p>As their varied strategies suggest , there is more than one way to respond to a disaster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wall Street Journal Section of Penn Treebank</head><p>Treebank-3 (LDC1999T42) /07/WSJ_0799.POS tagging may highly impact the results of these applications.</p><p>Most of the POS tagging methods that can achieve state-of-the-art performance are based on supervised learning algorithms ( <ref type="bibr" target="#b10">Gimpel et al., 2011)</ref>. Although these methods can achieve good performance for in-domain data, their per- formance usually drops quickly when processing data from a domain that is different from that of the training data <ref type="bibr" target="#b2">(Caruana and Niculescu-Mizil, 2006</ref>). To achieve better performance, we usually need to manually label a large amount of in- domain data. However, the task of construct- ing labeled data is time-consuming and tedious. Currently, various methods have been proposed to solve this problem using out-of-domain data, including domain adaptation <ref type="bibr" target="#b4">(Daum√© III, 2009;</ref><ref type="bibr" target="#b11">Gui et al., 2017)</ref>, multi-task learning <ref type="bibr" target="#b0">(Ben-David et al., 2007)</ref>, and dual learning ( <ref type="bibr" target="#b3">Chandrasekaran et al., 2014</ref>).</p><p>Most existing methods aim to learn the shared representations or parameters, which can reduce the classification or regression model errors of each task/domain. However, these methods usu- ally ignore the fact that each domain has domain- specific features that should not be shared.  the language characteristics, in spite of the ex- pressions that are similar to the newswire text, Twitter has some informal expressions that cannot be shared, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. <ref type="bibr" target="#b14">Hu et al. (2013)</ref> investigated the characteristics of language on Twitter and found that Twitter users not only tended to mimic the linguistic practices of tradi- tional media, like news, they also appeared to be developing linguistically unique styles. We also believe that the tweets simply follow the standard language rules, and at some point eventually devi- ate from those. Thus, tweets are a combination of formal expressions and informal expressions with conversions between different expression styles.</p><note type="other">Main LSTM layer Hyper LSTM layer MLP layer Character-level CNN layer Prediction layer Input layer ~</note><p>Based on the above observations, we believe that annotated sentences of newswire text can be selectively used to help tag contextual seg- ments of tweets, especially for formal expressions. To achieve this goal, in this work we adopt bidirectional long short-term memory (bi-LSTM) networks <ref type="bibr" target="#b26">(Schuster and Paliwal, 1997)</ref>, which have been successfully used for various sequence tagging problems. However, different from previ- ous methods, the formal expressions and informal expressions in a sentence should be separately modeled. Inspired by recent work on dynamic parameter prediction ( <ref type="bibr" target="#b12">Ha et al., 2016)</ref>, we pro- pose a method to generate the context-specific parameters of the bi-LSTM based on different styles of context for POS tagging. We evaluated our models on three different corpora. The results demonstrated that the proposed method can benefit from annotated newswire text data and achieve competitive performance. In addition, we visualized the context distributions and the change of parameters. The visualization results verified the fact that different contexts produce different parameters for POS tagging.</p><p>The main contributions of the paper can be summarized as follows.</p><p>1. We study problems in the segment modeling method to apply domain adaptation to the POS tagging task. Based on the observations from a linguistic perspective, we found that there are many shared expressions between the newswire and tweets, and some expres- sions cannot be shared.</p><p>2. A novel neural network architecture based on the bi-LSTM was proposed to perform the task. Different parameters were applied in different contexts.</p><p>3. Experimental results demonstrated that the proposed method can benefit from different domains. We also conducted qualitative and quantitative analyses to show why our model can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Twitter is responsible for colorful linguistic ex- pressions, and full of the conversions between for- mal expressions and informal expressions. To ad- dress this problem, we propose the Dynamic Con- version Neural Networks (DCNN), which can dy- namically generate different parameters for POS tagging based on different contexts as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Our model mainly consists of four parts: (1) a CNN layer for extracting word representations x i , (2) an MLP layer for producing low-dimensional context representations t i , (3) a hyper LSTM layer for generating the weights W of a main LSTM, and (4) the main LSTM layer with dynamic parameters for POS tagging. The architecture of the main network is the same with any sequence labeling model, which learns to map the word representations to the corresponding labels. However, the parameters of the main network can be modified according to our purpose. we use a low-dimensional context distribution vector t i as the input of the hyper LSTM to generate the weights of the main LSTM. Thus, the weights of the main LSTM will be subject to a change in context vector. Therefore, the main LSTM can predict the POS tag based on the different parameters. <ref type="bibr" target="#b12">Ha et al. (2016)</ref> also proposed a HyperRNN network, in which the hyper net is influenced by the main net. This is inconsistent with our motivation. Different from ( <ref type="bibr" target="#b12">Ha et al., 2016)</ref>, to make the parameters of the main LSTM totally controlled by the context distribution, we changed the architecture by cutting off the data path from the main LSTM to the hyper LSTM. This method can prevent the hidden states of the main LSTM from influencing the hyper LSTM. In addition, we add an extra layer of learning the context representations based on features returned by a CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word and Context Representations</head><p>Out-of-vocabulary words are frequently used in Twitter. Moreover, new symbols, abbreviations, and words are constantly being created. These make word representations difficult to address. Thus, robust methods should be used to extract the morphological and shape information from words.</p><p>Inspired by <ref type="bibr" target="#b23">(Santos and Zadrozny, 2014</ref>), we adopted character-level convolutional neural net- works (CNN layer) to tackle this problem, which can take all of the characters of the word into consideration and output important orthographic features <ref type="bibr" target="#b23">(Santos and Zadrozny, 2014)</ref>. Suppose that we are given the sentence X = {w 1 , w 2 , . . . } with vocabulary V of words. We use multiple filters with varying widths to obtain the ortho- graphic feature vector c i for word w i . Then, the orthographic feature vector c i is concatenated to the word embedding w i to form word representa- tion x i as the input of the main LSTM. Utilizing a bi-LSTM to model sentences, the model can extract the sequential relations and contextual information.</p><p>The context is a fixed window of words around target word. The context representations are learned by the MLP later. To do this, we apply a fully connected neural network, which takes sequential word representations in a fixed window as input to generate a low-dimensional vector t i as follows:</p><formula xml:id="formula_0">t i = sof tmax(M LP [x i‚àír ...x i‚àí1 ‚äï x i ‚äï x i+1 ...x i+r ]),</formula><p>(1) where [¬∑ ‚äï ¬∑] represents concatenation operation. r represents the length from central word x i to the edge of the window. M LP is the multilayer perceptrons function, which transfers the context matrix to a low-dimensional vector. We apply M LP to every window of contexts. sof tmax denotes the softmax function that converts the context vector into a probability distribution. The goal is to learn an MLP layer that, given sequential word representations, estimates a distribution over the contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive Weight Generation</head><p>The identical weights at each time step will limit the expressiveness of recurrent neural network (RNN) ( <ref type="bibr" target="#b12">Ha et al., 2016)</ref>. To overcome the limitation, our model uses a small network (hyper network) taking low-dimensional context repre- sentations as inputs to dynamically generate the parameters of a large network (main network) for POS tagging. Different with ( <ref type="bibr" target="#b12">Ha et al., 2016)</ref>, at every time step, the hyper LSTM only takes the context representation t i as an input and generates the hidden stat√™ h i as an output. This hidden stat√™ h i is used to generate the weights for the main LSTM at the same time step. The hyper LSTM and main LSTM are jointly trained with backpropagation and gradient descent. Next, we will give a more formal description of the weight generation.</p><p>The hyper LSTM is a standard LSTM (Hochre- iter and Schmidhuber, 1997), which takes context vectors as inputs and outputs hidden states. The hyper LSTM is defined as follows:</p><formula xml:id="formula_1">Ô£Æ Ô£Ø Ô£Ø Ô£∞ ÀÜ g i ÀÜ i i ÀÜ f i ÀÜ o i Ô£π Ô£∫ Ô£∫ Ô£ª = Ô£´ Ô£¨ Ô£¨ Ô£¨ Ô£¨ Ô£≠ Ô£Æ Ô£Ø Ô£Ø Ô£Ø Ô£Ø Ô£∞ W ÀÜ g t , W ÀÜ g ÀÜ h W ÀÜ i t , W ÀÜ i ÀÜ h W ÀÜ f t , W ÀÜ f ÀÜ h W ÀÜ o t , W ÀÜ o ÀÜ h Ô£π Ô£∫ Ô£∫ Ô£∫ Ô£∫ Ô£ª ‚Ä¢ t i ÀÜ h i‚àí1 + Ô£Æ Ô£Ø Ô£Ø Ô£Ø Ô£∞ ÀÜ b ÀÜ g ÀÜ b ÀÜ i ÀÜ b ÀÜ f ÀÜ b ÀÜ o Ô£π Ô£∫ Ô£∫ Ô£∫ Ô£ª Ô£∂ Ô£∑ Ô£∑ Ô£∑ Ô£∑ Ô£∏ ÀÜ c i = œÜ(ÀÜ g i ) œÉ( ÀÜ i i ) + ÀÜ c i‚àí1 œÉ( ÀÜ f i ) ÀÜ h i = œÉ(ÀÜ o i ) œÜ (ÀÜ c i ) , (2)</formula><p>where œÜ denotes the tanh function, and œÉ is the sigmoid function. and ‚Ä¢ represent the Hadamard product and matrix product, respec- tively. We assume thatÀÜythatÀÜ thatÀÜy is one of {ÀÜg{ÀÜg, ÀÜ i, ÀÜ f , ÀÜ o}. The hyper LSTM has N ÀÜ h hidden units, and N t is the dimensionality of</p><formula xml:id="formula_2">t i . Then, W ÀÜ y t ‚àà R N ÀÜ h √óNt , W ÀÜ y ÀÜ h ‚àà R N ÀÜ h √óNÀÜh√óNÀÜ √óNÀÜh , ÀÜ b ÀÜ y ‚àà R N ÀÜ h</formula><p>are the parameters of the hyper LSTM and stay invariable during one sentential sequence.</p><p>Inspired by <ref type="bibr" target="#b12">(Ha et al., 2016)</ref>, we adopted a weight scaling vector d which is a linear projec- tion ofÀÜhofÀÜ ofÀÜh i . d is used to linearly scale each row of the weight matrix in the standard LSTM. Because the context vector t is produced by the different contexts at each time step, the hidden state h i and d i will change corresponding to t i . Thus, the main LSTM can be modified as follows:</p><formula xml:id="formula_3">Ô£Æ Ô£∞ d y i,x d y i,h d y i,b Ô£π Ô£ª = Ô£Æ Ô£∞ M LP y x ( ÀÜ h i ) M LP y h ( ÀÜ h i ) M LP y b ( ÀÜ h i ) Ô£π Ô£ª y i = d y i,x ‚äó W y x , d y i,h ‚äó W y h ‚Ä¢ x i h i‚àí1 + d y i,b ‚äó b y c i =œÜ(g i ) œÉ(i i ) + c i‚àí1 œÉ(f i ) h i =œÉ(o i ) œÜ (c i ) , (3)</formula><p>where ‚äó represents the element-wise product with broadcasting. y is one of {g, i, f, o}. Generally, N ÀÜ h and N t are much smaller than N h and N x , respectively. Thus, the size of the parameters in the hyper LSTM is hundreds of times less than that of the standard LSTM.</p><p>According to the above functions, if the model is given contexts with different styles, it will generate different parameters for all kinds of gates in the main LSTM. Then, the outputs of the main LSTM are used to predict the POS tags of the central words with the cross entropy loss: where z i is the one-hot vector of the POS tagging label corresponding to x i . ÀÜ z i is the output of the top softmax layer:</p><formula xml:id="formula_4">L P OS = ‚àí i z i * logÀÜzlogÀÜ logÀÜz i ,<label>(4)</label></formula><formula xml:id="formula_5">ÀÜ z i = sof tmax(M LP (h i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In this section, we will first detail the datasets we used. Then, we will describe several baseline methods, including a number of classic taggers and a series of deep learning sequence labeling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Following ( <ref type="bibr" target="#b6">Derczynski et al., 2013)</ref>, we use RIT-Twitter (Ritter et al., 2011) as our main dataset. The RIT-Twitter was split into training, development and evaluation sets (RIT-Train, RIT- Dev, RIT-Test). The splitting method was shown in <ref type="bibr" target="#b6">(Derczynski et al., 2013)</ref>. In order to verify the validity of our model, we also tested it on two more datasets, NPSChat <ref type="bibr" target="#b9">(Forsythand and Martell, 2007)</ref>, and ARK-Twitter (Gimpel et al., 2011) using standard splits. The tag-sets of the RIT-Twitter and NPSChat are PTB-like, while that of the ARK-Twitter is specific. In order to use WSJ labeled data in experiments on ARK- Twitter, we performed the mapping from PTB tag- sets to ARK tag-sets, according to the PTB POS Tagging Guidelines <ref type="bibr" target="#b22">(Santorini, 1990)</ref> and ARK Guidelines 1 . The mapping proceeded from fine to coarse. For pretraining the word embedding, we con- structed a dataset containing 30 million tweets, from Twitter using its API. We introduced a newswire dataset containing 1173K tokens as the written language dataset, namely the Wall Street Journal (WSJ) from the Penn TreeBank v3 ( <ref type="bibr" target="#b16">Marcus et al., 1993)</ref>. During training, we mixed each of RIT-Twitter, NPSChat and ARK- Twitter with WSJ into three kinds of training data.</p><p>The detailed data statistics of the above datasets used in this work are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Competitor Methods</head><p>We applied several classic and state-of-the-art methods for comparison. In addition, we used a series of deep learning sequence labeling methods as baselines for comparison, as follows:</p><p>Stanford POS Tagger is a widely used part- of-speech taggers described in ( <ref type="bibr" target="#b27">Toutanova et al., 2003)</ref>. It demonstrates the broad use of features and appropriate model regularization, which pro- duces a superior level of performance (97.24%). In this work, we trained it using two different sets: sections 0-18 of the WSJ (Stanford-WSJ) and a mixed corpus of WSJ, IRC, and Twitter (Stanford-MIX).  Bidirectional LSTM (Bi-LSTM) ( <ref type="bibr" target="#b28">Wang et al., 2015</ref>) has been widely used in a variety of sequence labeling tasks. In this work, we also evaluated it as a baseline.</p><p>Bi-HyperLSTM ( <ref type="bibr" target="#b12">Ha et al., 2016</ref>) was used as a substitute for the standard Bi-LSTM. What makes the Bi-HyperLSTM model different from the proposed model is that we used context distribution to generate the parameters of main LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Initialization and Hyperparameter</head><p>The word embeddings for all the models were initialized with the word2vec tool ( <ref type="bibr" target="#b17">Mikolov et al., 2013</ref>) on 30 million tweets. The other parameters excluding the word embeddings, such as the parameters in LSTM and MLP, were initialized by randomly sampling from a uniform distribution in <ref type="bibr">[-0.05, 0.05]</ref>.</p><p>The dimensionality of the word embedding was set at 200. The dimensionality for the randomly initialized character embedding was set at 25. We adopted a hyper LSTM with 160 hidden neurons to produce the weights of each gates of the main LSTM with 250 hidden neurons. The dimensionality of the context vector was set at 10.</p><p>Our DCNN could be trained end-to-end with backpropagation and gradient-based optimization was performed using the Adam update rule <ref type="bibr" target="#b15">(Kingma and Ba, 2014</ref>) with learning rate 0.0001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Analysis</head><p>In this section, we will report the experimental results and a detailed analysis of the results for the three different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on RIT-Twitter</head><p>The RIT-Twitter was introduced in ( <ref type="bibr" target="#b20">Ritter et al., 2011)</ref>. This dataset uses a tagset based on the Penn Treebank tagset with several Twitter-specific tags: retweets, @usernames, hashtags, and urls. <ref type="table" target="#tab_2">Table 2</ref> lists the results of our method compared with other methods on this dataset. The first part shows the results of the classic methods. From the result of Stanford-WSJ, we can see that although it can achieve a superior level of performance (97.24%) on the WSJ dataset, the accuracy drops significantly to 73.37% when applied to the Twitter dataset. If we add some in- domain data to the training set, the Stanford-MIX can improve by 10% compared to the Stanford- WSJ. The same phenomenon can be observed from T-POS tagger. If we apply more features, like clustering, bootstrapping and lexical features, the T-POS, GATE tagger and ARK tagger can achieve better performances. Although TPANN achieve accuracy of 90.92%, it incorporates additional a large amount of in-domain unlabeled data. Our method is more competitive because of the use of much fewer data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Training Set RIT-Test RIT-Dev Stanford-WSJ ( <ref type="bibr" target="#b27">Toutanova et al., 2003)</ref> - 73.37% Stanford-MIX - 83.14% T-POS-WSJ ( <ref type="bibr" target="#b20">Ritter et al., 2011)</ref> 81.30% T-POS-RIT 84.55% 84.83% T-POS-MIX 88.30% - GATE Tagger <ref type="figure" target="#fig_0">(Derczynski et al., 2013)</ref> 88.69% 89.37% ARK Tagger ( <ref type="bibr" target="#b19">Owoputi et al., 2013)</ref> 90.40% - TPANN ( <ref type="bibr" target="#b11">Gui et al., 2017)</ref> 90  The second part shows the results of the deep learning methods trained on the RIT-Train dataset. We can see that if the sequence labeling methods are just trained on the RIT-Train dataset, their accuracies can exceed those of most conventional taggers. Thus, the deep learning methods are competitive and avoid feature engineering. Com- pared with other models, the DCNN achieved best performance among the models just trained on RIT-Train dataset.</p><p>The third part shows the results of the deep learning methods trained on the mixed dataset of the RIT-Train and WSJ. As observed, when we added the WSJ data to train the models, all of them could obtain different degrees of improvement. Moreover, our model could make better use of the out-of-domain data and obtained the best result. Compared with the ARK tagger, which achieved the previous best result in conventional methods, our model was almost 0.78% better. The error reduction rate was more than 8%. Our model also outperformed the TPANN, which incorporated additional unlabeled in-domain data.</p><p>From the perspective of utilizing a low- dimensional context vector, we provided the same information (word information and context information) for all of the deep learning models as shown in <ref type="table" target="#tab_2">Table 2</ref>. However, except for the DCNN, the other models were incapable of utilizing the context information. Most of the models could not obtain obvious improvement. In contrast, our DCNN could make better use of the context information to generate more appropriate parameters for POS tagging. Next, we will analyze the behavior how the DCNN changes parameters when encountering different context vectors.</p><p>Intuitively, contexts with different language expression styles should be transformed into dif- ferent vectors. <ref type="figure" target="#fig_5">Figure 3</ref>     set to 10. However, the values of the last three dimensions are close to zero. Consequently, we set this hyperparameter to 7 and we achieve a higher accuracy of 91.27%. <ref type="figure" target="#fig_6">Figure 4</ref> shows how the weight matrix W o h in output gate gets changed when the model inputs different kinds of contexts. Through making a comparison among the sentences I, II and III. we can find that although the sentences II and III are both from Twitter, whereas the sentence I is from WSJ, If the style of sentence II is close to that of sentence I, then the model will produce similar weight values to achieve the task. If the style of sentence I is different from that of sentence III, then the model will produce different parameters more suitable for Twitter-specific sentences. The</p><formula xml:id="formula_6">|W o h3 W o h2 | &lt; l</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a t e x i t s h a 1 _ b a s e 6 4 = " 2 m M U y y P U s w M N O 5 1 P S k X K C a 7 Y H p o = " &gt; A A A C 2 n i c j V H L S s N A F D 2 N r 1 p f V X H l J l g E N 5 a k F n R Z c O O y g n 1 A W 0 u S T t v Q N A n J R C h t N + 7 E r T / g V j 9 I / A P 9 C + + M U 1 C L 6 I Q k Z 8 6 9 5 8 z c e + 3 Q c 2 N u G K 8 p b W F x a X k l v Z p Z W 9 / Y 3 M p u 7 1 T j I</head><formula xml:id="formula_7">I k c V n E C L 4 j q t h U z z / V Z h b v c Y / U w Y t b Q 9 l j N H p y L e O 2 G R b E b + F d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. <ref type="bibr" target="#b8">Forsyth (2007)</ref> 90.8% ARK Tagger 93.4% ¬± 0.3% <ref type="bibr" target="#b11">Gui et al. (2017)</ref> 94.1% Bi-LSTM(IRC) 90.3% Bi-LSTM(WSJ + IRC) 93.2% DCNN 94.0% similar phenomena can be found in other gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on NPSChat</head><p>The NPSChat Corpus <ref type="bibr" target="#b9">(Forsythand and Martell, 2007</ref>) is a PTB-POS annotated dataset of Internet Relay Chat (IRC) room messages from 2006. The corpus consists of 10,567 posts out of approxi- mately 500,000 posts gathered from various online chat services in accordance with their terms of service. The authors of the corpus made several decisions during the process that were unique to the chat domain regarding some abbreviations, emotions and misspelled words. For example, LOL and :-) were frequently encountered in the chat messages. Because these expressions con- veyed emotion, they were treated as individual tokens and tagged as interjections (UH). <ref type="table" target="#tab_5">Table 3</ref> lists the results of different taggers eval- uated on NPSChat. Our method was tested using the same setup as the experiments in <ref type="bibr" target="#b8">(Forsyth, 2007)</ref>. The training part contained 90% of the data. The testing part contained the remain- ing 10%. Based on the results, we can see that our method could achieve the best accu- racy (94.0%), which was significantly better than 90.8% <ref type="bibr" target="#b8">(Forsyth, 2007)</ref>. They trained the tagger on a mix of several corpora tagged with the Penn Treebank tag set. Our method also outperformed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Acc. <ref type="bibr" target="#b10">Gimpel et al. (2011)</ref> 89.17% <ref type="bibr" target="#b11">Gui et al. (2017)</ref> 92.8% ARK Tagger 93.2% ARK <ref type="table" target="#tab_2">Tagger ‚Ä†  92.38%  Bi-LSTM(OCT27)</ref> 90.59% Bi-LSTM(WSJ + IRC + OCT27) 91.57% DCNN(WSJ + IRC + OCT27) 92.42% the ARK tagger, which applies various external corpus and features, e.g., Brown clustering, PTB, Freebase lists of celebrities, and video games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on ARK-Twitter</head><p>The ARK-Twitter that contains 34K tokens uses a novel tagset. The training set (OCT27) is provided in ( <ref type="bibr" target="#b10">Gimpel et al., 2011</ref>). It is a dataset of POS-tagged tweets consisting almost entirely of tweets sampled from one particular day ( <ref type="bibr">October 27, 2010)</ref>. However, the test set was introduced in ( <ref type="bibr" target="#b19">Owoputi et al., 2013)</ref>, and contains 574 tweets (DAILY547). The DAILY547 consists of one random English tweet from every day between January 1, 2011 and June 30, 2012. Thus, the distribution between the training set and test set may be slightly different. For example, a substantial fraction of the messages in the training data are about a basketball game that occurred on that day.</p><p>The results of the ARK tagger and TweetNLP Tagger in <ref type="table" target="#tab_6">Table 4</ref> are reported in ( <ref type="bibr" target="#b19">Owoputi et al., 2013)</ref>.</p><p>We can see that our method could significantly outperform the TweetNLP Tagger. However, our method was worse than the ARK tagger. By analyzing the incorrect results, we found that 20.3% of the errors occurred between nouns and proper nouns. Because our model does not incorporate any knowledge of proper nouns, it is difficult for it to recognize proper nouns from datasets. As reported in ( <ref type="bibr" target="#b19">Owoputi et al., 2013)</ref>, if ARK-tagger does not add tag dictionary features and name list features, its performance will drop to 92.38%, which is lower than that of the DCNN. Thus, our model is also competitive when lacking of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>At a very early time, <ref type="bibr" target="#b24">Schmidhuber (1992)</ref> began to explore the concept of fast weights, in which one network can produce context-dependent weight changes for a second network <ref type="bibr" target="#b24">(Schmidhuber, 1992</ref><ref type="bibr" target="#b25">(Schmidhuber, , 1993</ref>. Moreover, they provided the theo- retical possibility of a recurrent network version. Recently, numerous studies have been conducted in this field ( <ref type="bibr" target="#b18">Moczulski et al., 2015;</ref><ref type="bibr" target="#b7">Fernando et al., 2016)</ref>. De <ref type="bibr" target="#b5">Brabandere et al. (2016)</ref> introduced a new framework called the dynamic filter network where the filters in the CNN are generated dynamically. <ref type="bibr" target="#b12">Ha et al. (2016)</ref> explored the use of this approach in recurrent networks. Our work uses a different mechanism to generate pa- rameters, which can make the parameters subject to a change in context representations. we cut off the data path from the main LSTM to the hyper LSTM. This method can prevent the hidden states of the main LSTM from influencing the hyper LSTM.</p><p>Recently, deep learning has achieved promising results on POS tagging. <ref type="bibr" target="#b23">Santos and Zadrozny (2014)</ref> used a CNN to construct a character-based model for English (PTB) and Portuguese. <ref type="bibr" target="#b28">Wang et al. (2015)</ref> used the bi-LSTM on WSJ and reported a state-of-the-art performance. However, because of a lack of training data and an uncon- strained writing style, these models encountered resistance in the implementation process on Twit- ter. In this work, we focused on the linguistic correlation between Twitter and newswire and took the linguistic characteristics into consid- eration. To selectively utilize out-of-domain data, we used a low-dimensional context vector to generate different parameters for text with different expression styles and obtained better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we study the problem of incor- porating labeled newswire texts for Twitter POS tagging tasks. From a linguistic perspective, we find that Twitter users not only tend to mimic the formal expressions of traditional media, like news, but they also appear to be developing linguistically informal styles. Hence, we predict that labeled data from the newswire should selectively be used to help tag contextual segments of tweets. To achieve this task, we introduce a novel deep neural network architecture that can dynamically generate different parameters based on different expression styles for POS tagging. To evaluate the performance of the proposed method, we compare the method with previous state-of-the-art methods on three different datasets. Experimental results demonstrate that the proposed method can achieve better performance in most cases. We also visualize some parameters learned for the proposed method to demonstrate the motivation for this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of WSJ and tweets. Segments with red highlights can be regarded as the similar expressions. Segments with blue highlights correspond to expressions that cannot be learned from WSJ.</figDesc><graphic url="image-1.png" coords="1,308.86,277.49,215.11,53.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model architecture of the proposed dynamic conversion neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>T</head><label></label><figDesc>-POS (Ritter et al., 2011) adopts hierarchi- cal clustering and Brown clustering methods to address the issue of OOV words and lexical variations. It also uses conditional random fields and other standard sets of features to perform the task. In this work, we trained it using three different sets: the WSJ (T-POS-WSJ), RIT-Train (T-POS-RIT) and a mixed corpus of WSJ, IRC, and RIT-Train (T-POS-MIX).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>GATE Tagger (Derczynski et al., 2013) uses an approach that combines the available taggers for different tagsets. The tagger adopts a vote- constrained bootstrapping method with unlabeled data and assigns prior probabilities to handle of unknown words and slang. ARK Tagger (Owoputi et al., 2013) is a system that reports the best accuracy on ARK-Twitter. It uses unsupervised word clustering and a variety of lexical features. TPANN (Gui et al., 2017) applies adversarial networks and autoencoder to model labeled out- of-domain data, unlabeled in-domain data and labeled in-domain data and achieved the best performance on RIT-Twitter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of context distribution. The left sentence comes from WSJ, and the right one comes from Twitter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization of the comparison of weight matrices. We denote the weight matrix of output gate as W o h. Each subgraph represents the result of elementwise subtraction |W o hi ‚àíW o hj |, where |¬∑| means absolute value, i and j are the sequence numbers of sentences. If the two sentences have a similar expression style, then the absolute value would be close to zero represented by white color. We use i ‚àº j to represent it. On the contrary, We use i ‚àº j to represent the different expression styles. We only visualize the weights on the last time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Token level accuracies of different methods on RIT-Test and RIT-Dev. The first part demonstrates the results of classic methods. The second part demonstrates a series of deep learning methods trained on RIT-train. The third part demonstrates the same deep learning methods train on the mixed dataset of RIT-train and WSJ. DCNN refers to our dynamic conversion neural network. Other models are described in the Section 3.2. The symbol * represents the model concatenates the context vectors with the word representations as inputs.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>visualizes the context distribution. Subfigure (a) shows the context vector extracted from WSJ. We can see that the formal expressions are mainly concentrated in the middle of the four dimensions. This phenomenon can be observed in the subfigure (b), where the formal expressions in the Twitter are concentrated in the middle of the same dimensions and the informal expressions are concentrated in another three dimensions. Notice that in our experimental setup, the dimensionality of the context vector is</figDesc><table>RT 
@NickSilly 
: 
Fun 
! 
RT 
@JackFMDFW 
: 
Put 
on 
your 
Boogie 
Shoes 
and 
Get 
Down 
Tonight 
with 
KC 
and 
The 
Sunshinr 
Band 
. 
#Dallas 
#concerts 
http 
:/ 
‚Ä¶ 

1 

3 

5 

7 

9 

Compound 
yields 
assume 
reinvestment 
of 
dividends 
and 
that 
the 
current 
yield 
continues 
for 
a 
year 
. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Accuracy comparison of different methods on 
NPSChat Corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy comparison of different methods on 
ARK-Twitter Corpus. The symbol  ‚Ä† represents ARK 
Tagger trained without tagdicts and namelists. 

</table></figure>

			<note place="foot" n="1"> http://www.ark.cs.cmu.edu/TweetNLP/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National Key R&amp;D Program (No.2017YFB1002104), National Natural Science Foundation of <ref type="bibr">China (No.61751201, 61532011, 61473092, and 61472088)</ref>, and STCSM (No.16JC1420401, 17JC1420200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Twitter mood predicts the stock market</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huina</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical comparison of supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ICML</title>
		<meeting>the 23rd ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dual-learning systems during speech category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Gyol</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Todd</forename><surname>Maddox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">488</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<title level="m">Frustratingly easy domain adaptation</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twitter part-of-speech tagging for all: Overcoming sparse and noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalina</forename><surname>Bontcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolution by evolution: Differentiable pattern producing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving automated lexical and discourse analysis of online chat dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lexical and discourse analysis of online chat dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig H</forename><surname>Forsythand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSC 2007</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging for twitter: Annotation, features, and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Heilman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: Human Language Technologies: short papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging for twitter with adversarial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2016</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dude, srsly?: The surprisingly formal nature of twitter&apos;s language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subbarao</forename><surname>Kambhampati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Jeremy Appleyard, and Nando de Freitas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05946</idno>
	</analytic>
	<monogr>
		<title level="m">Acdc: A structured efficient linear layer</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: an experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Earthquake shakes twitter users: real-time event detection by social sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW 2010</title>
		<meeting>WWW 2010</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging guidelines for the penn treebank project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>CIS</publisher>
			<biblScope unit="page">570</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Technical Reports</note>
	<note>3rd revision</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A self-referential weight matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<meeting>the International Conference on Artificial Neural Networks<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="446" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging with bidirectional long short-term memory recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.06168</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Early detection of disease outbreaks using the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumanan</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John S Brownstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMAJ</title>
		<imprint>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="829" to="831" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
