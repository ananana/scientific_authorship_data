<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering &quot;Simple&quot; Questions)</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Comcast Labs</orgName>
								<address>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Jojic</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Comcast Labs</orgName>
								<address>
									<postCode>20005</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering &quot;Simple&quot; Questions)</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2866" to="2872"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results-even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast&apos;s X1 entertainment platform with millions of users every day.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>First-order factoid question answering (QA) as- sumes that the question can be answered by a single fact in a knowledge base (KB). For ex- ample, "How old is Tom Hanks" is about the <ref type="bibr">[age]</ref> of <ref type="bibr">[Tom Hanks]</ref>. Also referred to as simple questions by <ref type="bibr" target="#b4">Bordes et al. (2015)</ref>, recent attempts that apply either complex linguistic reasoning or attention-based complex neural network architec- tures achieve up to 76% accuracy on benchmark sets ( <ref type="bibr" target="#b10">Golub and He, 2016;</ref><ref type="bibr" target="#b26">Yin et al., 2016)</ref>. While it is tempting to study QA systems that can handle more complicated questions, it is hard to reach rea- sonably high precision for unrestricted questions. For more than a decade, successful industry ap- plications of QA have focused on first-order ques- tions. This bears the question: are users even in- terested in asking questions beyond first-order (or are these use cases more suitable for interactive dialogue)? Based on voice logs from a major en- tertainment platform with millions of users every day, Comcast X1, we find that most existing use cases of QA fall into the first-order category.</p><p>Our strategy is to tailor our approach to first- order QA by making strong assumptions about the problem structure. In particular, we assume that the answer to a first-order question is a sin- gle property of a single entity in the KB, and de- compose the task into two subproblems: (a) de- tecting entities in the question and (b) classify- ing the question as one of the relation types in the KB. We simply train a vanilla recurrent neu- ral network (RNN) to solve each subproblem <ref type="bibr" target="#b9">(Elman, 1990</ref>). Despite its simplicity, our approach (RNN-QA) achieves the highest reported accu- racy on the SimpleQuestions dataset. While recent literature has focused on building more complex neural network architectures with attention mech- anisms, attempting to generalize to broader QA, we enforce stricter assumptions on the problem structure, thereby reducing complexity. This also means that our solution is efficient, another criti- cal requirement for real-time QA applications. In fact, we present a performance analysis of RNN- QA on Comcast's X1 entertainment system, used by millions of customers every day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>If knowledge is presented in a structured form (e.g., knowledge base (KB)), the standard ap-proach to QA is to transform the question and knowledge into a compatible form, and perform reasoning to determine which fact in the KB an- swers a given question. Examples of this approach include pattern-based question analyzers <ref type="bibr" target="#b5">(Buscaldi et al., 2010)</ref>, combination of syntactic pars- ing and semantic role labeling ( <ref type="bibr" target="#b2">Bilotti et al., 2007</ref><ref type="bibr" target="#b1">Bilotti et al., , 2010</ref>, as well as lambda calculus <ref type="bibr" target="#b0">(Berant et al., 2013</ref>) and combinatory categorical gram- mars (CCG) ( <ref type="bibr" target="#b22">Reddy et al., 2014)</ref>. A downside of these approaches is the reliance on linguistic re- sources/heuristics, making them language-and/or domain-specific. Even though <ref type="bibr" target="#b22">Reddy et al. (2014)</ref> claim that their approach requires less supervision than prior work, it still relies on many English- specific heuristics and hand-crafted features. Also, their most accurate model uses a corpus of para- phrases to generalize to linguistic diversity. Lin- guistic parsers can also be too slow for real-time applications.</p><p>In contrast, an RNN can detect entities in the question with high accuracy and low latency. The only required resources are word embeddings and a set of questions with entity words tagged. The former can be easily trained for any lan- guage/domain in an unsupervised fashion, given a large text corpus without annotations ( <ref type="bibr" target="#b18">Mikolov et al., 2013;</ref><ref type="bibr" target="#b20">Pennington et al., 2014</ref>). The lat- ter is a relatively simple annotation task that ex- ists for many languages and domains, and it can also be synthetically generated. Many researchers have explored similar techniques for general NLP tasks <ref type="bibr" target="#b7">(Collobert et al., 2011</ref>), such as named entity recognition ( <ref type="bibr" target="#b17">Lu et al., 2015;</ref><ref type="bibr" target="#b12">Hammerton, 2003)</ref>, sequence labeling <ref type="bibr" target="#b11">(Graves, 2008;</ref><ref type="bibr" target="#b6">Chung et al., 2014)</ref>, part-of-speech tagging ( <ref type="bibr" target="#b15">Huang et al., 2015;</ref><ref type="bibr" target="#b23">Wang et al., 2015</ref>), chunking <ref type="bibr" target="#b15">(Huang et al., 2015)</ref>.</p><p>Deep learning techniques have been studied ex- tensively for constructing parallel neural networks for modeling a joint probability distribution for question-answer pairs ( <ref type="bibr" target="#b14">Hsu et al., 2016;</ref><ref type="bibr" target="#b25">Yang et al., 2014;</ref><ref type="bibr" target="#b19">Mueller and Thyagarajan, 2016</ref>) and re-ranking answers output by a retrieval engine ( <ref type="bibr" target="#b21">Rao et al., 2016;</ref><ref type="bibr" target="#b24">Yang et al., 2016)</ref>. These more complex approaches might be needed for general-purpose QA and sentence sim- ilarity, where one cannot make assumptions about the structure of the input or knowledge. How- ever, as noted in Section 1, first-order factoid ques- tions can be represented by an entity and a relation type, and the answer is usually stored in a struc- tured knowledge base. <ref type="bibr" target="#b8">Dong et al. (2015)</ref> sim- ilarly assume that the answer to a question is at most two hops away from the target entity. How- ever, they do not propose how to obtain the target entity, since it is provided as part of their dataset. <ref type="bibr" target="#b3">Bordes et al. (2014)</ref> take advantage of the KB structure by projecting entities, relations, and sub- graphs into the same latent space. In addition to finding the target entity, the other key information to first-order QA is the relation type correspond- ing to the question. Many researchers have shown that classifying the question into one of the pre- defined types (e.g., based on patterns ( <ref type="bibr" target="#b27">Zhang and Lee, 2003)</ref> or support vector machines ( <ref type="bibr" target="#b5">Buscaldi et al., 2010)</ref>) improves QA accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>(a) From Question to Structured Query. Our approach relies on a knowledge base, containing a large set of facts, each one representing a binary [subject, relation, object] relationship. Since we assume first-order questions, the answer can be re- trieved from a single fact. For instance, "How old is Sarah Michelle Gellar?" can be answered by the fact: <ref type="bibr">[Sarah Michelle Gellar,bornOn,4/14/1977]</ref> The main idea is to dissect a first-order factoid natural-language question by converting it into a structured query: {entity "Sarah Michelle Gellar", relation: bornOn}. The process can be modular- ized into two machine learning problems, namely entity detection and relation prediction. In the former, the objective is to tag each question word as either entity or not. In the latter, the objective is to classify the question into one of the K relation types. We modeled both using an RNN.</p><p>We use a standard RNN architecture: Each word in the question passes through an embed- ding lookup layer E, projecting the one-hot vector into a d-dimensional vector x t . A recurrent layer combines this input representation with the hid- den layer representation from the previous word and applies a non-linear transformation to com- pute the hidden layer representation for the cur- rent word. The hidden representation of the final recurrent layer is projected to the output space of k dimensions and normalized into a probability dis- tribution via soft-max.</p><p>In relation prediction, the question is classified into one of the 1837 classes (i.e., relation types in Freebase). In the entity detection task, each word is classified as either entity or context (i.e., k = 2). Given a new question, we run the two RNN models to construct the structured query. Once every question word is classified as entity (de- noted by E) or context (denoted by C), we can ex- tract entity phrase(s) by grouping consecutive en- tity words. For example, for question "How old is Michelle Gellar", the output of entity detection is [C C C E E], from which we can extract a sin- gle entity "Michelle Gellar". The output of rela- tion prediction is bornOn. The inferred struc- tured query q becomes the following: {entityText: "michelle gellar", relation: bornOn} (b) Entity Linking. The textual reference to the entity (entityText in q) needs to be linked to an ac- tual entity node in our KB. In order to achieve that, we build an inverted index I entity that maps all n- grams of an entity (n ∈ {1, 2, 3}) to the entity's alias text (e.g., name or title), each with an associ- ated T F -IDF score. We also map the exact text (n = ∞) to be able to prioritize exact matches.</p><p>Following our running example, let us demon- strate how we construct I entity . Let us assume there is a node e i in our KB that refers to the actress "Sarah Michelle Gellar". The alias of this entity node is the name, which has three un- igrams ("sarah", "michelle", "gellar"), two bi- grams ("sarah michelle", "michelle gellar") and a single trigram (i.e., the entire name). Each one of these n-grams gets indexed in I entity with T F - IDF weights. Here is how the weights would be computed for unigram "sarah" and bigram "michelle gellar" (⇒ denotes mapping):</p><p>I entity ("sarah") ⇒ {node : e i , score : T F -IDF ("sarah", "sarah michelle gellar")} I entity ("michelle gellar") ⇒ {node : e i , score : T F -IDF ("michelle gellar", "sarah michelle gellar")} This is performed for every n-gram (n ∈ {1, 2, 3, ∞}) of every entity node in the KB. As- suming there is an entity node, say e j , for the ac- tress "Sarah Jessica Parker", we would end up cre- ating a second mapping from unigram "sarah": I entity ("sarah") ⇒ {node : e j , score : T F -IDF ("sarah", "sarah jessica parker")} In other words, "sarah" would be linked to both e i and e j , with corresponding T F -IDF weights.</p><p>Once the index I entity is built, we can link en- tityText from the structured query (e.g., "michelle gellar") to the intended entity in the KB (e.g., e i ). Starting with n = ∞, we iterate over n-grams of entityText and query I entity , which returns all matching entities in the KB with associated T F - IDF relevance scores. For each n-gram, retrieved entities are appended to the candidate set C. We continue this process with decreasing value of n (i.e., n ∈ {∞, 3, 2, 1})</p><p>Early termination happens if C is non-empty and n is less than or equal to the number of to- kens in entityText. The latter criterion is to avoid cases where we find an exact match but there are also partial matches that might be more relevant: For "jurassic park", for n = ∞, we get an exact match to the original movie "Jurassic Park". But we would also like to retrieve "Jurassic Park II" as a candidate entity, which is only possible if we keep processing until n = 2. (c) Answer Selection. Once we have a list of can- didate entities C, we use each candidate node e cand as a starting point to reach candidate answers.</p><p>A graph reachability index I reach is built for mapping each entity node e to all nodes e that are reachable, with the associated path p(e, e ). For the purpose of the current approach, we limit our search to a single hop away, but this index can be easily expanded to support a wider search. We use I reach to retrieve all nodes e that are reach- able from e cand , where the path from is consistent with the predicted relation r (i.e., r ∈ p(e cand , e )). These are added to the candidate answer set A. For example, in the example above, node e i 2 would have been added to the answer set A, since the path [bornOn] matches the predicted relation in the structured query. After repeating this process for each entity in C, the highest-scored node in A is our best answer to the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Data. Evaluation of RNN-QA was carried out on SimpleQuestions, which uses a subset of Freebase containing 17.8M million facts, 4M unique enti- ties, and 7523 relation types. Indexes I entity and I reach are built based on this knowledge base.</p><p>SimpleQuestions was built by <ref type="bibr" target="#b3">(Bordes et al., 2014</ref>) to serve as a larger and more diverse fac- toid QA dataset. <ref type="bibr">1</ref> Freebase facts are sampled in a way to ensure a diverse set of questions, then given to human annotators to create questions from, and get labeled with corresponding entity and relation type. There are a total of 1837 unique relation types that appear in SimpleQuestions.</p><p>Training. We fixed the embedding layer based on the pre-trained 300-dimensional Google News embedding, 2 since the data size is too small for training embeddings. Out-of-vocabulary words were assigned to a random vector (sampled from uniform distribution). Parameters were learned via stochastic gradient descent, using categori- cal cross-entropy as objective. In order to han- dle variable-length input, we limit the input to N tokens and prepend a special pad word if in- put has fewer. <ref type="bibr">3</ref> We tried a variety of configura- tions for the RNN: four choices for the type of RNN layer (GRU or LSTM, bidirectional or not); depth from 1 to 3; and drop-out ratio from 0 to 0.5, yielding a total of 48 possible configurations. For each possible setting, we trained the model on the training portion and used the validation portion to avoid over-fitting. After running all 48 experi- ments, the most optimal setting was selected by micro-averaged F-score of predicted entities (en- tity detection) or accuracy (relation prediction) on the validation set. We concluded that the opti- mal model is a 2-layer bidirectional LSTM (BiL- STM2) for entity detection and BiGRU2 for rela- tion prediction. Drop-out was 10% in both cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>End-to-End QA. For evaluation, we apply the re- lation prediction and entity detection models on each test question, yielding a structured query q = {entityT ext: t e , relation: r} (Section 3a). Entity linking gives us a list of candidate entity nodes (Section 3b). For each candidate entity e cand , we can limit our relation choices to the set of unique relation types that some candidate en- tity e cand is associated with. This helps eliminate the artificial ambiguity due to overlapping rela-tion types as well as the spurious ambiguity due to redundancies in a typical knowledge base. Even though there are 1837 relation types in Freebase, the number of relation types that we need to con- sider per question (on average) drops to 36. The highest-scored answer node is selected by find- ing the highest scored entity node e that has an outward edge of type r (Section 3c). We follow <ref type="bibr" target="#b4">Bordes et al. (2015)</ref> in comparing the predicted entity-relation pair to the ground truth. A ques- tion is counted as correct if and only if the entity we select (i.e., e) and the relation we predict (i.e, r) match the ground truth. <ref type="table">Table 1</ref> summarizes end-to-end experimental results. We use the best models based on valida- tion set accuracy and compare it to three prior ap- proaches: a specialized network architecture that explicitly memorizes facts ( <ref type="bibr" target="#b4">Bordes et al., 2015</ref>), a network that learns how to convolve sequence of characters in the question ( <ref type="bibr" target="#b10">Golub and He, 2016)</ref>, and a complex network with attention mechanisms to learn most important parts of the question ( <ref type="bibr" target="#b26">Yin et al., 2016)</ref>. Our approach outperforms the state of the art in accuracy (i.e., precision at top 1) by 11.9 points (15.6% relative).  <ref type="table">Table 1</ref>: Top-1 accuracy on test portion of Simple- Questions. Ablation study on last three rows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Last three rows quantify the impact of each component via an ablation study, in which we re- place either entity detection (ED) or relation pre- diction (RP) models with a naive baseline: (i) we assign the relation that appears most frequently in training data (i.e., bornOn), and/or (ii) we tag the entire question as an entity (and then perform the n-gram entity linking). Results confirm that RP is absolutely critical, since both datasets include a diverse and well-balanced set of relation types. When we applied the naive ED baseline, our re- sults drop significantly, but they are still compa- rable to prior results. Given that most prior work do not use the network to detect entities, we can deduce that our RNN-based entity detection is the reason our approach performs so well. Error Analysis. In order to better understand the weaknesses of our approach, we performed a blame analysis: Among 2537 errors in the test set, 15% can be blamed on entity detection -the rela- tion type was correctly predicted, but the detected entity did not match the ground truth. The reverse is true for 48% cases. <ref type="bibr">4</ref> We manually labeled a sample of 50 instances from each blame scenario. When entity detection is to blame, 20% was due to spelling inconsistencies between question and KB, which can be resolved with better text nor- malization during indexing (e.g., "la kings" refers to "Los Angeles Kings"). We found 16% of the detected entities to be correct, even though it was not the same as the ground truth (e.g., either "New York" or "New York City" is correct in "what can do in new york?"); 18% are inherently ambigu- ous and need clarification (e.g., "where bin laden got killed?" might mean "Osama" or "Salem"). When blame is on relation prediction, we found that the predicted relation is reasonable (albeit dif- ferent than ground truth) 29% of the time (e.g., "what was nikola tesla known for" can be classi- fied as profession or notable for).</p><p>RNN-QA in Practice. In addition to matching the state of the art in effectiveness, we also claimed that our simple architecture provides an efficient and modular solution. We demonstrate this by applying our model (without any modifications) to the entertainment domain and deploying it to the Comcast X1 platform serving millions of cus- tomers every day. Training data was generated synthetically based on an internal entertainment KB. For evaluation, 295 unique question-answer pairs were randomly sampled from real usage logs of the platform.</p><p>We can draw two important conclusions from <ref type="table">Table 2</ref>: First of all, we find that almost all of the user-generated natural-language questions (278/295∼95%) are first-order questions, support- ing the significance of first-order QA as a task. Second, we show that even if we simply use an open-sourced deep learning toolkit (keras.io) for implementation and limit the computational re- sources to 2 CPU cores per thread, RNN-QA an- swers 75% of questions correctly with very rea- sonable latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error</head><p>Count <ref type="table">Correct  220  Incorrect entity  16  Incorrect relation  42  Not first-order question  17  Total Latency  76±16 ms   Table 2</ref>: Evaluation of RNN-QA on real questions from X platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future work</head><p>We described a simple yet effective approach for QA, focusing primarily on first-order factual ques- tions. Although we understand the benefit of ex- ploring task-agnostic approaches that aim to cap- ture semantics in a more general way (e.g., <ref type="bibr" target="#b16">(Kumar et al., 2015)</ref>), it is also important to acknowl- edge that there is no "one-size-fits-all" solution as of yet.</p><p>One of the main novelties of our work is to de- compose the task into two subproblems, entity de- tection and relation prediction, and provide solu- tions for each in the form of a RNN. In both cases, we have found that bidirectional networks are ben- eficial, and that two layers are sufficiently deep to balance the model's ability to fit versus its ability to generalize.</p><p>While an ablation study revealed the importance of both entity detection and relation prediction, we are hoping to further study the degree of which im- provements in either component affect QA accu- racy. Drop-out was tuned to 10% based on valida- tion accuracy. While we have not implemented at- tention directly on our model, we can compare our results side by side on the same benchmark task against prior work with complex attention mecha- nisms (e.g., <ref type="bibr" target="#b26">(Yin et al., 2016)</ref>). Given the proven strength of attention mechanisms, we were sur- prised to find our simple approach to be clearly superior on SimpleQuestions.</p><p>Even though deep learning has opened the po- tential for more generic solutions, we believe that taking advantage of problem structure yields a more accurate and efficient solution. While first- order QA might seem like a solved problem, there is clearly still room for improvement. By revealing that 95% of real use cases fit into this paradigm, we hope to convince the reader that this is a valu- able problem that requires more attention.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I</head><label></label><figDesc>reach (e i ) ⇒ {node:e i 1 , text:The Grudge, path:[actedIn]} {node:e i 2 , text:4/14/1977, path:[bornOn]} {node:e i 3 , text:F. Prinze, path:[marriedTo]}</figDesc></figure>

			<note place="foot" n="1"> 75910/10845/21687 question-answer pairs for training/validation/test is an order of magnitude larger than comparable datasets. Vocabulary size is 55K as opposed to around 3K for WebQuestions (Berant et al., 2013). 2 word2vec.googlecode.com 3 Input length (N ) was set to 36, the maximum number of tokens across training and validation splits.</note>

			<note place="foot" n="4"> In remaining 37% incorrect answers, both models fail, so the blame is shared.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rank Learning for Factoid Question Answering with Linguistic and Semantic Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Matthew W Bilotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Elsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10</title>
		<meeting>the 19th ACM International Conference on Information and Knowledge Management, CIKM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured Retrieval for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Matthew W Bilotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="351" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3676</idno>
		<title level="m">Question answering with subgraph embeddings</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1506.02075</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Answering Questions with an N-gram Based Passage Retrieval Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Manuel Gómezsoriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="134" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Question answering over freebase with multicolumn convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="260" to="269" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Character-level question answering with attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00727</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Technical University Munich</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Named entity recognition with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hammerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003</title>
		<meeting>the seventh conference on Natural language learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="172" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recurrent neural network encoder with attention for community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Glass</surname></persName>
		</author>
		<idno>abs/1603.07044</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1506.07285</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Twisted recurrent network for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zefu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bay Area Machine Learning Symposium</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Siamese recurrent architectures for learning sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Thyagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA.</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-02-12" />
			<biblScope unit="page" from="2786" to="2792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation for answer selection with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1913" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without questionanswer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging with bidirectional long short-term memory recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.06168</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">anmm: Ranking short answer texts with attention-based neural matching model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint relational embeddings for knowledge-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Chul</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haechang</forename><surname>Rim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="645" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03391</idno>
		<title level="m">Simple question answering by attentive convolutional neural network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Question Classification Using Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Sun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03</title>
		<meeting>the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval, SIGIR &apos;03<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
