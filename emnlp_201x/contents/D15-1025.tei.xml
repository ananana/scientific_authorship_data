<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-lexical neural architecture for fine-grained POS Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris-Sud and LIMSI-CNRS</orgName>
								<address>
									<postCode>91403</postCode>
									<settlement>Rue John von Neumann, Orsay cedex France</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Löser</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris-Sud and LIMSI-CNRS</orgName>
								<address>
									<postCode>91403</postCode>
									<settlement>Rue John von Neumann, Orsay cedex France</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Université Paris-Sud and LIMSI-CNRS</orgName>
								<address>
									<postCode>91403</postCode>
									<settlement>Rue John von Neumann, Orsay cedex France</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-lexical neural architecture for fine-grained POS Tagging</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we explore a POS tagging application of neural architectures that can infer word representations from the raw character stream. It relies on two modelling stages that are jointly learnt: a convolutional network that infers a word representation directly from the character stream, followed by a prediction stage. Models are evaluated on a POS and morphological tagging task for German. Experimental results show that the convolu-tional network can infer meaningful word representations, while for the prediction stage, a well designed and structured strategy allows the model to outperform state-of-the-art results, without any feature engineering .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most modern statistical models for natural lan- guage processing (NLP) applications are strongly or fully lexicalized, for instance part-of-speech (POS) and named entity taggers, as well as lan- guage models, and parsers. In these models, the observed word form is considered as the elemen- tary unit, while its morphological properties re- main neglected. As a result, the vocabulary ob- served on training data heavily restricts the gener- alization power of lexicalized models.</p><p>Designing subword-level systems is appealing for several reasons. First, words sharing morpho- logical properties often share grammatical func- tion and meaning, and leveraging that information can yield improved word representations. Sec- ond, a subword-level analysis can address the out- of-vocabulary issue i.e the fact that word-level models fail to meaningfully process unseen word forms. This allows a better processing of morpho- logically rich languages in which there is a com- binatorial explosion of word forms, most of which are not observed during training. Finally, using subword units could allow processing of noisy text such as user-generated content on the Web, where abbreviations, slang usage and spelling mistakes cause the number of word types to explode.</p><p>This work investigates models that do not rely on a fixed vocabulary to make a linguistic predic- tion. Our main focus in this paper is POS tag- ging, yet the proposed approach could be applied to a wide variety of language processing tasks. Our main contribution is to show that neural net- works can successfully learn unlexicalized mod- els that infer a useful word representation from the character stream. This approach achieves state of-the-art performance on a German POS tagging task. This task is difficult because German is a morphologically rich language 1 , as reflected by the large number of morphological tags (255) in our study, yielding a grand total of more than 600 POS+MORPH tags. An aggravating factor is that these morphological categories are overtly marked by a handful of highly ambiguous inflec- tion marks (suffixes). We therefore believe that this case study is well suited to assess both the rep- resentation and prediction power of our models.</p><p>The architecture we explore in section 2 differs from previous work that only consider the charac- ter level. Following <ref type="bibr" target="#b13">(Santos and Zadrozny, 2014)</ref>, it consists in two stages that are jointly learnt. The lower stage is a convolutional network that infers a word embedding from a character string of ar- bitrary size, while the higher network infers the POS tags based on this word embedding sequence. For the latter, we investigate different architec- tures of increasing complexities: from a feedfor- ward and context-free inference to a bi-recurrent network that predicts the global sequence. Exper- imental results (section 4) show that the proposed approach can achieve state of the art performance and that the choice of architecture for the predic- tion part of the model has a significant impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Network Architectures</head><p>The different architectures we propose act in two stages to infer, for a sentence s = {w 1 , . . . , w |s| }, a sequence of tags {t 1 , . . . , t |s| }. Each tag belongs to the tagset T . The first stage is designed to rep- resent each word locally, and focuses on capturing the meaningful morphological information. In the second stage, we investigate different ways to pre- dict the tag sequence that differ in how the global information is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">From character to word level</head><p>To obtain word embeddings, the usual approach introduced by <ref type="bibr" target="#b0">(Bengio et al., 2003</ref>) relies on a fixed vocabulary W and each word w ∈ W is mapped to a vector of n f real valued features by a look-up matrix W ∈ R |W| * n f . To avoid the use of a fixed vocabulary, we propose to derive a word representation from a sequence of character em- bedding: if C denotes the finite set of characters, each character is mapped on a vector of n c features gathered in the look-up matrix C.</p><p>To infer a word embedding , we use a convo- lution layer ( <ref type="bibr" target="#b17">Waibel et al., 1990;</ref>), build as in ( <ref type="bibr" target="#b13">Santos and Zadrozny, 2014)</ref>. As illustrated in <ref type="figure" target="#fig_0">figure 1</ref>, a word w is a character sequence {c 1 , .., c |w| } represented by their embed- dings {C c 1 , .., C c |w| }, where C c i denotes the row in C associated to the character c i . A convolu- tion filter W conv ∈ R n f × R dc * nc is applied over a sliding window of d c characters, producing local features :</p><formula xml:id="formula_0">x n = W conv (C c n−dc+1 : .. : C cn ) T + b conv ,</formula><p>where x n is a vector of size n f obtained for each position n in the word 2 . The i-th element of the embedding of w is the maximum over the i-th ele- ments of the feature vectors :</p><formula xml:id="formula_1">[f ] i = tanh( max 1≤n≤|s| [x n ] i )</formula><p>Using a maximum after a sliding convolution win- dow ensures that the embedding combines local features from the whole word, and selects the more 2 Two padding character tokens are used to deal with bor- der effects. The first is added at the beginning and the second at the end of the word, as many times as it is necessary to ob- tain the same number of windows than the length of the word. Their embeddings are added to C. </p><formula xml:id="formula_2">n c W conv × (.) T + b conv n f e max(.) S h e n f 1 f 2 f 3 f 4 f 5 C c1 C c2 C c3 C c4 C c5 C sow C eow</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">From words to prediction</head><p>To predict the tag sequence associated to a sen- tence s, we first use a feedforward architecture, with a single hidden layer. To compute the proba- bility of tagging the n-th word in the sentence with tag t i , we use a window of d w word embeddings 3 centered around the word w n :</p><formula xml:id="formula_3">x n = f n− dw −1 2 : ... : f n+ dw −1 2 ,</formula><p>followed by a hidden and output layers:</p><formula xml:id="formula_4">s n = W o tanh(W h x n + b h ) + b o .<label>(1)</label></formula><p>The parameters of the hidden an output layers are respectively</p><formula xml:id="formula_5">W h , b h and W o , b o .</formula><p>We also experiment with a a bidirectional re- current layer, as described in ( . The forward and backward passes allow each prediction to be conditioned on the complete past and future contexts, instead of merely a neigh- boring window. As illustrated in <ref type="figure" target="#fig_1">figure 2</ref>, the for- ward hidden state, at position n, will be computed using the previous forward hidden state and the word embedding in position n: </p><formula xml:id="formula_6">− → h n = tanh( −−→ W f h f n + −−→ W hh −−→ h n−1 + b h ) n f |T | n h f1 f2 f3 f4 f5</formula><formula xml:id="formula_7">s n = W o ( − → h n : ← − h n ) + b o .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference and Training</head><p>To infer the tag sequence from the sequence of output layers defined by equations 1 or 2, we ex- plore two strategies. The first simply applies a softmax function to the output layer of the net- work described in the previous section. In this case, each tag prediction is made independently of the surrounding predictions. For sequence labeling, a more appropriate so- lution relies on the approach of (Collobert, 2011), also used in ( <ref type="bibr" target="#b13">Santos and Zadrozny, 2014</ref>). Let con- sider each possible tag sequence {t 1 , . . . , t |s| } as a possible path over a sequence of hidden states. We can add a transition matrix W trans and then com- pute the score of a sequence as follows:</p><formula xml:id="formula_8">s({t} |s| 1 , {w} |s| 1 ) = 1≤n≤|s| W trans t n−1 ,tn + [s n ] tn</formula><p>The Viterbi algorithm <ref type="bibr" target="#b16">(Viterbi, 1967)</ref> offers an ex- act solution to infer the path that gives the max- imum score. It is worth noticing that both these strategies can be applied to the feedforward and bidirectional recurrent networks. For both strate- gies, the whole network can estimate conditional log-likelihood of a tag sequence given a sentence s and the set of parameters θ. This criterion can then be optimized using a stochastic gradient as- cent with the back-propagation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>The choice to consider words from the charac- ter level has recently been more and more ex- plored. While its raw application to language modeling did not achieve clear improvement over the word-based models ( <ref type="bibr" target="#b11">Mikolov et al., 2012</ref>), this approach shown impressive results for text gen- eration <ref type="bibr" target="#b15">(Sutskever et al., 2011;</ref><ref type="bibr" target="#b8">Graves, 2013)</ref>. However, for this line of work, the main issue is to learn long range dependencies at the character level since the word level is not considered by the model. More recently, the character level was con- sidered as more interpretable and convenient way to explore and understand recurrent net- works ( <ref type="bibr" target="#b9">Karpathy et al., 2015</ref>). In (Zhang and Le- Cun, 2015), the authors build a text understand- ing model that does not require any knowledge and uses hierarchical feature extraction. Here the character level allows the model to ignore the def- inition a priori of a vocabulary and let the model build its own representation of a sentence or a doc- ument, directly from the character level. To some extent, our work can be considered as an extension of their work, tailored for POS tagging.</p><p>( <ref type="bibr" target="#b13">Santos and Zadrozny, 2014</ref>) applies a very sim- ilar model to the POS tagging of <ref type="bibr">Portuguese and English. (Luong et al., 2013</ref>) also descends lower than the word level, using a dictionary of mor- phemes and recursive neural networks to model the structure of the words. Similarly, this allows a better representation of rare and complex words, evaluated on a word similarity task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Experiments are carried out on the Part-of-Speech and Morphological tagging tasks using the Ger- man corpus TIGER Treebank ( <ref type="bibr">Brants et al., 2002</ref>). To the best of our knowledge, the best results on this task were published in ( <ref type="bibr" target="#b12">Mueller et al., 2013</ref>), who applied a high-order CRF that includes an in- tensive feature engineering to five different lan- guages. German was highlighted as having 'the most ambiguous morphology'. The corpus, <ref type="bibr">de</ref>  <ref type="table">Table 1</ref>: Comparison of the feedforward and bidirectional recurrent architectures for predictions, with different settings. The non-lexical encoding is convolutional. CRF refers to state-of-the-art system of ( <ref type="bibr" target="#b12">Mueller et al., 2013</ref>). Simple and Struct. respectively denote the position-by-position and structured prediction. * indicates our best configuration. scribed in details in ( <ref type="bibr" target="#b6">Fraser et al., 2013)</ref>, contains a training set of 40472 sentences, a development and a test set of both 5000 sentences. We consider the two tagging tasks, with first a coarse tagset (54 tags), and then a morpho-syntactical rich tagset (619 items observed on the the training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental settings</head><p>All the models are implemented 4 with the Theano library ( <ref type="bibr" target="#b1">Bergstra et al., 2010)</ref>. For optimization, we use Adagrad ( <ref type="bibr" target="#b5">Duchi et al., 2011</ref>), with a learn- ing rate of 0.1. The other hyperparameters are: the window sizes, d c and d w , respectively set to 5 and 9, the dimension of character embeddings, word embeddings and of the hidden layer, n c , n f and n h , that are respectively of 100, 200 and 200 5 . The models were trained on 7 epochs. Parame- ter initialization and corpus ordering are random, and the results presented are the average and stan- dard deviation of the POS Tagging error rate over 5 runs. <ref type="bibr">4</ref> Implementation is available at https://github. com/MatthieuLabeau/NonlexNN 5 For both the learning rate and the embedding sizes, re- sults does not differ in a significant way in a large range of hy- perparameters, and their impact resides more in convergence speed and computation time</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The first experiment aims to evaluate the efficiency of a convolutional encoding with the basic feed- forward architecture for prediction. We compare a completely non-lexicalized model which relies only on a character-level encoding with a lexical- ized model where we use conventional word em- beddings stored with a fixed vocabulary 6 . Re- sults are reported in <ref type="table">Table 1</ref> along with with the state-of-the-art results published in <ref type="bibr" target="#b12">(Mueller et al., 2013)</ref>. Results show that a character-level en- coding yields better results than the conventional word-level encoding. Moreover, the structured in- ference allows the model to achieve accuracy rea- sonably close to the performance of a high-order CRF that uses handcrafted features. Finally, the model that uses the concatenation of both the char- acter and word-level embeddings outperforms the state-of-the-art system on the more difficult task, without any feature engineering.</p><p>To give an idea of how a simple model would perform on such task, the reader can refer to ( <ref type="bibr" target="#b14">Schmid and Laws, 2008)</ref> and <ref type="bibr" target="#b12">(Mueller et al., 2013</ref>). For instance in the former, by choosing the most probable tag position-by-position, the error rate on the development set of the TIGER dataset is 32.7 for the simple POS Tagging task.</p><p>We further analyze the results by looking at the error rates respectively on known and un- known words <ref type="bibr">7</ref> . From table 2, we observe that the number of unknown words wrongly labeled is divided by 3 for POS and almost divided by 2 for POS+Morph tagging, showing the ability of character-level encoding to generalize to new words. Moreover, a strictly non-lexical encoding makes slightly more mistakes on words already seen, whereas the model that concatenates both embeddings will make less mistakes for both un- known and known words.</p><p>This shows that information from the context and from the morphology are complementary, which is conjectured in <ref type="bibr" target="#b12">(Mueller et al., 2013</ref>) by using a morphological analyzer in complement of higher-order CRF. <ref type="bibr">Lex</ref>  <ref type="table">Table 2</ref>: Error counts for known/unknown words in the test set, with a structured feedforward pre- diction model for the tagging task.</p><p>In the second set of experiments, we evaluate the convolutional encoding with a bidirectional re- current network for prediction. Results are pre- sented in the second half of <ref type="table">Table 1</ref>. Surprisingly, this architecture performs poorly with simple in- ference, but clearly improves when predicting a structured output using the Viterbi algorithm, both for training and testing. Moreover, a non-lexical model trained to infer a tag sequence with the Viterbi algorithm achieves results that are close to the state-of-the-art, thus validating our approach. We consider that this improvement comes from the synergy between using a global training objective with a global hidden representation, complexify- ing the model but allowing a more efficient solu- tion. Finally, the model that uses the combination of both the character and word-level embeddings yields the best results. It is interesting to notice that the predictive architecture has no influence on the results of the simple task when the prediction is structured, but improves them on the difficult task. This also shows that the contribution of word em- beddings to our model corresponds to a difference of 1.5 to 2 points in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we explored new models that can in- fer meaningful word representations from the raw character stream, allowing the model to exploit the morphological properties of words without using any handcrafted features or external tools. These models can therefore efficiently process words that were unseen in the training data. The evaluation was carried out on a POS and morphological tag- ging task for German. We described different ar- chitectures that act in two stages: the first stage is a convolutional network that infers a word represen- tation directly from the character stream, while the second stage performs the prediction. For the pre- diction stage, we investigated different solutions showing that a bidirectional recurrent network can outperform state-of-the-art results when using a structured inference algorithm.</p><p>Our results showed that character-level encod- ing can address the unknown words problem for morphologically complex languages. In the fu- ture, we plan to extend these models to other tasks such as syntactic parsing and machine translation. Moreover, we will also investigate other architec- tures to infer word embeddings from the character level. For instance, preliminary experiments show that bidirectional recurrent network can achieve very competitive and promising results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the layer for characterlevel encoding of words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bidirectional recurrent architecture for tag prediction. The upper part is used in the case of structured inference. −−→ W f h and −−→ W hh are the transition matrices of the forward part of the layer, and b h is the bias. The backward hidden states are computed similarly, and the hidden states of each direction are concatenated to pass through an output layer:</figDesc></figure>

			<note place="foot" n="1"> Besides inflected forms, German is characterized by a possibly infinite and evolving set of compound nouns.</note>

			<note place="foot" n="3"> Similarly, we use special word tokens for padding.</note>

			<note place="foot" n="6"> Every word that appears in the training set.</note>

			<note place="foot" n="7"> Unknown words refer to words present in the development or test sets, but not in the training set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their helpful comments and suggestions. This work has been partly funded by the Eu-ropean Unions Horizon 2020 research and in-novation programme under grant agreement No. 645452 (QT21).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and 236 GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<publisher>Oral Presentation</publisher>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wolfgang Lezius, and George Smith. 2002. The TIGER treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on treebanks and linguistic theories</title>
		<meeting>the workshop on treebanks and linguistic theories</meeting>
		<imprint>
			<biblScope unit="page" from="24" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning for efficient discriminative parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge sources for constituent parsing of German, a morphologically rich and less-configurational language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="85" />
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<imprint>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="2013-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>abs/1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visualizing and understanding recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1506.02078</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08-08" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haison</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient higher-order CRFs for morphological tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
	<note>Seattle, Washington, USA, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning character-level representations for part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<editor>Tony Jebara and Eric P. Xing</editor>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1818" to="1826" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Laws</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Computational Linguistics</title>
		<meeting>the 22Nd International Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
	<note>COLING &apos;08</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11</title>
		<editor>Lise Getoor and Tobias Scheffer</editor>
		<meeting>the 28th International Conference on Machine Learning (ICML-11), ICML &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theor</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Readings in Speech Recognition, chapter Phoneme Recognition Using Time-delay Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geofrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="393" to="404" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Text understanding from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1502.01710</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
