<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Translate for Multilingual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
							<email>ferhan ture@cable.comcast.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Comcast Labs</orgName>
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>1110 Vermont Ave NW Ste 600 Washington, 10 Moulton St Cambridge</addrLine>
									<postCode>20005, 02138</postCode>
									<settlement>Raytheon</settlement>
									<region>DC, MA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
							<email>eboschee@bbn.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Comcast Labs</orgName>
								<orgName type="institution">BBN Technologies</orgName>
								<address>
									<addrLine>1110 Vermont Ave NW Ste 600 Washington, 10 Moulton St Cambridge</addrLine>
									<postCode>20005, 02138</postCode>
									<settlement>Raytheon</settlement>
									<region>DC, MA</region>
									<country>USA, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Translate for Multilingual Question Answering</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="573" to="584"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, context-based, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong base-line (p &lt; 0.05): translating all text into En-glish, then training a classifier based only on English (original or translated) text.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering (QA) is a specific form of the information retrieval (IR) task, where the goal is to find relevant well-formed answers to a posed question. Most QA pipelines consist of three main stages: (a) preprocessing the question and collec- tion, (b) retrieval of candidate answers in the col- lection, and (c) ranking answers with respect to their relevance to the question and return the top N an- swers. The types of questions can range from fac- toid (e.g., "What is the capital of France?") to causal (e.g., "Why are trees green?"), and opinion ques- tions (e.g., "Should USA lower the drinking age?").</p><p>The most common approach to multilingual QA (MLQA) has been to translate all content into its * This work was completed while author was an employee of Raytheon BBN Technologies. most probable English translation via machine trans- lation (MT) systems. This strong baseline, which we refer to as one-best MT (1MT), has been successful in prior work ( <ref type="bibr" target="#b15">Hartrumpf et al., 2009;</ref><ref type="bibr" target="#b21">Lin and Kuo, 2010;</ref><ref type="bibr" target="#b38">Shima and Mitamura, 2010)</ref>. However, re- cent advances in cross-lingual IR (CLIR) show that one can do better by representing the translation space as a probability distribution <ref type="bibr" target="#b41">(Ture and Lin, 2014</ref>). In addition, MT systems perform substan- tially worse with user-generated text, such as web forums (Van der <ref type="bibr" target="#b42">Wees et al., 2015)</ref>, which provide extra motivation to consider alternative translation approaches for higher recall. To our knowledge, it has yet to be shown whether these recent advance- ments in CLIR transfer to MLQA.</p><p>We introduce a novel answer ranking approach for MLQA (i.e., Learning to Translate or L2T), a model that learns the optimal translation of question and/or candidate answer, based on how well it discrimi- nates between good and bad answers. We achieve this by introducing a set of features that encapsulate lexical and semantic similarities between a question and a candidate answer through various translation strategies (Section 3.1). The model then learns fea- ture weights for each combination of translation di- rection and method, through a discriminative train- ing process <ref type="bibr">(Section 3.2)</ref>. Once a model is trained, it can be used for MLQA, by sorting each candidate answer in the collection by model score. Instead of learning a single model to score candidate answers in any language, it might be meaningful to train a separate model that can learn to discriminate be- tween good and bad answers in each language. This can let each model learn feature weights custom to the language, therefore allowing a more fine-grained ranking (Section 3.4). We call this alternative ap- proach <ref type="bibr">Learning to Custom Translate (L2CT)</ref>.</p><p>Experiments on the DARPA Broad Operational Language Technologies (BOLT) IR task 1 confirm that L2T yields statistically significant improve- ments over a strong baseline (p &lt; 0.05), in three out of four experiments. L2CT outperformed the base- line as well, but was not more effective than L2T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For the last decade or so, research in QA has mostly been driven by annual evaluation campaigns like TREC, 2 CLEF, <ref type="bibr">3</ref> and NTCIR. <ref type="bibr">4</ref> Most earlier work re- lied on either rule-based approaches where a set of rules were manually crafted for each type of ques- tion, or IR-like approaches where each pair of ques- tion and candidate answer was scored using retrieval functions (e.g., <ref type="bibr">BM25 (Robertson et al., 2004)</ref>). On the other hand, training a classifier for ranking can- didate answers allows the exploitation of various features extracted from the question, candidate an- swer, and surrounding context ( <ref type="bibr" target="#b23">Madnani et al., 2007;</ref><ref type="bibr" target="#b43">Zhang et al., 2007)</ref>. In fact, an explicit comparison at 2007 TREC confirmed the superiority of machine learning-based (ML-based) approaches (F-measure 35.9% vs 38.7%) ( <ref type="bibr" target="#b43">Zhang et al., 2007)</ref>. Learning-to- rank approaches have also been applied to QA suc- cessfully ( <ref type="bibr" target="#b1">Agarwal et al., 2012)</ref>.</p><p>Previous ML-based approaches have introduced useful features from many aspects of natural lan- guage, including lexical ( <ref type="bibr" target="#b7">Brill et al., 2001;</ref><ref type="bibr" target="#b3">Attardi et al., 2001</ref>), syntactic ( <ref type="bibr" target="#b2">Alfonseca et al., 2001;</ref><ref type="bibr" target="#b17">Katz et al., 2005</ref>), semantic ( <ref type="bibr" target="#b9">Cui et al., 2005;</ref><ref type="bibr" target="#b17">Katz et al., 2005;</ref><ref type="bibr" target="#b2">Alfonseca et al., 2001;</ref><ref type="bibr" target="#b16">Hovy et al., 2001</ref>), and discourse features, such as coref- erence resolution <ref type="bibr" target="#b27">(Morton, 1999)</ref>, or identifying temporal/spatial references ( <ref type="bibr" target="#b35">Saquete et al., 2005;</ref><ref type="bibr" target="#b14">Harabagiu and Bejan, 2005</ref>), which are especially useful for "why" and "how" questions ( <ref type="bibr" target="#b20">Kolomiyets and Moens, 2011</ref>). Additionally, semantic role la- beling and dependency trees are other forms of semantic analysis used widely in NLP applica- tions <ref type="bibr" target="#b37">(Shen and Lapata, 2007;</ref><ref type="bibr" target="#b9">Cui et al., 2005</ref>).</p><p>When dealing with multilingual collections, most prior approaches translate all text into English be- forehand, then treat the task as monolingual retrieval (previously referred to as 1MT). At recent evalua- tion campaigns like CLEF and NTCIR, 5 almost all teams simply obtained the one-best question trans- lation, treating some online MT system as a black box <ref type="bibr" target="#b0">(Adafre and van Genabith, 2009;</ref><ref type="bibr" target="#b15">Hartrumpf et al., 2009;</ref><ref type="bibr" target="#b23">Martinez-Gonzalez et al., 2009;</ref><ref type="bibr" target="#b21">Lin and Kuo, 2010;</ref><ref type="bibr" target="#b38">Shima and Mitamura, 2010)</ref>, with few notable exceptions that took term importance <ref type="bibr" target="#b32">(Ren et al., 2010)</ref>, or semantics ( <ref type="bibr" target="#b28">Munoz-Terol et al., 2009)</ref> into account. Even for non-factoid MLQA, most prior work does not focus on the translation com- ponent ( <ref type="bibr" target="#b22">Luo et al., 2013;</ref><ref type="bibr" target="#b8">Chaturvedi et al., 2014</ref>).</p><p>Contributions. Ture and Lin described three meth- ods for translating queries into the collection lan- guage in a probabilistic manner, improving docu- ment retrieval effectiveness over a one-best transla- tion approach <ref type="bibr">(2014)</ref>. Extending this idea to MLQA appears as a logical next step, yet most prior work relies solely on the one-best translation of questions or answers ( <ref type="bibr" target="#b19">Ko et al., 2010b;</ref><ref type="bibr" target="#b13">García-Cumbreras et al., 2012;</ref><ref type="bibr" target="#b8">Chaturvedi et al., 2014</ref>), or selects the best translation out of few options ( <ref type="bibr" target="#b34">Sacaleanu et al., 2008;</ref><ref type="bibr" target="#b26">Mitamura et al., 2006</ref>). Mehdad et al. re- ported improvements by including the top ten trans- lations (instead of the single best) and computing a distance-based entailment score with each <ref type="bibr">(2010)</ref>. While Espla-Gomis et al. argue that using MT as a black box is more convenient (and modular) (2012), there are potential benefits from a closer integra- tion between statistical MT and multilingual re- trieval <ref type="bibr" target="#b29">(Nie, 2010)</ref>. To the best of our knowledge, there is no prior work in the literature, where the optimal query and/or answer translation is learned via machine learning. This is our main contribu- tion, with which we outperform the state of the art.</p><p>In addition to learning the optimal translation, we learn the optimal subset of the training data for a given task, where the criteria of whether we include a certain data instance is based on either the source language of the sentence, or the language in which the sentence was annotated. Training data selection strategies have not been studied extensively in the QA literature, therefore the effectiveness of our sim- ple language-related criteria can provide useful in- sights to the community.</p><p>When there are multiple independent approaches for ranking question-answer pairs, it is required to perform a post-retrieval merge: each approach generates a ranked list of answers, which are then merged into a final ranked list. This type of sys- tem combination approach has been applied to var- ious settings in QA research. Merging at the document-level is common in IR systems (e.g., <ref type="bibr" target="#b39">(Tsai et al., 2008)</ref>), and has shown to improve multilin- gual QA performance as well ( <ref type="bibr" target="#b13">García-Cumbreras et al., 2012</ref>). Many QA systems combine an- swers obtained by different variants of the under- lying model (e.g., <ref type="bibr" target="#b7">(Brill et al., 2001</ref>) for monolin- gual, ( <ref type="bibr" target="#b18">Ko et al., 2010a;</ref><ref type="bibr" target="#b19">Ko et al., 2010b</ref>) for multi- lingual QA). We are not aware, however, of any prior work that has explored the merging of answers that are generated by language-specific ranking mod- els. Although this does not show increased effec- tiveness in our experiments, we believe that it brings a new perspective to the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Our work is focused on a specific stage of the QA pipeline, namely answer ranking: Given a natural-language question q and k candidate answers s 1 , . . . , s k , we score each answer in terms of its rel- evance to q. In our case, candidate answers are sen- tences extracted from all documents retrieved in the previous stage of the pipeline (using Indri <ref type="bibr" target="#b25">(Metzler and Croft, 2005)</ref>). Hereafter, sentence and answer might be used interchangeably.</p><p>While our approach is not language-specific, we assume (for simplicity) that questions are in English, whereas sentences are in either English, Arabic, or Chinese. Non-English answers are translated back into English before returning to user.</p><p>Our approach is not limited to any question type, factoid or non-factoid. Our main motivation is to provide good QA quality on any multilingual Web collection. This entails finding answers to questions where there is no single answer, and for which hu- man agreement is low. We aim to build a system that can successfully retrieve relevant information from open-domain and informal-language content.</p><p>In this scenario, two assumptions made by many of the prior approaches fail: 1) We can accurately classify questions via tem- plate patterns ( <ref type="bibr">Chaturvedi et al.</ref> argue that this does not hold for non-factoid questions <ref type="formula" target="#formula_0">(2014))</ref> 2) We can accurately determine the relevance of an answer, based on its automatic translation into English (Wees et al. show how recall decreases when translating user-generated text <ref type="formula" target="#formula_0">(2015))</ref> To avoid these assumptions, we opted for a more adaptable approach, in which question-answer rele- vance is modeled as a function of features, intended to capture the relationship between the question and sentence text. Also, instead of relying solely on a single potentially incorrect English translation, we increase our chances of a hit by translating both the question and the candidate answer, using four dif- ferent translation methods. Our main features, de- scribed throughout this section, are based on lexical similarity computed using these translations. The classifier is trained on a large number of question- answer pairs, each labeled by a human annotator with a binary relevance label. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation</head><p>In MLQA, since questions and answers are in differ- ent languages, most approaches translate both into an intermediary language (usually English). Due to the error-prone nature of MT, valuable information often gets "lost in translation". These errors are es- pecially noticeable when translating informal text or less-studied languages (Van der Wees et al., 2015).</p><p>Translation Direction. We perform a two-way translation to better retain the original meaning: in addition to translating each non-English sen- tence into English, we also translate the English questions into Arabic and Chinese (using multiple translation methods, described below). For each question-answer pair, we have two "views": com- paring translated question to the original sentence (i.e., collection-language (CL) view); and compar- ing original question to the translated sentence (i.e., question-language (QL) view).</p><p>Translation Method. When translating text for re- trieval tasks like QA, including a variety of alterna-tive translations is as important as finding the most accurate translation, especially for non-factoid ques- tions, where capturing (potentially multiple) under- lying topics is essential. Recent work in cross- language IR (CLIR) has shown that incorporating probabilities from the internal representations of an MT system to "translate" the question can accom- plish this, outperforming standard one-best transla- tion ( <ref type="bibr" target="#b41">Ture and Lin, 2014</ref>). We hypothesize that these improvements transfer to multilingual QA as well.</p><p>In addition to translation directions, we explored four translation methods for converting the English question into a probabilistic representation (in Ara- bic and Chinese). Each method builds a probability distribution for every question word, expressing the translation space in the collection language. More details of first three methods can be found in <ref type="bibr" target="#b41">(Ture and Lin, 2014</ref>), while fourth method is a novel query translation method adapted from the neural network translation model described in <ref type="bibr" target="#b11">(Devlin et al., 2014)</ref>. Word: In MT, a word alignment is a many-to- many mapping between source-and target-language words, learned without supervision, at the beginning of the training pipeline <ref type="bibr" target="#b31">(Och, 2003)</ref>. These align- ments can be converted into word translation proba- bilities for CLIR <ref type="bibr" target="#b10">(Darwish and Oard, 2003)</ref>. For example, in an English-Arabic parallel corpus, if an English word appears m times in total and is aligned to a certain Arabic word k times, we assign a probability of k m for this translation. This simple idea has performed greatly in IR for generating a probability distribution for query word translations. Grammar: Probabilities are derived from a syn- chronous context-free grammar, which is a typical translation model found in MT systems <ref type="bibr" target="#b41">(Ture and Lin, 2014</ref>). The grammar contains rules r that follow the form α|β|A|(r), stating that source- language word α can be translated into target- language word β with an associated likelihood value (r) (A represents word alignments). For each rule r that applies to the question, we identify each source word s j . From the word alignment information in- cluded in the rule, we can find all target words that s j is aligned to. By processing all the rules to ac- cumulate likelihood values, we construct translation probabilities for each word in the question. 10-best: Statistical MT systems retrieve a ranked list of translations, not a single best. Ture and Lin ex- ploited this to obtain word translation probabilities from the top 10 translations of the question <ref type="bibr">(2014)</ref>. For each question word w, we can extract which grammar rules were used to produce the translation -once we have the rules, word alignments allow us to find all target-language words that w translates into. By doing this for each question translation, we construct a probability distribution that defines the translation space of each question word. Context: Neural network-based MT models learn context-dependent word translation probabilities - the probability of a target word is dependent on the source word it aligns to, as well as a 5-word window of context <ref type="bibr" target="#b11">(Devlin et al., 2014</ref>). For example, if the Spanish word "placer" is aligned to the English word "pleasure", the model will not only learn from this word-to-word alignment but also consider the source sentence context (e.g., "Fue un placer conocerte y tenerte unos meses."). However, since short ques- tions might lack full sentence context, our model should have the flexibility to translate under par- tial or no context. Instead of training the NN-base translation model on full, well-formed sentences, we custom-fit it for question translation: words in the context window are randomly masked by replacing it with a special filler token &lt;F&gt;. This teaches the model how to accurately translate with full, partial context, or no context. For the above example, we generate partial contexts such as "fue un placer &lt;F&gt; y" or "&lt;F&gt; &lt;F&gt; placer conocerte y". Since there are many possibilities, if the context window is large, we randomly sample a few of the possibilities (e.g., 4 out of 9) per training word.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we display the probabilistic structure produced the grammar-based translation method, when implemented as described above. Each En- glish word in the question is translated into a prob- abilistic structure, consisting of Chinese words and corresponding probabilities that represent how much weight the method decides to put on that specific word. Similar structures are learned with the other three translation methods.</p><p>We are not aware of any other MLQA approach that represents the question-answer pair based on their probabilistic translation space.  </p><note type="other">child:'['0.32'0.25'0.21'0.15'...']' labor:</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>child$labor$ child$$$$$$$$$$$$$$$$$$$$$$$$children$$$$$$$$$$$$$$$$$$$$child$ labor$$$$$$$$$$$$$$$$$$$$$$$$$labor$$$$$$$$$$$$$$$$$$$$$$$$labor$force</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>Given two different translation directions (CL and QL), and four different translation methods (Word, Grammar, 10-best, Context), our strategy is to lever- age a machine learning process to determine how helpful each signal is with respect to the end task. For this, we introduced separate question-answer similarity features based on each combination of translation direction and method. Collection-language Features. In order to compute a single real-valued vector to represent the question in the collection language (LexCL), we start with the probabilistic structure representing the question translation (e.g., <ref type="figure" target="#fig_0">Figure 1</ref> is one such structure when the translation method is grammar-based). For each word in the collection-language vocabulary, we compute a weight by averaging its probability across the terms in the probabilistic structure.</p><p>v q grammar (w) = avg i Pr(w|q i )</p><p>where w is a non-Engish word and Pr(w|q i ) is the probability of w in the probability distribution cor- responding to the i th query term. <ref type="figure" target="#fig_3">Figure 2</ref> shows the real-valued vector computed based on the probabilistic question translation in <ref type="figure" target="#fig_0">Figure 1</ref>. The Chinese word translated as "child la- bor" has a weight of 0.32, 0.36, and 0 in the proba- bility distributions of the three query terms, respec- tively. Averaging these three values results in the fi- nal weight of 0.23 in v q grammar in <ref type="figure" target="#fig_3">Figure 2</ref>. Notice that these weights are normalized by construction.</p><p>Similarly, a candidate answer s in Chinese is rep- resented by normalized word frequencies:</p><formula xml:id="formula_1">v s (w) = freq(w|s) w freq(w |s)<label>(2)</label></formula><p>Given the two vectors, we compute the co- sine similarity. Same process is repeated for the  other three translation methods. The four lexical collection-language similarity features are collec- tively called LexCL.  Question-language Features. As mentioned be- fore, we also obtain a similarity value by translating the sentence (s 1best ) and computing the cosine sim- ilarity with the original question (q). v q and v s 1best are computed using Equation 2. Although it is pos- sible to translate the sentence into English using the same four methods, we only used the one-best trans- lation due to the computational cost. Hence, we have only one lexical similarity feature in the QL view (call LexQL). The computation process for the five lexical sim- ilarity features is summarized in <ref type="table" target="#tab_1">Table 1</ref>. Af- ter computation, feature weights are learned via a maximum-entropy model. <ref type="bibr">7</ref> Although not included in the figure or table, we also include the same set of features from the sentence preceding the answer (within the corresponding forum post), in order to represent the larger discourse.</p><formula xml:id="formula_2">(v q ) repr. (v s ) Value LexCL v q word v s v q 10best v s cosine (v q , v s ) v q context v s v q grammar v s LexQL v q v s 1best</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Selection</head><p>In order to train a machine learning model with our novel features, we need positive and negative exam- ples of question-answer pairs (i.e., (q, s)). For this, for each training question, our approach is to hire human annotators to label sentences retrieved from the non-English collections used in our evaluation. It is possible to label the sentences in the source lan- guage (i.e., Arabic or Chinese) or in the question language (i.e., translated into English). In this sec- tion, we explore the question of whether it is useful to distinguish between these two independently cre- ated labels, and whether this redundancy can be used to improve the machine learning process.</p><p>We hypothesize two reasons why selecting train- ing data based on language might benefit MLQA: i) The translation of non-English candidate an- swers might lack in quality, so annotators are likely to judge some relevant answers as non-relevant. Hence, training a classifier on this data might lead to a tendency to favor English answers. ii) For the question-answer pairs that were annotated in both languages, we can remove noisy (i.e., la- beled inconsistently by annotators) instances from the training set.</p><p>The question of annotation is an unavoidable part of evaluation of MLQA systems, so finding the opti- mal subset for training is a relevant problem. In or- der to explore further, we generated six subsets with respect to (a) the original language of the answer, or (b) the language of annotation (i.e., based on origi- nal text or its English translation): en: Sentences from the English corpus. ar/ch: Sentences from the Arabic / Chinese corpus (regardless of how it was judged). consist: All sentences except those that were judged inconsistently. src+: Sentences judged only in original text, or judged in both consistently. en+: Sentences that are either judged only in En- glish, or judged in both original and English transla- tion consistently. all: All sentences.</p><p>These subsets were determined based on linguis- tically motivated heuristics, but choosing the most suitable one (for a given task) is done via machine learning (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Language-specific Ranking</head><p>Scoring Arabic sentences with respect to a question is inherently different than scoring English (or Chi- nese) sentences. The quality of resources, grammar, etc., as well as other internal dynamics might differ greatly across languages. We hypothesize that there is no one-size-fits-all model, so the parameters that work best for English retrieval might not be as useful when scoring sentences in Arabic, and/or Chinese.</p><p>Our proposed solution is to apply a separate clas- sifier, custom-tuned to each collection, and retrieve three single-language ranked lists (i.e., in English, Arabic, and Chinese). In addition to comparing each custom-tuned, language-specific classifier to a sin- gle, language-independent one, we also use this idea to propose an approach for MLQA: L2CT(n) Retrieve answers from each language us- ing separate classifiers (call these lists English-only, Arabic-only, and Chinese-only), take the best an- swers from each language, then merge them into a mixed-language set of n answers.</p><p>We compare this to the standard approach: L2T(n) Retrieve up to n mixed-language answers using a single classifier.</p><p>Four heuristics were explored for merging lists in the L2CT approach. 8 Two common approaches are uniform and alternate merging <ref type="bibr" target="#b36">(Savoy, 2004)</ref>: Uniform: A straightforward merge can be achieved by using the classifier scores (i.e., probability of an- swer relevance, given question) to sort all answers, across all languages, and include the top n in the fi- nal list of answers. Classifier scores are normalized into the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> range for comparability. Alternate: We alternate between the lists, picking one answer at a time from each, stopping when the limit n has been reached.</p><p>Since answers are expected in English, there is a natural preference for answers that were originally written English, avoiding noisy text due to transla- tion errors. However, it is also important not to re- strict answers entirely to English sources, since that would defeat the purpose of searching in a multi- lingual collection. We implemented the following methods to account for language preferences: English first: We keep all sufficiently-confident (i.e., normalized score above a fixed threshold) answers from the English-only list first, and start including answers from Arabic-and Chinese-only lists only if the limit of n answers has not been reached.</p><p>Weighted: Similar to Uniform, but we weight the normalized scores before sorting. The optimal weights can be learned by using a grid-search pro- cedure and a cross-validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In order to perform controlled experiments and gain more insights, we split our evaluation into four separate tasks: three tasks focus on retrieving an- swers from posts written in a specified language (English-only, Arabic-only, or Chinese-only) 9 , and the last task is not restricted to any language <ref type="bibr">(Mixedlanguage)</ref>. All experiments were conducted on the DARPA BOLT-IR task. The collection consists of 12.6M Arabic, 7.5M Chinese, and 9.6M English Web forum posts. All runs use a set of 45 non- factoid (mostly opinion and causal) English ques- tions, from a range of topics. All questions and fo- rum posts were processed with an information ex- traction (IE) toolkit <ref type="bibr" target="#b6">(Boschee et al., 2005</ref>), which performs sentence-splitting, named entity recogni- tion, coreference resolution, parsing, and part-of- speech tagging.</p><p>All non-English posts were translated into En- glish (one-best only), and all questions were trans- lated into Arabic and Chinese (probabilistic transla- tion methods from Section 3.1). For all experiments, we used the same state-of-the-art English↔Arabic (En-Ar) and English↔Chinese (En-Ch) MT sys- tems ( <ref type="bibr" target="#b11">Devlin et al., 2014</ref>). Models were trained on parallel corpora from NIST OpenMT 2012, in addition to parallel forum data collected as part of the BOLT program (10M En-Ar words; 30M En- Ch words). Word alignments were learned with GIZA++ (Och and Ney, 2003) (five iterations of IBM Models 1-4 and HMM).</p><p>After all preprocessing, features were computed using the original post and question text, and their translations. Training data were created by having annotators label all sentences of the top 200 docu- ments retrieved by Indri from each collection (for each question). Due to the nature of retrieval tasks, training data usually contains an unbalanced portion of negative examples. Hence, we split the data into balanced subsets (each sharing the same set of pos- itively labeled data) and train multiple classifiers, <ref type="bibr">9</ref> Shortened as Eng, Arz, and Cmn, respectively. then take a majority vote when predicting.</p><p>For testing, we froze the set of candidate answers and applied the trained classifier to each question- answer pair, generating a ranked list of answers for each question. This ranked list was evaluated by av- erage precision (AP). <ref type="bibr">10</ref> Due to the size and redun- dancy of the collections, we sometimes end up with over 1000 known relevant answers for a question. So it is neither reasonable nor meaningful to com- pute AP until we reach 100% recall (e.g., 11-point AP) for these cases. Instead, we computed AP-k, by accumulating precision values at every relevant an- swer until we get k relevant answers. <ref type="bibr">11</ref> In order to provide a single metric for the test set, it is common to report the mean average precision (MAP), which in this case is the average of the AP-k values across all questions.</p><p>Baseline. As described earlier, the baseline system computes similarity between question text and the one-best translation of the candidate answer (we run the sentence through our state-of-the-art MT sys- tem). After translation, we compute similarity via scoring the match between the parse of the ques- tion text and the parse of the candidate answer, us- ing our finely-tuned IE toolkit [reference removed for anonymization]. This results in three different similarity features: matching the tree node similar- ity, edge similarity, and full tree similarity. Fea- ture weights are then learned by training this clas- sifier discriminatively on the training data described above. This already performs competitively, outper- forming the simpler baseline where we compute a single similarity score between question and trans- lated text, and matching the performance of the system by Chaturvedi et al. on the BOLT evalua- tion <ref type="bibr">(2014)</ref>. Baseline MAP values are reported on the leftmost column of <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Data effect. In the baseline approach, we do not perform any data selection, and use all available data for training the classifier. In order to test our hypothesis that selecting a linguistically-motivated subset of the training data might help, we used 10- fold cross-validation to choose the optimal data set (among seven options described in Section 3.3). Re- sults indicate that including English or Arabic sen- tences when training a classifier for Chinese-only QA is a bad idea, since effectiveness increases when restricted to Chinese sentences (lang=ch). On the other hand, for the remaining three tasks, the most effective training data set is annot=en+consist. These selections are consistent across all ten folds, and the difference is statistically significant for all but Arabic-only. The second column in <ref type="table" target="#tab_2">Table 2</ref> dis- plays the MAP achieved when data selection is ap- plied before training the baseline model. Feature effect. To measure the impact of our novel features, we trained classifiers using either LexCL, LexQL, or both feature sets (Section 3.2). In these experiments, the data is fixed to the optimal subset found earlier. Results are summarized on right side of <ref type="table" target="#tab_2">Table 2</ref>. Statistically significants improvements over Baseline/Baseline+Data selection are indicated with single/double underlining.</p><p>For Arabic-only QA, adding LexQL features yields greatest improvements over the baseline, while the same statement holds for LexCL features for the Chinese-only task. For the English-only and mixed-language tasks, the most significant in- crease in MAP is observed with all of our proba- bilistic bilingual features. For all but Arabic-only QA, the MAP is statistically significantly better (p &lt; 0.05) than the baseline; for Chinese-only and mixed- language tasks, it also outperforms baseline plus data selection (p &lt; 0.05). 12 All of this indicates the effectiveness of our probabilistic question trans- lation, as well as our data selection strategy.  Understanding the contribution of each of the four <ref type="bibr">12</ref> Note that bilingual features are not expected to help on the English-only task, and the improvements come solely from data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p>LexCL features is also important. To gain insight, we trained a classifier using all LexCL features (us- ing the optimal data subset learned earlier for each task), and then incrementally removed one of the features, and tested on the same task. This con- trolled experiment revealed that the word translation feature is most useful for Chinese-only QA (i.e., re- moving it produces largest drop in MAP, 0.6 points), whereas context translation appears to be most use- ful (by a slighter margin) in Arabic-only QA. In the former case, the diversity provided by word transla- tion might be useful at increasing recall in retriev- ing Chinese answers. In retrieving Arabic answers, using context to disambiguate the translation might be useful at increasing precision. This result further emphasizes the importance of a customized transla- tion approach for MLQA.</p><p>Furthermore, to test the effectiveness of the prob- abilistic translation approach (Section 3.1), we re- placed all LexCL features with a single lexical sim- ilarity feature computed from the one-best ques- tion translation. This resulted in lower MAP: 0.427 to 0.423 for Arabic-only, and 0.451 to 0.425 for Chinese-only task (p &lt; 0.01), supporting the hy- pothesis that probabilistic translation is more effec- tive than the widely-used one-best translation. In fact, almost all gains in Chinese-only QA seems to be coming from the probabilistic translation.</p><p>For a robustness test, we let cross-validation se- lect the best combination of (data, feature), mimick- ing a less controlled, real-world setting. In this case, the best MAP for the Arabic-, Chinese-, English- only, and Mixed-language tasks are 0.403, 0.448, 0.657, and 0.679, respectively. In all but Arabic- only, these are statistically significantly better (p &lt; 0.05) than not tuning the feature set or training data (i.e., Baseline). This result suggests that our ap- proach can be used for any MLQA task out of the box, and provide improvements.</p><p>Learning to Custom Translate (L2CT). We took the ranked list of answers output by each language- specific model, and merged all of them into a ranked list of mixed-language answers. For the weighted heuristic, we tried three values for the weight. In <ref type="table" target="#tab_4">Table 3</ref>, we see that training separate classifiers for each subtask does not bring overall improve- ments to the end task. Amongst merging strategies, the most effective were weighted (weights for each query learned by performing a grid-search on other queries) and English first -however, both are sta- tistically indistinguishable from the single classifier baseline. In the latter case, the percentage of English answers is highest (88%), which might not be desir- able. Depending on the application, the ratio of lan- guages can be adjusted with an appropriate merging method. For instance, alternate and norm heuristics tend to represent languages almost equally.  Even though we get lower MAP in the overall task, <ref type="table" target="#tab_2">Table 2</ref> suggests that it is worthwhile customiz- ing classifiers for each subtask (e.g., the Chinese re- sponses in the ranked list of L2CT are more relevant than Single.). The question of how to effectively combine the results into a mixed-language list, how- ever, remains an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We introduced L2T, a novel approach for MLQA, inspired from recent success in CLIR research. To our knowledge, this is the first use of probabilistic translation methods for this task, and the first at- tempt at using machine learning to learn the optimal question translation.</p><p>We also proposed L2CT, which uses language- specific classifiers to treat the ranking of English, Arabic, and Chinese answers as three separate sub- tasks, by applying a separate classifier for each language. While post-retrieval merging has been studied in the past, we have not come across any work that applies this idea specifically to create a language-aware ranking for MLQA.</p><p>Our experimental analysis shows the importance of data selection when dealing with annotations on source and translated text, and the effect of com- bining translation methods. L2T improved answer ranking effectiveness significantly for Chinese-only, English-only, and mixed-language QA.</p><p>Although results did not support the hypothesis that learning a custom classifier for the retrieval of each language would outperform the single classi- fier baseline, we think that more research is needed to fully understand how language-specific modeling can benefit MLQA. More sophisticated merging of multiple ranked lists of answers need to be explored. Learning to rank between answers from different languages might be more effective than heuristics. This would allow us to predict the final language ra- tio, based on many features (e.g., general collection statistics, quality of candidate answers, question cat- egory and complexity, MT system confidence levels) to merge question-answer pairs.</p><p>An even more comprehensive use of machine learning would be to learn word-level translation scores, instead of relying on translation probabili- ties from the bilingual dictionary, resulting in a fully customized translation. Similar approaches have ap- peared in learning-to-rank literature for monolingual IR ( <ref type="bibr" target="#b5">Bendersky et al., 2010)</ref>, but not for multilingual retrieval. Another extension of this work would be to apply the same translation for translating answers into the question language (in addition to question translation). By doing this, we would be able to cap- ture the semantics of each answer much better, since we have discussed that one-best translation discards a lot of potentially useful information.</p><p>Finally, since one of the take-away messages of our work is that a deeper understanding of linguistic context can improve QA effectiveness via more so- phisticated question translation, we are hoping to see even more improvements by creating features based on word embeddings. One potential next step is to learn bilingual embeddings directly for the task of QA, for which we have started adapting some re- lated work ( <ref type="bibr" target="#b4">Bai et al., 2010</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Probabilistic grammar-based translation of example question. The example question "Tell me about child labor in Africa" is simplified by our preprocessing engine to "child labor africa".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Vector representation of grammar-translated question (qgrammar) and sentence (s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>List of features used in L2T, and how the values are 

computed from vector representations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : L2T evaluated using MAP with 10-fold cross-</head><label>2</label><figDesc></figDesc><table>validation for each task. A statistically significant increase 

over Baseline/Base+Data is shown by single/double underlin-

ing (p &lt; 0.05). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : L2T vs. L2CT for multilingual QA.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.darpa.mil/Our_Work/I2O/Programs 2 http://trec.nist.gov 3 http://www.clef-initiative.eu 4 http://research.nii.ac.jp/ntcir/index.html</note>

			<note place="foot" n="5"> Most recent MLQA tracks were in 2008 (CLEF) and 2010 (NTCIR).</note>

			<note place="foot" n="6"> Annotators score each answer from 1 to 5. We label any score of 3 or higher as relevant.</note>

			<note place="foot" n="7"> Support vector machines yielded worse results.</note>

			<note place="foot" n="8"> In addition to these heuristics, the optimal merge could be learned from training data, as a &quot;learning to rank&quot; problem. This is out of the scope of this paper, but we plan to explore the idea in the future.</note>

			<note place="foot" n="10"> Many other metrics (e.g., NDCG, R-precision) were explored during BOLT, and results were very similar. 11 k was fixed to 20 in our evaluation, although we verified that conclusions do not change with varying k.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Jacob Devlin has provided great help in the design and implementation of the context-based question translation approach. We would also like to thank the anonymous reviewers for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anselmo Peñas, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sisayfissaha</forename><surname>Adafre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith ; Nicola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">F</forename><surname>Garethj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Carol Peters, Thomas Deselaers</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
	<note>Dublin city university at qaclef</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to Rank for Robust Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Subbian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="833" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A prototype question answering system using syntactic and semantic information for answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">De</forename><surname>Boni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José-Luis</forename><surname>Jaravalencia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piqasso: Pisa question answering system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Attardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Cisternino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Formica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TREC</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to rank with (a lot of) word features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiko</forename><surname>Sadamasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="314" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10</title>
		<meeting>the Third ACM International Conference on Web Search and Data Mining, WSDM &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zamanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Intelligence Analysis</title>
		<meeting>the International Conference on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-intensive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Text REtrieval Conference</title>
		<meeting>the Tenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint Question Clustering and Relevance Prediction for Open Domain Non-factoid Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><forename type="middle">M</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hema</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on World Wide Web, WWW &apos;14</title>
		<meeting>the 23rd International Conference on World Wide Web, WWW &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="503" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Question answering passage retrieval using dependency relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renxu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatseng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Probabilistic structured query methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Darwish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-22" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">UAlacant: Using Online Machine Translation for Cross-lingual Textual Entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Miquelespì A-Gomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sánchez-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forcada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval &apos;12</title>
		<meeting>the Sixth International Workshop on Semantic Evaluation, SemEval &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="472" to="476" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Architecture and Evaluation of BRUJA, a Multilingual Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ´ A</forename><surname>García-Cumbreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L A Ureña</forename><surname>Martínez-Santiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Question answering based on temporal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Harabagiu And Cosmin Adrian Bejan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-2005 workshop on inference for textual question answering</title>
		<meeting>the AAAI-2005 workshop on inference for textual question answering</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Anselmo Pe˜A±asPe˜ Pe˜A±as, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Hartrumpf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gl˜acknergl˜</forename><surname>Ingo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes Leveling ; Nicola</forename><surname>Gl˜ackner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">F</forename><surname>Garethj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Carol Peters, Thomas Deselaers</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="421" to="428" />
		</imprint>
	</monogr>
	<note>Efficient question answering with question decomposition and multiple answer streams</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward semantics-based answer pinpointing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first international conference on Human language technology research</title>
		<meeting>the first international conference on Human language technology research</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>ChinYew Lin, and Deepak Ravichandran</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactic and semantic decomposition strategies for question answering from multiple resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Borchardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Felshin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI 2005 workshop on inference for textual question answering</title>
		<meeting>the AAAI 2005 workshop on inference for textual question answering</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Combining Evidence with a Probabilistic Framework for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic Models for Answerranking in Multilingual Question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Jeongwoo</forename><surname>September</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
	<note>ACM Trans. Inf. Syst.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on question answering technology from an information retrieval perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Kolomiyets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="5412" to="5434" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Description of the ntou complex qa system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Min</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-8 Workshop</title>
		<meeting>NTCIR-8 Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Finding what matters in questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Hema Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Castelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Maskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT&apos;13</title>
		<meeting>NAACLHLT&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anselmo Pe˜A±asPe˜ Pe˜A±as, and Vivien Petras, editors, Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Madnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; ˜ Angel</forename><surname>Martinez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>De Pablo-Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Concepcion</forename><surname>Polo-Bayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mar˜aateresamar˜</forename><surname>Mar˜aateresa Vicente-Diez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paloma</forename><surname>Martinez-Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Luis Martinezfernandez ; Nicola</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><surname>Gonzalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">F</forename><surname>Garethj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kurimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Carol Peters, Thomas Deselaers</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">5706</biblScope>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
		<respStmt>
			<orgName>TREC 2007 ciQA Task: University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>The miracle team at the clef 2008 multilingual question answering track</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards Cross-lingual Textual Entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Markov random field model for term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05</title>
		<meeting>the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Keyword translation accuracy and cross-lingual question answering in chinese and japanese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Shima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multilingual Question Answering, MLQA &apos;06</title>
		<meeting>the Workshop on Multilingual Question Answering, MLQA &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using Coreference for Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas S Morton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Coreference and Its Applications, CorefApp &apos;99</title>
		<meeting>the Workshop on Coreference and Its Applications, CorefApp &apos;99<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="85" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Integrating logic forms and anaphora resolution in the aliqan system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Munoz-Terol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Puchol-Blasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pardino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Manuel</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katia</forename><surname>Vila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ferrandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Peral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricio</forename><surname>Martinezbarco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluating Systems for Multilingual and Multimodal Information Access</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="438" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-language information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="125" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Whu question answering system at ntcir-8 aclia task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Han Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-8 Workshop</title>
		<meeting>NTCIR-8 Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple {BM25} extension to multiple weighted fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dfki-lt at qaclef</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Spurk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF 2008 Working Notes, Working Notes</title>
		<editor>Carol Peters and et al.</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluation of Complex Temporal Questions in CLEF-QA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Saquete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martínez-Barco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llopis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Conference on Cross-Language Evaluation Forum: Multilingual Information Access for Text, Speech and Images, CLEF&apos;04</title>
		<meeting>the 5th Conference on Cross-Language Evaluation Forum: Multilingual Information Access for Text, Speech and Images, CLEF&apos;04<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="591" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combining multiple strategies for effective monolingual and cross-language retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacques</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="121" to="148" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using semantic roles to improve question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bootstrap pattern learning for open-domain clqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Shima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR-8 Workshop</title>
		<meeting>NTCIR-8 Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A study of learning a merge model for multilingual information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st</title>
		<meeting>the 31st</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="195" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploiting representations from statistical machine translation for crosslanguage information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno>19:1-19:32</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What&apos;s in a domain? analyzing genre and topic differences in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marlies</forename><surname>Van Der Wees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Weerkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="560" to="566" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Michigan State University at the 2007 TREC ciQA Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Emelander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth Text Retrieval Conference</title>
		<meeting>the Sixteenth Text Retrieval Conference<address><addrLine>Gaithersburg, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
