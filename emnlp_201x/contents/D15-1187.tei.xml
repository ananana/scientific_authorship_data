<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Negation Focus Identification with Word-Topic Graph Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Zou</surname></persName>
							<email>zoubowei@gmail.com, {qmzhu, gdzhou}@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Natural Language Processing Lab</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Negation Focus Identification with Word-Topic Graph Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Due to the commonality in natural language, negation focus plays a critical role in deep understanding of context. However, existing studies for negation focus identification major on supervised learning which is time-consuming and expensive due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased Pag-eRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification. *</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Negation is used to reverse the polarity of part of statements that are otherwise affirmative by de- fault ( <ref type="bibr" target="#b1">Blanco and Moldovan, 2011)</ref>, which is common in natural language. Negation focus is defined as the special part in sentence, which is most prominently or explicitly negated by a neg- ative expression. For example, sentence (1) could be interpreted as He stopped, but not until he got to Jackson Hole with a positive part he stopped and a negative part until he got to Jackson Hole.</p><p>(1) He didn't stop until he got to Jackson Hole.</p><p>Our previous work ( <ref type="bibr" target="#b12">Zou et al., 2014</ref>) showed that contextual information plays a critical role on negation focus identification. For better illus- tration of this conclusion, they manually analyze the evidences for 100 negation focuses. It is sur- * Corresponding author prising that 77 focuses can be identified with help of contextual information. This indicates that negation focus is often related with what authors repeatedly states in context. In this paper, we thus focus on graph-based ranking methods ( <ref type="bibr" target="#b7">Mihalcea and Tarau, 2004</ref>) which first build a word graph according to word co-occurrences within document, and then use random walk al- gorithms (e.g., PageRank) to measure word im- portance.</p><p>However, for negation focus identification, the graph-based methods may suffer from the fol- lowing two problems: (a) the words in graph- based methods are strongly connected by co- occurrence rather than semantic content, which do not necessarily guarantee that they are rele- vant to the negation focus in context; and (b) identifying a negation focus may be affected by not only the relatedness of surrounding words but also its importance in current document which is not considered in standard random walk algorithms.</p><p>To address the above problems, we propose a word-topic graph model by adding a topical layer on the original word layer to capture the seman- tic clues from both lexical and topic perspectives. Besides, a document-sensitive PageRank algo- rithm is also proposed to optimize the graph model by considering the document's major top- ics. Experimental results indicate that our word- topic graph model outperforms other baseline methods. Moreover, our model is unsupervised and requires only un-annotated text for training.</p><p>The rest of this paper is organized as follows. Section 2 overviews the related work. Section 3 introduces our word-topic graph model with con- textual discourse information. Section 4 reports the experimental results and analysis. Finally, we conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>So far there is little work on negation focus iden- tification, which was pioneered by <ref type="bibr" target="#b1">Blanco and Moldovan (2011)</ref> who investigated the negation phenomenon in semantic relations and proposed a supervised learning approach to identify the focus of a negation expression. However, alt- hough <ref type="bibr" target="#b9">Morante and Blanco (2012)</ref> proposed ne- gation focus identification as one of the *SEM'2012 shared tasks, only one team <ref type="bibr" target="#b11">(Rosenberg and Bergler, 2012</ref>) participated in. They identified negation focus by three heuristic rules.</p><p>Our previous work ( <ref type="bibr" target="#b12">Zou et al., 2014</ref>) demon- strates the effectiveness of contextual infor- mation for negation focus identification. On this basis, we further optimize the graph model in both the topical layer and the PageRank algo- rithm in this paper.</p><p>In recent years, many algorithms are widely used to incorporate word graph models and topi- cal information within random walk. Our work is originally inspired by <ref type="bibr" target="#b6">Liu et al. (2010)</ref>. Their method runs decomposed Topical PageRank (TPR) for each topic separately, and then calcu- lates the word scores with respect to different topics. When setting the edge weights, only word co-occurrence is considered. Different from their work, our word-topic graph model runs on a two- layers (word layer and topical layer) graph model and sets the edge weights by measuring both word similarity and topic distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>The word-topic graph model consists of word layer and topical layer, as shown in <ref type="figure">Figure 1</ref>. While the word layer is constructed according to word co-occurrence within a sliding window, which expresses the cohesion relationship be- tween words in the context, the topical layer is to refine the graph model over the discourse con- textual information. <ref type="figure">Figure 1</ref>. Word-topic graph model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Constructing Word Layer</head><p>The word layer is constructed according to word co-occurrence within a sliding window, which expresses the cohesion relationship between words in the context. It can be denoted as L word (W, E w ), where vertex set W={w i } represents the set of words in one document and link set E w ={e ij |w i , w j ∈W} represents the set of directed edges between these words. Note that only con- tent words are considered. Namely, we consider nouns, verbs, adjectives, and adverbs.</p><p>The link directions are added from the first word pointing to other words within a sliding s- width sentence window. Directed edge ew ij is weighted to represent the relatedness between word w i and word w j in a document with transi- tion probability P w (j|i) from i to j, which is nor- malized as follows:</p><formula xml:id="formula_0">: ( , ) ( | ) , ( , ) i k i j w i k k w w sim w w P j i sim w w    (1)</formula><p>where the denominator represents the out-degree of vertex w i , and sim(w i ,w j ) denotes the similari- ty between word w i and w j . In this paper, both corpus-based and knowledge-based methods are evaluated to calculate the similarity between words.  Word co-occurrence. If word w i and word w j occur in a s-width sliding sentence window, sim(w i ,w j ) increases 1.  WordNet similarity <ref type="bibr" target="#b8">(Miller, 1995)</ref>. In this paper, we adopt the path similarity function to measure relatedness of nouns and verbs, and adopt the similar to function to measure relatedness of adjectives and adverbs by us- ing the NLTK toolkit 1 ( <ref type="bibr" target="#b0">Bird et al., 2009</ref>). Note that sim(w i ,w i ) = 0 to avoid self-transition, and sim(w i ,w j ) and sim(w j ,w i ) may not be equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminaries for Topical Layer</head><p>To infer the latent topic distributions of words, Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b2">Blei et al., 2003)</ref>, a typical of topic model, is directly ap- plied. By the set of topics which derive from a corpus, we can obtain:  P(t|w), the probability of topic t given word w, which indicates how much that word w focuses on topic t, and  P(t|d), the probability of topic t given doc- ument d, which indicates how much that document d focuses on topic t.</p><p>Then, the similarity between two words or be- tween word w i and document d can be measured by the similarity between their corresponding topic distributions. Formally, we denote a topic distribution as θ, and measure the similarity by using:</p><p> Dot-product. We consider the topic distribu- tions as vectors and apply the dot-product, a geometrically motivated function, to calcu- late the similarity: <ref type="bibr">, 1991)</ref>. Considering the asymmetry (Eq. <ref type="formula">(4)</ref>), we obtain a symmetrized measure by Eq.(5).</p><formula xml:id="formula_1">( , ) ( | ) ( | ), i j i j k w w w w k i k j t T Inner P t w P t w           (2) ( , ) ( | ) ( | ). i i k w d w d k i k t T Inner P t w P t d           (3)  Kullback Leibler (KL) divergence (Lin</formula><formula xml:id="formula_2">2 ( ) ( , ) ( )log . ( ) k i k i j i k t T j k P t D Pt P t      (4) ( , ) ( , )<label>( , )</label></formula><p>.</p><formula xml:id="formula_3">KL i j i j j i D D D        <label>(5)</label></formula><formula xml:id="formula_4">Note that D KL (θ i , θ j ) is undefined if P i (t k )=0</formula><p>or P j (t k )=0 for any t k ∈T. For this reason, only the topics which make both P i (t k )≠0 and P j (t k )≠0 are adopted to calculate the KL divergence between two topic distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-Topic Graph Model</head><p>The word layer can well capture the relatedness between words, but just partially model the nega- tion focus since it is more directly related with topic than content. Therefore, we add one more layer to refine the graph model over the topical information, as shown in <ref type="figure">Figure 1</ref>. Formally, the word-topic graph is defined as G topic (W, T, E w , E t ), where vertex set T={t i } represents the set of top- ics in all of documents in corpus and link set E t ={et ij |w i ∈W, t j ∈T} represents the set of undi- rected edges between words and topics. Considering that the topical layer can provide more contextual semantic information, we refine the relatedness between words by using a topical transition probability P t (j|i) which is calculated by two kinds of measurements: </p><p>Here, the similarity is measured by the dot- product or the KL divergence (using reciprocals). On this basis, the word transition probability P w (j|i) is updated as following:</p><formula xml:id="formula_6">' ( | ) ( | ) (1 ) ( | ). w w t P j i P j i P j i       <label>(7)</label></formula><p>where µ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> is the coefficient controlling the relative contributions from the lexical infor- mation and the topical information. Moreover, the weights of word vertices are calculated by a PageRank algorithm. In standard PageRank ( <ref type="bibr" target="#b10">Page et al., 1998)</ref>, words are set to be the same value, which indicates there is equal importance to all of words in a document. How- ever, intuitively, we should allocate higher weights to those words with high relevance to the document. Therefore, we assign a document- sensitive value to word w i :</p><formula xml:id="formula_7">( , ) ( ) ( , ) i k k w d d i w d w W sim R w sim       <label>(8)</label></formula><p>and calculate the weights of word vertices itera- tively by using a biased PageRank algorithm:</p><formula xml:id="formula_8">( 1) ( ) ' ( ) ( ) ( | ) (1 ) ( ). n n i j w j i d i Score w Score w P j i R w         (9)</formula><p>All of the PageRank algorithms are terminated when the number of iterations reaches 100 or the difference of each vertex between consecutive iterations is less than 0.001.</p><p>Finally, according to the annotation guidelines ( <ref type="bibr" target="#b1">Blanco and Moldovan, 2011)</ref>, the focus is al- ways a full text of a semantic role. Thus, we se- lect all of semantic roles in sentence as candidate focuses for ranking. The ranking score of a can- didate focus f is computed by averaging the scores of all words inside the candidate:</p><formula xml:id="formula_9">( ) ( ) , ( , ) i i w f avg score w score f Count f    <label>(10)</label></formula><p>where count(f,•) denotes the number of content words within the candidate. Then the top ranked candidate is chosen as the negation focus.</p><p>the Penn TreeBank 3 to obtain the corresponding document as its discourse context. For fair com- parison, we adopt the same partition as *SEM'2012 shared task in our experiments. We evaluate our results in terms of accuracy. To see whether an improvement is statistically significant, we conduct significance testing using the paired t-test.</p><p>For estimating the topical transition probabil- ity P t (j|i) and the document-sensitive value R d (w i ), we employ GibbsLDA++ 4 , an LDA model using Gibbs Sampling technique for parameter estima- tion and inference <ref type="bibr" target="#b3">(Griffiths, 2002</ref>). We set the parameters α = 50/T and β = 0.1 as <ref type="bibr" target="#b4">Griffiths and Steyvers (2004)</ref> suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Influences of Parameters</head><p>There are two major parameters in our models that may influence the performance, including: (a) the damping factor µ of the word transition prob- ability P ' w (j|i) (Eq. <ref type="formula" target="#formula_6">(7)</ref>) and (b) the damping fac- tor λ of the word-topic graph model (Eq. <ref type="formula">(9)</ref>). <ref type="figure" target="#fig_1">Figure 2</ref> shows the accuracy when varying µ from 0.1 to 0.9 with an interval of 0.1 and when varying λ from 0.05 to 1 with an interval of 0.05. We notice that the best performance is achieved when µ=0.6. It indicates that the direct lexical information contributes slightly more than the topical information. The results also show the complementarity between these two kinds of in- formation on negation focus identification.</p><p>For λ, it has very little, if any, effect on per- formance, when λ is set from 0.5 to 0.85. It indi- cates that the contextual information (the first term in Eq. <ref type="formula">(9)</ref>) contributes more than the docu- ment information (the second term in Eq. <ref type="formula">(9)</ref>) on negation focus identification. Moreover, the results also show that these two parameters have little impact in a certain range on performance (µ:0.4~0.6; λ:0.5~0.85), which suggests that the approach is robust to a certain extent. Therefore, we set µ=0.6 and λ=0.7 in the following experiments.</p><p>Besides, we also evaluate the other minor pa- rameters in our model. Due to space limit, we do not report all of results here and set parameters to the following values: setting window size s=1 (the previous and next sentences) and the number of topic T=40, adopting the word co-occurrence similarity to calculate the similarity between words, and using dot-product to measure both P t (j|i) and R d (w i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Other Methods</head><p>In the word-topic graph models, two primary improvements are proposed: (a) updating the word transition probability P w (j|i) by adding a topical layer ("TL"), and (b) assigning a docu- ment-sensitive value to word node ("DS").  <ref type="table">Table 1</ref> shows that the word-topic graph mod- el (WTGM) is significantly better (+16.78%, p&lt;0.01) than the graph model with only word layer (WLM), which justifies the effectiveness of the topical layer. In addition, the results also in- dicate that the word-topic graph model not only takes the topical information into account ("TL"), but also considers the semantic relationship in current document ("DS").</p><p>We select two supervised baseline methods to compare with our word-topic graph model. One is a decision tree-based system described in <ref type="bibr" target="#b1">Blanco and Moldovan (2011)</ref>, and the other one is a SVM-based system which takes advantage of both syntactic features and contextual features ( <ref type="bibr" target="#b12">Zou et al., 2014</ref>  <ref type="table">Table 2</ref> shows that our word-topic graph model performs significantly better than the two others by 6.19% (p&lt;0.01) and 2.25% (p&lt;0.01), respectively. The results support our viewpoint that the topical information in context can help to find the negation focus, and the word-topic graph model we proposed is effective. Moreo- ver, it is also worth noting that our method is unsupervised, which does not need the prior knowledge for training, while the other two su- pervised baselines employ the golden features, such as the POS tag, constituent tree, and de- pendency tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an unsupervised word- topic graph model, which represents and measures the word importance by using contex- tual information from both lexical and topical perspectives. And then, we propose a document- sensitive biased PageRank algorithm to calculate the ranking scores of negation focus candidates. Experimental results show that our method achieves better performance than other baselines without any annotated data.</p><p>The main shortcoming of our method is that not all of negation focus can be identified by the context. As our statistics, at least 17% of samples are hard to be determined by human beings when ignoring the information in current sentence. Therefore, in future work, we will focus on in- vestigating an effective method to integrate the local lexical/syntactic information and the global contextual discourse information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Influence of the damping factors µ and λ. Moreover, the results also show that these two parameters have little impact in a certain range on performance (µ:0.4~0.6; λ:0.5~0.85), which suggests that the approach is robust to a certain</figDesc><graphic url="image-2.png" coords="4,70.95,546.24,218.75,114.54" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.nltk.org/</note>

			<note place="foot" n="4"> Experimental Results To evaluate the performance of our word-topic graph model for negation focus identification, we carry out experiments on the *SEM&apos;2012 shared task corpus 2. As a freely downloadable resource, the corpus is annotated on top of PropBank, which uses the WSJ section of the Penn TreeBank. In particular, negation focus annotation on this corpus is restricted to verbal negations (Blanco and Moldovan, 2011). In total, this corpus provides 3,544 instances of negation focus annotations. Although for each instance, the corpus only provides the current sentence, the previous and next sentences as its context, we sort to 2 http://www.clips.ua.ac.be/sem2012-st-neg/</note>

			<note place="foot" n="3"> http://www.cis.upenn.edu/~treebank/ 4 http://gibbslda.sourceforge.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is supported by the National Natu-ral Science Foundation of China, No.61272260, No.61331011, No.61273320, and the Major Pro-ject of College Natural Science Foundation of Jiangsu Province, No.11KJA520003. The au-thors would like to thank the anonymous review-ers for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Natural Language Processing with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Reilly Media Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Representation of Negation Using Focus Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-19" />
			<biblScope unit="page" from="581" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gibbs sampling in the generative model of Latent Dirichlet Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Stanford University</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Divergence measures based on Shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic Keyphrase Extraction via Topic Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>MIT, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
			<biblScope unit="page" from="9" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">*SEM 2012 Shared Task: Resolving the Scope and Focus of Negation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roser</forename><surname>Morante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-07" />
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UConcordia: CLaC Negation Focus Detection at *Sem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Bergler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the First Joint Conference on Lexical and Computational Semantics<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-07" />
			<biblScope unit="page" from="294" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Negation Focus Identification with Contextual Discourse Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
