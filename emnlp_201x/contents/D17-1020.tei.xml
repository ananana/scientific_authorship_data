<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affinity-Preserving Random Walk for Multi-Document Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Collaborative Innovation Center for Language Ability</orgName>
								<address>
									<postCode>221009</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affinity-Preserving Random Walk for Multi-Document Summarization</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="210" to="220"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summa-rization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summariza-tion task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-document summarization provides users with summary that reflects the main information in a set of given documents. The documents are often related and talk about more than one top- ics. Generic multi-document summarization and topic-focused multi-document summarization are two typical kinds of summarization. The former is a summarization delivering the main information of the documents with no bias while the latter is a one delivering the main information biased to a given topic description (a few sentences or even phrases). Most existing summarization systems are designed for these two kinds of summariza- tion.</p><p>There are two goals for generic multi-document summarization. The first one is saliency. Sum- mary sentences should be central sentences that capture the majority of information in a docu- ment cluster. The sentences with little informa- tion about the document cluster should not be in- cluded in the summary. The second one is di- versity. The information overlap between sum- mary sentences should be as minimal as possi- ble due to the length limit of summary. In other words, the information coverage of summary is a determinant, which requires that the summary sentences should cover diverse aspects of infor- mation. Besides the two goals, there is another goal for the topic-focused summarization and that is relevancy. It requires that the summary sen- tences be relevant to the topic description. A se- ries of conferences and workshops on automatic text summarization (e.g. NTCIR, DUC), special topic sessions in ACL, EMNLP and SIGIR have advanced the techniques to achieve these goals and many approaches have been proposed so far.</p><p>In this paper, we focus on the extractive summa- rization methods, which extract the summary sen- tences from the input document cluster. We pro- pose affinity-preserving random walk for multi- document summarization. The method is a graph- based ranking method, which takes into account the global information collectively computed from the entire sentence affinity graph. Different from the previous graph-based ranking methods, our method adopts "global normalization" to trans- form sentence affinity matrix into sentence tran- sition matrix and formulates the sentence rank- ing process in an absorbing random walk model. Meanwhile, the adjustable affinity-preserving ran- dom walk is proposed to facilitate the diversity of summary by adjusting the sentence transition ma- trix after each iteration of random walk. Experi- mental results on DUC generic and topic-focused multi-document summarization tasks show the competitive performance of our method. To our best knowledge, our system has the best ROUGE- 2 recall on both tasks among all existing graph-based ranking methods, which defeats most other methods.</p><p>We summarize our contributions as follows. (1) We preserve the original affinity relations be- tween sentences in a novel affinity-preserving ran- dom walk view for multi-document summariza- tion. The preservation of affinity leads to a more salient summary. And it is applicable to both generic and topic-focused summarization. <ref type="formula">(2)</ref> We propose adjustable affinity-preserving random walk to enforce the diversity constraint of summa- rization in the random walk process. (3) Experi- ments on DUC 2003 and DUC 2004 tasks demon- strate the competitive performance of our method.</p><p>The rest of the paper is organized as follows. Section 2 discusses the related work. Section 3 describes traditional random walk model for sum- marization. Section 4 proposes affinity-preserving random walk for the saliency goal of summa- rization and this section also proposes adjustable affinity-preserving random walk to produce both salient and diverse summary. Section 5 gives our evaluation results and the conclusion is made in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our method belongs to the graph-based ranking methods to select sentences in the documents to produce the summary. <ref type="bibr" target="#b8">Erkan and Radev (2004)</ref> proposed LexPageRank to compute the sentence saliency based on the concept of eigenvector cen- trality. It constructs the sentence affinity graph and computes the sentence saliency based on an al- gorithm similar to <ref type="bibr">PageRank (Page et al., 1999</ref>). Like PageRank, the affinity matrix is converted into the row-stochastic matrix, which is used as the transition matrix of random walk on the weighted graph. <ref type="bibr" target="#b20">Wan (2007)</ref> proposed manifold ranking for topic-focused multi-document summarization. It makes full use of both the relationships among all sentences in the documents and the relationships between the given topic description and the sen- tences. Manifold ranking conducts a different nor- malization on the sentence affinity matrix to guar- antee the algorithm's convergence. GRASSHOP- PER ( <ref type="bibr" target="#b25">Zhu et al., 2007)</ref> ranks sentences with an emphasis on the diversity constraint of summa- rization. It turns already ranked sentences into absorbing states, which effectively prevents re- dundant sentences from receiving a high rank. The algorithm is based on an absorbing random walk and produces only one summary sentence af- ter one particular random walk becomes station- ary. And the normalization from sentence affinity matrix to sentence transition matrix is the same as PageRank. <ref type="bibr">DivRank (Mei et al., 2010</ref>) is a method to balance the saliency and diversity of the top ranked sentences in a reinforced random walk model. Also, the normalization in DivRank from affinity matrix to transition matrix is the same as PageRank. Another notable diversified graph- based ranking method GCD <ref type="bibr" target="#b7">(Dubey et al., 2011</ref>) relies on large amounts of training data to learn edge conductances.</p><p>Our method formulates the multi-document summarization as an affinity-preserving random walk and uses the "global normalization" to trans- form sentence affinity matrix into sentence transi- tion matrix, which is different from all those pro- posed methods. And the adjustable transition ma- trix in our method balances the saliency and diver- sity goals of summarization. Like GRASSHOP- PER, our method relies on the absorbing random walk model. The difference is that our method does not turn the sentence vertex into absorbing state but add an absorbing vertex to the original sentence affinity graph. And all summary sen- tences are extracted after the random walk reaches a stationary state in our method. Like DivRank, the sentence transition matrix is adjustable in our method to enforce the diversity constraint of sum- marization. However, our method differs from Di- vRank in the mechanism to adjust the transition matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Traditional Random Walk for Summarization</head><p>Suppose G = (S, E) is a graph with vertex set S and edge set E ⊂ S 2 . Suppose there is a con- ductance c(s i , s j ) &gt; 0 associated with each edge (s i , s j ) ∈ E and c(s i , s j ) = 0 associated with the set S 2 − E (the conductance of nonexistent edge is zero). Let</p><formula xml:id="formula_0">C(s i ) = s j ∈S c(s i , s j ), s i ∈ S<label>(1)</label></formula><p>so that C(s i ) is the total conductance of the edges coming from s i . And the traditional random walk on graph is defined as Definition 3.1. The discrete-time Markov chain X = (X 0 , X 1 , X 2 , ...) with state space and tran-sition probability matrix P given by</p><formula xml:id="formula_1">P (s i , s j ) = c(s i , s j ) C(s i ) , (s i , s j ) ∈ S 2 (2)</formula><p>is called a random walk on the graph G.</p><p>This chain governs a particle moving along the vertices of G. If the particle in the state X m is at vertex s i ∈ S, it will be at a neighbor of s i in the next state X m+1 , which is chosen randomly in proportion to the conductance. We can prove that</p><formula xml:id="formula_2">s j ∈S P (s i , s j ) = 1 for any s i ∈ S (C(s i ) = 0) so P is a row-stochastic matrix by P = D −1 W, where D is a diagonal matrix with entries D ii = C(s i ) and W is the adjacency matrix of G where W ij = c(s i , s j ).</formula><p>For the summarization task, G is the sentence affinity graph. The vertex set S = {s 1 , s 2 , ..., s n } contains every sentence in the document cluster and the edge set E contains the pairwise affinity between sentences. We use the tf *isf formula to calculate the weight associated with each term occurring in the sentence, where tf is the term fre- quency in the sentence and isf is the inverse sen- tence frequency of the term among all sentences. isf is often calculated as 1+log(n/n t ), where n is the total number of sentences and n t is the number of sentences containing the term t. W ij is com- puted using the standard cosine measure ( <ref type="bibr">BaezaYates et al., 1999</ref>).</p><formula xml:id="formula_3">W ij = sim cosine (v i , v j ) = v i · v j v i 2 × v j 2<label>(3)</label></formula><p>where v i and v j are the corresponding term vec- tors of s i and s j . Two vertices are connected if their affinity is larger than 0 and W ii is set as 0 to avoid self transition. We get an undirected graph G as well as a symmetric sentence affinity matrix W in this way. Then we transform W into P by P = D −1 W and use the stationary distribution of random walk as sentence ranking scores. The tra- ditional random walk model is a simple practice of PageRank algorithm for multi-document sum- marization.</p><p>4 Affinity-Preserving Random Walk for Summarization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prior of Multi-Document Summarization</head><p>In the above traditional random walk on graph, the normalization from affinity matrix W to tran- sition matrix P is to make P a row-stochastic ma- trix. This can be interpreted as a democratic nor- malization because the surfer of a traditional ran- dom walk visits neighbors of a vertex with prob- ability 1. The surfer has to choose a neighbor to visit next although it is a random choice w.r.t. the conductance distribution of the vertex. How- ever this democratic normalization is not suitable for multi-document summarization due to the fact that most sentences are not salient and should not be normalized democratically as the few salient ones. The prior here is that the number ratio of good candidate sentences over bad candidate sen- tences is very low due to the summary length limit. Good candidate sentences are the sentences highly overlapping with sentences in the reference sum- mary written by humans. And the remaining sen- tences are bad candidate sentences. The demo- cratic normalization of P = D −1 W will extend the adverse effect of bad candidate sentences and suppress the effect of good candidate sentences, because the total conductance of the bad candi- date sentence is usually smaller than that of the good candidate sentence. In this case, the random surfer has to choose a neighbor to visit even when she is currently at a bad candidate sentence, which will direct her to visit other bad candidate sen- tences neighboring to the current sentence. The invariant behavior of the surfer at all vertices in the graph is not consistent with the prior which makes a distinction between good and bad candidate sen- tences. It may pervert the random walk process to get an ideal distribution in which only few sen- tences are assigned with a high ranking score. It is worth noting here that manifold ranking ( <ref type="bibr" target="#b20">Wan et al., 2007</ref>) for summarization uses a dif- ferent normalization:</p><formula xml:id="formula_4">P = D − 1 2 WD − 1 2 .</formula><p>It is a symmetric normalization on both endpoints of an edge, which makes P a suitable choice in the manifold ranking process to smooth the scores of neighboring vertices. The symmetric normaliza- tion can be deduced from the objective function of manifold ranking (Zhou et al., 2003) and does not make a distinction between the good and bad candidate sentences. It is also not consistent with the prior. We can conclude that existing graph- based ranking methods can not well characterize the prior of multi-document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Affinity-Preserving Random Walk</head><p>We need a new normalization method that dis- tinguishes between good and bad candidate sen-í µí±® í µí±® í µí±¨Figure µí±¨Figure 4.1: Sentence graphs for summarization. G: sentence affinity graph constructed from the document cluster. G A : sentence augmented graph with an absorbing vertex s 0 . C max equals to C(s 1 ) indicating that sentence s 1 has the maximum con- ductance, so there is no edge (s 1 , s 0 ).</p><p>tences to satisfy the prior of multi-document sum- marization. Affinity-preserving random walk has an intrinsic mechanism that preserves the original affinity relations between sentences in the docu- ments, which is proposed and defined as follows.</p><p>Definition 4.1. For the graph G, the vertex set S has (n + 1) vertices:</p><formula xml:id="formula_5">s 0 , s 1 , s 2 , ..., s n . The max- imum conductance C max = max i=1,2,...n C(s i ).</formula><p>Of the (n + 1) vertices, s 0 is an absorbing vertex with c(s i , s 0 ) 0, c(s 0 , s i ) = 0, and c(s 0 , s 0 ) = C max for i = 1, 2, ..., n. The remaining vertices are the normal vertices with c(s i , s j ) 0 for i, j = 1, 2, ..., n. The discrete-time Markov chain X = (X 0 , X 1 , X 2 , ...) with state space and tran- sition probability matrix P given by</p><formula xml:id="formula_6">P (s i , s j ) = c(s i , s j ) C max P (s i , s 0 ) = 1 − C(s i ) C max , P (s 0 , s i ) = 0 P (s 0 , s 0 ) = 1 for i, j = 1, 2, ..., n<label>(4)</label></formula><p>is called an affinity-preserving random walk on the graph G.</p><p>For our summarization task, we construct a sen- tence augmented graph G A (as shown in <ref type="figure">Fig- ure 4</ref>.1) by adding an absorbing vertex s 0 to the sentence affinity graph G. The unabsorbed ver- tices s i (i = 1, 2, ..., n) represent sentences in the documents. The affinity-preserving random walk process as defined above is implemented on G A to rank sentences in the documents. In the affinity-preserving random walk, once the surfer reaches the absorbing vertex, she cannot walk out of there. Because P (s i , s 0 ) is small for the ver- tex s i with a large conductance, it is less likely for the surfer at s i to walk into s 0 . As for the ver- tex with a small conductance, the surfer has a ten- dency to be absorbed by s 0 . The absorbing ver- tex here plays a role of soaking unreliable rank- ing scores from large numbers of bad candidate sentences and highlighting the few good candidate sentences. The affinity matrix W is normalized by its first norm (equivalent to C max ) in the affinity- preserving random walk, which results in a kind of "soft" stochastic matrix for n unabsorbed vertices. "soft" here means that the sum of row elements in the matrix can be less than 1. By contrast, P in the traditional random walk is a "hard" stochastic ma- trix in which every sum of row elements has to be 1. Meanwhile, P in this absorbing Markov chain <ref type="bibr" target="#b19">(Seneta, 2006</ref>) preserves the original affinity rela- tions in W as all sentences are globally normal- ized by the same factor (i.e. C max ). We call this approach an "affinity-preserving random walk" as it is used in ( <ref type="bibr" target="#b3">Cho et al., 2010)</ref>, which deals with a graph matching problem that aims at assigning 1-vs-1 correspondences between two graphs. The similar idea is also applied in the case of ontology matching ( <ref type="bibr" target="#b22">Xiang et al., 2015)</ref>. Transition matrix P including the absorbing vertex is formulated in ( <ref type="bibr" target="#b3">Cho et al., 2010</ref>) as follows</p><formula xml:id="formula_7">P = 1 0 T e − c/W 1 W/W 1<label>(5)</label></formula><p>where 0 T is a 1 × n vector with all elements 0, e is an n × 1 vector with all elements 1, c = [C(s 1 ), C(s 2 ), ..., C(s n )] T is a vector containing the conductances of n sentences and W/W 1 is the n × n soft substochastic matrix. How- ever, the stationary distribution of such a random walk on graph with one absorbing vertex is always 1 0 T , which is not a good characterization of the sentence ranking distribution. We turn to the quasi-stationary distribution ¯ x ( <ref type="bibr" target="#b3">Cho et al., 2010;</ref><ref type="bibr" target="#b6">Darroch and Seneta, 1965)</ref> of absorbing random walk for ranking sentences. ¯ x (K) is defined as</p><formula xml:id="formula_8">¯ x (K) i = P rob(X (K) = s i |X (K) = s 0 ) = x (K) i j x (K) j<label>(6)</label></formula><p>where X (K) denotes the position of random surfer at time K. It can be proven that ¯ x (K) has its stationary distribution ¯ x if W is irreducible <ref type="bibr" target="#b6">(Darroch and Seneta, 1965)</ref>. We remove the sentences that have the total conductance 0 (i.e. the isolated sentences) when constructing the sentence affinity graph G. In this way, G will be strongly connected and has an irreducible adjacency matrix W. We introduce the teleport vector y as used in personalized PageRank ( <ref type="bibr" target="#b18">Page et al., 1999;</ref><ref type="bibr" target="#b10">Haveliwala, 2002;</ref><ref type="bibr" target="#b12">Jeh and Widom, 2003)</ref> </p><note type="other">for the summa- rization task. In the generic summarization case, we define the vector y in a way that reflects the position of each sentence in a document. If the sentence s i+1 is right after the sentence s i in the same document, then y i+1 y</note><formula xml:id="formula_9">i = λ −1 , λ &gt; 1<label>(7)</label></formula><p>where λ is the decay factor. In the topic-focused summarization case, we incorporate the topic de- scription as a vertex in the random walk process, which is a standard way of achieving the rele- vancy goal of this kind of summarization. Vector y is defined to be [y 1 , y 2 , ..., y n , y n+1 ] T in which y i = 0(1 i n) and y n+1 = 1 when the first n elements represent sentences in the documents and the last one represents the topic description. We normalize y by its first norm to get a prior sentence ranking for multi-document summariza- tion. Based on W and y, sentence ranking scores in affinity-preserving random walk can be formu- lated in a recursive form as follows</p><formula xml:id="formula_10">x = µW T /W 1 x + (1 − µ)y µW T /W 1 x + (1 − µ)y 1<label>(8)</label></formula><p>where x = [Score(s i )] n×1 is the vector of sen- tence ranking scores. µ is the damping factor that trades off between two actions: the transition ac- cording to W T /W 1 and the teleport specified by y. Transpose operation in Eq. <ref type="formula" target="#formula_10">(8)</ref> can be removed because of symmetry of W. The final transition matrix of affinity-preserving random walk is given by A = µW/W 1 + (1 − µ)y · e T and x should be normalized by its first norm after each itera- tion of random walk. Like PageRank, the quasi- stationary distribution is obtained by the normal- ized principal eigenvector of A. For implementation, the initial ranking scores of all sentences are set to 1/n and the iterative pro- cess in Eq.(8) is adopted to compute new ranking scores of sentences. Usually convergence of the it- erative algorithm is achieved when the difference between scores computed at two successive itera- tions falls below a given threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adjustable Affinity-Preserving Random Walk for Summarization</head><p>Affinity-preserving random walk preserves the affinity relations between sentences and gives high ranking scores to the salient sentences. However, the diversity constraint of summarization has not been taken into account. The surfer of affinity- preserving random walk has no knowledge about what a diverse summary should be. If we just take redundancy removing as the post-processing separate step to improve diversity, sentences that highly overlap with other summary sentences may be chosen and sentences that include information about different topics may be submerged. This phenomenon can be explained by the theorem as follows. </p><p>when µ = 1.</p><p>Proof. In mathematics, for a given symmetric real matrix A (when µ = 1) and nonzero real vector x, the Rayleigh quotient R(A, x) is defined as</p><formula xml:id="formula_12">R(A, x) = x T Ax x T x</formula><p>and it reaches its maximum value when x is the principal eigenvector of A. If x 2 = 1, R(A, x) is equivalent to x T A x. So the solution x is the principal eigenvector of A. From Section 4.2, ¯ x is the normalized principal eigenvector of A. ¯ x and x have the relation in Eq.(9). The conclusion is made.</p><p>From Theorem 4.1, affinity-preserving random walk tends to produce a stationary distribution in which the total sum of affinity between sentences (i.e. x T Ax) is large if there is a subtle teleport- ing effect. It will lead to a summary consisting of many sentences overlapping with each other, which clearly violates the diversity constraint of summarization. Good candidate sentences may not have high affinity with other sentences and are likely to be submerged by affinity-preserving ran- dom walk. Conversely, some bad candidate sen- tences could have high affinity with others and will be highlighted by the random walk process. An extreme example is that a cluster of sentences will all get high ranking scores if they are very sim- ilar to each other. However, only one sentence in this cluster should be included in the summary and the others should be suppressed. The random surfer is caught in a trap of larger sentence clus- ter, which operates against the exploration of good candidates in smaller cluster.</p><p>We introduce adjustable affinity-preserving ran- dom walk to enforce the diversity constraint of summarization. In the original affinity-preserving random walk, sentence transition matrix A is fixed and set as µW/W 1 +(1−µ)y·e T . Edge (s i , s 0 ) (if C(s i ) = C max ) always exists and has an in- variant conductance c(s i , s 0 ), which means that the surfer at s i walks into s 0 in the same manner for the entire random walk process. The random surfer makes her decision only based on invariant A to select salient sentences and form the sum- mary. To equip the surfer with knowledge about what a diverse summary should be, we propose to adjust sentence transition matrix A in each itera- tion of random walk. The key point is that good candidate sentences should be normalized locally while bad ones should be normalized globally in the transformation from affinity matrix W to tran- sition matrix A.</p><p>A "virtual" summary V is produced based on x in each iteration of affinity-preserving random walk. "Virtual" here means that V is a summary based on transient distribution x, which differs from the final summary based on quasi-stationary distribution ¯ x. The method of diversity penalty im- position ( <ref type="bibr" target="#b20">Wan et al., 2007</ref>) is used to produce V , which is denoted by the producingSummary func- tion in Algorithm 4.1. It is a simple greedy al- gorithm to select sentences that are both salient and diverse, which often plays a role of greedy post-processing step to produce the final summary. Rather, we use it to produce virtual summary V that satisfies both the saliency and diversity con- straints based on a specific iteration. V is an indi- cator for the diversity constraint of summarization.</p><p>The sentence transition matrix A in the iteration (K +1) is then constructed with help of the virtual summary V in the iteration K. Here, different nor- malization methods are used to transform W into A. If V includes the sentence s i , elements in the corresponding i-th row of W will be normalized by the sum of the row (i.e. C(s i )). Otherwise, el- ements will be normalized by the maximum sum of row elements in W (i.e. C max ). In this way, "local normalization" is adopted for the sentences in V while "global normalization" is adopted for the sentences not in V . We differentiate the nor- malization methods to lead the surfer of affinity- preserving random walk to explore more in the neighborhood of the sentences in V rather than end in the absorbing vertex s 0 . As a result, the sen- tences that satisfy the saliency and diversity con- straints will be highlighted even though they are in a small sentence cluster. We characterize differ- The damping factor, µ; Output: The multi-document summary, V ;</p><formula xml:id="formula_13">1 A ← µW/C max + (1 − µ)y · e T 2 Initialize the starting distribution x as uniform 3 for i ← 1, 2, ..., M do 4 if i &gt; B then 5 V ← producingSummary(x) 6 D ← adjustingNormalization(V ) 7 A ← µ(D −1 W) T + (1 − µ)y · e T 8 ¯ x ← Ax 9 ¯ x ← ¯ x/¯ x 1 10 if ¯ x − x 1 &lt; ε then 11 break 12 x ← ¯ x 13 V ← producingSummary(x) 14 Return V ent normalizations in the diagonal matrix D. D ii is C(s i ) if s i ∈ V and D ii is C max if s i /</formula><p>∈ V ,which is denoted by the adjustingNormalization function in Algorithm 4.1. D in the current iteration is here dependent on x in the previous iteration. We will have different sentence augmented graphs G A in each iteration. <ref type="figure">Figure 4.2</ref> shows an example of G A (K) and G A (K + 1) in the respective itera- tions K and (K + 1). The probability distribution in the adjustable affinity-preserving random walk is updated as follows. <ref type="figure">Figure 4</ref>.2: Sentence augmented graphs for sum- marization in two successive iterations. G A (K): augmented graph in the iteration K. Virtual sum- mary V = {s 1 , s 3 , s 5 }, which is constructed from x in the iteration (K−1) by producingSummary.</p><formula xml:id="formula_14">x = µ(D −1 W) T x + (1 − µ)y µ(D −1 W) T x + (1 − µ)y 1<label>(10)</label></formula><formula xml:id="formula_15">D = diag([C(s 1 ), C max , C(s 3 ), C max , C(s 5 )]). G A (K + 1)</formula><p>: augmented graph in the iteration (K+1). V = {s 1 , s 4 , s 5 }, which is constructed from x in the iteration K by producingSummary.</p><formula xml:id="formula_16">D = diag([C(s 1 ), C max , C max , C(s 4 ), C(s 5 )]).</formula><p>In both cases, C max equals to C(s 1 ) indicating that sentence s 1 has the maximum conductance.</p><p>This is an adjustable Markov chain for which the transition matrix A is</p><formula xml:id="formula_17">µ(D −1 W) T + (1 − µ)y · e T .</formula><p>In this setting, A is dependent on the transient distribution x in the previous iteration, which dif- fers from the invariant transition matrix in Eq.(8).</p><p>As the diversity constraint is embedded in A, subsequent random walks move to the solution that induces a better summary. The algorithm of the adjustable affinity-preserving random walk for multi-document summarization is demonstrated in Algorithm 4.1. The parameter B in Algorithm 4.1 is used to produce a transient distribution which is reliable enough to adjust the transition matrix. To get the final multi-document summary, we use the same producingSummary function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Sets</head><p>Generic and topic-focused multi-document sum- marization have been the main tasks in DUC 1 . Task 2 of DUC 2004 is a generic summarization task and task 3 of DUC 2003 is a topic-focused summarization task. Both tasks are used for per- formance evaluation of our method. In the exper- iments, task 2 of DUC 2003 is used for the pa- 1 http://www-nlpir.nist.gov/projects/duc/intro.html rameter tuning of our method. We preprocess the document data sets by removing stopwords from each sentence and stemming the remaining words using the Porter's stemmer 2 . Also, the sentences containing the said clause (if a said, says, told, tells word and quotation marks appear simultaneously) are filtered out.</p><p>For evaluation, four reference summaries gen- erated by human judges for each document cluster are provided by DUC as the ground truth. A brief summary over the evaluation datasets is shown in <ref type="table" target="#tab_3">Table 5</ref>.1. According to <ref type="bibr" target="#b11">(Hong et al., 2014</ref>), we adjust the length limit of summary in DUC 2004 from 665 bytes to 100 words as it provides the same setting for system evaluations.  <ref type="table" target="#tab_3">Table 5</ref>.1: Summary of data sets used in our ex- periments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metric</head><p>We use the ROUGE-1.5.5 (Lin and Hovy, 2003) toolkit for evaluation, which has been officially adopted by DUC for automatic summarization evaluation. The toolkit measures summary qual- ity by counting overlapping units such as the n- gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE-N is an n-gram based measure and the ROUGE-N recall is computed as follows</p><formula xml:id="formula_18">ROUGE-NR = S∈{RefSum} n-gram∈S Count match (n-gram) S∈{RefSum} n-gram∈S Count(n-gram) (11)</formula><p>where n stands for the length of the n-gram, and Count match (n-gram) is the maximum number of n- grams co-occurring in the candidate summary and the set of reference summaries. Count(n-gram) is the number of n-grams in the reference sum- maries. We conduct our ROUGE experiments following the recommended standard in <ref type="bibr" target="#b17">(Owczarzak et al., 2012;</ref><ref type="bibr" target="#b11">Hong et al., 2014)</ref>  <ref type="bibr">3</ref> . We compute ROUGE-2 recall with stemming and stopwords not removed, which provides the best agreement with manual evaluations. We also compute ROUGE-1 recall which has the highest recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to iden- tify the better summary in a pair <ref type="bibr" target="#b17">(Owczarzak et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>In the experiments, the parameters of our method are set as follows: the decay factor λ is 2, the max- imum number of iteration M is 100, the number of starting iteration B is 30, the damping factor µ is 0.85 and the minimum error ε is 1E-30.  <ref type="table" target="#tab_3">Table 5</ref>.2: System comparisons on task 2 of DUC 2004 (%). *: Graph-based ranking methods. <ref type="table" target="#tab_3">Table 5</ref>.2 shows the performance of our method and other eleven well-known systems on task 2 of DUC 2004 according to ROUGE-1,2,4 recall, sorted by ROUGE-2 recall in the ascending order. Some of the results are from <ref type="bibr" target="#b11">(Hong et al., 2014</ref>). Cont. <ref type="bibr">LexPageRank (Erkan and Radev, 2004</ref>) is a graph-based ranking method and a representative of traditional random walk approach. Here we em- ploy the continuous version of LexPageRank. Fre- qSum ( <ref type="bibr" target="#b16">Nenkova et al., 2006</ref>) is a simple approach to approximate the importance of words with their probability in the input and then select sentences with high average word probability. CLASSY 04 <ref type="bibr" target="#b4">(Conroy et al., 2004</ref>) was the participant of the of- ficial DUC 2004 evaluation with the best evalua- tion score. It employs a Hidden Markov Model us- ing topic signature feature and requires a linguis- tic preprocessing component. CLASSY 11 <ref type="bibr" target="#b5">(Conroy et al., 2011</ref>) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization prob- lem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. <ref type="bibr">ICSISumm (Gillick and Favre, 2009</ref>) aims at find- ing the globally optimal summary by formulating the summarization task in Integer Linear Program- ming. WFS-NMF ( <ref type="bibr" target="#b21">Wang et al., 2010</ref>) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking mod- els mentioned in Section 2. APRW and AAPRW are our methods. APRW is the method of affinity- preserving random walk described in Section 4.2 and AAPRW is the method of adjustable affinity- preserving random walk described in Section 4.3.    <ref type="bibr" target="#b24">Zhou and Hovy, 2003;</ref><ref type="bibr" target="#b2">Chali et al., 2003)</ref>. Manifold Ranking is the method proposed in ( <ref type="bibr" target="#b20">Wan et al., 2007)</ref> to make use of both the re- lationships among all sentences in the documents and the relationships between the given topic de-scription and the sentences. APRW and AAPRW are our methods.</p><formula xml:id="formula_19">System R-1 R-2 R-4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>From <ref type="table" target="#tab_3">Tables 5.2 and 5</ref>.3, our method has the best ROUGE-2 score among all graph-based rank- ing methods for generic multi-document summa- rization, and it also has the best ROUGE-2 score for topic-focused multi-document summarization. AAPRW has the ROUGE-2 score 10.06% on DUC 2004 task 2, which is 0.28% higher than the best system ICSISumm reported by <ref type="bibr" target="#b11">(Hong et al., 2014</ref>) and 1.1% higher than the official best sys- tem CLASSY 04. WFS-NMF has the overall best score on DUC 2004 task 2 due to the sentence feature selection and the weights on the document side, which is reported by ( <ref type="bibr" target="#b21">Wang et al., 2010;</ref><ref type="bibr" target="#b0">Alguliev et al., 2013)</ref>. AAPRW has the ROUGE-2 score 8.21% on DUC 2003 task 3, which is 0.53% higher than Manifold Ranking and 0.9% higher than the official best system S16. In DUC 2004 AAPRW has 0.67% more ROUGE-2 score than APRW and the gap is 0.49% in DUC 2003, which proves the effectiveness of the adjustable transi- tion matrix in the random walk process. It is worth mentioning that our method has the best ROUGE- 4 score on the DUC 2003 topic-focused summa- rization task.</p><p>We conducted the two-sided Wilcoxon signed- rank tests between each pair of AAPRW and other methods. For the generic summarization in DUC 2004, our method provides a significant improve- ment over the official best system CLASSY 04 on ROUGE-2 (with p-value lower than 0.05). For the query-focused summarization in DUC 2003, our method also provides a significant improvement over S17, S13 and S16 on ROUGE-2.</p><p>In order to further investigate the influences of the parameter in our proposed method, the damp- ing factor µ is varied from 0 to 1. Figures 5.1 and 5.2 show the ROUGE-1 and ROUGE-2 re- call curves of our method on the two data sets, re- spectively. We can see from the figures that the damping factor has an effect on the performance of multi-document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper we propose the adjustable affinity- preserving random walk for generic and topic- focused multi-document summarization, which deals with the saliency and diversity goals in a uni- fied framework. Experiments demonstrate the ef- fectiveness of our method.  In the future work, we will focus on the self transition of adjustable affinity preserving random walk, which could be used to remove the redun- dancy between summary sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 4 . 1 .</head><label>41</label><figDesc>Suppose ¯ x is the quasi-stationary distribution of affinity-preserving random walk as defined in Sec.4.2 and x is the solution of a continuous quadratic optimization problem argmax(x T A x) s.t. x ∈ [0, 1] n , x 2 = 1 and A has definition in Sec.4.2. The following equa- tion holds ¯ x = x/x 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The sentence affinity matrix, W; The starting and maximum number of iteration, B and M ; The teleport vector, y;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5.1: ROUGE-1 recall scores vs. µ of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>3: System comparisons on task 3 of DUC 
2003 (%). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table>3 shows the evaluation results on task 3 
of DUC 2003 according to ROUGE-1,2,4 recall, 
sorted also by ROUGE-2 recall in the ascending 
order. S13, S16 and S17 are the system IDs of the 
top performing systems in the official DUC 2003 
evaluation, whose details are described in DUC 
publications (</table></figure>

			<note place="foot">í µí±® í µí±¨(µí±¨(í µí±² + í µí¿) í µí±® í µí±¨(µí±¨(í µí±²)</note>

			<note place="foot" n="2"> https://tartarus.org/martin/PorterStemmer/</note>

			<note place="foot" n="3"> ROUGE-1.5.5 with the parameters:-n 4-m-a-l 100-x-c 95-r 1000-f A-p 0.5-t 0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank our three anonymous reviewers for their helpful advice on various aspects of this work. This research is sup-ported by National Key Basic Research Pro-gram of China (No.2014CB340504) and Na-tional Natural Science Foundation of China (No.61375074,61273318). The contact authors for this paper are Zhifang Sui and Baobao Chang.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multiple documents summarization 218 based on evolutionary optimization algorithm. Expert Systems with Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasim M Alguliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramiz M Aliguliyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isazade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1675" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The university of lethbridge text summarizer at duc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yllias</forename><surname>Chali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maheedhar</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanak</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenshuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Proceedings of the HLT/NAACL workshop on Automatic Summarization/Document Understanding Conference</title>
		<imprint>
			<publisher>DUC</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reweighted random walks for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Left-brain/rightbrain multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">D</forename><surname>John M Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne</forename><forename type="middle">P</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Document Understanding Conference</title>
		<meeting>the Document Understanding Conference</meeting>
		<imprint>
			<publisher>DUC</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Classy 2011 at tac: Guided and multi-lingual summaries and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">D</forename><surname>John M Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schlesinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianne P O&amp;apos;</forename><surname>Peter A Rankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TAC</publisher>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On quasistationary distributions in absorbing discrete-time finite markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Darroch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seneta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="100" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diversity in ranking via resistive graph centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lexrank: Graph-based lexical centrality as salience in text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günes</forename><surname>Erkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir R Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="457" to="479" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Langauge Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topic-sensitive pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haveliwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on World Wide Web</title>
		<meeting>the 11th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A repository of state of the art and competitive baseline summaries for generic news summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1608" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
		<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic evaluation of summaries using n-gram cooccurrence statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
		<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="71" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Divrank: the interplay of prestige and diversity in information networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>Acm</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1009" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="573" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An assessment of the accuracy of automatic evaluation in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Non-negative matrices and Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Seneta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Manifold-ranking based topic-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2903" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weighted feature subset non-negative matrix factorization and its applications to document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2010 IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An ontology matching approach based on affinity-preserving random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuncheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1471" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Headline summarization at isi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Understanding Conference (DUC-2003)</title>
		<meeting><address><addrLine>Edmonton, Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving diversity in ranking using absorbing random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Van Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrzejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
