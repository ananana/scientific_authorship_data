<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word Translation Prediction for Morphologically Rich Languages with Bilingual Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam Science</orgName>
								<address>
									<addrLine>Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam Science</orgName>
								<address>
									<addrLine>Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam Science</orgName>
								<address>
									<addrLine>Park 904</addrLine>
									<postCode>1098 XH</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Word Translation Prediction for Morphologically Rich Languages with Bilingual Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1676" to="1688"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Translating into morphologically rich languages is a particularly difficult problem in machine translation due to the high degree of inflectional ambiguity in the target language, often only poorly captured by existing word translation models. We present a general approach that exploits source-side contexts of foreign words to improve translation prediction accuracy. Our approach is based on a probabilistic neural network which does not require linguistic annotation nor manual feature engineering. We report significant improvements in word translation prediction accuracy for three morphologically rich target languages. In addition, preliminary results for integrating our approach into a large-scale English-Russian statistical machine translation system show small but statistically significant improvements in translation quality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to make context-sensitive translation decisions is one of the major strengths of phrase- based SMT (PSMT). However, the way PSMT ex- ploits source-language context has several limita- tions as pointed out, for instance, by <ref type="bibr" target="#b38">Quirk and Menezes (2006)</ref> and <ref type="bibr" target="#b13">Durrani et al. (2013)</ref>. First, the amount of context used to translate a given input word depends on the phrase segmentation, with hypotheses resulting from different segmen- tations competing with one another. Another issue is that, given a phrase segmentation, each source phrase is translated independently from the oth- ers, which can be problematic especially for short phrases. As a result, the predictive translation of a source phrase does not access useful linguistic clues in the source sentence that are outside of the scope of the phrase.</p><p>Lexical weighting tackles the problem of un- reliable phrase probabilities, typically associated with long phrases, but does not alleviate the prob- lem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very ef- fective but can only leverage target language con- text. Moreover, decisions that are taken at early decoding stages-such as the common practice of retaining only top n translation options for each source span-depend only on the translation models and on the target context available in the phrase.</p><p>Source context based translation models <ref type="bibr" target="#b16">(Gimpel and Smith, 2008;</ref><ref type="bibr" target="#b33">Mauser et al., 2009;</ref><ref type="bibr" target="#b22">Jeong et al., 2010;</ref><ref type="bibr" target="#b18">Haque et al., 2011</ref>) naturally ad- dress these limitations. These models can ex- ploit a boundless context of the input text, but they assume that target words can be predicted in- dependently from each other, which makes them easy to integrate into state-of-the-art PSMT sys- tems. Even though the independence assump- tion is made on the target side, these models have shown the benefits of utilizing source context, es- pecially in translating into morphologically rich languages. One drawback of previous research on this topic, though, is that it relied on rich sets of manually designed features, which in turn required the availability of linguistic annotation tools like POS taggers and syntactic parsers.</p><p>In this paper, we specifically focus on im- proving the prediction accuracy for word transla- tions. Achieving high levels of word translation accuracy is particularly challenging for language pairs where the source language is morphologi- cally poor, such as English, and the target lan- guage is morphologically rich, such as Russian, i.e., language pairs with a high degree of surface realization ambiguity ( <ref type="bibr" target="#b34">Minkov et al., 2007)</ref>. To address this problem we propose a general ap- proach based on bilingual neural networks (BNN) exploiting source-side contextual information.</p><p>This paper makes a number of contributions: Unlike previous approaches our models do not re- quire any form of linguistic annotation <ref type="bibr" target="#b34">(Minkov et al., 2007;</ref><ref type="bibr" target="#b26">Kholy and Habash, 2012;</ref><ref type="bibr" target="#b8">Chahuneau et al., 2013)</ref>, nor do they require any feature en- gineering ( <ref type="bibr" target="#b16">Gimpel and Smith, 2008)</ref>. Moreover, besides directly predicting fully inflected forms as <ref type="bibr" target="#b22">Jeong et al. (2010)</ref>, our approach can also model stem and suffix prediction explicitly. Pre- diction accuracy is evaluated with respect to three morphologically rich target languages <ref type="bibr">(Bulgarian, Czech, and Russian)</ref> showing that our approach consistently yields substantial improvements over a competitive baseline. We also show that these improvements in prediction accuracy can be ben- eficial in an end-to-end machine translation sce- nario by integrating into a large-scale English- Russian PSMT system. Finally, a detailed analysis shows that our approach induces a positive bias on phrase translation probabilities leading to a better ranking of the translation options employed by the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lexical coverage of SMT models</head><p>The first question we ask is whether translation can be improved by a more accurate selection of the translation options already existing in the SMT models, as opposed to generating new options. To answer this question we measure the lexical coverage of a baseline PSMT system trained on English-Russian. <ref type="bibr">1</ref> We choose this language pair because of the morphological richness on the tar- get side: Russian is characterized by a highly in- flectional morphology with a particularly complex nominal declension (six core cases, three genders and two number categories). As suggested by <ref type="bibr" target="#b17">Green and DeNero (2012)</ref>, we compute the re- call of reference tokens in the set of target to- kens that the decoder could produce in a trans- lation of the source, that is the target tokens of all phrase pairs that matched the input sentence and that were actually used for decoding. <ref type="bibr">2</ref> We call this the decoder's lexical search space. Then, we compare the reference/space recall against the reference/MT-output recall: that is, the percent- age of reference tokens that also appeared in the 1-best translation output by the SMT system. Re- sults for the WMT12 benchmark are presented in <ref type="table">Table 1</ref>. From the first two rows, we see that only a rather small part of the correct target tokens avail- able to the decoder are actually produced in the 1-best MT output (50% against 86%). Although our word-level analysis does not directly estimate phrase-level coverage, these numbers suggest that a large potential for translation improvement lies in better lexical selection during decoding.</p><p>Token recall: reference/MT-search-space 86.0% reference/MT-output 50.0% stem-only reference/MT-output 12.3% of which reachable 11.2% <ref type="table">Table 1</ref>: Lexical coverage analysis of the baseline SMT system (English-Russian wmt12).</p><p>To quantify the importance of morphology, we count how many reference tokens matched the MT output only at the stem level <ref type="bibr">3</ref> and for how many of those the correct surface form existed in the search space (reachable matches). These two numbers represent the upper bound of the im- provement achievable by a model only predicting suffixes given the target stems. As shown in Ta- ble 1, such a model could potentially increase the reference/MT-output recall by 12.3% with genera- tion of new inflected forms, and by 11.2% without. Thus, also when it comes to morphology, gener- ation seems to be of secondary importance com- pared to better selection in our experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Predicting word translations in context</head><p>It is standard practice in PSMT to use word- to-word translation probabilities as an additional phrase score. More specifically, state-of-the-art PSMT systems employ the maximum-likelihood estimate of the context-independent probability of a target word given its aligned source word P (t j |s i ) for each word alignment link a ij .</p><p>[конституционность] [индиана закон]</p><formula xml:id="formula_0">constitutionality of the] [indiana law] [.]</formula><p>[.]</p><p>[the <ref type="figure">Figure 1</ref>: Fragment of English sentence and its in- correct Russian translation produced by the base- line SMT system. Square brackets indicate phrase boundaries.</p><p>The main goal of our work is to improve the estimation of such probabilities by exploiting the context of s i , which in turn we expect will re- sult in better phrase translation selection. <ref type="figure">Figure  1</ref> illustrates this idea: the translation of "law" in this example has a wrong case-nominative in- stead of genitive. Due to the rare word "Indi- ana/индиана", the target LM must backoff to the bigram history and does not penalize this choice sufficiently. However, a model that has access to the word "of" in the near source context could bias the translation of "law" to the correct case.</p><p>We then model P (t j |c s i ) with source context c s i defined as a fixed-length word sequence cen- tered around s i :</p><formula xml:id="formula_1">c s i = s i−k , ..., s i , .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.., s i+k</head><p>Our definition of context is similar to the n − 1 word history used in n-gram LMs. Similarly to previous work in source context-sensitive trans- lation modeling ( <ref type="bibr" target="#b22">Jeong et al., 2010;</ref><ref type="bibr" target="#b8">Chahuneau et al., 2013)</ref>, target words are predicted indepen- dently from each other, which allows for an ef- ficient decoding integration. We are particularly interested in translating into morphologically rich languages where source context can provide useful information for the prediction of target translation, for example, the gender of the subject in a source sentence constrains the morphology of the transla- tion of the source verb. Therefore, we integrate the notions of stem and suffix directly into the model. We assume the availability of a word segmenta- tion function g that takes a target word t as in- put and returns its stem and suffix: g(t) = (σ, µ). Then, the conditional probability p(t j |c s i ) can be decomposed into stem probability and suffix prob- ability:</p><formula xml:id="formula_2">p(t j |c s i ) = p(σ j |c s i )p(µ j |c s i , σ j )<label>(1)</label></formula><p>These two probabilities can be estimated sepa- rately, which yields the two subtasks:</p><p>1. predict target stem σ given source context c s ;</p><p>2. predict target suffix µ given source context c s and target stem σ.</p><p>Based on the results of our analysis, we focus on the selection of existing translation candidates. We then restrict our prediction on a set of pos- sible target candidates depending on the task in- stead of considering all target words in the vocab- ulary. More specifically, for each source word s i , our candidate generation function returns the set of target words T s = {t 1 , . . . , t m } that were aligned to s i in the parallel training corpus, which in turn corresponds to the set of target words that the SMT system can produce for a given source. In practice, we use a pruned version of T s to speed up training and reduce noise (see details in Section 5).</p><p>As for the morphological models, given T s and g, we can obtain L s = {σ 1 , . . . , σ k }, the set of possible target stem translations of s, and M σ = {µ 1 , . . . , µ l }, the set of possible suffixes for a tar- get stem σ. The use of L s , and M σ is similar to stemming and inflection operations in ( <ref type="bibr" target="#b48">Toutanova et al., 2008</ref>) while the set T s is similar to the GEN function in ( <ref type="bibr" target="#b22">Jeong et al., 2010</ref>). <ref type="bibr">4</ref> Our approach differs crucially from previous work ( <ref type="bibr" target="#b34">Minkov et al., 2007;</ref><ref type="bibr" target="#b8">Chahuneau et al., 2013</ref>) in that it does not require linguistic fea- tures such as part-of-speech and syntactic tree on the source side. The proposed models automati- cally learn features that are relevant for each of the modeled tasks, directly from word-aligned data. To make the approach completely language inde- pendent, the word segmentation function g can be trained with an unsupervised segmentation tool. The effects of using different word segmentation techniques are discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bilingual neural networks for translation prediction</head><p>Probabilistic neural network (NN), or continuous space, language models have received increas- ing attention over the last few years and have been applied to several natural language process- ing tasks ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b44">Socher et al., 2011;</ref><ref type="bibr" target="#b45">Socher et al., 2012</ref>). Within statistical machine translation, they have been used for monolingual target language modeling ( <ref type="bibr" target="#b40">Schwenk et al., 2006;</ref><ref type="bibr" target="#b31">Le et al., 2011;</ref><ref type="bibr" target="#b12">Duh et al., 2013;</ref><ref type="bibr" target="#b49">Vaswani et al., 2013)</ref>, n-gram translation modeling ( <ref type="bibr" target="#b46">Son et al., 2012</ref>), phrase translation modeling <ref type="bibr" target="#b41">(Schwenk, 2012;</ref><ref type="bibr" target="#b50">Zou et al., 2013;</ref>) and minimal translation modeling ( <ref type="bibr" target="#b21">Hu et al., 2014</ref>). The recurrent neural network LMs of <ref type="bibr" target="#b0">Auli et al. (2013)</ref> are primarily trained to predict target word sequences. However, they also experiment with an additional input layer representing source side context. Our models differ from most previous work in neural language modeling in that we predict a tar- get translation given a source context while pre- vious models predict the next word given a tar- get word history. Unlike previous work in phrase translation modeling with NNs, our models have the advantage of accessing source context that can fall outside the phrase boundaries.</p><p>We now describe our models in a general set- ting, predicting target translations given a source context, where target translations can be either words, stems or suffixes. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neural network architecture</head><p>Following a common approach in deep learning for NLP ( <ref type="bibr" target="#b1">Bengio et al., 2003;</ref><ref type="bibr" target="#b11">Collobert and Weston, 2008)</ref>, we represent each source word s i by a column vector r s i ∈ R d . Given a source con- text c s i = s i−k , ..., s i , ..., s i+k of k words on the left and k words on the right of s i , the context rep- resentation r cs i ∈ R (2k+1)×d is obtained by con- catenating the vector representations of all words in c s i :</p><formula xml:id="formula_3">r cs i = r s i−k ... r s i+k</formula><p>Our main BNN architecture for word or stem prediction ( <ref type="figure" target="#fig_0">Figure 2a</ref>) is a feed-forward neural network (FFNN) with one hidden layer, a matrix W 1 ∈ R n×(2k+1)d connecting the input layer to the hidden layer, a matrix W 2 ∈ R |Vt|×n connect- ing the hidden layer to the output layer, and a bias vector b 2 ∈ R |Vt| where |V t | is the size of target translations vocabulary. The target translation dis- tribution P BNN (t|c s i ) for a given source context c s i is computed by a forward pass:</p><formula xml:id="formula_4">softmax W 2 φ(W 1 r cs i ) + b 2 (2)</formula><p>where φ is a nonlinearity (tanh, sigmoid or rec- tified linear units). The parameters of the neural <ref type="bibr">5</ref> The source code of our models is available at https:</p><formula xml:id="formula_5">//bitbucket.org/ketran network are θ = {r s i , W 1 , W 2 , b 2 }.</formula><p>The suffix prediction BNN is obtained by adding the target stem representation r σ to the in- put layer (see <ref type="figure" target="#fig_0">Figure 2b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model variants</head><p>We encounter two major issues with FFNNs: (i) They do not provide a natural mechanism to com- pute word surface conditional probability p(t|c s ) given individual stem probability p(σ|c s ) and suf- fix probability p(µ|c s , σ), and (ii) FFNNs do not provide the flexibility to capture long dependen- cies among words if they lie outside the source context window. Hence, we consider two BNN variants: a log-bilinear model (LBL) and a con- volutional neural network model (ConvNet). LBL could potentially address (i) by factorizing target representations into target stem and suffix repre- sentations whereas ConvNets offer the advantage of modeling variable input length (ii) <ref type="bibr" target="#b24">(Kalchbrenner et al., 2014)</ref>.</p><p>Log-bilinear model. The FFNN models stem and suffix probabilities separately. A log-bilinear model instead could directly model word predic- tion through a factored representation of target words, similarly to <ref type="bibr" target="#b6">Botha and Blunsom (2014)</ref>. Thus, no probability mass would be wasted over stem-suffix combinations that are not in the candi- date generation function. The LBL model speci- fies the conditional distribution for the word trans- lation t j ∈ T s i given a source context c s i :</p><formula xml:id="formula_6">P θ (t j |c s i ) = exp(s θ (t j , c s i )) t j ∈Ts i exp(s θ (t j , c s i ))<label>(3)</label></formula><p>We use an additional set of word representations q t j ∈ R n for target translations t j . The LBL model computes a predictive representationˆqrepresentationˆ representationˆq of a source context c s i by taking a linear combination of the source word representations r s i+m with the position-dependent weight matrices C m ∈ R n×d :</p><formula xml:id="formula_7">ˆ q = k m=−k C m r s i+m<label>(4)</label></formula><p>The score function s θ (t j , c s i ) measures the simi- larity between the predictive representationˆqrepresentationˆ representationˆq and the target representation q t j :</p><formula xml:id="formula_8">s θ (t j , c s i ) = ˆ q T q t j + b T h q t j + b t j (5) P ✓ (t|c si ) r s ik r si r s i+k W 1 W 2 (x)</formula><p>(a) BNN for word prediction. Here b t j is the bias term associated with target word t j . b h ∈ R n are the representation bi- ases. s θ (t j , c s i ) can be seen as the negative en- ergy function of the target translation t j and its context c s i . The parameters of the model thus</p><formula xml:id="formula_9">P ✓ (µ|, c si ) r s ik r si r s i+k W 1 W 2 (x) r (b) BNN for suffix prediction.</formula><formula xml:id="formula_10">are θ = {r s i , C m , q t j , b h , b t j }.</formula><p>Our log-bilinear model is a modification of the log-bilinear model proposed for n-gram language modeling in <ref type="bibr" target="#b35">(Mnih and Hinton, 2007)</ref>. </p><formula xml:id="formula_11">m cs i = r s i−k ; . . . ; r s i+k<label>(6)</label></formula><note type="other">ˆ q r s 1 r s 2 r s 3 r s 4 r s 5 r s 6 r s 0 Figure 3: Convolutional neural network model. Edges with the same color indicate the same ker- nel weight matrix. Each convolutional layer L i consists of a one- dimensional filter m i ∈ R d×2 . Each row of m i</note><p>is convolved with the corresponding row in the previous layer resulting in a weight matrix whose number of columns decreases by one. Thus after 2k convolutional layers, the network transforms the source context matrix m cs i to a feature vec- torˆqtorˆ torˆq ∈ R d . A fully connected layer with weight matrix W followed by a softmax layer are placed after the last convolutional layer L 2k to perform classification. The parameters of the convolutional neural network model are θ = {r s i , m j , W}. Here, we focus on a fixed length input, how- ever convolutional neural networks may be used to model variable length input ( <ref type="bibr" target="#b24">Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b23">Kalchbrenner and Blunsom, 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>In training, for each example (t, c s ), we maximize the conditional probability P θ (t|c s ) of a correct target label t. The contribution of the training ex- ample (t, c s ) to the gradient of the log conditional probability is given by:</p><formula xml:id="formula_12">∂ ∂θ log P θ (t|c s ) = ∂ ∂θ s θ (t|c s ) − t ∈Ts P θ (t |c s ) ∂ ∂θ s θ (t , c s )</formula><p>Note that in the gradient, we do not sum over all target translations T but a set of possible candi- dates T s of a source word s. In practice |T s | ≤ 200 with our pruning settings (see Section 5.1), thus training time for one example does not depend on the vocabulary size. Our training criterion can be seen as a form of contrastive estimation <ref type="bibr" target="#b43">(Smith and Eisner, 2005</ref>), however we explicitly move the probability mass from competing candidates to the correct translation candidate, thus obtaining more reliable estimates of the conditional probabilities. The BNN parameters are initialized randomly according to a zero-mean Gaussian. We regularize the models with L 2 . As an alternative to the L 2 regularizer, we also experiment with dropout <ref type="bibr" target="#b19">(Hinton et al., 2012)</ref>, where the neurons are randomly zeroed out with dropout rate p. This technique is known to be useful in computer vision tasks but has been rarely used in NLP tasks. In FFNN, we use dropout after the hidden layer, while in Con- vNet, dropout applies after the last convolutional layer. The dropout rate p is set to 0.3 in our exper-iments. We use rectified nonlinearities 6 in FFNN and after each convolutional layer in ConvNet. We train our BNN models with the standard stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluating word translation prediction</head><p>In this section, we assess the ability of our BNN models to predict the correct translation of a word in context. In addition to English-Russian, we also consider translation prediction for Czech and Bul- garian. As members of the Slavic language fam- ily, Czech and Bulgarian are also characterized by highly inflectional morphology. Czech, like Rus- sian, displays a very rich nominal inflection with as many as 14 declension paradigms. Bulgarian, unlike Russian, is not affected by case distinctions but is characterized by a definiteness suffix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>The following parallel corpora are used to train the BNN models:</p><p>• English-Russian: WMT13 data (News Com- mentary and Yandex corpora);</p><p>• English-Czech: CzEng 1.0 corpus (Bojar et al., 2012) (Web Pages and News sections);</p><p>• English-Bulgarian: a mix of crawled news data, TED talks and Europarl proceedings.</p><p>Detailed corpus statistics are given in <ref type="table">Table 2</ref>. For each language pair, accuracies are measured on a held-out set of 10K parallel sentences. To prepare the candidate generation function, each dataset is first word-aligned with GIZA++, then a bilingual lexicon with maximum-likelihood probabilities (P mle ) is built from the symmetrized alignment. After some frequency and signifi- cance pruning, 7 the top 200 translations sorted by P mle (t|s) · P mle (s|t) are kept as candidate word translations for each source word in the vocabu- lary. Word alignments are also used to train the BNN models: each alignment link constitutes a training sample, with no special treatment of un- aligned words and 1-to-many alignments.</p><p>The context window size k is set to 3 (cor- responding to 7-gram) and the dimensionality of source word representations to 100 in all experi- ments. The number of hidden units in our feed- forward neural networks and the target translation embedding size in LBL models are set to 200. All models are trained for 10 iterations with learning rate set to 0.001.  <ref type="table">Table 2</ref>: BNN training corpora statistics: number of sentences, tokens, and type/token ratio (T/T).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Ru En-Cs En-Bg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Word, stem and suffix prediction accuracy</head><p>We measure accuracy at top-n, i. e. the number of times the correct translation was in the top n candidates sorted by a model. For each subtask- word, stem and suffix prediction-the BNN model is compared to the context-independent maximum-likelihood baseline P mle (t|s) on which the PSMT lexical weighting score is based. Note that this is a more realistic baseline than the uni- form models sometimes reported in the litera- ture. The oracle corresponds to the percentage of aligned source-target word pairs in the held-out set that are covered by the candidate generation func- tion. Out of the missing links, about 4% is due to lexicon pruning. Results for all three language pairs are presented in <ref type="table" target="#tab_2">Table 3</ref>. In this series of experiments, the morphological BNNs utilize un- supervised segmentation models trained on each target language following <ref type="bibr" target="#b32">Lee et al. (2011)</ref>. <ref type="bibr">8</ref> As shown in <ref type="table" target="#tab_2">Table 3</ref>, the BNN models outper- form the baseline by a large margin in all tasks and languages. In particular, word prediction accuracy at top-1 increases by +6.4%, +24.6% and +9.0% absolute in English-Russian, English-Czech and English-Bulgarian respectively, without the use of any features based on linguistic annotation. While the baseline and oracle differences among lan- guages can be explained by different levels of overlap between training and held-out set, we can- not easily explain why the Czech BNN perfor- mance is so much higher. When comparing the  three prediction subtasks, we find that word pre- diction is the hardest task as expected. Stem pre- diction accuracies are considerably higher than word prediction accuracies in Russian, but almost equal in the other two languages. Finally, base- line accuracies for suffix prediction are by far the highest, ranging between 71.2% and 81.5%, which is primarily explained by a smaller num- ber of candidates to choose from. Also on this task, the BNN model achieves considerable gains of +5.8%, +13.1% and +6.2% at top-1, without the need of manual feature engineering. From these figures, it is hard to predict whether word BNNs or morphological BNNs will have a better effect on SMT performance. On one hand, the word-level BNN achieves the highest gain over the MLE baseline. On the other, the stem-and suffix-level BNNs provide two separate scoring functions, whose weights can be directly tuned for translation quality. A preliminary answer to this question is given by the SMT experiments pre- sented in Section 6.</p><note type="other">Model En-Ru En-Cs En-</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of word segmentation</head><p>This section analyzes the effect of using different segmentation techniques. We consider two super- vised tagging methods that produce lemma and in- flection tag for each token in a context-sensitive manner: TreeTagger ( <ref type="bibr" target="#b42">Sharoff et al., 2008</ref>) for Rus- sian and the Morce tagger ( <ref type="bibr" target="#b47">Spoustová et al., 2007</ref>) for Czech. <ref type="bibr">9</ref> Finally, we employ the Russian Snow- ball rule-based stemmer as a light-weight context-  insensitive segmentation technique. <ref type="bibr" target="#b4">10</ref> As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, accuracies for both stem and suffix prediction vary noticeably with the seg- mentation used. However, higher stem accuracies corresponds to lower suffix accuracies and vice versa, which can be mainly due to a general pref- erence of a tool to segment more or less than an- other. In summary, the unsupervised segmentation methods and the light-weight stemmer appear to perform comparably to the supervised methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of training data size</head><p>We examine the predictive power of our models with respect to the size of training data. <ref type="table">Table 4</ref> shows the accuracies of stem and suffix models trained on 200K and 1M English-Russian sentence pairs with unsupervised word segmentation. Sur- prisingly, we observe only a minor loss when we decrease the training data size, which suggests that our models are robust even on a small data set.  <ref type="table">Table 4</ref>: Accuracy at top-1/top-3 (%) of stem and suffix BNNs with different training data sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Fine-grained evaluation</head><p>We evaluate the suffix BNN model at the part-of- speech (POS) level. <ref type="table">Table 5</ref> provides suffix pre- diction accuracy per POS for En-Ru. For this analysis, Russian data is segmented by TreeTag-ger. Additionally, we report the average number of suffixes per stem given the part-of-speech.</p><p>Our results are consistent with the findings of <ref type="bibr" target="#b8">Chahuneau et al. (2013)</ref>: <ref type="bibr">11</ref> the prediction of ad- jectives is more difficult than that of other POS while Russian verb prediction is relatively easier in spite of the higher number of suffixes per stem. These differences reflect the importance of source versus target context features in the prediction of the target inflection: For instance, adjectives agree in gender with the nouns they modify, but this may be only inferred from the target context.  <ref type="table">Table 5</ref>: Suffix prediction accuracy at top-1 (%), breakdown by category (A: adjectives, V: verbs, N: nouns, M: numerals and P: pronouns). |M σ | denotes the average number of suffixes per stem. <ref type="table" target="#tab_5">Table 6</ref> shows the stem and suffix accuracies of BNN variants on English-Czech. Although none of the variants outperform our main FFNN archi- tecture, we observe similar performances by the LBL on stem prediction, and by the ConvNet on suffix prediction. This suggests that future work could exploit their additional flexibilities (see Sec- tion 4.2) to improve the BNN predictive power. As for the low suffix accuracy by the LBL, it can be explained by the absence of nonlinearity transformation. Nonlinearity is important for the suffix model where the prediction of target suf- fix µ j often does not depend linearly on s i and σ j . The predictive representation of target stem in the LBL stem model, however, mainly depends on the source representation r s i through a position dependent weight matrix C 0 . Thus, we observe a smaller accuracy drop in the stem model than in the suffix model. Conversely, the ConvNet per- forms poorly on stem prediction because it cap- tures the meaning of the whole source context in- stead of emphasizing the importance of the source word s i as the main predictor of the target transla- tion t j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Neural Network variants</head><p>Unexpectedly, no improvement is obtained by the use of dropout regularizer (see Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Stem  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SMT experiments</head><p>While the main objective of this paper is to im- prove prediction accuracy of word translations, see Section 5, we are also interested in know- ing to which extent these improvements carry over within an end-to-end machine translation task. To this end, we integrate our translation prediction models described in Section 4 into our existing English-Russian SMT system. For each phrase pair matching the input, the phrase BNN score P BNN-p is computed as follows:</p><formula xml:id="formula_13">P BNN-p (˜ s, ˜ t, a) = |˜s||˜s| i=1      1 |{a i }| j∈{a i } P BNN (t j |c s i ) if |{a i }| &gt; 0 P mle (NULL|s i ) otherwise</formula><p>where a is the word-level alignment of the phrase pair (˜ s, ˜ t) and {a i } is the set of target positions aligned to s i . If a source-target link cannot be scored by the BNN model, we give it a P BNN probability of 1 and increment a separate count feature ε. Note that the same phrase pair can get different BNN scores if used in different source side contexts.</p><p>Our baseline is an in-house phrase-based ( <ref type="bibr" target="#b27">Koehn et al., 2003</ref>) statistical machine transla- tion system very similar to Moses ( <ref type="bibr" target="#b28">Koehn et al., 2007)</ref>. All system runs use hierarchical lexicalized reordering ( <ref type="bibr" target="#b14">Galley and Manning, 2008;</ref><ref type="bibr" target="#b10">Cherry et al., 2012)</ref>, distinguishing between monotone, swap, and discontinuous reordering, all with re- spect to left-to-right and right-to-left decoding. Other features include linear distortion, bidirec- tional lexical weighting ( <ref type="bibr" target="#b27">Koehn et al., 2003</ref>), word and phrase penalties, and finally a word-level 5- gram target LM trained on all available mono- lingual data with modified Kneser-Ney smooth- ing <ref type="bibr" target="#b9">(Chen and Goodman, 1999</ref>  <ref type="table">Table 7</ref>: SMT training and test data statistics. All numbers refer to tokenized, lowercased data.</p><p>limit is set to 6 and for each source phrase the top 30 translation candidates are considered. When translating into a morphologically rich language, data sparsity issues in the target language become particularly apparent. To compensate for this we also experiment with a 5-gram suffix-based LM in addition to the surface-based LM ( <ref type="bibr" target="#b36">Müller et al., 2012;</ref><ref type="bibr" target="#b2">Bisazza and Monz, 2014)</ref>.</p><p>The BNN models are integrated as additional log-probability feature functions (log P BNN-p ): one feature for the word prediction model or two features for the stem and suffix models respec- tively, plus the penalty feature ε. <ref type="table">Table 7</ref> shows the data used to train our English- Russian SMT system. The feature weights for all approaches were tuned by using pairwise rank- ing optimization (Hopkins and May, 2011) on the wmt12 benchmark <ref type="bibr" target="#b7">(Callison-Burch et al., 2012)</ref>. During tuning, 14 PRO parameter estimation runs are performed in parallel on different samples of the n-best list after each decoder iteration. The weights of the individual PRO runs are then av- eraged and passed on to the next decoding itera- tion. Performing weight estimation independently for a number of samples corrects for some of the instability that can be caused by individual sam- ples. The wmt13 set ( <ref type="bibr" target="#b5">Bojar et al., 2013</ref>) was used for testing. We use approximate randomization <ref type="bibr" target="#b37">(Noreen, 1989)</ref> to test for statistically significant differences between runs ( <ref type="bibr" target="#b39">Riezler and Maxwell, 2005</ref>).</p><p>Translation quality is measured with case- insensitive BLEU[%] using one reference trans- lation. As shown in <ref type="table" target="#tab_8">Table 8</ref>, statistically signif- icant improvements over the respective baseline (Baseline and Base+suffLM) are marked at the p &lt; .01 level. Integrating our bilingual neural net- work approach into our SMT system yields small but statistically significant improvements of 0.4 BLEU over a competitive baseline. We can also  To better understand the BNN effect on the SMT system, we analyze the set of phrase pairs that are employed by the decoder to translate each sentence. This set is ranked by the weighted com- bination of phrase translation and lexical weight- ing scores, target language model score and, if available, phrase BNN scores. As shown in Ta- ble 9, the morphological BNN models have a pos- itive effect on the decoder's lexical search space increasing the recall of reference tokens among the top 1 and 3 phrase translation candidates. The mean reciprocal rank (MRR) also improves from 0.655 to 0.662. Looking at the 1-best SMT output, we observe a slight increase of reference/output recall (50.0% to 50.7%), which is less than the in- crease we observe for the top 1 translation candi- dates (57.6% to 59.0%  <ref type="table">Table 9</ref>: Target word coverage analysis of the English-Russian SMT system before and after adding the morphological BNN models.</p><p>like the target LM, that are based on traditional maximum-likelihood estimates. While the suffix- based LMs proved beneficial in our experiments, we speculate that higher gains could be obtained by coupling our approach with a morphology- aware neural LM like the one recently presented by <ref type="bibr" target="#b6">Botha and Blunsom (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>While most relevant literature has been discussed in earlier sections, the following approaches are particularly related to ours: <ref type="bibr" target="#b34">Minkov et al. (2007)</ref> and <ref type="bibr" target="#b48">Toutanova et al. (2008)</ref> address target inflec- tion prediction with a log-linear model based on rich morphological and syntactic features. Their model exploits target context and is applied to inflect the output of a stem-based SMT system, whereas our models predict target words (or pairs of stem-suffix) independently and are integrated into decoding. <ref type="bibr" target="#b8">Chahuneau et al. (2013)</ref> address the same problem with another feature-rich dis- criminative model that can be integrated in decod- ing, like ours, but they also use it to inflect on- the-fly stemmed phrases. It is not clear what part of their SMT improvements is due to the gener- ation of new phrases or to better scoring. <ref type="bibr" target="#b22">Jeong et al. (2010)</ref> predict surface word forms in con- text, similarly to our word BNN, and integrate the scores into the SMT system. Unlike us, they rely on linguistic feature-rich log-linear models to do that. <ref type="bibr" target="#b16">Gimpel and Smith (2008)</ref> propose a similar approach to directly predict phrases in context, in- stead of words. All those approaches employed features that capture the global structure of source sentences, like dependency relations. By contrast, our mod- els access only local context in the source sen- tence but they achieve accuracy gains comparably to models that also use global sentence structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have proposed a general approach to predict word translations in context using bilingual neu- ral network architectures. Unlike previous NN ap- proaches, we model word, stem and suffix dis- tributions in the target language given context in the source language. Instead of relying on man- ually engineered features, our models automati- cally learn abstract word representations and fea- tures that are relevant for the modeled task directly from word-aligned parallel data. Our preliminary results with LBL and ConvNet architectures sug- gest that potential improvement may be achieved by factorizing target representations or by dynam- ically modeling source context size. Evaluated on three morphologically rich languages, our ap- proach achieves considerable gains in word, stem and suffix accuracy over a context-independent maximum-likelihood baseline. Finally, we have shown that the proposed BNN models can be tightly integrated into a phrase-based SMT sys- tem, resulting in small but statistically significant BLEU improvement over a competitive, large- scale English-Russian baseline.</p><p>Our analysis shows that the number of correct target words occurring in highly scored phrase translation candidates increases after integrating the morphological BNNs. However, only few of these end up in the 1-best translation output. Fu- ture work will investigate the benefits of coupling our BNN models with target language models that also exploit abstract word representations, such as <ref type="bibr" target="#b6">Botha and Blunsom (2014)</ref> and <ref type="bibr" target="#b0">Auli et al. (2013)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Feed-forward BNN architectures for predicting target translations: (a) word model (similar to stem model), and (b) suffix model with an additional vector representation r σ for target stems σ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Convolutional neural network model. This model (Figure 3 )</head><label>3</label><figDesc>computes the predictive repre- sentationˆqsentationˆ sentationˆq by applying a sequence of 2k convo- lutional layers {L 1 , . . . , L 2k }. The source context c s i is represented as a matrix m cs i ∈ R d×(2k+1) :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9</head><label></label><figDesc>Annotation included in the CzEng 1.0 corpus release.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of different word segmentation techniques (U: unsupervised, S: supervised, R: rule-based stemmer) on stem and suffix prediction accuracy. The dark part of each bar stands for top1, the light one for top-3 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BNN prediction accuracy (top-1/top-3) 
compared to a context-independent maximum-
likelihood baseline. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Accuracies at top-1/top-3 (%) of stem and 
suffix models. +do indicates dropout instead of L 2 
regularizer. FFNN is our main architecture. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Effect of our BNN models on English-
Russian translation quality (BLEU[%]). 

see that it is beneficial to add a suffix-based lan-
guage model to the baseline system. The biggest 
improvement is obtained by combining the suffix-
based language model and our BNN approach, 
yielding 0.7 BLEU over a competitive, state-of-
the-art baseline, of which 0.4 BLEU are due to our 
BNNs. Finally, one can see that the BNNs mod-
eling stems and suffixes separately perform bet-
ter than a BNN directly predicting fully inflected 
forms. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>). One possible explanation is that the new, more accurate translation distribu- tions are overruled by other SMT model scores,</figDesc><table>Token recall (wmt12): 
Baseline +BNN 
reference/MT-search-space [top-1] 57.6% 59.0% 
reference/MT-search-space [top-3] 70.7% 70.9% 
reference/MT-search-space [top-30] 86.0% 85.0% 
reference/MT-search-space [MRR] 0.655 
0.662 
reference/MT-output 
50.0% 50.7% 
stem-only reference/MT-output 
12.3% 11.5% 
of which reachable 
11.2% 10.3% 

</table></figure>

			<note place="foot" n="1"> Training data and SMT setup are described in Section 6.</note>

			<note place="foot" n="2"> This corresponds to the top 30 phrases sorted by weighted phrase, lexical and LM probabilities, for each source span. Koehn (2004) and our own experience suggest that using more phrases has little or no impact on MT quality. 3 Word segmentation for this analysis is performed by the Russian Snowball stemmer, see also Section 5.3.</note>

			<note place="foot" n="4"> Note that our suffix generation function Mσ is restricted to the forms observed in the target monolingual data, but not to those aligned to a source word s, which opens the possibility of generating inflected forms that are missing from the translation models. We leave this possibility to future work.</note>

			<note place="foot" n="6"> We find that using rectified linear units gives better results than sigmoid and tanh. 7 Each lexicon is pruned with minimum word frequency 5, minimum source-target word pair frequency 2, minimum log odds ratio 10.</note>

			<note place="foot" n="8"> We use the C++ implementation available at http:// groups.csail.mit.edu/rbg/code/morphsyn</note>

			<note place="foot" n="10"> http://snowball.tartarus.org/ algorithms/russian/stemmer.html</note>

			<note place="foot" n="11"> Chahuneau et al. (2013) report an average accuracy of 63.1% for the prediction of A, V, N, M suffixes. When we train our model on the same dataset (news-commentary) we obtain a comparable result (64.7% vs 63.1%).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213 and 612.001.218. We would like to thank Ekaterina Garmash for helping with the error analysis of the English-Russian translations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Classbased language modeling for translating into morphologically rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1918" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdeněk</forename><surname>Žabokrtský</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Majliš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Maršík</surname></persName>
		</author>
		<editor>Michal Novák, Martin Popel, and Aleš Tamchyna</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Proceedings of LREC2012, Istanbul, Turkey, May. ELRA, European Language Resources Association</title>
		<meeting>LREC2012, Istanbul, Turkey, May. ELRA, European Language Resources Association</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Findings of the 2013 Workshop on Statistical Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compositional Morphology for Word Representations and Language Modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning, Beijing</title>
		<meeting>the 31st International Conference on Machine Learning, Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Findings of the 2012 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="10" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Translating into morphologically rich languages with synthetic phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1677" to="1687" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On hierarchical re-ordering and permutation parsing for phrase-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International Conference on Machine Learning</title>
		<meeting>the 25th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptation data selection using neural language models: Experiments in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="678" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model with minimal translation units, but decode with phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple and effective hierarchical phrase reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP &apos;08: Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting><address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="699" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rich sourceside context for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation<address><addrLine>Columbus, Ohio, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A class-based agreement model for generating accurately inflected translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics, ACL &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrating sourcelanguage context into phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejwanul</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antal</forename><surname>Sudip Kumar Naskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Way</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="285" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminative lexicon model for complex morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Ninth Conference of the Association for Machine Translation in the Americas</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Translate, predict or generate: Modeling rich morphology in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 16th Conference of the European Association for Machine Translation (EAMT)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="127" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pharaoh: A beam search decoder for phrase-based statistical machine translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<editor>Robert E. Frederking and</editor>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">Proceedings of the 6th Conference of the Association for Machine Translations in the Americas (AMTA 2004)</title>
		<editor>Kathryn B. Taylor</editor>
		<meeting>the 6th Conference of the Association for Machine Translations in the Americas (AMTA 2004)</meeting>
		<imprint>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured output layer neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Son</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Oparin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Proceedings of ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling syntactic context improves morphological segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Yoong Keok Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fifteenth Conference on Computational Natural Language Learning<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Extending statistical machine translation with discriminative and trigger-based lexicon models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
	<note>EMNLP &apos;09. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating complex morphology for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einat</forename><surname>Minkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="128" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A comparative investigation of morphological language modeling for the languages of the European Union</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="386" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Computer Intensive Methods for Testing Hypotheses. An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Noreen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>WileyInterscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Do we need phrases? challenging the conventional wisdom in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting>the Human Language Technology Conference of the NAACL, Main Conference<address><addrLine>New York City, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On some pitfalls in automatic evaluation and significance testing for MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Maxwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Continuous space language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dechelotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Luc</forename><surname>Gauvain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Conference</title>
		<meeting>the COLING/ACL 2006 Conference<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">European Language Resources Association (ELRA)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Kopotev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaz</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagmar</forename><surname>Divjak</surname></persName>
		</author>
		<ptr target="http://www.lrec-conf.org/proceedings/lrec2008/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC&apos;08)<address><addrLine>Marrakech, Morocco</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Designing and evaluating a russian tagset</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Continuous space translation models with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hai Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The best of two worlds: Cooperation of statistical and rule-based taggers for czech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drahomíra</forename><surname>Spoustová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Votrubec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Krbec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Květoň</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Balto-Slavonic Natural Language Processing</title>
		<meeting>the Workshop on Balto-Slavonic Natural Language Processing<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Applying morphology generation models to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achim</forename><surname>Ruopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
