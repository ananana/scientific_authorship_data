<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reducing Dimensions of Tensors in Type-Driven Distributional Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Fˇagˇarˇas¸an</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fˇagˇ</forename><surname>Fˇagˇarˇ</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fˇagˇarˇas</forename><surname>Fˇagˇarˇas¸an</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Computer Laboratory University of Cambridge Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reducing Dimensions of Tensors in Type-Driven Distributional Semantics</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1036" to="1046"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Compositional distributional semantics is a subfield of Computational Linguistics which investigates methods for representing the meanings of phrases and sentences. In this paper, we explore implementations of a framework based on Combinatory Categorial Grammar (CCG), in which words with certain grammatical types have meanings represented by multi-linear maps (i.e. multi-dimensional arrays, or tensors). An obstacle to full implementation of the framework is the size of these tensors. We examine the performance of lower dimensional approximations of transitive verb tensors on a sentence plausi-bility/selectional preference task. We find that the matrices perform as well as, and sometimes even better than, full tensors, allowing a reduction in the number of parameters needed to model the framework.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An emerging subfield of computational linguis- tics is concerned with learning compositional dis- tributional representations of meaning ( <ref type="bibr" target="#b23">Mitchell and Lapata, 2008;</ref><ref type="bibr" target="#b2">Baroni and Zamparelli, 2010;</ref><ref type="bibr" target="#b10">Coecke et al., 2010;</ref><ref type="bibr" target="#b15">Grefenstette and Sadrzadeh, 2011;</ref><ref type="bibr" target="#b9">Clarke, 2012;</ref><ref type="bibr" target="#b29">Socher et al., 2012;</ref><ref type="bibr" target="#b8">Clark, 2013)</ref>. The advantage of such representations lies in their potential to combine the benefits of dis- tributional approachs to word meaning <ref type="bibr" target="#b27">(Schütze, 1998;</ref><ref type="bibr" target="#b33">Turney and Pantel, 2010</ref>) with the more tra- ditional compositional methods from formal se- mantics ( <ref type="bibr" target="#b12">Dowty et al., 1981)</ref>. Distributional repre- sentations have the properties of robustness, learn- ability from data, ease of handling ambiguity, and the ability to represent gradations of mean- ing; whereas compositional models handle the un- bounded nature of natural language, as well as providing established accounts of logical words, quantification, and inference.</p><p>One promising approach which attempts to combine elements of compositional and distribu- tional semantics is by <ref type="bibr" target="#b10">Coecke et al. (2010)</ref>. The underlying idea is to take the type-driven approach from formal semantics -in particular the idea that the meanings of complex grammatical types should be represented as functions -and ap- ply it to distributional representations. Since the mathematics of distributional semantics is pro- vided by linear algebra, a natural set of functions to consider is the set of linear maps. <ref type="bibr">Coecke et al.</ref> recognize that there is a natural correspon- dence from complex grammatical types to tensors (multi-linear maps), so that the meaning of an ad- jective, for example, is represented by a matrix (a 2nd-order tensor) <ref type="bibr">1</ref> and the meaning of a transitive verb is represented by a 3rd-order tensor.</p><p>Coecke et al. use the grammar of pregroups as the syntactic machinery to construct distribu- tional meaning representations, since both pre- groups and vector spaces can be seen as exam- ples of the same abstract structure, which leads to a particularly clean mathematical description of the compositional process. However, the approach applies more generally, for example to other forms of categorial grammar, such as Combinatory Cate- gorial Grammar <ref type="bibr" target="#b31">(Steedman, 2000;</ref><ref type="bibr" target="#b21">Maillard et al., 2014)</ref>, and also to phrase-structure grammars in a way that a formal linguist would recognize ( <ref type="bibr" target="#b3">Baroni et al., 2014</ref>). Clark (2013) provides a descrip- tion of the tensor-based framework aimed more at computational linguists, relying only on the math- ematics of multi-linear algebra rather than the cat- egory theory used in <ref type="bibr" target="#b10">Coecke et al. (2010)</ref>. Sec- tion 2 repeats some of this description.</p><p>A major open question associated with the tensor-based semantic framework is how to learn the tensors representing the meanings of words with complex types, such as verbs and adjec- tives. The framework is essentially a composi- tional framework, providing a recipe for how to combine distributional representations, but leav- ing open what the underlying vector spaces are and how they can be acquired. One significant chal- lenge is an engineering one: in a wide-coverage grammar, which is able to handle naturally occur- ring text, there will be a) a large lexicon with many word-category pairs requiring tensor representa- tions; and b) many higher-order tensors with large numbers of parameters which need to be learned. In this paper we take a first step towards learning such representations, by learning tensors for tran- sitive verbs.</p><p>One feature of the tensor-based framework is that it allows the meanings of words and phrases with different basic types, for example nouns and sentences, to live in different vector spaces. This means that the sentence space is task specific, and must be defined in advance. For example, to calcu- late sentence similarity, we would have to learn a vector space where distances between vectors rep- resenting the meanings of sentences reflect simi- larity scores assigned by human annotators.</p><p>In this paper we describe an initial investi- gation into the learning of word meanings with complex syntactic types, together with a simple sentence space. The space we consider is the "plausibility space" described by <ref type="bibr" target="#b8">Clark (2013)</ref>, to- gether with sentences of the form subject-verb- object. This space is defined to distinguish se- mantically plausible sentences (e.g. Animals eat plants) from implausible ones (e.g. Animals eat planets). Plausibility can be either represented as a single continuous variable between 0 and 1, or as a two-dimensional probability distribution over the classes plausible () and implausible (⊥). Whether we consider a one-or two-dimensional sentence space depends on the architecture of the logistic regression classifier that is used to learn the verb (Section 3).</p><p>We begin with this simple plausibility sentence space to determine if, in fact, the tensor-based rep- resentation can be learned to a sufficiently useful degree. Other simple sentence spaces which can perhaps be represented using one or two variables include a "sentence space" for the sentiment anal- ysis task <ref type="bibr" target="#b30">(Socher et al., 2013)</ref>, where one variable represents positive sentiment and the other nega- tive. We also expect that the insights gained from research on this task can be applied to more com- plex sentence spaces, for example a semantic simi- larity space which will require more than two vari- ables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Syntactic Types to Tensors</head><p>The syntactic type of a transitive verb in English is (S \NP )/NP (using notation from Steedman (2000)), meaning that a transitive verb is a func- tion which takes an NP argument to the right, an NP argument to the left, and results in a sentence S . Such categories with slashes are complex cate- gories; S and NP are basic or atomic categories. Interpreting such categories under the Coecke et al. framework is straightforward. First, for each atomic category there is a corresponding vector space; in this case the sentence space S and the noun space N. <ref type="bibr">2</ref> Hence the meaning of a noun or noun phrase, for example people, will be a vector in the noun space:</p><p>−−−→ people ∈ N. In order to obtain the meaning of a transitive verb, each slash is re- placed with a tensor product operator, so that the meaning of eat, for example, is a 3rd-order tensor: eat ∈ S ⊗ N ⊗ N. Just as in the syntactic case, the meaning of a transitive verb is a function (a multi-linear map) which takes two noun vectors as arguments and returns a sentence vector.</p><p>Meanings combine using tensor contraction, which can be thought of as a multi-linear gen- eralisation of matrix multiplication <ref type="bibr" target="#b17">(Grefenstette, 2013)</ref>. Consider first the adjective-noun case, for example black cat. The syntactic type of black is N /N ; hence its meaning is a 2nd-order tensor (matrix): black ∈ N ⊗ N. In the syntax, N /N combines with N using the rule of forward appli- cation (N /N N ⇒ N ), which is an instance of function application. Function application is also used in the tensor-based semantics, which, for a matrix and vector argument, corresponds to ma- trix multiplication. <ref type="figure">Figure 1</ref> shows how the syntactic types com- bine with a transitive verb, and the corresponding tensor-based semantic types. Note that, after the verb has combined with its object NP , the type of the verb phrase is S \NP , with a correspond- ing meaning tensor (matrix) in S ⊗ N. This ma- trix then combines with the subject vector, through  <ref type="figure">Figure 1</ref>: Syntactic reduction and tensor-based se- mantic types for a transitive verb sentence matrix multiplication, to give a sentence vector. In practice, using for example the wide- coverage grammar from <ref type="bibr">CCGbank (Hockenmaier and Steedman, 2007</ref>), there will be many types with more than 3 slashes, with corresponding higher-order tensors.</p><formula xml:id="formula_0">NP (S \NP )/NP NP N S ⊗ N ⊗ N N &gt; S \NP S ⊗ N &lt; S S</formula><p>For example, a com- mon category for a preposition is the follow- ing: ((S \NP )\(S \NP ))/NP , which would be assigned to WITH in eat WITH a fork. (The way to read the syntactic type is as follows: with re- quires an NP argument to the right -a fork in this example -and then a verb phrase to the left -eat with type S \NP -resulting in a verb phrase S \NP .) The corresponding meaning ten- sor lives in the tensor space S ⊗ N ⊗ S ⊗ N ⊗ N, i.e. a 5th-order tensor. Categories with even more slashes are not uncommon, for example ((N /N )/(N /N ))/((N /N )/(N /N )). Clearly learning parameters for such tensors is highly challenging, and it is likely that lower dimensional approximations will be required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this paper we compare five different methods for modelling the type-driven semantic represen- tation of subject-verb-object sentences. The ten- sor is a function that encodes the meaning of a verb. It takes two vectors from the K-dimensional noun space as input, and produces a representa- tion of the sentence in the S-dimensional sentence space. In this paper, we consider a plausibility space where S is either a single variable or a two- dimensional space over two classes: plausible () and implausible (⊥).</p><p>The first method (Tensor) follows Krishna- murthy and Mitchell (2013) by learning a tensor as parameters in a softmax classifier. We introduce three related methods (2Mat, SKMat, KKMat), all of which model the verb as a matrix or a pair of matrices ( <ref type="figure" target="#fig_1">Figure 2</ref>). <ref type="table" target="#tab_0">Table 1</ref> gives the number of parameters for each method. Tensor, 2Mat, and SKMat all have a two-dimensional S space, while KKMat produces a scalar value. In all of these learning-based methods the derivatives were ob- tained via the chain rule with respect to each set of parameters and gradient descent was performed using the Adagrad algorithm <ref type="bibr" target="#b13">(Duchi et al., 2011</ref>). We also reimplement a distributional method (DMat), which was previously used in SVO experiments with the type-driven framework ( <ref type="bibr" target="#b15">Grefenstette and Sadrzadeh, 2011</ref>). While the other methods are trained as plausibility classi- fiers, in DMat we estimate the class boundary from cosine similarity via training data (see expla- nation below).</p><formula xml:id="formula_1">Tensor 2Mat SKMat KKMat DMat V 2K 2 4K 2K K 2 K 2 Θ 4 8 4 0 0</formula><p>Tensor If subject (n s ) and object (n o ) nouns are K-dimensional vectors and the plausibility vec- tor is S-dimensional with S = 2, we can learn the values of the K × K × S tensor represent- ing the verb as parameters (V) of a regression al- gorithm. To represent this space as a distribution over two classes (,⊥) we apply a sigmoid func- tion (σ) to restrict the output to the <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> range and the softmax activation function (g) to balance the class probabilities. The full parameter set which needs to be optimised for is B = {V, Θ}, where Θ = {θ , θ ⊥ } are the softmax parameters for the two classes. For each verb we optimise the KL-divergence L between the training labels t i and classifier predictions using the following reg- ularised objective:</p><formula xml:id="formula_2">O(B) = N i=1 L t i , g σ hV n i s , n i o , Θ + λ 2 ||B|| 2 (1)</formula><p>where n i s and n i o are the subject and object of the training instance i ∈ N , and</p><formula xml:id="formula_3">h V n i s , n i o = (n i s )V(n i o ) T describes tensor contraction. The function h V is described diagrammatically in Fig- ure 2-(a)</formula><p>, where the verb tensor parameters are drawn as a cube with the subject and object noun vectors as operands on either side. The output is a two-dimensional vector which is then trans- formed using the sigmoid and softmax functions. 2Mat An alternative approach is to decouple the interaction between the object and subject by learning a pair of S × K (S = 2) matrices (V s , V o ) for each of the input noun vectors (one ma- trix for the subject slot of the verb and one for the object slot). The resulting S-vectors are concate- nated, after the subject and object nouns have been combined with their matrices, and combined with the softmax component to produce the output dis- tribution. Therefore the objective function is the same as in Equation 1, but h V is defined as:</p><formula xml:id="formula_4">K S K K K S (a) K K S K S K 2*S x S S x (b) K K S K K 0 0 0 0 0 0 K S S x x (c) K K K K x x (d)</formula><formula xml:id="formula_5">h V n i s , n i o = (n i s )V T s || V o (n i o ) T T</formula><p>where || represents vector concatenation. The in- tention is to test whether we can learn the verb without directly multiplying subject and object features, n i s and n j o . The function h V is shown in <ref type="figure" target="#fig_1">Figure 2</ref>-(b), where the verb tensor parameters are drawn as two 2 × K matrices, one of which inter- acts with the subject and the other with the object noun vector. The output is a four-dimensional vec- tor whose values are then restricted to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> using the sigmoid function and then transformed into a two-dimensional distribution over the classes us- ing the softmax function.</p><p>SKMat A third option for generating a sentence vector with S = 2 dimensions is to consider the verb as an S × K matrix. If we transform the ob- ject vector into a K × K matrix with the noun on the diagonal and zeroes elsewhere, we can com- bine the verb and object to produce a new S × K matrix, which is encoding the meaning of the verb phrase. We can then complete the sentence re- duction by multiplying the subject vector with this verb phrase vector to produce an S-dimensional sentence vector. Formally, we define SKMat as:</p><formula xml:id="formula_6">h V n i s , n i o = n i s Vdiag(n i o ) T</formula><p>and use it in Equation 1. The function h V is described in <ref type="figure" target="#fig_1">Figure 2-(c)</ref>, where the verb ten- sor parameters are drawn as a matrix, the sub- ject as a vector, and the object as a diagonal ma-trix. The graphic demonstrates the two-step com- bination and the intermediate S × K verb phrase matrix, as well as the the noun vector product that results in a two-dimensional vector which is then transformed using the sigmoid and softmax functions. Whilst the tensor method captures the interactions between all pairs of context features (n si · n oj ), SKMat only captures the interactions between matching features (n si · n oi ).</p><p>KKMat Given a two-class problem, such as plausibility classification, the softmax implemen- tation is overparameterised because the class membership can be estimated with a single vari- able. To produce a scalar output, we can learn the parameters for a single K × K matrix (V) using standard logistic regression with the mean squared error cost function:</p><formula xml:id="formula_7">O(V) = − 1 m N i=1 t i log hV n i s , n i o + (1 − t i ) log hV n i s , n i o where h V n i s , n i o = (n i s )V(n i o ) T and the objec- tive is regularised: O(V) + λ 2 ||V|| 2 .</formula><p>This function is shown in <ref type="figure" target="#fig_1">Figure 2-(d)</ref>, where the verb parame- ters are shown as a matrix, while the subject and object are vectors. The output is a single scalar, which is then transformed with the sigmoid func- tion. Values over 0.5 are considered plausible.</p><p>DMat The final method produces a scalar as in KKMat, but is distributional and based on corpus counts rather than regression-based. Grefenstette and Sadrzadeh (2011) introduced a corpus-based approach for generating a K × K matrix for each verb from an average of Kronecker products of the subject and object vectors from the positively la- belled subset of the training data. The intuition is that, for example, the matrix for eat may have a high value for the contextual topic pair describing animate subjects and edible objects. To determine the plausibility of a new subject-object pair for a particular verb, we calculate the Kronecker prod- uct of the subject and object noun vectors for this pair, and compare the resulting matrix with the av- erage verb matrix using cosine similarity.</p><p>For label prediction, we calculate the similar- ity between each of the training data pairs and the learned average matrix. Unlike for KKmat, the class cutoff is estimated at the break-even point of the receiver operator characteristic (ROC) gen- erated by comparing the training labels with this cosine similarity value. The break-even point is when the true positive rate is equal to the false pos- itive rate. In practice it would be more accurate to estimate the cutoff on a validation dataset, but some of the verbs have so few training instances that this was not possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In order to examine the quality of learning we run several experiments where we compare the differ- ent methods. In these experiments we consider the DMat method as the baseline. Some of the experiments employ cross-validation, in particular five repetitions of 2-fold cross validation (5x2cv), which has been shown to be statistically more ro- bust than the traditional 10-fold cross validation <ref type="bibr" target="#b0">(Alpaydin, 1999;</ref><ref type="bibr">Ulas¸etUlas¸Ulas¸et al., 2012</ref>). The results of 5x2cv experiments can be compared using the reg- ular paired t-test, but the specially designed 5x2cv F-test has been proven to produce fewer statistical errors <ref type="bibr">(Ulas¸etUlas¸Ulas¸et al., 2012</ref>).</p><p>The performance was evaluated using the area under the ROC (AUC) and the F 1 measure (based on precision and recall over the plausible class). The AUC evaluates whether a method is ranking positive examples above negative ones, regardless of the class cutoff value. F 1 shows how accurately a method assigns the correct class label. Another way to interpret the results is to consider the AUC as the measure of the quality of the parameters in the verb matrix or tensor, while the F-score indi- cates how well the softmax, the sigmoid, and the DMat cutoff algorithm are estimating class partic- ipation.</p><p>Ex-1. In the first experiment, we compare the different transitive verb representations by running 5x2cv experiments on ten verbs chosen to cover a range of concreteness and frequency values (Sec- tion 4.2).</p><p>Ex-2. In the initial experiments we found that some models had low performance, so we applied the column normalisation technique, which is of- ten used with regression learning to standardise the numerical range of features:</p><formula xml:id="formula_8">x := x − min( x) max( x) − min( x)<label>(2)</label></formula><p>This preserves the relative values of features be- tween training samples, while moving the values to the [0,1] range.</p><p>Ex-3. There are varying numbers of training ex- amples for each of the verbs, so we repeated the 5x2cv with datasets of 52 training points for each verb, since this is the size of the smallest dataset of the verb CENSOR. The points were randomly sam- pled from the datasets used in the first experiment. Finally, the four verbs with the largest datasets were used to examine how the performance of the methods changes as the amount of training data increases. The 4,000 training samples were ran- domised and half were used for testing. We sam- pled between 10 and 1000 training triples from the other half <ref type="figure" target="#fig_4">(Figure 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Noun vectors</head><p>Distributional semantic models (Turney and Pan- tel, 2010) encode word meaning in a vector for- mat by counting co-occurrences with other words within a specified context window. We con- structed the vectors from the October 2013 dump of Wikipedia articles, which was tokenised us- ing the Stanford NLP tools 3 , lemmatised with the Morpha lemmatiser ( <ref type="bibr" target="#b22">Minnen et al., 2001</ref>), and parsed with the C&amp;C parser <ref type="bibr" target="#b6">(Clark and Curran, 2007)</ref>. In this paper we use sentence boundaries to define context windows and the top 10,000 most frequent lemmatised words in the whole corpus (excluding stopwords) as context words. The raw co-occurrence counts are re-weighted using the standard tTest weighting scheme <ref type="bibr" target="#b11">(Curran, 2004)</ref>, where f w i c j is the number of times target noun w i occurs with context word c j :</p><formula xml:id="formula_9">tT est( w i , c j ) = p(w i , c j ) − p(w i )p(c j ) p(w i )p(c j )<label>(3)</label></formula><p>where</p><formula xml:id="formula_10">p(w i ) = j fw i c j k l fw k c l , p(c j ) = i fw i c j k l fw k c l , and p(w i , c j ) = fw i c j k l fw k c l .</formula><p>Using all 10,000 context words would result in a large number of parameters for each verb ten- sor, and so we apply singular value decomposition (SVD) <ref type="bibr" target="#b33">(Turney and Pantel, 2010</ref>) with 40 latent dimensions to the target-context word matrix. We use context selection (with N = 140) and row normalisation as described in <ref type="bibr" target="#b24">Polajnar and Clark (2014)</ref> to markedly improve the performance of SVD on smaller dimensions (K) and enable us to train the verb tensors using very low-dimensional <ref type="bibr">3</ref>   noun vectors. Performance of the noun vectors was measured on standard word similarity datasets and the results were comparable to those reported by <ref type="bibr" target="#b24">Polajnar and Clark (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training data</head><p>In order to generate training data we made use of two large corpora: the Google Syntactic N- grams (GSN) <ref type="bibr" target="#b14">(Goldberg and Orwant, 2013</ref>) and the Wikipedia October 2013 dump. We first chose ten transitive verbs with different concreteness scores ( <ref type="bibr" target="#b4">Brysbaert et al., 2013</ref>) and frequencies, in order to obtain a variety of verb types. Then the positive (plausible) SVO examples were extracted from the GSN corpus. More precisely, we col- lected all distinct syntactic trigrams of the form nsubj ROOT dobj, where the root of the phrase was one of our target verbs. We lemmatised the words using the NLTK 4 lemmatiser and filtered these ex- amples to retain only the ones that contain nouns that also occur in Wikipedia, obtaining the counts reported in <ref type="table" target="#tab_2">Table 2</ref>. For every positive training example, we con- structed a negative (implausible) one by replac- ing both the subject and the object with a con- founder, using a standard technique from the se- lectional preference literature (Chambers and Ju- rafsky, 2010). A confounder was generated by choosing a random noun from the same frequency bucket as the original noun. <ref type="bibr">5</ref> Frequency buckets of size 10 were constructed by collecting noun fre- quency counts from the Wikipedia corpus. For ex-  <ref type="table">Table 3</ref>: The best AUC and F 1 results for all the verbs, where † denotes statistical significance compared to DMat and ‡ denotes significance compared to Tensor according to the 5x2cv F-test with p &lt; 0.05.</p><note type="other">80.30 50.84 80.90 31.99 84.55 79.73 73.71 81.10 54.09 82.52 91.24 71.24 ‡ 87.46 76.67 ‡ 92.22 78.57 47.62 80.65 39.50 78.90 80.30 55.79 81.03 46.69 81.79</note><p>ample, for the plausible triple animal EAT plant, we generate the implausible triple mountain EAT product. Some verbs were well represented in the corpus, so we used up to the top 2,000 most fre- quent triples for training. </p><formula xml:id="formula_11">0 0.5 1 AUC A P P L Y C E N S O R C O M B D E P O S E E A T I D E A L I Z E I N C U B A T E J U S T I F Y R E D U C E W I P E Tensor Tensor* SKMat SKMat* −0.2 0 0.2 0.4 0.6 0.8 1 1.2 F−Score A P P L Y C E N S O R C O M B D E P O S E E A T I D E A L I Z E I N C U B A T E J U S T I F Y R E D U C E W I P E Tensor Tensor* SKMat SKMat*</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>The results from Ex-1 are summarised in Ta- ble 3. We can see that linear regression can lead to models that are able to distinguish between plausible and implausible SVO triples. The Ten- sor method outperforms DMat, which was pre- viously shown to produce reasonable verb repre- sentations in related experiments ( <ref type="bibr" target="#b15">Grefenstette and Sadrzadeh, 2011</ref>). 2Mat and KKMat, in turn, outperform Tensor demonstrating that it is pos- sible to learn lower dimensional approximations of the tensor-based framework. 2Mat is an appro- priate approximation for functions with two inputs and a sentence space of any dimensionality, while KKMat is only appropriate for a single valued sentence space, such as the plausibility or senti- ment space. Due to method variance and dataset size there are very few AUC results that are sig- nificantly better than DMat and even fewer that outperform Tensor. All methods perform poorly on the verb IDEALIZE, probably because it has the lowest concreteness value and is in one of the smallest datasets. This verb is also particularly dif- ficult because it does not select strongly for either its subject or object, and so some of the pseudo- negative examples are in fact somewhat plausible (e.g. town IDEALIZE authority or child IDEALIZE racehorse). In general, this would indicate that more concrete verbs are easier to learn, as they have a clearer pattern of preferred property types, but there is no distinct correlation.</p><p>The results of the normalisation experiments (Ex-2) are shown in <ref type="table" target="#tab_5">Table 4</ref>. We can see that the SKMat method, which performed poorly in Ex- 1 notably improves with normalisation. Tensor AUC scores also improve through normalisation, but the F-scores decrease. The rest of the methods, and in particular DMat are negatively affected by column normalisation. The results from Ex-1 and Ex-2 for SKMat and Tensor are summarised in   When considering the size of the datasets <ref type="bibr">(Ex3)</ref>, it would seem from <ref type="table" target="#tab_7">Table 5</ref> that 2Mat is able to learn from less data than DMat or Tensor. While this may be true over a 5x2cv experiment on small data, <ref type="figure" target="#fig_4">Figure 4</ref> shows that this view may be overly simplistic and that different training examples can influence learning. Analysis of errors shows that the baseline method mostly generates false nega- tive errors (i.e. predicting implausible when the gold standard label is plausible). In contrast, Ten- sor produces almost equal numbers of false posi- tives and false negatives, but sometimes produces false negatives with low frequency nouns (e.g. bourgeoisie IDEALIZE work), presumably because there is not enough information in the noun vec- tor to decide on the correct class. It also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by train- ing with data where only one noun is confounded or by treating negative data as possibly positive (Lee and Liu, 2003).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>Current methods which derive distributed repre- sentations for phrases, for example the work of <ref type="bibr" target="#b29">Socher et al. (2012)</ref>, typically use only matrix rep- resentations, and also assume that words, phrases and sentences all live in the same vector space. The tensor-based semantic framework is more flexible, in that it allows different spaces for dif- ferent grammatical types, which results from it be-  ing tied more closely to a type-driven syntactic de- scription; however, this flexibility comes at a cost, since there are many more paramaters to learn. Various communities are beginning to recog- nize the additional power that tensor representa- tions can provide, through the capturing of interac- tions that are difficult to represent with vectors and matrices (see e.g. ( <ref type="bibr" target="#b25">Ranzato et al., 2010;</ref><ref type="bibr" target="#b32">Sutskever et al., 2009</ref>; Van de Cruys et al., 2012)). Hierar- chical recursive structures in language potentially represent a large number of such interactions -the obvious example for this paper being the interac- tion between a transitive verb's subject and object -and present a significant challenge for machine learning.</p><p>This paper is a practical extension of the work in <ref type="bibr" target="#b19">Krishnamurthy and Mitchell (2013)</ref>, which in- troduced learning of CCG-based function tensors with logistic regression on a compositional se- mantics task, but was implemented as a proof-of- concept with vectors of length 2 and on small, manually created datasets based on propositional logic examples. Here, we go beyond this by learn- ing tensors using corpus data and by deriving sev- eral different matrix representations for the verb in the subject-verb-object (SVO) sentence.</p><p>This work can also be thought of as applying neural network learning techniques to the clas- sic problem of selectional preference acquisition, since the design of the pseudo-disambiguation ex- periments is taken from the literature on selec- tional preferences <ref type="bibr" target="#b7">(Clark and Weir, 2002;</ref><ref type="bibr" target="#b5">Chambers and Jurafsky, 2010</ref>). We do not compare di- rectly with methods from this literature, e.g. those based on WordNet <ref type="bibr" target="#b26">(Resnik, 1996;</ref><ref type="bibr" target="#b7">Clark and Weir, 2002</ref>) or topic modelling techniques <ref type="bibr" target="#b28">(Seaghdha, 2010)</ref>, since our goal in this paper is not to ex- tend the state-of-the-art in that area, but rather to use selectional preference acquisition as a test bed for the tensor-based semantic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we introduced three dimensionally reduced representations of the transitive verb ten- sor defined in the type-driven framework for com- positional distributional semantics ( <ref type="bibr" target="#b10">Coecke et al., 2010)</ref>. In a comprehensive experiment on ten dif- ferent verbs we find no significant difference be- tween the full tensor representation and the re- duced representations. The SKMat and 2Mat rep- resentations have the lowest number of parame- ters and offer a promising avenue of research for more complex sentence structures and sentence spaces. KKMat and DMat also had high scores on some verbs, but these representations are appli- cable only in spaces where a single-value output is appropriate.</p><p>In experiments where we varied the amount of training data, we found that in general more con- crete verbs can learn from less data. Low con- creteness verbs require particular care with dataset design, since some of the seemingly random ex- amples can be plausible. This problem may be circumvented by using semi-supervised learning techniques.</p><p>We also found that simple numerical tech- niques, such as column normalisation, can markedly alter the values and quality of learning. On our data, column normalisation has a side- effect of removing the negative values that were introduced by the use of tTest weighting measure. The use of the PPMI weighting scheme and non- negative matrix factorisation (NMF) ( ; Van de Cruys, 2010) could lead to a similar effect, and should be investigated. Further numerical techniques for improving the estimation of the class decision boundary, and consequently the F-score, will also constitute future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrations of the h V function for the regression-based methods (a)-Tensor, (b)-2Mat, (c)SKMat, (d)-KKMat. The operation in (a) is tensor contraction, T denotes transpose, and × denotes matrix multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of column normalisation (*) on Tensor and SKMat. Top table shows AUC and the bottom F 1-score, while the error bars indicate standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. This figure also shows that AUC values have much lower variance, but that high variance in F-score leads to results that are not statistically significant. When considering the size of the datasets (Ex3), it would seem from Table 5 that 2Mat is able to learn from less data than DMat or Tensor. While this may be true over a 5x2cv experiment on small data, Figure 4 shows that this view may be overly simplistic and that different training examples can influence learning. Analysis of errors shows that the baseline method mostly generates false negative errors (i.e. predicting implausible when the gold standard label is plausible). In contrast, Tensor produces almost equal numbers of false positives and false negatives, but sometimes produces false negatives with low frequency nouns (e.g. bourgeoisie IDEALIZE work), presumably because there is not enough information in the noun vector to decide on the correct class. It also produces some false positive errors when either of the nouns is plausible (but the triple is implausible), which would suggest results may be improved by training with data where only one noun is confounded or by treating negative data as possibly positive (Lee and Liu, 2003).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of DMat, Tensor, and 2Mat methods as the number of training instances increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Number of parameters per method.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The 10 chosen verbs together with their 
concreteness scores. The number of positive SVO 
examples was capped at 2000. Frequency is the 
frequency of the verb in the GSN corpus. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The best AUC and F 1 results for all the verbs with normalised vectors, where  † denotes statistical 
significance compared to DMat and  ‡ denotes significance compared to Tensor according to the 5x2cv 
F-test with p &lt; 0.05. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results show average of 5x2cv AUC on 
small data (26 positive + 26 negative per verb). 
None of the results are significant. 

</table></figure>

			<note place="foot" n="1"> This same insight lies behind the work of Baroni and Zamparelli (2010).</note>

			<note place="foot" n="2"> In practice, for example using the CCG parser of Clark and Curran (2007), there will be additional atomic categories, such as PP , but not many more.</note>

			<note place="foot" n="4"> http://nltk.org/ 5 Note that the random selection of the confounder could result in a plausible negative example by chance, but manual inspection of a subset of the data suggests this happens infrequently for those verbs which select strongly for their arguments, but more often for those verbs that don&apos;t.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combined 5x2 CV F-test for comparing supervised classification learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ethem Alpaydin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1885" to="1892" />
			<date type="published" when="1999-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Matlab tensor toolbox version 2.5. Available online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP-10)</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1183" to="1193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Frege in space: A program for compositional distributional semantics. Linguistic Issues in Language Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="5" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">Beth</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<title level="m">Concreteness ratings for 40 thousand generally known English word lemmas. Behavior research methods</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving the use of pseudo-words for evaluating selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="552" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-based probability estimation using a semantic hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="206" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Type-driven syntax and semantics for composing meaning vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Quantum Physics and Linguistics: A Compositional, Diagrammatic Discourse</title>
		<editor>Chris Heunen, Mehrnoosh Sadrzadeh, and Edward Grefenstette</editor>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="359" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A context-theoretic framework for compositionality in distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daoud</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="71" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mathematical foundations for a compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Analysis (Lambek Festschrift)</title>
		<editor>J. van Bentham, M. Moortgat, and W. Buszkowski, editors</editor>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="345" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">From Distributional to Semantic Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Dowty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
	<note>Introduction to Montague Semantics. Dordrecht</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A dataset of syntactic-ngrams over time from a very large corpus of English books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Orwant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Atlanta,Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Experimental support for a categorical compositional distributional model of meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1394" to="1404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-step regression learning for compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrnoosh</forename><surname>Sadrzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics</title>
		<meeting>the 10th International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IWCS 2013</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vector space semantic parsing: A framework for compositional vector space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning with positive and unlabeled examples using weighted logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Machine Learning (ICML)</title>
		<meeting>the Twentieth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A type-driven tensor-based semantics for CCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS)</title>
		<meeting>the EACL 2014 Type Theory and Natural Language Semantics Workshop (TTNLS)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Applied morphological processing of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Pearce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="223" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vector-based models of semantic composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08</title>
		<meeting>ACL-08<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="236" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving distributional semantic vectors through context selection and normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Conference of the European Chapter of the Association for Computational Linguistics, EACL&apos;14</title>
		<meeting><address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Factored 3-way restricted boltzmann machines for modeling natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS)<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selectional constraints: An information-theoretic model and its computational realization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="127" to="159" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic word sense discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="124" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent variable models of selectional preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diarmuid O Seaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Jeju, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Seattle</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modelling relational data using bayesian clustered tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS 2009)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS 2009)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cost-conscious comparison of supervised learning algorithms over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aydın</forename><surname>Ulas¸</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olcay</forename><surname>Ulas¸</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethem</forename><surname>Taner Yıldız</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alpaydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1772" to="1781" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-way tensor factorization for unsupervised lexical acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A non-negative tensor factorization model for selectional preference induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="437" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
