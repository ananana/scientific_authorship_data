<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Bastings</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ILCC</orgName>
								<orgName type="institution" key="instit2">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima&amp;apos;an</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">ILLC</orgName>
								<orgName type="institution" key="instit2">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1957" to="1967"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidi-rectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) is one of suc- cess stories of deep learning in natural language processing, with recent NMT systems outperform- ing traditional phrase-based approaches on many language pairs ( <ref type="bibr" target="#b33">Sennrich et al., 2016a</ref>). State-of- the-art NMT systems rely on sequential encoder- decoders ( <ref type="bibr" target="#b43">Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref> and lack any explicit modeling of syntax or any hierarchical structure of language. One poten- tial reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorpo- rating structured information in neural encoders, including RNNs. Despite some successes, tech- niques explored so far either incorporate syntactic information in NMT models in a relatively indi- rect way (e.g., multi-task learning ( <ref type="bibr" target="#b25">Luong et al., 2015a;</ref><ref type="bibr">Nadejde et al., 2017;</ref><ref type="bibr" target="#b11">Eriguchi et al., 2017;</ref><ref type="bibr" target="#b14">Hashimoto and Tsuruoka, 2017)</ref>) or may be too restrictive in modeling the interface between syn- tax and the translation task (e.g., learning repre- sentations of linguistic phrases ( <ref type="bibr" target="#b10">Eriguchi et al., 2016)</ref>). Our goal is to provide the encoder with access to rich syntactic information but let it de- cide which aspects of syntax are beneficial for MT, without placing rigid constraints on the in- teraction between syntax and the translation task. This goal is in line with claims that rigid syntac- tic constraints typically hurt MT ( <ref type="bibr" target="#b46">Zollmann and Venugopal, 2006;</ref><ref type="bibr" target="#b36">Smith and Eisner, 2006;</ref><ref type="bibr" target="#b3">Chiang, 2010)</ref>, and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.</p><p>Attention-based NMT systems ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b26">Luong et al., 2015b</ref>) represent source sen- tence words as latent-feature vectors in the en- coder and use these vectors when generating a translation. Our goal is to automatically incorpo- rate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation out- put. Since vectors correspond to words, it is natu- ral for us to use dependency syntax. Dependency trees (see <ref type="figure" target="#fig_0">Figure 1</ref>) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.</p><p>In order to produce syntax-aware feature representations of words, we exploit graph- convolutional networks (GCNs) ( <ref type="bibr" target="#b7">Duvenaud et al., 2015;</ref><ref type="bibr" target="#b6">Defferrard et al., 2016;</ref><ref type="bibr" target="#b21">Kearnes et al., 2016;</ref><ref type="bibr" target="#b23">Kipf and Welling, 2016)</ref>. GCNs can be regarded as computing a latent-feature representation of a node (i.e. a real-valued vector) based on its k- th order neighborhood (i.e. nodes at most k hops aways from the node) ( <ref type="bibr" target="#b13">Gilmer et al., 2017</ref>). They are generally simple and computationally inexpen- sive. We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, re- cently shown effective in the context of semantic role labeling <ref type="bibr" target="#b27">(Marcheggiani and Titov, 2017)</ref>. Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoder- decoder framework. As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effec- tive when used as layers on top of recurrent neu- ral network (RNN) or convolutional neural net- work (CNN) encoders <ref type="bibr" target="#b12">(Gehring et al., 2016)</ref>, en- riching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntac- tic phenomena (e.g., subject-verb agreement) rea- sonably well on their own, without explicit tree- bank supervision ( <ref type="bibr" target="#b24">Linzen et al., 2016;</ref><ref type="bibr" target="#b35">Shi et al., 2016)</ref>. Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for English- German and English-Czech, respectively.</p><p>In principle, GCNs are flexible enough to incor- porate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations ( <ref type="bibr" target="#b42">Surdeanu et al., 2008)</ref>, AMR semantic graphs ( <ref type="bibr" target="#b2">Banarescu et al., 2012</ref>) and co-reference chains). For ex- ample, unlike recursive neural networks <ref type="bibr" target="#b38">(Socher et al., 2013</ref>), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general inves- tigation for future work.</p><p>Our main contributions can be summarized as follows:</p><p>• we introduce a method for incorporating structure into NMT using syntactic GCNs;</p><p>• we show that GCNs can be used along with RNN and CNN encoders;</p><p>• we show that incorporating structure is ben- eficial for machine translation on English- Czech and English-German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Notation. We use x for vectors, x 1:t for a se- quence of t vectors, and X for matrices. The i-th value of vector x is denoted by x i . We use • for vector concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>In NMT <ref type="bibr" target="#b19">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b43">Sutskever et al., 2014;</ref><ref type="bibr" target="#b5">Cho et al., 2014b)</ref>, given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y 1:Ty |x 1:Tx ) of translat- ing a source sentence x 1:Tx (a sequence of T x words) into a target sentence y 1:Ty . NMT mod- els typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism. We will now briefly describe the components that we use in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Encoders</head><p>An encoder is a function that takes as input the source sentence and produces a representation en- coding its semantic content. We describe recur- rent, convolutional and bag-of-words encoders.</p><p>Recurrent. Recurrent neural networks (RNNs) <ref type="bibr" target="#b9">(Elman, 1990</ref>) model sequential data. They re- ceive one input vector at each time step and up- date their hidden state to summarize all inputs up to that point. Given an input sequence x 1:Tx = x 1 , x 2 , . . . , x Tx of word embeddings an RNN is defined recursively as follows:</p><formula xml:id="formula_0">RNN(x 1:t ) = f (x t , RNN(x 1:t−1 ))</formula><p>where f is a nonlinear function such as an LSTM <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997</ref>) or a GRU ( <ref type="bibr" target="#b5">Cho et al., 2014b</ref>). We will use the function RNN as an abstract mapping from an input sequence x 1:T to final hidden state RNN(x 1:Tx ), regardless of the used nonlinearity. To not only summarize the past of a word, but also its future, a bidirec- tional RNN <ref type="bibr" target="#b31">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b18">Irsoy and Cardie, 2014</ref>) is often used. A bidirectional RNN reads the input sentence in two directions and then concatenates the states for each time step:</p><formula xml:id="formula_1">BIRNN(x 1:Tx , t) = RNN F (x 1:t ) • RNN B (x Tx:t )</formula><p>where RNN F and RNN B are the forward and backward RNNs, respectively. For further details we refer to the encoder of <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>.</p><p>Convolutional. Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word <ref type="bibr" target="#b12">(Gehring et al., 2016)</ref>. One advantage of this ap- proach over RNNs is that it allows for fast parallel computation, while sacrificing non-local context. To remedy the loss of context, multiple CNN lay- ers can be stacked. Formally, given an input se- quence x 1:Tx , we define a CNN as follows:</p><formula xml:id="formula_2">CNN(x 1:Tx , t) = f (x t−−w/2 , .., x t , .., x t+w/2 )</formula><p>where f is a nonlinear function, typically a lin- ear transformation followed by ReLU, and w is the size of the window.</p><p>Bag-of-Words. In a bag-of-words (BoW) en- coder every word is simply represented by its word embedding. To give the decoder some sense of word position, position embeddings (PE) may be added. There are different strategies for defining position embeddings, and in this paper we choose to learn a vector for each absolute word position up to a certain maximum length. We then repre- sent the t-th word in a sequence as follows:</p><formula xml:id="formula_3">BOW(x 1:Tx , t) = x t + p t</formula><p>where x t is the word embedding and p t is the t-th position embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Decoder</head><p>A decoder produces the target sentence condi- tioned on the representation of the source sentence induced by the encoder. In <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> the decoder is implemented as an RNN condi- tioned on an additional input c i , the context vector, which is dynamically computed at each time step using an attention mechanism. The probability of a target word y i is now a function of the decoder RNN state, the previous target word embedding, and the context vector. The model is trained end-to-end for maximum log likelihood of the next target word given its context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Networks</head><p>We will now describe the Graph Convolutional Networks (GCNs) of <ref type="bibr" target="#b23">Kipf and Welling (2016)</ref>. For a comprehensive overview of alternative GCN architectures see <ref type="bibr" target="#b13">Gilmer et al. (2017)</ref>.</p><p>A GCN is a multilayer neural network that operates directly on a graph, encoding informa- tion about the neighborhood of a node as a real- valued vector. In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its imme- diate neighbors. When multiple GCN layers are stacked, information about larger neighborhoods gets integrated. For example, in the second layer, a node will receive information from its immediate neighbors, but this information already includes information from their respective neighbors. By choosing the number of GCN layers, we regulate the distance the information travels: with k lay- ers a node receives information from neighbors at most k hops away.</p><p>Formally, consider an undirected graph G = (V, E), where V is a set of n nodes, and E is a set of edges. Every node is assumed to be con- nected to itself, i.e. ∀v ∈ V : (v, v) ∈ E. Now, let X ∈ R d×n be a matrix containing all n nodes with their features, where d is the dimensionality of the feature vectors. In our case, X will contain word embeddings, but in general it can contain any kind of features. For a 1-layer GCN, the new node representations are computed as follows:</p><formula xml:id="formula_4">h v = ρ u∈N (v) W x u + b</formula><p>where W ∈ R d×d is a weight matrix and b ∈ R d a bias vector. 1 ρ is an activation function, e.g. a ReLU. N (v) is the set of neighbors of v, which we assume here to always include v itself. As stated before, to allow information to flow over multiple hops, we need to stack GCN layers. The recursive computation is as follows:</p><formula xml:id="formula_5">h (j+1) v = ρ u∈N (v) W (j) h (j) u + b (j)</formula><p>where j indexes the layer, and h <ref type="figure">Figure 2</ref>: A 2-layer syntactic GCN on top of a convolutional encoder. Loop connections are depicted with dashed edges, syntactic ones with solid (dependents to heads) and dotted (heads to dependents) edges. Gates and some labels are omitted for clarity.</p><formula xml:id="formula_6">(0) v = x v . W ( 0 ) d et W ( 0 ) n su b j W (0 ) do bj W ( 0 ) d et W ( 1 ) d et W ( 1 ) n su b j W (1 ) do bj W ( 1 ) d et *PAD* The monkey eats a banana *PAD* h (0) h (1) h (2) GCN CNN</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Syntactic GCNs</head><p>Marcheggiani and Titov (2017) generalize GCNs to operate on directed and labeled graphs. <ref type="bibr">2</ref> This makes it possible to use linguistic structures such as dependency trees, where directionality and edge labels play an important role. They also integrate edge-wise gates which let the model regulate con- tributions of individual dependency edges. We will briefly describe these modifications.</p><p>Directionality. In order to deal with direction- ality of edges, separate weight matrices are used for incoming and outgoing edges. We follow the convention that in dependency trees heads point to their dependents, and thus outgoing edges are used for head-to-dependent connections, and in- coming edges are used for dependent-to-head con- nections. Modifying the recursive computation for directionality, we arrive at:</p><formula xml:id="formula_7">h (j+1) v = ρ u∈N (v) W (j) dir(u,v) h (j) u + b (j) dir(u,v)</formula><p>where dir(u, v) selects the weight matrix associ- ated with the directionality of the edge connecting u and v (i.e. W IN for u-to-v, W OUT for v-to-u, and W LOOP for v-to-v). Note that self loops are modeled separately, so there are now three times as many parameters as in a non-directional GCN.</p><p>Labels. Making the GCN sensitive to labels is straightforward given the above modifications for directionality. Instead of using separate matrices for each direction, separate matrices are now de- fined for each direction and label combination:</p><formula xml:id="formula_8">h (j+1) v = ρ u∈N (v) W (j) lab(u,v) h (j) u + b (j) lab(u,v)</formula><p>where we incorporate the directionality of an edge directly in its label. Importantly, to prevent over-parametrization, only bias terms are made label-specific, in other words: W lab(u,v) = W dir(u,v) . The resulting syn- tactic GCN is illustrated in <ref type="figure">Figure 2</ref> (shown on top of a CNN, as we will explain in the subsequent section).</p><p>Edge-wise gating. Syntactic GCNs also include gates, which can down-weight the contribution of individual edges. They also allow the model to deal with noisy predicted structure, i.e. to ignore potentially erroneous syntactic edges. For each edge, a scalar gate is calculated as follows:</p><formula xml:id="formula_9">g (j) u,v = σ h (j) u · ˆ w (j) dir(u,v) + ˆ b (j) lab(u,v)</formula><p>where σ is the logistic sigmoid function, andˆw andˆ andˆw</p><formula xml:id="formula_10">(j) dir(u,v) ∈ R d andˆbandˆandˆb (j)</formula><p>lab(u,v) ∈ R are learned pa- rameters for the gate. The computation becomes:</p><formula xml:id="formula_11">h (j+1) v =ρ u∈N (v) g (j) u,v W (j) dir(u,v) h (j) u + b (j) lab(u,v)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Convolutional Encoders</head><p>In this work we focus on exploiting structural in- formation on the source side, i.e. in the encoder. We hypothesize that using an encoder that incor- porates syntax will lead to more informative rep- resentations of words, and that these representa- tions, when used as context vectors by the decoder, will lead to an improvement in translation qual- ity. Consequently, in all our models, we use the decoder of <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> and keep this part of the model constant. As is now common practice, we do not use a maxout layer in the de- coder, but apart from this we do not deviate from the original definition. In all models we make use of GRUs (Cho et al., 2014b) as our RNN units. Our models vary in the encoder part, where we exploit the power of GCNs to induce syntactically- aware representations. We now define a series of encoders of increasing complexity.</p><p>BoW + GCN. In our first and simplest model, we propose a bag-of-words encoder (with position embeddings, see §2.1.1), with a GCN on top. In other words, inputs h (0) are a sum of embeddings of a word and its position in a sentence. Since the original BoW encoder captures the linear order- ing information only in a very crude way (through the position embeddings), the structural informa- tion provided by GCN should be highly beneficial.</p><p>Convolutional + GCN. In our second model, we use convolutional neural networks to learn word representations. CNNs are fast, but by def- inition only use a limited window of context. In- stead of the approach used by <ref type="bibr" target="#b12">Gehring et al. (2016)</ref> (i.e. stacking mulitple CNN layers on top of each other), we use a GCN to enrich the one-layer CNN representations. <ref type="figure">Figure 2</ref> shows this model. Note that, while the figure shows a CNN with a window size of 3, we will use a larger window size of 5 in our experiments. We expect this model to perform better than BoW + GCN, because of the additional local context captured by the CNN.</p><p>BiRNN + GCN. In our third and most powerful model, we employ bidirectional recurrent neural networks. In this model, we start by encoding the source sentence using a BiRNN (i.e. BiGRU), and use the resulting hidden states as input to a GCN. Instead of relying on linear order only, the GCN will allow the encoder to 'teleport' over parts of the input sentence, along dependency edges, con- necting words that otherwise might be far apart. The model might not only benefit from this tele- porting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see §2.3 for details). This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information with- out explicit supervision ( <ref type="bibr" target="#b24">Linzen et al., 2016</ref>), and hence they should be hard to improve on by incor- porating treebank syntax.</p><p>Marcheggiani and Titov (2017) did not observe improvements from using multiple GCN layers in semantic role labeling. However, we do expect that propagating information from further in the tree should be beneficial in principle. We hypoth- esize that the first layer is the most influential one, capturing most of the syntactic context, and that additional layers only modestly modify the repre- sentations. To ease optimization, we add a resid- ual connection <ref type="bibr" target="#b15">(He et al., 2016</ref>) between the GCN layers, when using more than one layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experiments are performed using the Neural Mon- key toolkit 3 (Helcl and Libovick´y <ref type="bibr" target="#b16">Libovick´y, 2017)</ref>, which implements the model of <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref> in TensorFlow. We use the Adam optimizer ( <ref type="bibr" target="#b22">Kingma and Ba, 2015</ref>) with a learning rate of 0.001 (0.0002 for CNN models). <ref type="bibr">4</ref> The batch size is set to 80. Between layers we apply dropout with a probability of 0.2, and in experiments with GCNs <ref type="bibr">5</ref> we use the same value for edge dropout. We train for 45 epochs, evaluating the BLEU per- formance of the model every epoch on the vali- dation set. For testing, we select the model with the highest validation BLEU. L2 regularization is used with a value of 10 −8 . All the model selection (incl. hyperparameter selections) was performed on the validation set. In all experiments we obtain translations using a greedy decoder, i.e. we se- lect the output token with the highest probability at each time step.</p><p>We will describe an artificial experiment in §4.1 and MT experiments in §4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reordering artificial sequences</head><p>Our goal here is to provide an intuition for the ca- pabilities of GCNs. We define a reordering task where randomly permuted sequences need to be put back into the original order. We encode the original order using edges, and test if GCNs can successfully exploit them. Note that this task is not meant to provide a fair comparison to RNNs. The input (besides the edges) simply does not carry any information about the original ordering, so RNNs cannot possibly solve this task.</p><p>Data. From a vocabulary of 26 types, we gen- erate random sequences of 3-10 tokens. We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels. Additionally, we point every to- ken to an arbitrary position in the sequence with a label from a distinct set of 5 'fake' labels. We sam- ple 25000 training and 1000 validation sequences.</p><p>Model. We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN on top. We use 32, 64 and 128 units for embeddings, GRU units and GCN layers, respectively.</p><p>Results. After 6 epochs of training, the model learns to put permuted sequences back into or- der, reaching a validation BLEU of 99.2. <ref type="figure" target="#fig_1">Fig- ure 3</ref> shows that the mean values of the bias terms of gates (i.e. ˆ b) for real and fake edges are far apart, suggesting that the GCN learns to dis- tinguish them. Interestingly, this illustrates why edge-wise gating is beneficial. A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases b would then be label-dependent. Conse- quently, it would only do a mediocre job in re- ordering. Although using label-specific matrices W would also help, this would not scale to the real scenario (see §2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Machine Translation</head><p>Data. For our experiments we use the En-De and En-Cs News Commentary v11 data from the WMT16 translation task. <ref type="bibr">6</ref> For En-De we also train on the full WMT16 data set. As our valida- tion set and test set we use newstest2015 and newstest2016, respectively.</p><p>Pre-processing. The English sides of the cor- pora are tokenized and parsed into dependency trees by SyntaxNet, 7 using the pre-trained Parsey McParseface model. <ref type="bibr">8</ref> The Czech and German sides are tokenized using the Moses tokenizer. <ref type="bibr">9</ref> Sentence pairs where either side is longer than 50 words are filtered out after tokenization.</p><p>Vocabularies. For the English sides, we con- struct vocabularies from all words except those with a training set frequency smaller than three. For Czech and German, to deal with rare words and phenomena such as inflection and compound- ing, we learn byte-pair encodings (BPE) as de- scribed by <ref type="bibr" target="#b34">Sennrich et al. (2016b)</ref>. Given the size of our data set, and following <ref type="bibr" target="#b44">Wu et al. (2016)</ref>, we use 8000 BPE merges to obtain robust frequencies for our subword units (16000 merges for full data experiment). Data set statistics are summarized in <ref type="table" target="#tab_0">Table 1 and vocabulary sizes in Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Val <ref type="table">Table 1</ref>: The number of sentences in our data sets.</p><note type="other">. Test English-German 226822 2169 2999 English-German (full) 4500966 2169 2999 English-Czech 181112 2656 2999</note><p>Hyperparameters. We use 256 units for word embeddings, 512 units for GRUs (800 for En-De full data set experiment), and 512 units for con- volutional layers (or equivalently, 512 'channels'). The dimensionality of the GCN layers is equiva-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target</head><p>English-German 37824 8099 (BPE) English-German (full) 50000 16000 (BPE) English-Czech 33786 8116 (BPE) lent to the dimensionality of their input. We report results for 2-layer GCNs, as we find them most ef- fective (see ablation studies below).</p><p>Baselines. We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN. See §2.1.1 for details.</p><p>Evaluation. We report (cased) BLEU results ( <ref type="bibr" target="#b29">Papineni et al., 2002</ref>) using multi-bleu, as well as Kendall τ reordering scores. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results</head><p>English-German.    <ref type="table">Table 5</ref>: Validation BLEU for English-German and English-Czech for 1-and 2-layer GCNs.</p><p>Effect of sentence length. We hypothesize that GCNs should be more beneficial for longer sen- tences: these are likely to contain long-distance syntactic dependencies which may not be ade- quately captured by RNNs but directly encoded in GCNs. To test this, we partition the validation data into five buckets and calculate BLEU for each of them. <ref type="figure" target="#fig_2">Figure 4</ref> shows that GCN-based models outperform their respective baselines rather uni- formly across all buckets. This is a surprising re- sult. One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs. Discussion. Results suggest that the syntax- aware representations provided by GCNs consis- tently lead to improved translation performance as measured by BLEU 4 (as well as TER and BEER). Consistent gains in terms of Kendall tau and BLEU 1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>We review various accounts to syntax in NMT as well as other convolutional encoders.</p><p>Syntactic features and/or constraints. Sen- nrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embed- dings. <ref type="bibr" target="#b10">Eriguchi et al. (2016)</ref> parse English sen- tences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. <ref type="bibr" target="#b39">Stahlberg et al. (2016)</ref> use a pruned lattice from a hierarchi- cal phrase-based model (hiero) to constrain NMT.</p><p>Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. <ref type="bibr" target="#b0">Aharoni and Goldberg (2017)</ref> propose neural string-to-tree by predicting linearized parse trees.</p><p>Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. <ref type="bibr" target="#b25">Luong et al. (2015a)</ref> predict linearized constituency parses as an additional task. <ref type="bibr" target="#b11">Eriguchi et al. (2017)</ref> multi-task with a target-side RNNG parser  and improve on various language pairs with English on the target side. <ref type="bibr">Nadejde et al. (2017)</ref> multi-task with CCG tagging, and also in- tegrate syntax on the target side by predicting a se- quence of words interleaved with CCG supertags.</p><p>Latent structure. <ref type="bibr" target="#b14">Hashimoto and Tsuruoka (2017)</ref> add a syntax-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also ex- periment with pre-training the encoder using tree- bank annotation and report modest improvements on English-Japanese. <ref type="bibr" target="#b45">Yogatama et al. (2016)</ref> in- troduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees.</p><p>Convolutional encoders. <ref type="bibr" target="#b12">Gehring et al. (2016)</ref> show that CNNs can be competitive to BiRNNs when used as encoders. To increase the receptive field of a word's context they stack multiple CNN layers. <ref type="bibr">Kalchbrenner et al. (2016)</ref> use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field. In con- trast to both approaches, we use a GCN informed by dependency structure to increase it. Finally, <ref type="bibr" target="#b4">Cho et al. (2014a)</ref> propose a recursive convolu- tional neural network which builds a tree out of the word leaf nodes, but which ends up compress- ing the source sentence in a single vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented a simple and effective approach to integrating syntax into neural machine trans- lation models and have shown consistent BLEU 4 improvements for two challenging language pairs: English-German and English-Czech. Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-yond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A dependency tree for the example sentence: "The monkey eats a banana."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean values of gate bias terms for real (useful) labels and for fake (non useful) labels suggest the GCN learns to distinguish them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Validation BLEU per sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : Vocabulary sizes.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 shows</head><label>3</label><figDesc></figDesc><table>test results 
on English-German. Unsurprisingly, the bag-of-
words baseline performs the worst. We expected 
the BoW+GCN model to make easy gains over 
this baseline, which is indeed what happens. The 
CNN baseline reaches a higher BLEU 4 score than 
the BoW models, but interestingly its BLEU 1 
score is lower than the BoW+GCN model. The 
CNN+GCN model improves over the CNN base-
line by +1.9 and +1.1 for BLEU 1 and BLEU 4 , re-
spectively. The BiRNN, the strongest baseline, 
reaches a BLEU 4 of 14.9. Interestingly, GCNs 
still manage to improve the result by +2.3 BLEU 1 
and +1.2 BLEU 4 points. Finally, we observe a big 
jump in BLEU 4 by using the full data set and beam 
search (beam 12). The BiRNN now reaches 23.3, 
while adding a GCN achieves a score of 23.9. 

English-Czech. Table 4 shows test results on 
English-Czech. While it is difficult to obtain high 
absolute BLEU scores on this dataset, we can still 
see similar relative improvements. Again the BoW 
baseline scores worst, with the BoW+GCN eas-
ily beating that result. The CNN baseline scores 
BLEU 4 of 8.1, but the CNN+GCN improves on 
that, this time by +1.0 and +0.6 for BLEU 1 and 
BLEU 4 , respectively. Interestingly, BLEU 1 scores 
for the BoW+GCN and CNN+GCN models are 

10 See Stanojevi´cStanojevi´c and Simaan (2015). TER (Snover et al., 
2006) and BEER (Stanojevi´cStanojevi´c and Sima'an, 2014) metrics, 
even though omitted due to space considerations, are con-
sistent with the reported results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Test results for English-German. 

higher than both baselines so far. Finally, the 
BiRNN baseline scores a BLEU 4 of 8.9, but it 
is again beaten by the BiRNN+GCN model with 
+1.9 BLEU 1 and +0.7 BLEU 4 . 

Kendall BLEU 1 BLEU 4 

BoW 
0.2498 
32.9 
6.0 
+ GCN 0.2561 
35.4 
7.5 

CNN 
0.2756 
35.1 
8.1 
+ GCN 0.2850 
36.1 
8.7 

BiRNN 
0.2961 
36.9 
8.9 
+ GCN 0.3046 
38.8 
9.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Test results for English-Czech.</head><label>4</label><figDesc></figDesc><table>Effect of GCN layers. How many GCN layers 
do we need? Every layer gives us an extra hop 
in the graph and expands the syntactic neighbor-
hood of a word. Table 5 shows validation BLEU 
performance as a function of the number of GCN 
layers. For English-German, using a 1-layer GCN 
improves BLEU-1, but surprisingly has little effect 
on BLEU 4 . Adding an additional layer gives im-
provements on both BLEU 1 and BLEU 4 of +1.3 
and +0.73, respectively. For English-Czech, per-
formance increases with each added GCN layer. 

En-De 
En-Cs 

BLEU1 BLEU4 BLEU1 BLEU4 

BiRNN 
44.2 
14.1 
37.8 
8.9 
+ GCN (1L) 
45.0 
14.1 
38.3 
9.6 
+ GCN (2L) 
46.3 
14.8 
39.6 
9.9 

</table></figure>

			<note place="foot" n="1"> We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).</note>

			<note place="foot" n="2"> For an alternative approach to integrating labels and directions, see applications of GCNs to statistical relation learning (Schlichtkrull et al., 2017).</note>

			<note place="foot" n="3"> https://github.com/ufal/neuralmonkey 4 Like Gehring et al. (2016) we note that Adam is too aggressive for CNN models, hence we use a lower learning rate. 5 GCN code at https://github.com/bastings/neuralmonkey</note>

			<note place="foot" n="7"> https://github.com/tensorflow/models/tree/master/syntaxnet 8 The used dependency parses can be reproduced by using the syntaxnet/demo.sh shell script. 9 https://github.com/moses-smt/mosesdecoder</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Michael Schlichtkrull and Thomas Kipf for their suggestions and comments. This work was supported by the European Re-search Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards String-to-Tree Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract meaning representation (amr) 1.0 specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to translate with source and target syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1443" to="1452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: EncoderDecoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.1259</idno>
	</analytic>
	<monogr>
		<title level="m">SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>California</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to Parse and Translate Improves Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1611.02344</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<title level="m">Neural Message Passing for Quantum Chemistry</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno>abs/1702.02265</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural monkey: An open-source tool for sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinion Mining with Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Aäron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno>abs/1610.10099</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="595" to="608" />
		</imprint>
	</monogr>
	<note>Journal of computer-aided molecular design</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Assessing the ability of lstms to learn syntaxsensitive dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Dupoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="535" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multitask Sequence to Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>abs/1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Philipp Koehn, and Alexandra Birch. 2017. Syntax-aware Neural Machine Translation Using CCG</title>
		<imprint/>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldip</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno>abs/1606.02892</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation (WMT16)</title>
		<meeting>the First Conference on Machine Translation (WMT16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Does string-based neural mt learn source syntax?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkit</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the Workshop on Statistical Machine Translation</title>
		<meeting>on the Workshop on Statistical Machine Translation<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evaluating mt systems with beer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="26" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fitting sentence level translation evaluation with many dense features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojevi´cstanojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="202" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The conll 2008 shared task on joint parsing of syntactic and semantic dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to compose words into sentences with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1611.09100</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Syntax augmented machine translation via chart parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Zollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venugopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation, StatMT &apos;06</title>
		<meeting>the Workshop on Statistical Machine Translation, StatMT &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="138" to="141" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
