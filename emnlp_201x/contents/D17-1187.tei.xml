<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Training for Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Information UC Berkeley</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
							<email>dbamman@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Information UC Berkeley</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><surname>Russell</surname></persName>
							<email>russell@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Division UC Berkeley</orgName>
								<orgName type="department" key="dep2">School of Information UC Berkeley</orgName>
								<orgName type="department" key="dep3">Computer Science Division UC Berkeley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Training for Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1778" to="1783"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Adversarial training is a mean of regu-larizing classification algorithms by generating adversarial noise to the training data. We apply adversarial training in relation extraction within the multi-instance multi-label learning framework. We evaluate various neural network architectures on two different datasets. Experimental results demonstrate that adversarial training is generally effective for both CNN and RNN models and significantly improves the precision of predicted relations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite the recent successes of deep neural net- works on various applications, neural network models tend to be overconfident about the noise in input signals. Adversarial examples ( <ref type="bibr">Szegedy et al., 2013)</ref> are examples generated by adding noise in the form of small perturbations to the original data, which are often indistinguishable for humans but drastically increase the loss incurred in a deep model. Adversarial training <ref type="bibr">(Goodfellow et al., 2014</ref>) is a technique for regularizing deep models by encouraging the neural network to correctly classify both unmodified examples and perturbed ones, which in practice not only en- hances the robustness of the neural network but also improves its generalizability. Previous work has largely applied adversarial training on straight- forward classification tasks, including image clas- sification ( <ref type="bibr">Goodfellow et al., 2014</ref>) and text clas- sification ( <ref type="bibr" target="#b4">Miyato et al., 2016)</ref>, where the goal is simply predicting a single label for every exam- ple and the training examples are able to provide strong supervision. It remains unclear whether ad- versarial training could be still effective for tasks with much weaker supervision, e.g., distant super- vision ( <ref type="bibr" target="#b3">Mintz et al., 2009)</ref>, or a different evalu- ation metric other than prediction accuracy (e.g., F1 score).</p><p>This paper focuses on the task of relation ex- traction, where the goal is to predict the relation that exists between a particular entity pair given several text mentions. One popular way to han- dle this problem is the multi-instance multi-label learning framework (MIML) ( <ref type="bibr">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b13">Surdeanu et al., 2012</ref>) with distant super- vision ( <ref type="bibr" target="#b3">Mintz et al., 2009)</ref>, where the mentions for an entity pair are aligned with the relations in Freebase ( <ref type="bibr">Bollacker et al., 2008)</ref>. In this setting, relation extraction is much harder than the canon- ical classification problem in two respects: (1) although distant supervision can provide a large amount of data, the training labels are very noisy, and due to the multi-instance framework, the su- pervision is much weaker; (2) the evaluation met- ric of relation extraction is often the precision- recall curve or F1 score, which cannot be repre- sented (and thereby optimized) directly in the loss function.</p><p>In order to evaluate the effectiveness of adver- sarial training for relation extraction, we apply it to two different architectures (a convoluational neu- ral network and a recurrent neural network) on two different datasets. Experimental results show that even on this harder task with much weaker super- vision, adversarial training can still improve the performance on all of the cases we studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural Relation Extraction: In recent years, neural network models have shown superior per- formance over approaches using hand-crafted fea- tures in various tasks. Convolutional neural net- works (CNN) are among the first deep mod- els that have been applied to relation extrac-tion ( <ref type="bibr" target="#b10">Santos et al., 2015;</ref><ref type="bibr" target="#b7">Nguyen and Grishman, 2015)</ref>. Variants of convolutional networks include piecewise-CNN (PCNN) ( <ref type="bibr" target="#b18">Zeng et al., 2014</ref>), split CNN ( <ref type="bibr">Adel et al., 2016)</ref>, CNN with sentence- wise pooling <ref type="bibr">(Jiang et al., 2016</ref>) and attention CNN ( <ref type="bibr" target="#b15">Wang et al., 2016)</ref>. Recurrent neural net- works (RNN) are another popular choice, and have been used in recent work in the form of recurrent CNNs <ref type="bibr">(Cai et al., 2016</ref>) and attention RNNs ( <ref type="bibr" target="#b19">Zhou et al., 2016</ref>). An instance-level selective attention mechanism was introduced for MIML by , and has significantly improved the predic- tion accuracy for several of these base deep mod- els.</p><p>Adversarial Training: Adversarial training (AT) ( <ref type="bibr">Goodfellow et al., 2014</ref>) was originally in- troduced in the context of image classification tasks where the input data is continuous. <ref type="bibr" target="#b5">Miyato et al. (2015</ref><ref type="bibr" target="#b4">Miyato et al. ( , 2016</ref> adapts AT to text classification by adding perturbations on word embeddings and also extends AT to a semi-supervised setting by minimizing the entropy of the predicted label dis- tributions on unlabeled data.</p><p>AT introduces an end-to-end and deterministic way of data perturbation by utilizing the gradi- ent information. There are also other works for regularizing classifiers by adding random noise to the data, such as dropout ( <ref type="bibr" target="#b12">Srivastava et al., 2014</ref>) and its variant for NLP tasks, word dropout ( <ref type="bibr">Iyyer et al., 2015)</ref>. <ref type="bibr" target="#b16">Xie et al. (2017)</ref> discusses vari- ous data noising techniques for language models. <ref type="bibr">Søgaard (2013)</ref> and <ref type="bibr" target="#b16">Li et al. (2017)</ref> focus on lin- guistic adversaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We first introduce MIML and then describe the base neural network models we consider: <ref type="bibr">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>In MIML, we consider the set of text sentences X = {x 1 , x 2 , . . . , x n } for each entity pair. Sup- posing we have R predefined relations (including NA) to extract, we want to predict the probabil- </p><formula xml:id="formula_0">+ + + í µí± í µí± (3)</formula><p>í µí± í µí± <ref type="formula" target="#formula_2">(1)</ref> í µí± í µí± ity of each of the R relations given the mentions. Formally, for each relation r, we want to predict</p><formula xml:id="formula_1">P (r | x 1 , . . . , x n ).</formula><p>Note that since an entity pair may have no re- lations, we introduce a special relation NA to the label set. Hence, we simply assume there will be at least one relation existing for every entity pair. During evaluation, we ignore the probability pre- dicted for the NA relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Architectures</head><p>Input Representation: For each sentence x i , we use pretrained word embeddings to project each word token into d w -dimensional space. Note that we also need to include the entity position infor- mation in x i . Here we introduce an extra feature vector p (w) i for each word w to encode the enti- ties' positions. One choice is the position embed- ding ( <ref type="bibr" target="#b18">Zeng et al., 2014</ref>): for each word w, we com- pute the relative distances to the two entities and embed the distances in two d p -dimensional vec- tors, which are then concatenated as p (w)</p><p>i . Position embedding introduces extra variables in the model and slows down the training time. We also inves- tigate a simpler choice, indicator encoding: when a word w is exactly an entity, we generate a d p - dimensional 1 vector and a 0 vector otherwise. In our experiments, position embedding is crucial for PCNN due to the spatial invariance of CNN. For RNN, position embedding helps little (likely be- cause an RNN has the capacity of exploiting tem- poral dependencies) so we adopt indicator encod- ing instead.</p><p>Sentence Encoder: For a sentence x i , we want to apply a non-linear transformation to the vector representation of x i to derive a feature vector s i = f (x i ; θ) given a set of parameters θ. We consider both PCNN and RNN as f (x i ; θ).</p><p>For PCNN, inheriting the settings from ( <ref type="bibr" target="#b18">Zeng et al., 2014</ref>), we adopt a convolution kernel with window size 3 and d s output channels and then apply piecewise pooling and ReLU <ref type="bibr" target="#b6">(Nair and Hinton, 2010)</ref> </p><note type="other">as an activation function to eventually obtain a 3 · d s -dimensional feature vector s i . For RNN, we adopt bidirectional GRU with d s hidden units and concatenate the hidden states of the last timesteps from both the forward and the backward RNN as a 2·d s -dimensional feature vec- tor s i . Selective Attention: Following Lin et al. (2016), for each relation r, we aim to softly se- lect an attended sentence s r by taking a weighted average of s 1 , s 2 , . . . , s n , namely s r = i α r i s i . Here α r denotes the attention weights w.r.t. re- lation r. For computing the weights, we define a query vector q r</note><p>for each relation r and com- pute α r = softmax(u r ) where u r i = tanh(s i ) q r . The query vector q r can be considered as the em- bedding vector for the relation r, which is jointly learned with other model parameters.</p><p>Loss Function: For an entity pair, we com- pute the probability of relation r by P (r | X; θ) = softmax(As r +b), where A is the projection matrix and b is the bias. For the multi-label setting, sup- pose K relations r 1 , . . . , r K exist for X. Simply taking the summation over the log probabilities of all those labels yields the final loss function</p><formula xml:id="formula_2">L(X; θ) = − K i=1 log P (r i | X; θ).<label>(1)</label></formula><p>Dropout: For regularizing the parameters, we apply dropout ( <ref type="bibr" target="#b12">Srivastava et al., 2014</ref>) to both the word embedding and the sentence feature vector s i . Note that we do not perform dropout on the position embedding p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Training</head><p>Adversarial training (AT) is a way of regulariz- ing the classifier to improve robustness to small worst-case perturbations by computing the gradi- ent direction of a loss function w.r.t. the data. AT generates continuous perturbations, so we add the adversarial noise at the level of the word embed- dings, similar to <ref type="bibr" target="#b4">Miyato et al. (2016)</ref>. Formally, consider the input data X and suppose the word embedding of all the words in X is V . AT adds a Dataset #Rel #Ent-Pair #Mention Sent-Len <ref type="table" target="#tab_2">NYT-Train  58  290429  577434  145  UW-Train  5  132419  546731  120   Table 1</ref>: Dataset statistics (#Rel includes NA).</p><p>small adversarial perturbation e adv to V and opti- mizes the following objective instead of Eq. <ref type="formula" target="#formula_2">(1)</ref>. <ref type="formula">(2)</ref> e adv = arg max</p><formula xml:id="formula_3">L adv (X; θ) = L(X + e adv ; θ), where</formula><formula xml:id="formula_4">e≤ L(X + e; ˆ θ) (3)</formula><p>Herê θ denotes a fixed copy of the current value of θ. Since Eq. <ref type="formula">(3)</ref> is computationally intractable for neural nets, <ref type="bibr">Goodfellow et al. (2014)</ref> proposes to approximate Eq.(3) by linearizing L(X; ˆ θ) near X:</p><formula xml:id="formula_5">e adv = g/g, where g = V L(X; ˆ θ). (4)</formula><p>Here V denotes the word embedding of all the words in X. Accordingly, in Eq. 4, g denotes the norm of gradients over all the words from all the sentences in X. In addition, we do not perturb the feature vector p for entity positions. A visual- ization of the process is demonstrated in <ref type="figure" target="#fig_1">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To measure the effectiveness of adversarial train- ing on relation extraction, we evaluate both the CNN (PCNN) and RNN (bi-GRU) models on two different datasets, the NYT dataset (NYT) developed by <ref type="bibr" target="#b9">Riedel et al. (2010)</ref> and the UW dataset (UW) by . All code is implemented in Tensorflow ( <ref type="bibr">Abadi et al., 2016</ref>) and available at https://github. com/jxwuyi/AtNRE. We adopt Adam opti- mizer ( <ref type="bibr">Kingma and Ba, 2014</ref>) with learning rate 0.001, batch size 50 and dropout rate 0.5. For adversarial training, the only parameter is . In each of the following experiments, we fixed all the hyper-parameters of the base model, performed a binary search solely on and showed the most ef- fective value of .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The statistics of the two datasets are summarized in <ref type="table">Table 1</ref>. We exclude sentences longer than Sent- Len during training and randomly split data for entity pairs with more than 500 mentions. Note that the number of target relations in these two datasets are significantly different, which helps   demonstrate the applicability of adversarial train- ing on various evaluation settings.</p><note type="other">Recall 0.1 0.2 0.3 0.4 AUC</note><p>Since the test set of the UW dataset only con- tains 200 sentences, we adopt a subset of the test set from the NYT dataset: all the entity pairs with the corresponding 4 relations in UW and another 1500 randomly selected NA pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Practical Performances</head><p>The NYT dataset:</p><p>We utilize the word embeddings released by , which has d w = 50 dimensions. For model parameters, we set d e = 5 (dimension of the entity position feature vector) and d s = 230 (dimension of sentence feature vector) for PCNN and d e = 3 and d s = 150 for RNN. For adver- sarial training, we choose = 0.01 for PCNN and = 0.02 for RNN. We empirically observed that when adding dropout to the word embeddings, PCNN performs significantly worse. Hence we only apply dropout to s i for PCNN. However, even with a dropout rate of 0.5, RNN still performs well. We conjecture that it is due to PCNN being more sensitive to input signals and the dimension- ality of the word embedding (d w = 50) being very small.</p><p>The precision-recall curves for different mod- els on the test set are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Since the precision drops significantly with large recalls on the NYT dataset, we emphasize a part of the curve with recall number smaller than 0.5 in the   figure. Adversarial training significantly improves the precision for both PCNN and RNN models. We also show the precision numbers for some par- ticular recalls as well as the AUC (for the whole PR curve) in <ref type="table" target="#tab_2">Table 2</ref>, where RNN generally leads to better precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The UW dataset:</head><p>We train a word embedding of d w = 200 di- mensions using Glove ( <ref type="bibr" target="#b8">Pennington et al., 2014</ref>) on the New York Times Corpus in this experi- ment. For model parameters, we set the entity fea- ture dimension d e = 5 and sentence feature di- mension d s = 250 for PCNN and d e = 3 and d s = 200 for RNN. For adversarial training, we choose = 0.05 for PCNN and = 0.5 for RNN. Since here word embedding dimension d w is larger than that used for the NYT dataset, which implies that we now have word embeddings with larger norms, accordingly the optimal value of increases. The precision-recall curves on the test data are shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, where adversarial train- ing again significantly improves the precision for both models. The precision numbers for some par- ticular recall values as well as the AUC numbers are demonstrated in <ref type="table" target="#tab_4">Table 3</ref>. Similarly RNN yields superior performances on the UW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>CNN vs RNN: In the experiments, RNN gener- ally produces more precise predictions than CNN due to its rich model capacity and also has high robustness to input embeddings. The CNN, in contrast, has far fewer parameters which leads to much faster training and testing, which suggests a practical trade-off. Notably, although the improvement under AUC by adversarial training are roughly the same for both RNN and CNN, the optimal value for RNN is always much larger than CNN. This implies that empirically RNN is more robust under adversarial attacks than CNN, which also helps RNN maintain higher precision as recall increases. Choice of : When = 0, the AT loss (Eq.(2)) de- generates to the original loss (Eq. <ref type="formula" target="#formula_2">(1)</ref>); when be- comes too large, the noise can change the seman- tics of a sentence 2 and make the model extremely hard to correctly classify the adversarial examples.</p><p>Notably, the optimal value of is much smaller than the norm of the word embedding, which im- plies adversarial training works most effectively when only producing tiny perturbations on word features while keeping the semantics of sentences unchanged <ref type="bibr">3</ref> . Connection to other approaches: <ref type="bibr" target="#b16">Li et al. (2017)</ref>; <ref type="bibr" target="#b16">Xie et al. (2017)</ref> proposes linguistic adver- saries techniques to enhance the robustness of the model by randomly changing the word tokens in a sentence. This explicitly modifies the semantics of a sentence. By contrast, adversarial training fo- cuses on smaller and continuous perturbations in the embedding space while preserving the seman- tics of sentences. Hence, adversarial training is complementary to linguistic adversaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>piece- wise CNN (Zeng et al., 2015) (PCNN) and bidi- rectional GRU (Cho et al., 2014) (RNN). We also utilize the selective attention mechanism in Lin et al. (2016) for both PCNN and RNN models. Adversarial training is presented at the end of this section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The computation graph of encoding a sentence x i with adversarial training. e i denotes the adversarial perturbation w.r.t. x i. Dropout is placed on the output of the variables in the doublelined rectangles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PR curves for PCNN (left) and RNN (right) on the NYT dataset with (blue) and without (green) adversarial training.</figDesc><graphic url="image-1.png" coords="4,73.19,172.67,106.58,93.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PR curves for PCNN (left) and RNN (right) on the UW dataset with (blue) and without (green) adversarial training.</figDesc><graphic url="image-3.png" coords="4,308.46,161.09,106.58,93.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Precisions of various models for differ- ent recalls on the NYT dataset, with best values in bold.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Precisions of various models for different recalls on the UW dataset, with best values in bold.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> We primarily focus on effectiveness of AT. Other techniques in Sec. 2 are complementary to our focus.</note>

			<note place="foot" n="2"> When is large enough, e.g., comparable to the norm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our anonymous review-ers for valuable discussions. We also appreciate the generous help from Angli Liu and Yankai Lin. This work is partially supported by the DARPA PPAML program, contract FA8750-14-C-0011.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2016.">Tensorflow: Large-scale machine learning on</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective crowd annotation for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL-HLT</title>
		<meeting>the NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semi-supervised text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07725</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relation extraction: Perspective from convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huu</forename><surname>Thien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modeling relations and their mentions without labeled text. Machine learning and knowledge discovery in databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<title level="m">Classifying relations by ranking with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging with antagonistic adversaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="640" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
		<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Relation classification via multi-level attention cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiming</forename><surname>Lévy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02573</idno>
		<title level="m">Data noising as smoothing in neural network language models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attentionbased bidirectional long short-term memory networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>The 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">207</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
