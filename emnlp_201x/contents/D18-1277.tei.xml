<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Challenge Set and Methods for Noun-Verb Ambiguity</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 2562</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Elkahky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kellie</forename><surname>Webster</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">Pitler</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Language</surname></persName>
						</author>
						<title level="a" type="main">A Challenge Set and Methods for Noun-Verb Ambiguity</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2562" to="2572"/>
							<date type="published">October 31-November 4, 2018. 2018. 2562</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>English part-of-speech taggers regularly make egregious errors related to noun-verb ambiguity , despite having achieved 97%+ accuracy on the WSJ Penn Treebank since 2002. These mistakes have been difficult to quantify and make taggers less useful to downstream tasks such as translation and text-to-speech synthesis. This paper creates a new dataset of over 30,000 naturally-occurring non-trivial examples of noun-verb ambiguity. Taggers within 1% of each other when measured on the WSJ have accuracies ranging from 57% to 75% accuracy on this challenge set. Enhancing the strongest existing tagger with contextual word embeddings and targeted training data improves its accuracy to 89%, a 14% absolute (52% relative) improvement. Downstream, using just this enhanced tagger yields a 28% reduction in error over the prior best learned model for homograph disambiguation for text-to-speech synthesis.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whether a word is functioning as a noun or a verb in a particular linguistic context critically affects the output of tasks including translation and text- to-speech synthesis. The English word close may be translated as either nah (adjective/non-verb) or schließen (verb) (example from <ref type="bibr" target="#b20">Sennrich and Haddow (2016)</ref>). In text-to-speech, the homograph lives is pronounced /laIvz/ (noun) or /lIvz/ (verb; example from <ref type="bibr" target="#b22">Sproat et al. (1992)</ref>).</p><p>While downstream applications require taggers be sensitive to non-local linguistic context, it is difficult to measure such sensitivity with current tagging evaluation. In the past 15 years since <ref type="bibr" target="#b2">Collins (2002)</ref>, many models have accuracy ex- ceeding 97% when measured on the WSJ Penn Treebank, which is within the level of human inter-annotator agreement for the corpus. Incorpo- rating non-local context via sentence-based repre- sentations <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>) or state-of-the- art contextual representations of tokens (ELMo, <ref type="bibr" target="#b17">Peters et al. (2018)</ref>) yields the same tagging ac- curacy as Collobert et al.'s limited window-based representation (97.3%). However, existing local models "regularly make egregious errors" <ref type="bibr" target="#b14">(Manning, 2011</ref>), notably on imperative detection <ref type="bibr">1</ref> . That is, the applicability of the part-of-speech la- beling task is limited by its standard evaluation not reflecting difficult cases which require contextual reasoning to resolve ambiguity.</p><p>In this paper, we address this mismatch by cre- ating a targeted intrinsic evaluation: a challenge dataset of over 30,000 naturally-occurring non- trivial examples of noun-verb ambiguity spanning multiple domains and containing many impera- tives that non-expert humans can annotate with high agreement (Section 2). We will publicly re- lease both the training and evaluation data 2 .</p><p>We further contribute a series of modeling ex- periments on this data. We first show that state- of-the-art taggers perform poorly on this challenge <ref type="table">(Table 1)</ref> and then investigate two simple and or- thogonal approaches to enhancing a state-of-the- art tagger: incorporating generic contextual em- beddings trained on billions of words, and incor- porating thousands of examples of training data targeted for this task. Both of these approaches yield large and complementary improvements: the combined methods give an accuracy of 89.1%, a 14% absolute improvement over a state-of-the-art tagger and a 31% absolute improvement over the widely used Stanford tagger. Section 3 provides an overview of the investigated taggers, experiments, and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>WSJ NV Existing Taggers <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref>  <ref type="bibr">97.24 57.6 Choi (2016)</ref> 97.64 71.2 <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> 97.33 70.4 <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref> 98.00±.12 74.0±1.2 Enhancements +ELMo 97.94±.08 82.1±0.9 +NV Data 97.98±.11 86.4±0.4 +ELMo+NV Data 97.97±.09 88.9±0.3 <ref type="table">Table 1</ref>: Empirical Results. All investigated new and existing taggers are within 1% of each other when measured on the WSJ test set. When evalu- ated on the Noun-Verb dataset, however, existing taggers range from 57% to 74%. Adding enhance- ments to the <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref> tagger gives over 14% absolute improvement. Best results and results insignificantly different from the best are bolded (two-tailed t-test).</p><p>Finally, we demonstrate that these tagging im- provements make a positive impact on the down- stream task of homograph disambiguation for text- to-speech (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Noun-Verb Dataset</head><p>Consider the ambiguous examples below:</p><p>(1) Certain insects can damage plumerias, such as mites, flies, or aphids. NOUN (2) Mark which area you want to distress. VERB All tested existing part-of-speech taggers <ref type="table">(Table 1)</ref> mistag both of these examples, tagging flies as a verb and Mark as a noun 3 . Looking at only the WSJ Penn Treebank, all occurrences of Mark are nouns, so a part-of-speech tagger that ignores con- text completely could appear to do quite well on this word type. Similarly, all occurrences of the word type share in the WSJ development set are noun instances. A baseline of selecting the most frequent tag per word type (ignoring all context) achieves 93.0% accuracy on the ambiguous tokens in the WSJ (Ta- ble 2). A simple tagger based on a single hidden layer feed-forward neural network with 128 units that uses a three word window around the focus <ref type="bibr">3</ref> The enhanced tagger that uses both contextual word em- beddings and data augmentation (+ELMo+NV Data in <ref type="table">Table  1</ref>) gets both Example (1) and Example (2) correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>NN <ref type="table" target="#tab_3">±3  Train  Dev  Majority Words  WSJ:NV WSJ:NV  93.0  97.0  NV  NV  70.1  77.6   Table 2</ref>: Taggers that use no context (Type Ma- jority) or very little context (NN ±3 words) can achieve high accuracies on the ambiguous tokens in the WSJ (WSJ:NV), but would fare much worse on the Noun-Verb dataset.</p><p>token as features achieves an accuracy of 97.0% on the WSJ ambiguous words (WSJ:NV). We therefore aim to create a dataset in which taggers would have to take into account the sur- rounding context in order to correctly tag ambigu- ous words, rather than relying on skewed priors per word type. We design a methodology for iden- tifying and labeling hard cases of noun-verb am- biguity. The result is a dataset of over 30,000 hand-labeled, natural, and non-trivial examples of noun-verb ambiguity, which we will make pub- licly available to facilitate research on modeling for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Collection Methodology</head><p>Our goal is to build a resource which captures a wide range of challenges that a part-of-speech tag- ger needs to handle in the wild. To produce this resource, we find large sources of naturally occur- ring examples with a diversity of challenges, iden- tify noun-verb ambiguity, find the non-trivial ex- amples, and finally acquire high-precision labels from humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Naturally Occurring Sources</head><p>All examples come from naturally occurring En- glish web text from three distinct genres. Typical examples from each are shown in <ref type="table">Table 3</ref>. These genres present a diverse range of challenges: genre 1 has long well-edited sentences, genre 2 makes heavy use of imperative verbs, and genre 3 con- tains largely headline style short sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Ambiguous Token Detection</head><p>We used an online dictionary to identify ambigu- ous word types (such as play) that can be either a noun or a verb. <ref type="bibr">4</ref> To find ambiguous instances of these types, we ran a CRF-based tagger simi- lar to <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> over the input sen-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representative Examples</head><p>Label Genre 1 "Man With a Vision" peaked at #91 in the UK, spending two weeks on the chart. NOUN 40.7% of the population benefit from public assistance as of 2004, up from 23.0% VERB in 2000. Genre 2</p><p>Your doctor may recommend a diet or exercise routine. NOUN Use within 3 days of cooking. VERB Genre 3</p><p>Safeguard Infrastructure From Electrical Surges &amp; Limit Downtime. NOUN Stop In Today Or Shop Online! VERB <ref type="table">Table 3</ref>: Noun and Verb examples from each genre. All examples are taken from the development set.</p><p>tences. We selected tokens tagged as either a noun or a verb <ref type="bibr">5</ref> and for which the k-best list for that to- ken contained both noun and verb 6 tags with close scores. We used a heuristic that the lower scoring tag had to have a score within 20% of the score of the higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Filtering Trivial Examples</head><p>Part-of-speech tagging is already a well- established task with plenty of existing labeled examples. Adding more examples similar to John watched a play would not affect the output predictions of taggers, which already tend to correctly label tokens as nouns if they follow determiners. Inspired by work on active learning <ref type="bibr" target="#b25">(Tomanek and Hahn, 2009;</ref><ref type="bibr" target="#b21">Small and Roth, 2010)</ref>, we focused our data collection efforts on difficult examples. To remove easy contexts, we excluded tokens preceded by a determiner or modal verb. Tokens 7 were additionally restricted to be neither adjectival modifiers 8 nor components of noun-compounds 9 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Diversification</head><p>Noun-verb disambiguation is a challenge for mod- ern POS taggers both because words can look si- multaneously noun-and verb-like to a model, but also because verbs (nouns) can falsely present as nouns (verbs). Our extraction methodology is <ref type="bibr">5</ref> Nouns and verbs were identified by mapping the fine-grained part-of-speech tag to its coarse-grained category ( <ref type="bibr" target="#b18">Petrov et al., 2012)</ref>: https://github. com/slavpetrov/universal-pos-tags/blob/ master/en-ptb.map <ref type="bibr">6</ref> We excluded VBN from the set of verb tags, as it often functions more similarly to non-verbs <ref type="bibr">7</ref>   well-designed to identify the former. To identify tokens on which models are falsely confident, we manually reviewed a sample of tokens discarded in extraction. We found that sentence-initial impera- tive verbs were very likely to be confidently tagged as nouns. To ensure that this important class of ambiguous tokens was included in our dataset, we made it a special extraction case and did not apply the above filters for trivial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5">Crowdsourced Annotation</head><p>We presented annotators with the extracted to- kens in their full sentence context. Annotators were asked to select whether the target word was a "Noun", a "Verb", "Ambiguous", or "Neither" (a noun or a verb). Full annotation guidelines will accompany the dataset release. Each exam- ple was annotated by at least three annotators for quality assurance. For batches with larger than av- erage proportions of non-unanimous annotations, the non-unanimous examples were sent to an ad- ditional two annotators for a total of five annota- tions. <ref type="table" target="#tab_1">Table 4</ref> shows that annotators generally had a high level of agreement with each other, with unanimous agreement on 71.4% of the examples and majority agreement on 98.7% of the exam- ples. Annotators achieved an average pace of 40 seconds per sentence.</p><p>Genre <ref type="table" target="#tab_1">Train Dev Test  1  8621 1081 2711  2  6160  919 2289  3  9473  400 1000  All  24254 2400 6000   Table 5</ref>: Noun-Verb dataset statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Final Dataset</head><p>To compile the final dataset, we rejected exam- ples in which there was no majority agreement or in which the majority label was "Ambiguous" or "Neither". This excluded 808 sentences and yielded a final dataset size of 32,654. We divided this into training, development, and test sets. <ref type="table">Table  5</ref> shows the dataset sizes and genre distributions. The genre distribution of the training set is inten- tionally different from that of the development and test sets, as realistically one will often have differ- ent distributions at training and test time, and fu- ture work may want to model this difference <ref type="bibr" target="#b4">(Donmez et al., 2010;</ref><ref type="bibr" target="#b23">Steinhardt and Liang, 2016)</ref>. We asked a professional linguist to indepen- dently label 200 examples and adjudicate any dif- ferences from the crowd-sourced labels with other professional linguists. The linguists found only 7 actual mistakes (3.5% of examples). Of the re- maining 96.5% plausible annotations, the linguist agreed with the crowd in 167 cases (83.5%), and found 26 disparities between PTB-style guidelines and plausible intuitive judgments (13%). All but one of the disparities involved a word ending in "ing" inside a noun phrase, such as "Manufactur- ing defects"). Also, all but two of the disparities were cases which the crowd source annotators la- beled as nouns while the PTB-style guidelines la- beled as verbs.</p><p>While humans can do well on these instances, <ref type="table">Table 2</ref> shows that baseline taggers that use little or no context have high error rates on this dataset, in contrast to the WSJ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluation of Taggers</head><p>In this section, we demonstrate empirically the limitations of several existing taggers on the new challenge dataset. We then take the most accu- rate, <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref>, and investigate how it can be enhanced to be much more discriminative in ambiguous contexts. We finish with some error analysis to inspire future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Training All experiments used the standard splits of the WSJ Penn Treebank and the new Noun-Verb dataset. Specifically, WSJ Sections 2- 21 were used to train all models; where indicated, this was augmented with the training portion of the Noun-Verb dataset. Neural models ( <ref type="bibr" target="#b5">Dozat et al. (2017)</ref>, <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref>, and extensions) used WSJ Section 22 for early stopping, and were run with n = 10 random restarts to compute standard deviations.</p><p>Evaluation Models are evaluated on the Noun- Verb test set. The development set was used for developing the proposed enhancements, as well as to do error analysis. To verify performance on the standard task, we also evaluate accuracy on WSJ Section 23, cf. <ref type="table">Table 1</ref> first column.</p><p>Our evaluation metric is VERB/NON-VERB classification accuracy over tokens which have gold annotations. To evaluate the taggers we map the fine-grained tag output using <ref type="bibr" target="#b18">Petrov et al. (2012)</ref>: tags with a coarse-grained VERB category map to the VERB label, and all other tags to the NON-VERB label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Existing Taggers</head><p>We evaluated four commonly used and/or state-of- the-art taggers on our task. The first investigated tagger is the Stanford POS tagger <ref type="bibr">10 (Toutanova et al., 2003)</ref>, part of the Stanford CoreNLP Toolkit ( ) and widely used. This pre- trained model is a log-linear model with features over the surrounding words and tags in a local win- dow around the focus word.</p><p>The second investigated tagger is the pub- licly available NLP4J, a pre-trained tagging model <ref type="bibr" target="#b1">(Choi, 2016)</ref>  <ref type="bibr">11</ref> . It used feature induction to expand the feature set during training by adding combina- tions of low-dimensional features. The approach achieved 97.64% on WSJ evaluation. It is worth noting that this model used a large automatically tagged corpus to get ambiguity classes for each <ref type="bibr">word and Choi (2016)</ref> showed that this extra piece of information was responsible for the largest part of the improvement.</p><p>The third tagger is <ref type="bibr" target="#b5">Dozat et al. (2017)</ref>, which won the UPOS portion of the CoNLL 2017 Shared Task on Universal Dependencies ( <ref type="bibr" target="#b27">Zeman et al., 2017</ref>) by a wide margin. It represents each word by a sum of its pretrained word embedding (glove <ref type="bibr" target="#b16">Pennington et al. (2014)</ref>), trained word embed- ding, and the output from an LSTM runs over word's characters. Those representations are sup- plied to a deep BiLSTM followed by a Multi- Layer Perceptron (MLP) layer. The output from the MLP layer is multiplied by a learned embed- ding for tags and the tag with the highest score is selected as the output.</p><p>Finally the fourth existing tagger is the Meta- BiLSTM ( <ref type="bibr" target="#b0">Bohnet et al., 2018)</ref>  <ref type="bibr">et al., 2014)</ref>) and trained word embeddings, a char- BiLSTM that consumes trained characters embed- ding and a Meta component that takes a concate- nation of word and character representations (at word boundaries) and feeds it to a Bi-LSTM fol- lowed by a MLP layer. The final output is com- puted using softmax over the Meta-MLP represen- tation but a multi-loss is also optimized at the char and word representations level.</p><note type="other">which is the cur- rent state of the art on both WSJ and CoNLL 2017 POS tagging evaluation. This model con- sists of three components, all of which run over the entire input sentence: a word-BiLSTM that takes a sum of pretrained (GloVe (Pennington</note><p>For <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> and <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref>, we trained the model on WSJ PTB training data to get comparable models to the two previous sys- tems. For <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> we used the default hyperparameters. For <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref>, the hyperparameters used are almost identical to the original paper. <ref type="bibr">12</ref> The first two taggers are linear models (with feature combinations) while the second two are neural models. Both <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> and <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref> take non-local context into account through BiLSTMs over the full sentence. However, these models might not use this model- ing power when trained on the WSJ, since local context is usually sufficient <ref type="table">(Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhancements</head><p>We take the best existing tagger ( <ref type="bibr" target="#b0">Bohnet et al., 2018)</ref> as our starting point to investigate the ef- ficacy of two simple enhancements and their com- bination for improving noun-verb disambiguation.</p><p>The first enhancement is to add generic, contex-tual word embeddings trained on a billion words of language modeling data ( <ref type="bibr" target="#b17">Peters et al., 2018)</ref>. The second enhancement is to add task-specific targeted training data, with thousands of examples derived from the Noun-Verb training set.</p><p>Contextual Word Embeddings (ELMo) The statistics of the new dataset, shown in <ref type="table">Table 2</ref>, suggest that this dataset might benefit from more contextual modeling. Although the basic Meta- BiLSTM model is already contextual, one can sus- pect based on the first row in <ref type="table">Table 2</ref> that WSJ training might lead the model to ignore wider con- text. One way to make the model use more con- textual information is to replace the word embed- ding layer with a contextual embedding. We used ELMo embeddings ( <ref type="bibr" target="#b17">Peters et al., 2018</ref>), which are generated by training a bi-directional language model on a large corpus of unlabeled data. The aim of using ELMo here is that we expect to get different embeddings for a word like "play" when it is used as a verb, as in "I will come and play", versus when it is used as a noun, as in "I liked the two-act play". We replaced the word embedding layer in the Word component with ELMo. <ref type="bibr">13</ref> As in <ref type="bibr" target="#b17">Peters et al. (2018)</ref>, we trained a task specific weighting of the three ELMo layers:</p><formula xml:id="formula_0">v (word) i = γ 2 j=0 s j h ELMo i,j ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">h ELMo i,j</formula><p>is the j-th layer ELMo embedding of word i, s j are softmax-normalized weights over the layers, and γ is a scalar parameter. We trained this model on the WSJ training data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Data Augmentation (NV Data) Our</head><p>Noun-Verb training data comes with gold binary labels ("Noun" or "Verb"). To add them to our current model, we took a simple approach to map the Noun-Verb labels into the fine-grained POS tagset used in the WSJ dataset. To do that, we ran the baseline tagger used to extract the anno- tated examples in §2.1 over the Noun-Verb train- ing data, and extracted all possible tags for the annotated words, sorted by their score. We then assigned to that word the highest scoring tag con- sistent with the coarse-grained tags. This resulted in a silver training dataset containing partially la- beled sentences, each with one word tagged by its  ELMo and Data Augmentation Together Fi- nally, we experimented with using both enhance- ments together. We trained the ELMo-enhanced model on the dataset augmented with the Noun- Verb training set examples. The motivating in- tuition for combining them is that the inclusion of the difficult Noun-Verb training set examples could encourage the model to make more use of ELMo embeddings than the model trained on the WSJ only. Another possibility is that these two types of enhancements are redundant and that one dominates the other. <ref type="table">Table 1</ref> shows the main results of both existing taggers and the enhanced models on both WSJ and the Noun-Verb Challenge Set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>Existing Taggers While all four selected tag- gers achieve accuracies above 97% on WSJ, they all struggle on our noun-verb challenge <ref type="table">(Table 1)</ref>. The widely used tagger of <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> has an accuracy of just 57.6%, below the 70.1% accuracy of a per-word type majority class base- line <ref type="table">(Table 2</ref>). The best performing tagger (Bohnet et al., 2018) was 3.9% above the next best model. However it still has an error rate of 25%. The ranking of the four taggers stays the same whether one uses the WSJ or the Noun-Verb Chal- lenge Set for evaluation. However, the magnitude of differences changes drastically. For example, on the WSJ test set, the differences between <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> and <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> appear insignificant: <ref type="bibr" target="#b5">Dozat et al. (2017)</ref> improves over <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> by 0.09% absolute (3% relative reduction in error). When measured on the Noun-Verb Challenge Set, the differences are stark: the tagger of Dozat et al. <ref type="formula" target="#formula_0">(2017)</ref> is 12.8% absolute more accurate, which is a 30% relative reduction in error.</p><p>Enhancements Experimental results in <ref type="table">Table 1</ref> show that ELMo gave 7.2% absolute improvement and did not significantly affect the WSJ results <ref type="bibr">14</ref> . This is further evidence that WSJ evaluation does not model ambiguities in cases where context mat- ters. Adding the silver Noun-Verb data to the baseline model gave 10% absolute improvement over the baseline. This is significant given that the model capacity remained unchanged. By contrast, hooking up ELMo added a very large multi-layer BiLSTM language model to the parameters. The best model was the model which used both ELMo embeddings and data augmentation. It achieved 13.1% absolute improvement over the state-of-the-art baseline of <ref type="bibr" target="#b0">Bohnet et al. (2018)</ref>, equivalent to over a 52% error reduction. This demonstrates that the improvement from ELMo is complementary to that from the additional Noun- Verb data.</p><p>Sentence-Initial Examples The trend in <ref type="table">Table 1</ref> is magnified in <ref type="table" target="#tab_3">Table 6</ref>, which shows develop- ment set accuracies separately for tokens that are sentence-initial (SI), which are often imperatives, and for tokens that are not SI.</p><p>On SI accuracy, none of the WSJ-trained base- lines could beat the most-frequent-tag baseline from the Noun-Verb training data. This shows that these sorts of examples, which are mostly im- peratives, are underrepresented in the WSJ cor- pus. ELMo embeddings were able to improve both SI and non-SI accuracies by roughly the same amount, but again, not as much as adding the Noun-Verb data, which gave a 21.7% boost to SI accuracy. The efficacy of the Noun-Verb data in this case shows that directed training examples can Tuning Set Model WSJ NV WSJ Test Set Bohnet et al. <ref type="formula" target="#formula_0">(2018)</ref> 98.00±0.12 97.98±0.13 +ELMo 97.94±0.08 97.85±0.16 +NV Data 97.98±0.11 97.94±0.14 +ELMo+NV Data 97.97±0.09 97.94±0.13</p><p>Noun <ref type="table">Table 7</ref>: Effect of using different tuning sets. As usual with early stopping, the best tuning set per- formance was used to evaluate the test set. Here, we evaluated the same experimental runs at two points: when the performance was best on the WSJ development set, and again when the perfor- mance was best on the Noun-Verb development set. The increase in Noun-Verb results is signif- icant at the p &lt; 0.001( †) and p &lt; 0.01( ‡) levels.</p><note type="other">-Verb Test Set Bohnet et al. (2018) 74.0±1.2 76.9±0.6 † +ELMo 82.1±0.9 83.4±0.5 † +NV Data 86.4±0.4 86.8±0.4 +ELMo+NV Data 88.9±0.3 89.3±0.2 ‡</note><p>be especially beneficial for fixing some common error patterns. <ref type="table">Table 7</ref> compares perfor- mance of the same experiments on the WSJ and Noun-Verb Challenge test sets, tuned either us- ing the WSJ or the Noun-Verb development set. The only effect of the change in tuning set was for the Noun-Verb tuning to cause the early stopping to sometimes be a little earlier. When we tuned on the Noun-Verb development set, the WSJ re- sults remained almost unchanged, while the Noun- Verb test set results increased significantly. We see that the performance on each dataset is best when matched with its tuning data. The effect was greatest on the unenhanced model, which im- proved 2.9% absolute on the Noun-Verb evalua- tion. The best overall Noun-Verb test set result was 89.3±0.2 when tuned this way. <ref type="table" target="#tab_4">Table 8</ref> shows representative examples that the best baseline run got wrong, along with the pre- dictions from the best runs for each of the differ- ent enhancements. While each enhancement re- duces all error types, adding Noun-Verb data im- proves imperatives in particular when compared with adding ELMo. This holds true even when im- peratives are not sentence-initial, like the practice example in <ref type="table" target="#tab_4">Table 8</ref>. Of the errors made by our best model, roughly a quarter occurred when the focus word was a con- junction. This provides additional evidence for the importance of modeling non-local context in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Tuning Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Homograph Disambiguation</head><p>To show the impact of our best models on a down- stream task, we used the text-to-speech homo- graph disambiguation task described in <ref type="bibr" target="#b7">Gorman et al. (2018)</ref>. The dataset contains 161 word types, each of which has up to three possible pronun- ciations. In that work, the authors built a linear model that used lexical features of the focus word and its surrounding words, POS tags, and capital- ization, to achieve 95.4% on this task. Here, we want to see the effectiveness of our taggers by us- ing just the POS tag of each word to determine its pronunciation category. To do this, we anno- tated the homograph disambiguation train and test data with with POS tags using each of our taggers. We collected counts from the training corpus of the form &lt;word, POS tag, word_sense, Count&gt;. These counts show how many times a given word got assigned to a certain word sense when it has a certain POS tag. We used those counts to select the most frequent pronunciation for each &lt;word, POS tag&gt; pair on the test data. Note that this approach will miss some word senses that cannot be deter- mined from the word and POS tag only, like the difference in pronunciation of the word "jesus" be- tween English: /"dZi:z@s/ and Spanish: /heI"su:s/. <ref type="table" target="#tab_5">Table 9</ref> shows results for the micro and macro accrucies among different word types in the same way <ref type="bibr" target="#b7">(Gorman et al., 2018</ref>  <ref type="formula" target="#formula_0">2018)</ref>, which uses a wider context and more features. This is probably due to having a stronger POS tagger than the one used in that model. It is also interesting to see the gap between <ref type="bibr" target="#b26">Toutanova et al. (2003)</ref> and the rest of baseline taggers which was measured only on the Noun-Verb evaluation and not in WSJ evaluation. The rest of the re- sults show that using either ELMo achieves a 1.3% absolute improvement over the baseline. while adding data augmentation achieves 0.3% absolute improvement over the baseline. Using both ELMo +ELMo Example Gold Base +ELMo +Data +Data Will gets his revenge by masquerading as Sue's NOUN MD NNP MD NNP hairdresser and forcibly shaving her head bald. Will putting a patch over my eye help to get the VERB NN VB NN VB object out of it? If you don't have a table, you can mount the frame NOUN VB VB NN NN on a desk, stand, or other structure that will hold the bike off the ground. For best results, practice hitting one note higher VERB NN NN VB VB than your standard range. Spirit actually suggests unpacking their smokes by NOUN VB VB VB NN rolling the cigarette between your fingers, filter to end, so that a pinch or so of tobacco comes out. Choose the highest combat level and duel.</p><p>VERB NN NN NN VB  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Related Work</head><p>Dataset Creation Prior work in crowd-sourcing syntactic annotations and using them in mod- els motivated the dataset creation portion of this work. <ref type="bibr" target="#b10">Jha et al. (2010)</ref> showed that non-linguists could reliably do aspects of syntactic annota- tion, and <ref type="bibr" target="#b9">Hovy et al. (2014)</ref> showed that non- experts could annotate universal part-of-speech tags ( <ref type="bibr" target="#b18">Petrov et al., 2012</ref>) almost as well as experts. <ref type="bibr" target="#b8">He et al. (2016)</ref> then showed that incorporating crowd-sourced annotations improves parsing by a noticeable margin on the subset of sentences in which the human judgments affected the parser's output. Inspired by this result, we focused our ef- forts on collecting annotations that were likely to change a tagger's predictions and humans can an- notate reliably. This work filtered out trivial examples via hand- written heuristics targeted towards examples that taggers generally get correct (Section 2.1). One interesting direction for future work would be to eliminate this manual step. One option could be to instead use automatically produced high-precision interpretable rules to filter out these examples, such as the Anchor explanations output by <ref type="bibr" target="#b19">Ribeiro et al. (2018)</ref>. <ref type="table">Table 1</ref> in that paper shows how the system can automatically induce that a part-of- speech tagging system will tag the word play as a NOUN in the sentence I went to a play yesterday because the previous word is a determiner.</p><p>Measurement Manning (2011) performed an error analysis for WSJ and discovered that 19% of the errors fall under "Difficult linguistics" which need non-local context modeling to be able to solve them. The negative results of <ref type="bibr" target="#b11">Kiddon et al. (2015)</ref> on using existing supervised part-of-speech taggers for imperative detection provided motiva- tion for focusing on noun-verb confusion. How- ever we are not aware of any prior work on try- ing to measure part-of-speech-tagging accuracy on hard ambiguities that are easily recognized by hu- man using diverse corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>This paper proposes a challenge set approach to evaluating part-of-speech taggers, and builds a new resource for doing so. We show that a part-of- speech tagger can be trained to be better at noun- verb ambiguity by using extra Noun-Verb targeted training data or by adding contextual word em- bedding. We also show that our evaluation data can measure improvements in Noun-Verb disam- biguation that standard evaluation dataset was not able to capture. Those previously unmeasured improvements in the Noun-Verb disambiguation are shown to lead to improvements in a down- stream task. Improvements were especially large on sentence-initial tokens, which are often imper- atives. Even with these improvements, there is still a large gap between the noun-verb accuracies and overall WSJ tagging accuracy. We expect that closing this gap will make incorporating syntax more useful across natural language understand- ing applications.</p><p>Future work can include exploring ways to in- corporate more context into the tagger, possibly by using information from dependency tree. Also investigating more downstream tasks and explore if this dataset can be used directly in downstream tasks in a way similar to what have been done in <ref type="bibr" target="#b24">Swayamdipta et al. (2017)</ref> and <ref type="bibr" target="#b6">(Eriguchi et al., 2017;</ref><ref type="bibr" target="#b15">Niehues and Cho, 2017;</ref><ref type="bibr" target="#b12">Kiperwasser and Ballesteros, 2018</ref>) for injecting syntax in seman- tic role labeling and translation tasks. A third di- rection for research would be using this dataset to evaluate different contextual modeling approaches and investigate the creation and using such con- text sensitive dataset to create simpler and smaller models that can capture a lot of contextual word representation.</p><p>Future work on dataset creation can include generating similar challenge datasets for differ- ent key ambiguities in NLP. A collection of such datasets could be one way to cover hard exam- ples that models do not get right but humans are good at. Such targeted datasets can complement the use of large unsupervised contextual embed- ding models. This can open an avenue to improve core NLP tasks on hard relevant ambiguities that allows making progress on downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) reported their results. The overall results show similar trend to what is observed in the Noun-Verb evaluation results. The Choi (2016), and Bohnet et al. (2018) baseline tag- gers perform close to the full model in Gorman et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Inter-annotator agreement rates. Unani-
mous examples had 3/3 agreement, while majority 
examples had 2/3, 3/5, or 4/5 in agreement. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Development set accuracies on sentence 
initial (SI) tokens compared with non-sentence-
initial (¬SI) tokens. 

fine-grained POS tag. We used this dataset to aug-
ment the WSJ training data. Since the Noun-Verb 
examples only contain one labeled token per sen-
tence, we assigned the unlabeled tokens a cost of 
zero in the cost function at training time. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Development set examples that reflect the types of errors the enhancements address. Base is the 
tagger of Bohnet et al. (2018), while the remaining columns show the impact of the enhancements. Tags 
consistent with the gold annotations are in bold and inconsistent are in italics. 

Model 
Micro 
Macro 
Best ML system 
Gorman et al. (2018) 
95.4 
95.1 
Existing Taggers 
Toutanova et al. (2003) 91.1 
91.5 
Choi (2016) 
95.8 
95.8 
Dozat et al. (2017) 
94.6 
94.7 
Bohnet et al. (2018) 
95.9±0.2 95.9±0.2 
Enhancements 
+ELMo 
96.7±0.2 96.7±0.2 
+NV Data 
96.2±0.2 96.2±0.2 
+ELMo+NV Data 
96.7±0.3 96.7±0.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Accurcies of different models on the ho-
mograph disambiguation test set. All enhance-
ments' improvements over (Bohnet et al., 2018) 
baseline are statistically significant p &lt; 0.008. 
Standard deviations are estimated from n = 10 
random restarts, and p-values were computed us-
ing a heteroscedastic two-tailed t-test. 

and data augmentation was not better than just us-
ing ELMo. Those improvements correspond to 
a 28% error reduction compared to the machine-
learned model in Gorman et al. (2018). 

</table></figure>

			<note place="foot" n="1"> In experiments with recipe data in Kiddon et al. (2015), an unsupervised system had an F1 score over 20% higher in absolute terms than supervised taggers. 2 http://goo.gl/language/noun-verb</note>

			<note place="foot" n="4"> We exclude a short stop list (do, name, state); the final list contains 24,170 word types.</note>

			<note place="foot" n="10"> https://nlp.stanford.edu/software/ tagger.shtml 11 https://github.com/emorynlp/nlp4j</note>

			<note place="foot" n="12"> Two hyperparameter differences: we used two layers instead of three for the word component and a learning rate decay of 0.99994 instead of 0.999994. These were fixed early on and not tuned.</note>

			<note place="foot" n="13"> We used the &quot;Original&quot; model from https:// allennlp.org/elmo.</note>

			<note place="foot" n="14"> We also ran the experiment using the &quot;Original (5.5B)&quot; ELMo model, trained on a larger and more diverse corpus. We did not find any significant difference between the two.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors gratefully acknowledge their anony-mous reviewers for their insightful questions and feedback. We are grateful to Alexander Clines, Kazoo Sone, Ashwin Kakarla, Daphne Luong, and Austin Tarango for their assistance in the creation of the dataset. Thanks also go to members of the Google AI Language group for their input to this work, including Slav Petrov, Michael Collins, and all who provided feedback in reading group ses-sions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gon¸calo</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">To Appear) Proceedings of ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic feature induction: The last gist to the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="281" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised supervised learning i: Estimating classification and regression errors without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Donmez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnakumar</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1323" to="1351" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the conll 2017 shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to parse and translate improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving homograph disambiguation with supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Mazovetskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human-in-the-loop parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2337" to="2342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Experiments with crowdsourced re-annotation of a pos tagging data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Corpus creation for new genres: A crowdsourced approach to pp attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mise en place: Unsupervised interpretation of instructional recipes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloé</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thandavam</forename><surname>Ganesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Ponnuraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scheduled multi-task learning: From syntax to translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Part-of-speech tagging from 97% to 100%: is it time for some linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher D Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="171" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploiting linguistic resources for neural machine translation using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anchors: High-precision modelagnostic explanations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Linguistic input features improve neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Margin-based active learning for structured predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A corpus-based synthesizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised risk estimation using only conditional independence structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Percy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3657" to="3665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Frame-semantic parsing with softmax-margin segmental rnns and a syntactic scaffold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09528</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semisupervised active learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1039" to="1047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature-rich part-ofspeech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">shared task: Multilingual parsing from raw text to universal dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memduh</forename><surname>Gokirmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Nedoluzhko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvie</forename><surname>Cinkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic Jr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaroslava</forename><surname>Hlavacova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Václava</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zdenka</forename><surname>Uresova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stina</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nizar</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><surname>Depaiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Droganova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Héctor Martínez</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<editor>Kayadelen, Mohammed Attia, Ali Elkahky, Zhuoran Yu, Emily Pitler, Saran Lertpradit, Michael Mandl, Jesse Kirchner, Hector Fernandez Alcalde, Jana Strnadová, Esha Banerjee, Ruli Manurung, Antonio Stella, Atsuko Shimada, Sookyoung Kwak, Gustavo Mendonca, Tatiana Lando, Rattima Nitisaroj, and Josie Li</editor>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Umut Sulubacak, Hans Uszkoreit, Vivien Macketanz, Aljoscha Burchardt, Kim Harris, Katrin Marheinecke, Georg Rehm, Tolga; Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
