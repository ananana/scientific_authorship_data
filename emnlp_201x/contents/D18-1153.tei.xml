<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
							<email>xiangren@usc.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA †</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1215" to="1225"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1215</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific task. As different layers of the model keep different information, we develop a layer selection method for model pruning using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In model training, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Benefited from the recent advances in neural net- works (NNs) and the access to nearly unlim- ited corpora, neural language models are able to achieve a good perplexity score and generate high- quality sentences. These LMs automatically cap- ture abundant linguistic information and patterns from large text corpora, and can be applied to fa- cilitate a wide range of NLP applications <ref type="bibr" target="#b29">(Rei, 2017;</ref><ref type="bibr" target="#b20">Liu et al., 2018;</ref><ref type="bibr" target="#b26">Peters et al., 2018)</ref>.</p><p>Recently, efforts have been made on learning contextualized representations with pre-trained language models (LMs) ( <ref type="bibr" target="#b26">Peters et al., 2018</ref> Figure 1: Leverage the dense connectivity to compress models via layer selection, and replace wide and shal- low RNNs with deep and narrow ones.</p><p>up to 30% relative error reductions. However, due to high variability of language, gigantic NNs (e.g., LSTMs with 8,192 hidden states) are preferred to construct informative LMs and extract multifar- ious linguistic information ( <ref type="bibr" target="#b25">Peters et al., 2017)</ref>. Even though these models can be integrated with- out retraining (using their forward pass only), they still result in heavy computation workloads during inference stage, making them prohibitive for real- world applications.</p><p>In this paper, we aim to compress LMs for the end task in a plug-in-and-play manner. Typ- ically, NN compression methods require the re- training of the whole model ( <ref type="bibr" target="#b23">Mellempudi et al., 2017)</ref>. However, neural language models are usu- ally composed of RNNs, and their backpropaga- tions require significantly more RAM than their inference. It would become even more cumber- some when the target task equips the coupled LMs to capture information in both directions. There- fore, these methods do not fit our scenario very well. Accordingly, we try to compress LMs while avoiding costly retraining.</p><p>Intuitively, layers of different depths would capture linguistic information of different lev- els. Meanwhile, since LMs are trained in a task- agnostic manner, not all layers and their extracted information are relevant to the end task. Hence, we propose to compress the model by layer se- lection, which retains useful layers for the target task and prunes irrelevant ones. However, for the widely-used stacked-LSTM, directly pruning any layers will eliminate all subsequent ones. To over- come this challenge, we introduce the dense con- nectivity. As shown in <ref type="figure">Fig. 1</ref>, it allows us to detach any layers while keeping all remaining ones, thus creating the basis to avoid retraining. Moreover, such connectivity can stretch shallow and wide LMs to be deep and narrow <ref type="bibr" target="#b11">(Huang et al., 2017)</ref>, and enable a more fine-grained layer selection.</p><p>Furthermore, we try to retain the effective- ness of the pruned model. Specifically, we mod- ify the L 1 regularization for encouraging the se- lection weights to be not only sparse but bi- nary, which protects the retained layer connections from shrinkage. Besides, we design a layer-wise dropout to make LMs more robust and better pre- pared for the layer selection.</p><p>We refer to our model as LD-Net, since the layer selection and the dense connectivity form the basis of our pruning methods. For evaluation, we apply LD-Net on two sequence labeling bench- mark datasets, and demonstrated the effectiveness of the proposed method. In the CoNLL03 Named Entity Recognition (NER) task, the F 1 score in- creases from 90.78±0.24% to 91.86±0.15% by integrating the unpruned LMs. Meanwhile, after pruning over 90% calculation workloads from the best performing model 1 (92.03%), the resulting model still yields 91.84±0.14%. Our implemen- tations and pre-trained models would be released for futher study 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LD-Net</head><p>Given a input sequence of T word-level tokens, {x 1 , x 2 , · · · , x T }, we use x t to denote the embed- ding of x t . For a L-layers NN, we mark the input and output of the l th layer at the t th time stamp as x l,t and h l,t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RNN and Dense Connectivity</head><p>We represent one RNN layer as a function:</p><formula xml:id="formula_0">h l,t = F l (x l,t , h l,t−1 )<label>(1)</label></formula><p>where F l is the recurrent unit of l th layer, it could be any RNNs variants, and the vanilla LSTMs is used in our experiments. As deeper NNs usually have more representa- tion power, RNN layers are often stacked together to form the final model by setting x l,t = h l−1,t . These vanilla stacked-RNN models, however, suf- fer from problems like the vanishing gradient, and it's hard to train very deep models.</p><p>Recently, the dense connectivity and residual connectivity have been proposed to handle these problems ( <ref type="bibr" target="#b9">He et al., 2016;</ref><ref type="bibr" target="#b11">Huang et al., 2017)</ref>. Specifically, dense connectivity refers to adding direct connections from any layer to all its subse- quent layers. As illustrated in <ref type="figure">Figure 1</ref>, the input of l th layer is composed of the original input and the output of all preceding layers as follows.</p><formula xml:id="formula_1">x l,t = [x t , h 1,t , · · · , h l−1,t ]</formula><p>Similarly, the final output of the L-layer RNN is</p><formula xml:id="formula_2">h t = [x t , h 1,t , · · · , h L,t ].</formula><p>With dense connectiv- ity, we can detach any single layer without elim- inating its subsequent layers (as in <ref type="figure">Fig. 1</ref>). Also, existing practices in computer vision demonstrate that such connectivities can lead to deep and nar- row NNs and distribute parameters into different layers. Moreover, different layers in LMs usually capture linguistic information of different levels. Hence, we can compress LMs for a specific task by pruning unrelated or unimportant layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Modeling</head><p>Language modeling aims to describe the sequence generation. Normally, the generation probability of the sequence {x 1 , · · · , x T } is defined in a "for- ward" manner:</p><formula xml:id="formula_3">p(x 1 , · · · , x T ) = T t=1 p(x t |x 1 , · · · , x t−1 ) (2)</formula><p>Where p(x t |x 1 , · · · , x t−1 ) is computed based on the output of RNN, h t . Due to the dense con- nectivity, h t is composed of outputs from different layers, which are designed to capture linguistic in- formation of different levels. Similar to the bot- tleneck layers employed in the DenseNet ( <ref type="bibr" target="#b11">Huang et al., 2017)</ref>, we add additional layers to unify such information. Accordingly, we add an projection layer with the ReLU activation function:</p><formula xml:id="formula_4">h * t = ReLU(W proj · h t + b proj )<label>(3)</label></formula><p>Based on h * t , it's intuitive to calculate p(x t |x 1 , · · · , x t−1 ) by the softmax function, i.e., softmax(W out · h * t + b). Since the training of language models needs nothing but the raw text, it has almost unlimited corpora. However, conducting training on ex- tensive corpora results in a huge dictionary, and makes calculating the vanilla softmax intractable. Several techniques have been proposed to handle this problem, including adaptive softmax ( <ref type="bibr" target="#b6">Grave et al., 2017)</ref>, slim word embedding ( <ref type="bibr" target="#b19">Li et al., 2018)</ref>, the sampled softmax and the noise con- trastive estimation <ref type="bibr">(Józefowicz et al., 2016)</ref>. Since the major focus of our paper does not lie in the language modeling task, we choose the adaptive softmax because of its practical efficiency when accelerated with GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contextualized Representations</head><p>As pre-trained LMs can describe the text genera- tion accurately, they can be utilized to extract in- formation and construct features for other tasks. These features, referred as contextualized repre- sentations, have been demonstrated to be essen- tially useful ( <ref type="bibr" target="#b26">Peters et al., 2018)</ref>. To capture in- formation from both directions, we utilized not only forward LMs, but also backward LMs. Back- ward LMs are based on Eqn. 4 instead of Eqn. 2. Similar to forward LMs, backward LMs approach p(x t |x t+1 , · · · , x T ) with NNs. For reference, the output of the RNN in backward LMs for x t is recorded as h r t .</p><formula xml:id="formula_5">p(x 1 , · · · , x n ) = T t=1 p(x t |x t+1 , · · · , x T ) (4)</formula><p>Ideally, the final output of LMs (e.g., h * t ) would be the same as the representation of the target word (e.g., x t+1 ); therefore, it may not contain much context information. Meanwhile, the output of the densely connected RNN (e.g., h t ) includes outputs from every layer, thus summarizing all ex- tracted features. Since the dimensions of h t could be too large for the end task, we add a non-linear transformation to calculate the contextualized rep- resentation (r t ):</p><formula xml:id="formula_6">r t = ReLU(W cr · [h t , h r t ] + b cr )<label>(5)</label></formula><p>Our proposed method bears the same intuition as the ELMo ( <ref type="bibr" target="#b26">Peters et al., 2018</ref>). ELMo is de- signed for the vanilla stacked-RNN, and tries to calculate a weighted average of different layers' outputs as the contextualized representation. Our method, benefited from the dense connectivity and its narrow structure, can directly combine the out- puts of different layers by concatenation. It does not assume the outputs of different layers to be in the same vector space, thus having more potential for transferring the constructed token representa- tions. More discussions are available in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Layer Selection</head><p>Typical model compression methods require re- training or gradient calculation. For the coupled LMs, these methods require even more computa- tion resources compared to the training of LMs, thus not fitting our scenario very well.</p><p>Benefited from the dense connectivity, we are able to train deep and narrow networks. Moreover, we can detach one of its layer without eliminating all subsequent layers (as in <ref type="figure">Fig. 1</ref>). Since different layers in NNs could capture different linguistic in- formation, only a few of them would be relevant or useful for a specific task. As a result, we try to compress these models by the task-guided layer selection. For i-th layer, we introduce a binary mask z i ∈ {0, 1} and calculate h l,t with Eqn. 6 instead of Eqn. 1.</p><formula xml:id="formula_7">h l,t = z i · F l (x l,t , h l,t−1 )<label>(6)</label></formula><p>With this setting, we can conduct a layer selection by optimizing the regularized empirical risk:</p><formula xml:id="formula_8">min L + λ 0 · R (7)</formula><p>where L is the empirical risk for the sequence la- beling task and R is the sparse regularization. The ideal choice for R would be the L 0 regu- larization of z, i.e., R 0 (z) = |z| 0 . However, it is not continuous and cannot be efficiently opti- mized. Hence, we relax z i from binary to a real value (i.e., 0 ≤ z i ≤ 1) and replace R 0 by:</p><formula xml:id="formula_9">R 1 = |z| 1</formula><p>Despite the sparsity achieved by R 1 , it could hurt the performance by shifting all z i far away from 1. Such shrinkage introduces additional noise in h l,t and x l,t , which may result in inef- fective pruned LMs. Since our goal is to conduct pruning without retraining, we further modify the L 1 regularization to achieve sparsity while allevi- ating its shrinkage effect. As the target of R is to make z sparse, it can be "turned-off" after achiev- ing a satisfying sparsity. Therefore, we extend R 1 to a margin-based regularization:</p><formula xml:id="formula_10">R 2 = δ(|z| 0 &gt; λ 1 )|z| 1</formula><p>In addition, we also want to make up the re- laxation made on z, i.e., relaxing its values from binary to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Accordingly, we add the penalty |z(1 − z)| 1 to encourage z to be binary (Murray and Ng, 2010) and modify R 2 into R 3 :</p><formula xml:id="formula_11">R 3 = δ(|z| 0 &gt; λ 1 )|z| 1 + |z(1 − z)| 1</formula><p>To compare R 1 , R 2 and R 3 , we visualize their penalty values in <ref type="figure" target="#fig_0">Fig. 2</ref>. The visualization is generated for a 3-dimensional z while the tar- geted sparsity, λ 1 , is set to 2. Comparing to R 1 , we can observe that R 2 enlarges the optimal point set from 0 to all z with a satisfying spar- sity, thus avoiding the over-shrinkage. To better demonstrate the effect of R 3 , we further visualize its penalties after achieving a satisfying sparsity (w.l.o.g., assuming z 3 = 0). One can observe that it penalizes non-binary z and favors binary values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Layer-wise Dropout</head><p>So far, we've customized the regularization term for the layer-wise pruning, which protects the re- tained connections among layers from shrinking. After that, we try to further retain the effective- ness of the compressed model. Specifically, we choose to prepare the LMs for the pruned inputs, thus making them more robust to pruning.</p><p>Accordingly, we conduct the training of LMs with a layer-wise dropout. As in <ref type="figure" target="#fig_2">Figure 3</ref>, a random part of layers in the LMs are randomly dropped during each batch. The outputs of the dropped layers will not be passed to their subse- quent recurrent layers, but will be sent to the pro- jection layer (Eqn. 3) for predicting the next word.  In other words, this dropout is only applied to the input of recurrent layers, which aims to imitate the pruned input without totally removing any layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence Labeling</head><p>In this section, we will introduce our sequence la- beling architecture, which is augmented with the contextualized representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Architecture</head><p>Following the recent studies ( <ref type="bibr" target="#b20">Liu et al., 2018;</ref><ref type="bibr" target="#b17">Kuru et al., 2016)</ref>, we construct the neural archi- tecture as in <ref type="figure" target="#fig_3">Fig. 4</ref>. Given the input sequence {x 1 , x 2 , · · · , x T }, for t th token (x t ), we assume its word embedding is w t , its label is y t , and its character-level input is {c i,1 , c i,2 , · · · , c i, }, where c i, is the space character following x t .</p><p>The character-level representations have be- come the required components for most of the state-of-the-art. Following the recent study ( <ref type="bibr" target="#b20">Liu et al., 2018)</ref>, we employ LSTMs to take the character-level input in a context-aware manner, and mark its output for x t as c t . Similar to the contextualized representation, c t usually has more dimensions than w t . To integrate them together, we set the output dimension of Eqn. 5 as the di- mension of w t , and project c t to a new space with the same dimension number. We mark the pro- jected character-level representation as c * t .  After projections, these vectors are concate- nated as v t = [c * t ; r t ; w t ], ∀i ∈ [1, T ] and fur- ther fed into the word-level LSTMs. We refer to their output as U = {u 1 , · · · , u T }. To ensure the model to predict valid label sequences, we ap- pend a first-order conditional random field (CRF) layer to the model ( <ref type="bibr" target="#b18">Lample et al., 2016)</ref>. Specifi- cally, the model defines the generation probability of y = {y 1 , · · · , y T } as</p><formula xml:id="formula_12">p(y|U) = T t=1 φ(y t−1 , y t , u t ) ˆ y∈Y(U) T t=1 φ(ˆ y t−1 , ˆ y t , u t )<label>(8)</label></formula><p>wherê y = {ˆy{ˆy 1 , . . . , ˆ y T } is a generic label se- quence, Y(U) is the set of all generic label se- quences for U and φ(y t−1 , y t , u t ) is the potential function. In our model, φ(y t−1 , y t , u t ) is defined as exp(W yt u t + b y t−1 ,yt ), where W yt and b y t−1 ,yt are the weight and bias parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training and Inference</head><p>We use the following negative log-likelihood as the empirical risk.</p><formula xml:id="formula_13">L = − U log p(y|U)<label>(9)</label></formula><p>For testing or decoding, we want to find the op- timal sequence y * that maximizes the likelihood.</p><formula xml:id="formula_14">y * = argmax y∈Y(U) p(y|U)<label>(10)</label></formula><p>Although the denominator of Eq. 8 is complicated, we can calculate Eqs. 9 and 10 efficiently by the Viterbi algorithm. For optimization, we decompose it into two steps, i.e., model training and model pruning. Model training. We set λ 0 to 0 and optimize the empirical risk without any regularization, i.e., min L. In this step, we conduct optimization with the stochastic gradient descent with momentum. Following ( <ref type="bibr" target="#b26">Peters et al., 2018)</ref>, dropout would be added to both the coupled LMs and the sequence labeling model. Model pruning. We conduct the pruning based on the checkpoint which has the best performance on the development set during the model train- ing. We set λ 0 to non-zero values and optimize min L + λ 0 R 3 by the projected gradient descent with momentum. Any layer i with z i = 0 would be deleted in the final model to complete the prun- ing. To get a better stability, dropout is only added to the sequence labeling model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We will first discuss the capability of the LD-Net as language models, then explore the effectiveness of its contextualized representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Language Modeling</head><p>For comparison, we conducted experiments on the one billion word benchmark dataset ( <ref type="bibr" target="#b0">Chelba et al., 2013</ref>) with both LD-Net (with 1,600 dimensional projection) and the vanilla stacked-LSTM. Both kinds of models use word embedding (random ini- tialized) of 300 dimension as input and use the adaptive softmax (with default setting) as an ap- proximation of the full softmax. Additionally, as preprocessing, we replace all tokens occurring equal or less than 3 times with as UNK, which shrinks the dictionary from 0.79M to 0.64M.</p><p>The optimization is performed by the Adam al- gorithm <ref type="bibr" target="#b15">(Kingma and Ba, 2014</ref>), the gradient is clipped at 5.0 and the learning rate is set to start from 0.001. The layer-wise dropout ratio is set to 0.5, the RNNs are unrolled for 20 steps with- out resetting the LSTM states, and the batch size is set to 128. Their performances are summa- rized in <ref type="table">Table 1</ref>, together with several LMs used in our sequence labeling baselines. For models with- out official reported parameter numbers, we esti- mate their values (marked with † ) by assuming they adopted the vanilla LSTM. Note that, for models 3, 5, 6, 7, 8, and 9, PPL refers to the averaged per- plexity of the forward and the backward LMs.</p><p>We can observe that, for those models tak- ing word embedding as the input, embedding composes the vast majority of model parame- ters. However, embedding can be embodied as a "sparse" layer which is computationally efficient. Instead, the intense calculations are conducted in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Ind Comparing LD-Net with other baselines, we think it achieves satisfactory performance with re- gard to the size of hidden states. It demonstrates the LD-Net's capability of capturing the underly- ing structure of natural language. Meanwhile, we find that the layer-wise dropout makes it harder to train LD-Net and its resulting model achieves less competitive results. However, as would be discussed in the next section, layer-wise dropout allows the resulting model to generate better con- textualized representations and be more robust to pruning, even with a higher perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sequence Labeling</head><p>Following TagLM ( <ref type="bibr" target="#b25">Peters et al., 2017)</ref>, we eval- uate our methods in two benchmark datasets, the CoNLL03 NER task <ref type="bibr" target="#b33">(Tjong Kim Sang and De Meulder, 2003</ref>) and the CoNLL00 Chunking task <ref type="bibr" target="#b32">(Tjong Kim Sang and Buchholz, 2000</ref>). CoNLL03 NER has four entity types and includes the standard training, development and test sets. CoNLL00 chunking defines eleven syntactic chunk types (e.g., NP and VP) in addition to Other. Since it only includes training and test sets, we sampled 1000 sentences from training set as a held-out development set ( <ref type="bibr" target="#b25">Peters et al., 2017)</ref>.</p><p>In both cases, we use the BIOES labeling scheme <ref type="bibr" target="#b28">(Ratinov and Roth, 2009)</ref> and use the micro-averaged F 1 as the evaluation metric. Based on the analysis conducted in the development set, we set λ 0 = 0.05 for the NER task, and λ 0 = 0.5 for the Chunking task. As discussed before, we conduct optimization with the stochastic gradient descent with momentum. We set the batch size, the momentum, and the learning rate to 10, 0.9, and η t = η 0 1+ρt respectively. Here, η 0 = 0.015 is the initial learning rate and ρ = 0.05 is the de- cay ratio. Dropout is applied in our model, and its ratio is set to 0.5. For a better stability, we use gra- dient clipping of 5.0. Furthermore, we employ the early stopping in the development set and report averaged score across five different runs.</p><p>Regarding the network structure, we use the 30-dimension character-level embedding. Both character-level and word-level RNNs are set to one-layer LSTMs with 150-dimension hidden states in each direction. The GloVe 100-dimension pre-trained word embedding 3 is used as the initial- ization of word embedding w t , and will be fine- tuned during the training. The layer selection vari- ables z i are initialized as 1, remained unchanged Network Avg.</p><p>#FLOPs  during the model training and only be updated dur- ing the model pruning. All other variables are ran- domly initialized <ref type="bibr" target="#b4">(Glorot and Bengio, 2010</ref>). Compared methods. The first baseline, referred as NoLM, is our sequence labeling model with- out the contextualized representations, i.e., calcu-</p><formula xml:id="formula_15">F 1 score (LMs Ind.#) ppl (·10 6 )<label>(avg±std</label></formula><formula xml:id="formula_16">lating v t as [c * t ; w t ] instead of [c * t ; r t ; w t ].</formula><p>Be- sides, <ref type="bibr">ELMo (Peters et al., 2018</ref>) is the major base- line. To make comparison more fair, we imple- mented the ELMo model and use it to calculate the r t in Eqn. 5 instead of [h t , h r t ]. Results of re- implemented models are referred with R-ELMo (λ is set to the recommended value, 0.1) and the results reported in its original paper are referred with O-ELMo. Additionally, since <ref type="bibr">TagLM (Peters et al., 2017</ref>) with one-layer NNs can be viewed as a special case of ELMo, we also include its results. Sequence labeling results. <ref type="table" target="#tab_4">Table 2</ref> and 3 sum- marizes the results of LD-Net and baselines. Be- sides the F 1 score and averaged perplexity, we also estimate FLOPs (i.e., the number of floating- point multiplication-adds) for the efficiency evalu- ation. Since our model takes both word-level and character-level inputs, we estimated the FLOPs value for a word-level input with 4.39 character- level inputs, while 4.39 is the averaged length of words in the CoNLL03 dataset.</p><p>Before the model pruning, LD-Net achieves a 96.05±0.08 F 1 score in the CoNLL00 Chunking task, yielding nearly 30% error reductions over the NoLM baseline. Also, it scores 91.86±0.15 F 1 in the CoNLL03 NER task with over 10% error re- ductions. Similar to the language modeling, we Network Avg. #FLOPs</p><formula xml:id="formula_17">F 1 score (LMs Ind.#) ppl (·10 6 )<label>(avg±std)</label></formula><p>NoLM <ref type="formula">(</ref>  observe that the most complicated models achieve the best perplexity and provide the most improve- ments in the target task. Still, considering the number of model parameters and the resulting per- plexity, our model demonstrates its effectiveness in generating contextualized representations. For example, comparing to our methods, R-ELMo <ref type="formula">(7)</ref> leverages LMs with the similar perplexity and pa- rameter number, but cannot get the same improve- ments with our method on both datasets. Actually, contextualized representations have strong connections with the skip-thought vec- tors ( <ref type="bibr" target="#b16">Kiros et al., 2015)</ref>. Skip-thought models try to embed sentences and are trained by predict- ing the previous and afterwards sentences. Sim- ilarly, LMs encode a specific context as the hid- den states of RNNs, and use them to predict fu- ture contexts. Specifically, we recognize the cell states of LSTMs are more like to be the sentence embedding ( <ref type="bibr" target="#b27">Radford et al., 2017</ref>), since they are only passed to the next time stamps. At the same time, because the hidden states would be passed to other layers, we think they are more like to be the token representations capturing necessary signals for predicting the next word or updating context representations <ref type="bibr">4</ref>   effective then ELMo, as concatenating could pre- serve all extracted signals while weighted average might cause information loss. Although the layer-wise dropout makes the model harder to train, their resulting LMs generate better contextualized representations, even with- out the same perplexity. Also, as discussed in <ref type="bibr" target="#b26">(Peters et al., 2018</ref><ref type="bibr" target="#b25">(Peters et al., , 2017</ref>, the performance of the contextualized representation can be further im- proved by training larger models or using the CNN to represent words.</p><p>For the pruning, we started from the model with the best performance on the development set (re- ferred with "origin"), and refer the performances of pruned models with "pruned" in <ref type="table" target="#tab_4">Table 2</ref> and 3. Essentially, we can observe the pruned models get rid of the vast majority of calculation while still re- taining a significant improvement. We will discuss more on the pruned models in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Speed Up Measurements</head><p>We use FLOPs for measuring the inference ef- ficiency as it reflects the time complexity <ref type="bibr" target="#b7">(Han et al., 2015)</ref>, and thus is independent of spe- cific implementations. For models with the same structure, improvements in FLOPs would result in monotonically decreasing inference time. How- ever, it may not reflect the actual efficiency of models due to the model differences in paral- lelism. Accordingly, we also tested wall-clock speeds of our implementations.</p><p>Our implementations are based on the PyTorch 0.3.1 5 , and all experiments are conducted on the CoNLL03 dataset with the Nvidia GTX 1080 GPU. Specifically, due to the limited size of CoNLL03 test set, we measure such speeds on the training set. As in <ref type="table" target="#tab_8">Table 4</ref>, we can observe that, the pruned model achieved about 5 times speed up. Although there is still a large margin between We think it implies that ELMo works as token representations instead of sentence representations 5 http://pytorch.org/ the actual speed-up and the FLOPs speed-up, we think the resulting decode speed (166K words/s) is sufficient for most real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Studies</head><p>Effect of the pruning ratio. To explore the effect of the pruning ratio, we adjust λ 1 and visualize the performance of pruned models v.s. their FLOPs # in <ref type="figure" target="#fig_4">Fig 5.</ref> We can observe that LD-Net outperforms its variants and demonstrates its effectiveness. As the pruning ratio becoming larger, we can observe the performance of LD-Net first increases a little, then starts dropping. Besides, in the CoNLL03 NER task, LMs can be pruned to a rel- atively small size without much loss of efficiency. As in <ref type="table" target="#tab_6">Table 3</ref>, we can observe that, after prun- ing over 90% calculations, the error of the re- sulting model only increases about 2%, yielding a competitive performance. As for the CoNLL00 Chunking task, the performance of LD-Net decays in a faster rate than that in the NER task. For ex- ample, after pruning over 80% calculations, the error of the resulting model increases about 13%. Considering the fact that this corpus is only half the size of the CoNLL03 NER dataset, we can ex- pect the resulting models have more dependencies on the LMs. Still, the pruned model achieves a 25% error reduction over the NoLM baseline. Layer selection pattern. We further studied the layer selection patterns. Specifically, we use the same setting of LD-Net (9) in <ref type="table" target="#tab_6">Table 3</ref>, conduct model pruning using for 50 times, and summa- rize the statics in <ref type="figure" target="#fig_5">Figure 6</ref>. We can observe that network layers formulate two clear clusters, one is likely to be preserved during the selection, and the other is likely to be pruned. This is consistent with our intuition that some layers are more impor- tant than others and the layer selection algorithm would pick up layers meaningfully. However, there is some randomness in the se- lection result. We conjugate that large networks trained with dropout can be viewed as a ensem- ble of small sub-networks ( <ref type="bibr" target="#b8">Hara et al., 2016)</ref>, also there would be several sub-networks having the similar function. Accordingly, we think the ran- domness mainly comes from such redundancy. Effectiveness of model pruning. <ref type="bibr" target="#b35">Zhu and Gupta (2017)</ref> observed pruned large models consistently outperform small models on various tasks (includ- ing LM). These observations are consistent with our experiments. For example, LD-Net achieves 91.84 after pruning on the CoNLL03 dataset. It outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54. Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs). Its per- plexity is 69 and its performance on the CoNLL03 dataset is 91.55 ± 0.19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Sequence labeling. Linguistic sequence labeling is one of the fundamental tasks in NLP, encom- passing various applications including POS tag- ging, chunking, and NER. Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted fea- tures ( <ref type="bibr" target="#b1">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b21">Ma and Hovy, 2016)</ref>.</p><p>Language modeling. Language modeling is a core task in NLP. Many attempts have been paid to develop better neural language models <ref type="bibr" target="#b36">(Zilly et al., 2017;</ref><ref type="bibr" target="#b13">Inan et al., 2016;</ref><ref type="bibr" target="#b5">Godin et al., 2017;</ref><ref type="bibr" target="#b22">Melis et al., 2017)</ref>. Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch <ref type="bibr" target="#b14">(Józefowicz et al., 2016;</ref><ref type="bibr" target="#b6">Grave et al., 2017;</ref><ref type="bibr" target="#b19">Li et al., 2018;</ref><ref type="bibr" target="#b31">Shazeer et al., 2017)</ref>. Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods. Some methods treat the language modeling as an additional su- pervision, and conduct co-training for knowledge transfer <ref type="bibr" target="#b3">(Dai and Le, 2015;</ref><ref type="bibr" target="#b20">Liu et al., 2018;</ref><ref type="bibr" target="#b29">Rei, 2017)</ref>. Others, including this paper, aim to con- struct additional features (referred as contextual- ized representations) with the pre-trained language models ( <ref type="bibr" target="#b25">Peters et al., 2017</ref><ref type="bibr" target="#b26">Peters et al., , 2018</ref>. Neural Network Acceleration. There are mainly three kinds of NN acceleration methods, i.e., prune network into smaller sizes <ref type="bibr" target="#b7">(Han et al., 2015;</ref><ref type="bibr" target="#b34">Wen et al., 2016)</ref>, converting float operation into customized low precision arithmetic ( <ref type="bibr" target="#b12">Hubara et al., 2018;</ref><ref type="bibr" target="#b2">Courbariaux et al., 2016)</ref>, and using shallower networks to mimic the output of deeper ones <ref type="bibr" target="#b10">(Hinton et al., 2015;</ref><ref type="bibr" target="#b30">Romero et al., 2014</ref>). However, most of them require costly retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Here, we proposed LD-Net, a novel framework for efficient contextualized representation. As demonstrated on two benchmarks, it can conduct the layer-wise pruning for a specific task. More- over, it requires neither the gradient oracle of LMs nor the costly retraining. In the future, we plan to apply LD-Net to other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Penalty values of various R for z with three dimensions. λ 1 has been set to 2 for R 2 and R 3 .</figDesc><graphic url="image-1.png" coords="4,72.00,62.80,453.55,99.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Layer-wise dropout conducted on a 4-layer densely connected RNN. (a) is the remained RNN. (b) is the original densely connected RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The proposed sequence labeling architecture with contextualized representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of pruned models in two tasks w.r.t. their efficiency (FLOPs).</figDesc><graphic url="image-2.png" coords="8,426.92,207.11,98.59,136.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The performance of pruned models in two tasks w.r.t. their efficiency (FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparisons in the CoNLL00 
Chunking task. LD-Net maked with  *  are trained with-
out pruning (layer selection). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance comparison in the CoNLL03 
NER task. Models marked with  † employed LSTMs 
with projection, which is more efficient than the vanilla 
LSTMs. LD-Net maked with  *  are trained without 
pruning (layer selection). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Speed comparison in the CoNLL03 NER task. We can observe that LD-Net (9, pruned) achieved about 5 
times speed up on the wall-clock time over LD-Net (9, origin). 

</table></figure>

			<note place="foot" n="3"> https://nlp.stanford.edu/projects/glove/</note>

			<note place="foot" n="4"> We tried to combine the cell states with the hidden states to construct the contextualized representations by concatenation or weighted average, but failed to get better performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Junliang Guo and all reviewers for their constructive comments. Research was sponsored by the Army Research Laboratory and was ac-complished under Cooperative Agreement Num-ber W911NF-09-2-0053 (the ARL Network Sci-ence CTA). The views and conclusions in this doc-ument are those of the authors and should not be interpreted as representing the official policies, ei-ther expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and dis-tribute reprints for Government purposes notwith-standing any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Thorsten Brants, Phillipp Koehn, and Tony Robinson</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ran El-Yaniv, and Yoshua Bengio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
	</analytic>
	<monogr>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving language modeling using densely connected recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fréderic</forename><surname>Godin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Dambre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>De Neve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient softmax approximation for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of dropout learning regarded as ensemble learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Saitoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayaru</forename><surname>Shouno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">187</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ran El-Yaniv, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tying word vectors and word classifiers: A loss framework for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khashayar</forename><surname>Hakan Inan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1611.01462</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Charner: Character-level named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Kuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Ozan Arkan Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="911" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Slim embedding layers for recurrent neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Kulhanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1711.09873</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1709.04109</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ternary neural networks with finegrained quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Mellempudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhisek</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<idno>abs/1705.01462</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for nonlinear optimization problems with binary variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kien-Ming</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="288" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations. In NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi-supervised multitask learning for sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6550</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2000 shared task: Chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning language in logic and CoNLL</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural language learning at NAACL-HLT</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1710.01878</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent Highway Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
