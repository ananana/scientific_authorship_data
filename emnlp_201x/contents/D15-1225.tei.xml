<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Unsupervised Bayesian Modelling Approach to Storyline Detection from News Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
							<email>d.zhou@seu.edu.cn, h.xu@seu.edu.cn, y.he@cantab.net</email>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering and Applied Science</orgName>
								<orgName type="institution">Aston University</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Unsupervised Bayesian Modelling Approach to Storyline Detection from News Articles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Storyline detection from news articles aims at summarizing events described under a certain news topic and revealing how those events evolve over time. It is a difficult task because it requires first the detection of events from news articles published in different time periods and then the construction of storylines by linking events into coherent news stories. Moreover , each storyline has different hierarchical structures which are dependent across epochs. Existing approaches often ignore the dependency of hierarchical structures in storyline generation. In this paper, we propose an unsupervised Bayesian model, called dynamic storyline detection model, to extract structured representations and evolution patterns of storylines. The proposed model is evaluated on a large scale news corpus. Experimental results show that our proposed model outperforms several baseline approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rapid development of online news media sites is accompanied by the generation of tremendous news reports. Facing such massive amount of news articles, it is crucial to develop an automat- ed tool which can provide a temporal summary of events and their evolutions related to a topic from news reports. Therefore, storyline detection, aim- ing at summarising the development of certain re- lated events, has been studied in order to help read- ers quickly understand the major events reported in news articles. It has attracted great attention re- cently. <ref type="bibr" target="#b4">Kawamae (2011)</ref> proposed a trend analy- sis model which used the difference between tem- poral words and other words in each documen- t to detect topic evolution over time. <ref type="bibr" target="#b0">Ahmed et al. (2011)</ref> proposed a unified framework to group temporally and topically related news articles in- to same storylines in order to reveal the tempo- ral evolution of events. <ref type="bibr" target="#b9">Tang and Yang (2012)</ref> de- veloped a topic-user-trend model, which incorpo- rates user interests into the generative process of web contents. <ref type="bibr" target="#b8">Radinsky and Horvitz (2013)</ref> built storylines based on text clustering and entity en- tropy to predict future events. <ref type="bibr" target="#b3">Huang and Huang (2013)</ref> developed a mixture-event-aspect model to model sub-events into local and global aspects and utilize an optimization method to generate story- lines. <ref type="bibr" target="#b10">Wang et al. (2013)</ref> proposed an evolutionary multi-branch tree clustering method for streaming text data in which the tree construction is casted as an online posterior estimation problem by con- sidering both the current tree and the previous tree simultaneously.</p><p>With the fast development of social media plat- forms, newsworthy events are widely scattered not only on traditional news media but also on social media ( <ref type="bibr" target="#b11">Zhou et al., 2015)</ref>. For example, Twit- ter, one of the most widely adopted social medi- a platforms, appears to cover nearly all newswire events ( <ref type="bibr" target="#b7">Petrovic et al., 2013)</ref>. Therefore, ap- proaches have also been proposed for storyline summarization on social media. Given a user in- put query of an ongoing event, <ref type="bibr" target="#b6">Lin et al. (2012)</ref> ex- tracted the storyline of an event by first obtaining relevant tweets and then generating storylines via graph optimization. In ( <ref type="bibr" target="#b5">Li and Li, 2013)</ref>, an evo- lutionary hierarchical Dirichlet process was pro- posed to capture the topic evolution pattern in sto- ryline summarization.</p><p>However, most of the aforementioned ap- proaches do not represent events in the form of structured representation. More importantly, they ignore the dependency of the hierarchical struc- tures of events at different epochs in a storyline. In this paper, we propose a dynamic storyline de- tection model to overcome the above limitations.</p><p>We assume that each document could belong to one storyline s, which is modelled as a joint dis- tribution over some named entities e and a set of topics z. Furthermore, to link events at different epochs and detect different types of storylines, the weighted sum of storyline distribution of previous epochs is employed as the prior of the current s- toryline distribution. The proposed model is eval- uated on a large scale news corpus. Experimental results show that our proposed model outperforms several baseline approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>To model the generation of a storyline in con- secutive time periods for a stream of documents, we propose an unsupervised latent variable mod- el, called dynamic storyline detection model (DS- DM), The graphical model of DSDM is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In this model, we assume that the storyline- topic-word, storyline-topic and storyline-entity probabilities at time t are dependent on the previous storyline-topic-word, storyline-topic and storyline-entity distributions in the last M epochs. For a certain period of time, we assume that each document could belong to one storyline s, which is modelled as a joint distribution over some named entities e and a set of topics z. This assumption es- sentially encourages documents published around similar time that involve the same named entities and discuss similar topics to be grouped into the same storyline. As the storyline distribution is shared across documents with the same named en- tities and similar topics, it essentially preserves the ambiguity that for example, documents compris- ing the same person and location may or may not belong to the same storyline.</p><p>The generative process of DSDM is shown be- low:</p><p>For each time period t from 1 to T :</p><p>• Draw a distribution over storylines π t s ∼ Dirichlet(γ t s ).</p><p>• For each storyline s ∈ {1...S}:</p><formula xml:id="formula_0">-Draw a distribution over topics θ t s ∼ Dirichlet(α t s ). -Draw a distribution over named entities ω t s ∼ Dirichlet(ϵ t s ). -For each topic k ∈ {1...K}, draw a word distri- bution φ t s,k ∼ Dirichlet(β t s ). • For each document d ∈ {1...D}: -Choose a storyline indicator s t d ∼ Multinomial(π t s ). -For each named entity e ∈ {1...E d }: * Choose a named entity e ∼ Multinomial(ω t s ). -For other word positions n ∈ {1...N d }: * Choose a topic zn ∼ Multinomial(θ t s ). * Choose a word wn ∼ Multinomial(φ t s,z ).</formula><p>We define an evolutionary matrix of storyline indicator s and topic z, σ t s,z,m , where each colum- n σ t s,z,m denotes storyline-topic-word distribution of storyline indicator s and topic z at epoch m, an evolutionary topic matrix of storyline indicator s, τ t s , where each column τ t s,m denotes storyline- topic distribution of storyline indicator at epoch m, an evolutionary entity matrix of storyline in- dicator s, υ t s , where each column υ t s,m denotes storyline-entity distribution of storyline indicator s.</p><p>We attach a vector of</p><formula xml:id="formula_1">M + 1 weights µ t s,z = {µ t s,z,m } M m=0 (µ t s,z,m &gt; 0, ∑ M m=0 µ t s,z,m = 1)</formula><p>, with its components representing the weights that each σ t s,z,m contributes to calculating the priors of φ t s,z . We do it similarly for θ t s and ω t s . The Dirich- let prior for the storyline-topic-word distribution, the storyline-topic distribution and the storyline- entity distribution, respectively, at epoch t are:</p><formula xml:id="formula_2">β t s,z = M ∑ m=0 µ t s,z,m × σ t s,z,m<label>(1)</label></formula><formula xml:id="formula_3">α t s = M ∑ m=0 µ t s,m × τ t s,m<label>(2)</label></formula><formula xml:id="formula_4">ϵ t s = M ∑ m=0 µ t s,m × υ t s,m<label>(3)</label></formula><p>In our experiments, the weight parameters are set to be the same regardless of storylines or top- ics. They are only dependent on the time win- dow using an exponential decay function, µ m = exp(−0.5 × m) where m stands for the mth epoch counting backwards in the past M epochs. That is, more recent documents would have a relatively stronger influence on the model parameters in the current epoch compared to earlier documents. It is also possible to estimate the weights directly from data. We leave it as our future work.</p><p>The storyline-topic-word distribution φ t s,z , the storyline-topic distribution θ t s and the storyline- entity distribution ω t s at the current epoch t are generated from the Dirichlet distribution param- eterized by</p><formula xml:id="formula_5">β t s,z , α t s , ϵ t s , φ t s,z ∼ Dir(β t s,z ), φ t s,k ∼ Dir(α t s ), ω t s ∼ Dir(ϵ t s )</formula><p>. With this formulation, we can ensure that the mean of the Dirichlet pa- rameter for the current epoch becomes proportion- al to the weighted sum of the word, topic distribu- tion, and entity distribution at previous epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference and Parameter Estimation</head><p>We use collapsed Gibbs sampling ( <ref type="bibr" target="#b2">Griffiths and Steyvers, 2004</ref>) to infer the parameters of the mod- el, given observed data D. Gibbs sampling is a Markov chain Monte Carlo method which allows us repeatedly sample from a Markov chain whose stationary distribution is the posterior of interest, s t d and z t d,n here, from the distribution over that variable given the current values of all other vari- ables and the data. Such samples can be used to empirically estimate the target distribution. Let- ting the subscript −d denote the quantity that ex- cludes counts in document d, the conditional pos- terior for s d is:</p><formula xml:id="formula_6">P (s t d = j|s t −d , z, w, Λ) ∝ {N j } −d + γ D −d + Sγ × E ∏ e=1 ∏n (d) j,e b=1 N j,e − b + ϵ t j,e ∏n E(d) j b=1 n E j − b + ∑ E e=1 ϵ t j,e × K ∏ k=1 ∏n (d) j,k b=1 n j,k − b + α t j,k ∏n (d) j b=1 n j − b + ∑ K k=1 α t j,k × V ∏ v=1 ∏n (d) j,k,v b=1 n j,k,v − b + β t j,k,v ∏n (d) j,k b=1 n j,k − b + ∑ V v=1 β t j,k,v ,</formula><p>where N j denotes the number of documents as- signed to storyline indicator j in the whole corpus, D is the total number of documents, n j,e is the number of times named entity e is assigned with storyline indicator j, n E j denotes the total number of named entities with storyline indicator j in the document collection, n j,k is the number of times words with topic label k with storyline indicator j, n j is the total number of words (excluding named entities) in the corpus with storyline indicator j, n j,k,v is the number of words v with storyline in- dicator j and topic label k in the document col- lection, counts with (d) notation denote the counts relating to document d only.</p><p>Letting the index x = (d, n) denote nth word in document d and the subscript −x denote a quantity that excludes data from the nth word position in document d. We only sample a topic z x if the nth word is not a named entity based on the following conditional posterior:</p><formula xml:id="formula_7">P (z t x = k|s d = j, z−x, w, Λ) ∝ {n t j,k }−x + α t j,k {nj}−x + ∑ K k=1 α t j,k × {n t j,k,wn }−x + β t j,k,v {n t j,k }−x + ∑ V v=1 β t j,k,v</formula><p>Once the latent variables s and z are known, we can easily estimate the model parameters π, Θ, φ, ψ, ω. We set the hyperparameters α = γ = 0.1, β = ϵ = 0.01 for the current epoch (i.e., m = 0), and gather statistics in the previous 7 epochs (i.e., M = 7) to set the Dirichlet priors for the storyline-topic-word distribution φ t s,z , the storyline-topic distribution θ t s and the storyline- entity distribution ω t s in the current epoch t, and run Gibbs sampler for 1000 iterations and stop the iteration once the log-likelihood of the training da- ta converges under the learned model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We crawled and parsed the GDELT Even- t Database 1 containing news articles published in May 2014. We manually annotated one-week da- ta containing 101,654 documents and identified 77 storylines for evaluation. We also report the result- s of our model on the one-month data containing 526,587 documents. But we only report the preci- sion and not recall of the storylines extracted since it is time consuming to identify all the true story- lines in such a large dataset. In our experiments, we used the Stanford Named Entity Recognizer for identifying the named entities. In addition, we removed common stopwords and only kept tokens which are verbs, nouns, or adjectives in these news articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We chose the following three methods as the base- line approaches.</p><p>1. K-Means + Cosine Similarity (KMCS): the method first applies K-Means to cluster news documents for each day, then link storylines detected in different days based on the cosine similarity measurement.</p><p>2. LDA + Cosine Similarity (LDCS): the method first splits news documents on a daily basis, then applies the Latent Dirichlet Allo- cation (LDA) model to detect the latent story- lines for the documents in each day, in which each storyline is modelled as a joint distribu- tion over named entities and words, and final- ly links storylines detected in different days using the cosine similarity measurement.</p><p>3. Dynamic LDA (DLDA) 2 : this is the dynam- ic LDA ( <ref type="bibr" target="#b1">Blei and Lafferty, 2006</ref>) where the topic-word distributions are linked across e- pochs based on the Markovian assumption. That is, the topic-word distribution at the cur- rent epoch is only influenced by the topic- word distribution in the previous epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metric</head><p>To evaluate the performance of the proposed ap- proach, we use precision, recall and F-score which are commonly used in evaluating information ex- traction systems. The precision is calculated based on the following criteria: 1) The entities and key- words extracted refer to the same storyline.</p><p>2) The duration of the storyline is correct. We assume that the start date (or end date) of a storyline is the pub- lication date of the first (or last) news article about it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>The proposed model is compared against the base- line approaches on the annotated one-week da- ta which consist of 77 storylines. The number of storylines, S, and the number of topics, K, are both set to 100. The number of historical e- pochs, M , which is taken into account for set- ting the Dirichlet priors for the storyline-topic- word, the storyline-topic and the storyline-entity distributions, is set to 7. The evaluation results of our proposed approach in comparison to the three baselines are presented in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: Performance comparison of the storyline extraction results in terms of Precision (%), Recall (%) and F-score (%).</p><p>It can be observed from <ref type="table">Table 1</ref> that simply using K-means to cluster news articles in each day and linking similar stories across differen- t days in hoping of identifying storylines gives the worst results. Using LDA to detect stories in each day improves the precision dramatically. The dynamic LDA model assumes topics (or sto- ries) in the current epoch evolves from the previ- ous epoch and further improves the storyline de- tection results significantly. Our proposed mod- el aims to capture the long distance dependen- cies in which the statistics gathered in the past 7 days are taken into account to set the Dirichlet pri- ors of the storyline-topic-word, storyline-topic and storyline-entity distributions in the current epoch. It gives the best performance and outperforms dy- namic LDA by nearly 7% in F-measure.</p><p>To study the impact of the number of topics on the performance of the proposed model, we con- ducted experiments on the one-month data with d- ifferent number of topics varying between 100 and 200. In all these experiments, the number of story- lines, S, is set to 200, based on the speculation that about 40 storylines in the annotated one-week data last for one month and about 40 new storylines oc- cur each week. Figure 2: Storyline about the patent infringement case between Apple and Samsung was extracted by the proposed Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Structured Browsing</head><p>We illustrate the evolution of storylines by using structured browsing, from which the structured in- formation (entity, topic, keywords) about story- lines and the duration of storylines can be easily observed. <ref type="figure">Figure 2</ref> shows the storyline about "The patent infringement case between Apple and Samsung". It can be observed that in the first two days, the hierarchical structure consists of entities (Apple, Samsung) and keywords (trial, patent, in- fringe). The case has gained significant attention in the next three days when US jury orders Sam- sung to pay Apple $119.6 million. It can be ob- served that the stories in the next three days also consist of entities (Apple, Samsung), but with d- ifferent keywords (award, patent, win). The last day's story gives an overall summary and consists of entities (Apple, Samsung) and keywords (jury, patent, company).</p><p>To further investigate the storylines detected by the proposed model, we randomly selected three detected storylines. The first one is about "the patent infringement case between Apple and Samsung". It is a short-term storyline lasting for 6 day as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The second one is about "India election", which is a long-term sto- ryline lasting for one month. The third one is about "Pistorius shoot Steenkamp", which is an inter- mittent storyline, lasting for a total of 22 days but with no relevant news reports in certain days as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. It can be observed that the pro- posed model can detect not only continuous but also intermittent storylines, which further demon- strates the advantage of the proposed model.  <ref type="formula" target="#formula_2">4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we have proposed an unsupervised Bayesian model to extract storylines from news corpus. Experimental results show that our pro- posed model is able to extract both continuous and intermittent storylines and outperforms a number of baselines. In future work, we will consider modelling background topics explicitly and inves- tigating more principled ways in setting the weight parameters of the statistics gathered in the histor- ical epochs. Moreover, we will also explore the impact of different scale of the dependencies from historical epochs on the distributions of the current epoch.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Dynamic Storyline Detection model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The number of documents on each day relating to the three storylines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Method Precision Recall F-score 
KMCS 
22.73 
32.47 
26.74 
LDCS 
34.29 
31.17 
32.66 
DLDA 
62.67 
61.03 
61.84 
DSDM 
70.67 
68.80 69.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows</head><label>2</label><figDesc>the precision of the proposed method under different number of topic- s. It can be observed that the performance of the proposed approach is quite stable across different number of topics.</figDesc><table>K 
100 
150 
200 
Precision 69.6% 70.2% 69.9% 

Table 2: The precision of our method with various 
number (K) of topics. </table></figure>

			<note place="foot" n="1"> http://data.gdeltproject.org/events/ index.html</note>

			<note place="foot" n="2"> Topic number is set to 100 for both DLDA and LDCS. Cluster number is also set to 100 for KMCS.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by the National Natural Sci-ence Foundation of China <ref type="formula">(61528302)</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified analysis of streaming news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choon Hui</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="267" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
		</imprint>
	</monogr>
	<note>Suppl. 1)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimized event storyline generation based on mixture-eventaspect model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian&amp;apos;en</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods on Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods on Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trend analysis model: trend consists of temporal words, topics, and timestamps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriaki</forename><surname>Kawamae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evolutionary hierarchical dirichlet process for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="556" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating event storylines from microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="175" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can twitter replace newswire for breaking news?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Shrimpton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>the 7th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining the web to predict future events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
		<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TUT: a statistical model for detecting trends, topics and user interests in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuning</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="972" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining evolutionary multi-branch trees from text streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="722" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An unsupervised framework of exploring events on twitter: Filtering, extraction and categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015)</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015)<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-01-25" />
			<biblScope unit="page" from="2468" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
