<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research Redmond</orgName>
								<address>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representing Text for Joint Embedding of Text and Knowledge Bases</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of tex-tual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among tex-tual relations with common sub-structure.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Representing information about real-world enti- ties and their relations in structured knowledge base (KB) form enables numerous applications. Large, collaboratively created knowledge bases have recently become available e.g., <ref type="bibr">Freebase (Bollacker et al., 2008)</ref>, <ref type="bibr">YAGO (Suchanek et al., 2007)</ref>, and DBPedia ( <ref type="bibr" target="#b0">Auer et al., 2007)</ref>, but even though they are impressively large, their coverage is far from complete. This has motivated research in automatically deriving new facts to extend a manually built knowledge base, by using infor- mation from the existing knowledge base, textual mentions of entities, and semi-structured data such as tables and web forms ( <ref type="bibr" target="#b17">Nickel et al., 2015)</ref>.</p><p>In this paper we build upon the work of <ref type="bibr" target="#b19">Riedel et al. (2013)</ref>, which jointly learns continuous rep- resentations for knowledge base and textual rela- tions. This common representation in the same vector space can serve as a kind of "universal schema" which admits joint inferences among * This research was conducted during the author's intern- ship at Microsoft Research. KBs and text. The textual relations represent the relationships between entities expressed in indi- vidual sentences (see <ref type="figure">Figure 1</ref> for an example). <ref type="bibr" target="#b19">Riedel et al. (2013)</ref> represented each textual men- tion of an entity pair by the lexicalized depen- dency path between the two entities (see <ref type="figure" target="#fig_1">Figure 2</ref>). Each such path is treated as a separate relation in a combined knowledge graph including both KB and textual relations. Following prior work in la- tent feature models for knowledge base comple- tion, every textual relation receives its own contin- uous representation, learned from the pattern of its co-occurrences in the knowledge graph. However, largely synonymous textual relations often share common sub-structure, and are com- posed of similar words and dependency arcs. For example, <ref type="table" target="#tab_0">Table 1</ref> shows a collection of dependency paths co-occurring with the per- son/organizations founded relation.</p><p>In this paper we model this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize perfor- mance on the knowledge base link prediction task.</p><p>We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free-base subset <ref type="bibr">FB15k (Bordes et al., 2013</ref>) and filtered to remove highly redundant relations ( <ref type="bibr" target="#b25">Toutanova and Chen, 2015)</ref>. The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb12 1 with Freebase entity mention annotations ( <ref type="bibr" target="#b5">Gabrilovich et al., 2013)</ref>.</p><p>We show that using a convolutional neural net- work to derive continuous representations for tex- tual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been a growing body of work on learn- ing to predict relations between entities without re- quiring sentence-level annotations of textual men- tions at training time. We group such related work into three groups based on whether KB, text, or both sources of information are used. Addition- ally, we discuss related work in the area of super- vised relation extraction using continuous repre- sentations of text, even though we do not use su- pervision at the level of textual mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge base completion</head><p>Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph fea- tures such as the path ranking algorithm <ref type="bibr" target="#b11">(Lao et al., 2011)</ref>, models based on continuous represen- tations (latent features), and model combinations ( <ref type="bibr" target="#b4">Dong et al., 2014</ref>). These models predict new facts in a given knowledge base, based on infor- mation from existing entities and relations. From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset. <ref type="bibr" target="#b30">Yang et al. (2015)</ref> showed that a simple variant of a bilinear model DISTMULT outperformed TRANSE ( ) and more richly parameterized models on this dataset. We therefore build upon the best per- forming prior model DISTMULT from this line of work, as well as additional models E and F devel- oped in the context of text-augmented knowledge graphs ( <ref type="bibr" target="#b19">Riedel et al., 2013)</ref>, and extend them to in- corporate compositional representations of textual relations.</p><p>1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated model- ing of prior knowledge from the knowledge base. <ref type="bibr" target="#b14">Mintz et al. (2009)</ref> demonstrated that both surface context and dependency path context were help- ful for the task, but did not model the composi- tional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables ( <ref type="bibr" target="#b18">Riedel et al., 2010;</ref><ref type="bibr" target="#b9">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b23">Surdeanu et al., 2012)</ref> or model the noise arising from the incom- pleteness of knowledge bases and text collections ( <ref type="bibr" target="#b21">Ritter et al., 2013)</ref>, inter alia. Our work focuses on representing the compositional structure of sen- tential context for learning joint continuous repre- sentations of text and knowledge bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining knowledge base and text information</head><p>A combination of knowledge base and textual in- formation was first shown to outperform either source alone in the framework of path-ranking al- gorithms in a combined knowledge base and text graph ( <ref type="bibr" target="#b12">Lao et al., 2012)</ref>. To alleviate the spar- sity of textual relations arising in such a com- bined graph, ( <ref type="bibr" target="#b6">Gardner et al., 2013;</ref><ref type="bibr" target="#b7">Gardner et al., 2014</ref>) showed how to incorporate clusters or con- tinuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Co- occurrence based textual relation representations were also learned in ( <ref type="bibr" target="#b15">Neelakantan et al., 2015)</ref>. <ref type="bibr" target="#b27">Wang et al. (2014a)</ref> combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual co- occurrences of entity pairs and the expressed tex- tual relations.  combined con- tinuous representations from a knowledge base and textual mentions for prediction of new rela- tions. The two representations were trained inde- pendently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional.</p><p>In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the sources of information. We directly build on the universal schema approach of <ref type="bibr" target="#b19">Riedel et al. (2013)</ref> as well as the universal schema extension of the DISTMULT model mentioned previously, to im- prove the representations of textual relations by capturing their compositional structure. Addition- ally, we evaluate the approach on a dataset that contains rich prior information from the training knowledge base, as well as a wealth of textual in- formation from a large document collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous representations for supervised relation extraction</head><p>In contrast to the work reviewed so far, work on sentence-level relation extraction using direct su- pervision has focused heavily on representing sen- tence context. Models using hand-crafted fea- tures have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance ( <ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b8">Gormley et al., 2015</ref>). Compared to work on representation learn- ing for sentence-level context, such as this recent work using LSTM models on constituency or de- pendency trees <ref type="bibr" target="#b24">(Tai et al., 2015)</ref>, our approach us- ing a one-hidden-layer convolutional neural net- work is relatively simple. However, even such a simple approach has been shown to be very com- petitive <ref type="bibr" target="#b10">(Kim, 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models for knowledge base completion</head><p>We begin by introducing notation to define the task, largely following the terminology in <ref type="bibr" target="#b17">Nickel et al. (2015)</ref>. We assume knowledge bases are rep- resented using RDF triples, in the form (subject, predicate, object), where the subject and object are entities and the predicate is the type of relation. For example, the KB fragment shown in <ref type="figure">Figure 1</ref> is shown as a knowledge graph, where the enti- ties are the nodes, and the relations are shown as directed labeled edges: we see three entities par- ticipating in three relation instances indicated by the edges. For brevity, we will denote triples by (e s , r, e o ), where e s and e o denote the subject and object entities, respectively.</p><p>The task is, given a training KB consisting of entities with some relations between them, to pre- dict new relations (links) that do not appear in the training KB. More specifically, we will build mod- els that rank candidate entities for given queries (e s , r, ?) or (?, r, e o ), which ask about the object Barack Obama is the 44th and currrent President of United States . or subject of a given relation. This task setting has been used in models for KB completion previously, e.g. ( <ref type="bibr" target="#b4">Dong et al., 2014;</ref><ref type="bibr" target="#b7">Gardner et al., 2014</ref>), even though it has not been standard in evaluations of distant supervision for relation extraction <ref type="bibr" target="#b14">(Mintz et al., 2009;</ref><ref type="bibr" target="#b19">Riedel et al., 2013)</ref>. The advantage of this evaluation set- ting is that it enables automatic evaluation without requiring humans to label candidate extractions, while making only a local closed world assump- tion for the completeness of the knowledge base -i.e., if one object e o for a certain subject / rela- tion pair (e s , r) is present in the knowledge base, it is assumed likely that all other objects (e s , r, e o ) will be present. Such an assumption is particularly justified for nearly functional relations.</p><p>To incorporate textual information, we follow prior work ( <ref type="bibr" target="#b12">Lao et al., 2012;</ref><ref type="bibr" target="#b19">Riedel et al., 2013)</ref> and represent both textual and knowledge base re- lations in a single graph of "universal" relations. To present the models for knowledge base completion based on such combined knowledge graphs, we first introduce some notation. Let E denote the set of entities in the knowledge graph and let R denote the set of relation types. We de- note each possible triple as T = (e s , r, e o ) where e s , e o ∈ E, r ∈ R, and model its presence with a binary random variable y T ∈ {0, 1} which in- dicates whether the triple exists. The models we build score possible triples (e s , r, e o ) using contin- uous representations (latent features) of the three elements of the triple. The models use scoring function f (e s , r, e o ) to represent the model's con- fidence in the existence of the triple. We present the models and then the loss function used to train their parameters.</p><formula xml:id="formula_0">· r (e s , e o ) F: · r s e s · r o e o + E: · ( r e s e o ) DISTMULT:</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Models</head><p>We begin with presenting the three models from prior work that this research builds upon. They all learn latent continuous representations of rela- tions and entities or entity pairs, and score possible triples based on the learned continuous represen- tations. Each of the models can be defined on a knowledge graph containing entities and KB rela- tions only, or on a knowledge graph additionally containing textual relations. We use models F and E from ( <ref type="bibr" target="#b19">Riedel et al., 2013</ref>) where they were used for a combined KB+text graph, and model DIST- MULT from <ref type="bibr" target="#b30">(Yang et al., 2015)</ref>, which was origi- nally used for a knowledge graph containing only KB relations.</p><p>As shown in <ref type="figure">Figure 3</ref>, model F learns a K- dimensional latent feature vector for each can- didate entity pair (e s , e o ), as well as a same- dimensional vector for each relation r, and the scoring function is simply defined as their inner product: f (e s , r, e o ) = v(r) v(e s , e o ). Therefore, different pairs sharing the same entity would not share parameters in this model. Model E does not have parameters for entity pairs, and instead has parameters for individual entities. It aims to capture the compatibility be- tween entities and the subject and object posi- tions of relations. For each relation type r, the model learns two latent feature vectors v(r s ) and v(r o ) of dimension K. For each entity (node) e i , the model also learns a latent feature vector of the same dimensionality. The score of a candi- date triple (e s , r, e o ) is defined as f (e s , r, e o ) = v(r s ) v(e s ) + v(r o ) v(e o ). It can be seen that when a subject entity is fixed in a query (e s , r, ?), the ranking of candidate object entity fillers ac- cording to f does not depend on the subject entity but only on the relation type r.</p><p>The third model DISTMULT, is a special form of a bilinear model like RESCAL <ref type="bibr" target="#b16">(Nickel et al., 2011)</ref>, where the non-diagonal entries in the rela- tion matrices are assumed to be zero. This model was proposed in <ref type="bibr" target="#b30">Yang et al. (2015)</ref> and was shown to outperform prior work on the FB15k dataset. In this model, each entity e i and each relation r is assigned a latent feature vector of dimension K.</p><formula xml:id="formula_1">The score of a candidate triple (e s , r, e o ) is defined as f (e s , r, e o ) = v(r) (v(e s ) • v(e o ))</formula><p>, where • denotes the element-wise vector product. In this model, entity pairs which share an entity also share parameters, and the ranking of candidate objects for queries (e s , r, ?) depends on the subject entity.</p><p>Denote N e = |E|, N r = |R|, and K = di- mension of latent feature vectors, then model E has KN e + 2KN r parameters and model DIST- MULT has KN e + KN r parameters. Model F has KN 2 e + KN r parameters, although most en- tity pairs will not co-occur in the knowledge base or text.</p><p>In the basic models, knowledge base and textual relations are treated uniformly, and each textual re- lation receives its own latent representation of di- mensionality K. When textual relations are added to the training knowledge graph, the total number of relations |R| grows substantially (it increases from 237 to more than 2.7 million for the dataset in this study), resulting in a substantial increase in the total number of independent parameters.</p><p>Note that in all of these models queries about the arguments of knowledge base relations (e s , r, ?) are answered by scoring functions look- ing only at the entity and KB relation represen- tations, without using representations of textual mentions. The textual mention information and representations are only used at training time to improve the learned representations of KB rela- tions and entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONV: Compositional Representations of Textual Relations</head><p>In the standard latent feature models discussed above, each textual relation is treated as an atomic unit receiving its own set of latent features. How- ever, many textual relations differ only slightly in the words or dependency arcs used to express the relation. For example, <ref type="table" target="#tab_0">Table 1</ref> shows sev- eral textual patterns that co-occurr with the re- lation person/organizations founded in the train- ing KB. While some dependency paths occur fre- quently, many very closely related ones have been observed only once. The statistical strength of the model could be improved if similar dependency paths have a shared parameterization. We build on work using similar intuitions for other tasks and learn compositional representations of textual re- lations based on their internal structure, so that the derived representations are accurate for the task of predicting knowledge base relations. We use a convolutional neural network applied to the lexicalized dependency paths treated as a se- quence of words and dependency arcs with direc- tion. <ref type="figure">Figure 4</ref> depicts the neural network archi- tecture. In the first layer, each word or directed la- beled arc is mapped to a continuous representation using an embedding matrix V. In the hidden layer, every window of three elements is mapped to a hidden vector using position-specific maps W, a bias vector b, and a tanh activation function. A max-pooling operation over the sequence is ap- plied to derive the final continuous representation for the dependency path.</p><p>The CONV representation of textual relations can be used to augment any of the three basic mod- els. The difference between a basic model and its CONV-augmented variant is in the parameteriza- tion of textual mentions. The basic models learn distinct latent feature vectors of dimensionality K for all textual relation types, whereas the CONV models derive the K-dimensional latent feature vectors for textual relation types as the activation at the top layer of the convolutional network in <ref type="figure">Figure 4</ref>, given the corresponding lexicalized de- pendency path as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training loss function</head><p>All basic and CONV-augmented models use the same training loss function. Our loss function is motivated by the link prediction task and the performance measures used. As previously men- tioned, the task is to predict the subject or ob- ject entity for given held-out triples (e s , r, e o ), i.e., to rank all entities with respect to their like- lihood of filling the respective position in the triple 2 . We would thus like the model to score cor- rect triples (e s , r, e o ) higher than incorrect triples (e , r, e o ) and (e s , r, e ) which differ from the cor- rect triple by one entity. Several approaches ( <ref type="bibr" target="#b17">Nickel et al., 2015</ref>) use a margin-based loss func- tion. We use an approximation to the negative log- likelihood of the correct entity filler instead <ref type="bibr">3</ref> . We define the conditional probabilities p(e o |e s , r) and p(e s |r, e o ) for object and subject entities given the relation and the other argument as follows:</p><p>p(e o |e s , r; Θ) = e f (es,r,eo;Θ) e ∈N eg(es,r,?) e f (es,r,e ;Θ)</p><p>Conditional probabilities for subject entities p(e s |e o , r; Θ) are defined analogously. Here Θ de- notes all the parameters of latent features. The denominator is defined using a set of entities that do not fill the object position in any relation triple (e s , r, ?) in the training knowledge graph. Since the number of such entities is impractically large, we sample negative triples from the full set. We also limit the candidate entities to ones that have types consistent with the position in the relation triple ( <ref type="bibr" target="#b3">Chang et al., 2014;</ref><ref type="bibr" target="#b30">Yang et al., 2015)</ref>, where the types are approximated follow- ing <ref type="bibr" target="#b25">Toutanova and Chen (2015)</ref>. Additionally, since the task of predicting textual relations is aux- iliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations ( <ref type="bibr" target="#b25">Toutanova and Chen, 2015)</ref>.</p><p>Denote T as a set of triples, we define the loss L(T ; Θ) as: Let T KB and T text represent the set of knowl- edge base triples and textual relation triples re- spectively. The final training loss function is de-    <ref type="figure">Figure 4</ref>: The convolutional neural network architecture for representing textual relations.</p><formula xml:id="formula_2">L(T ; Θ) = −</formula><formula xml:id="formula_3">v i = V e i h i = tanh(W 1 v i1 + W 0 v i + W 1 v i+1 + b) r = max{h i }</formula><p>fined as:</p><formula xml:id="formula_4">L(T KB ; Θ) + τ L(T text ; Θ) + λΘ 2 ,</formula><p>where λ is the regularization parameter, and τ is the weighing factor of the textual relations.</p><p>The parameters of all models are trained using a batch training algorithm. The gradients of the ba- sic models are straightforward to compute, and the gradients of the convolutional network parameters for the CONV-augmented models are also not hard to derive using back-propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Evaluation Protocol</head><p>We use the FB15k-237 4 dataset, which is a sub- set of FB15k ( ) that excludes redundant relations and direct training links for held-out triples, with the goal of making the task more realistic <ref type="bibr" target="#b25">(Toutanova and Chen, 2015)</ref>. The FB15k dataset has been used in multiple stud- ies on knowledge base completion ( <ref type="bibr" target="#b28">Wang et al., 2014b;</ref><ref type="bibr" target="#b30">Yang et al., 2015)</ref>. Textual relations for FB15k-237 are extracted from 200 million sen- tences in the ClueWeb12 corpus coupled with Freebase mention annotations ( <ref type="bibr" target="#b5">Gabrilovich et al., 2013)</ref>, and include textual links of all co-occurring entities from the KB set. After pruning 5 , there are 2.7 million unique textual relations that are added to the knowledge graph. The set of textual rela- tions is larger than the set used in <ref type="bibr" target="#b25">Toutanova and Chen (2015)</ref>  <ref type="figure" target="#fig_1">(25,000 versus 2.7 million)</ref>, leading to improved performance.</p><p>The number of relations and triples in the train- ing, validation and test portions of the data are given in <ref type="table" target="#tab_2">Table 2</ref>. The two rows list statistics for the KB and text portions of the data separately. The 2.7 million textual relations occur in 3.9 mil- lion text triples. Almost all entities occur in tex- tual relations (13,937 out of 14,541). The num- bers of triples for textual relations are shown as zero for the validation and test sets because we don't evaluate on prediction of textual relations (all text triples are used in training). The per- centage of KB triples that have textual relations for their pair of entities is 40.5% for the training, 26.6% for the validation, and 28.1% for the test set. While 26.6% of the validation set triples have textual mentions, the percentage with textual re- lations that have been seen in the training set is 18.4%. Having a mention increases the chance that a random entity pair has a relation from 0.1% to 5.0% -a fifty-fold increase.</p><p>Given a set of triples in a set disjoint from a training knowledge graph, we test models on pre- dicting the object of each triple, given the subject and relation type. We rank all entities in the train- ing knowledge base in order of their likelihood of filling the argument position. We report the mean reciprocal rank (MRR) of the correct entity, as well as HITS@10 -the percentage of test triples for which the correct entity is ranked in the top 10. We use filtered measures following the pro- tocol proposed in  -that is, when we rank entities for a given position, we re- move all other entities that are known to be part of an existing triple in the training, validation, or test set. This avoids penalizing the model for ranking other correct fillers higher than the tested entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details</head><p>We used a value of λ = 1 for the weight of the L 2 penalty for the main results in <ref type="table" target="#tab_3">Table 3</ref>, and present some results on the impact of λ at the end of this section. We used batch optimization af- ter initial experiments with AdaGrad showed in- ferior performance. L-BFGS ( <ref type="bibr" target="#b13">Liu and Nocedal, 1989)</ref> and RProp ( <ref type="bibr" target="#b20">Riedmiller and Braun, 1993)</ref> were found to converge to similar function values, with RProp converging significantly faster. We thus used RProp for optimization. We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set de- creased. For each model type, we chose the better of random and KB-only initialization. The word embeddings in the CONV models were initialized using the 50-dimensional vectors from <ref type="bibr" target="#b26">Turian et al. (2010)</ref> in the main experiments, with a slight positive impact. The effect of initialization is dis- cussed at the end of the section.</p><p>The number of negative examples for each triple was set to 200. Performance improved substan- tially when the number of negative examples was increased and reached a plateau around 200. We chose the optimal number of latent feature dimen- sions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500. We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}. The best dimen- sion for latent feature vectors was 10 for most KB- only models (not including model F), and 5 for the two model configurations including F. We used K = 10 for all KB+text models, as higher dimen- sion was also not helpful for them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental results</head><p>In <ref type="table" target="#tab_3">Table 3</ref> we show the performance of differ- ent models and their combinations 6 , both when using textual mentions (KB+text), and when us- ing only knowledge base relations (KB only). In the KB+text setting, we evaluate the contribution of the CONV representations of the textual rela- tions. The upper portion of the    <ref type="table">Table)</ref> were chosen to maximize MRR on the validation set. The reported numbers were obtained for the test set.</p><p>base relations, and are not using any information from textual mentions. The lower portion of the <ref type="table" target="#tab_1">Table shows</ref> the performance when textual rela- tions are added to the training knowledge graph and the corresponding training loss function. Note that all models predict based on the learned knowl- edge base relation and entity representations, and the textual relations are only used at training time when they can impact these representations. The performance of all models is shown as an overall MRR (scaled by 100) and HITS@10, as well as performance on the subset of triples that have textual mentions (column With mentions), and ones that do not (column Without mentions). Around 28% of the test triples have mentions and contribute toward the measures in the With men- tions column, and the other 72% of the test triples contribute to the Without mentions column.</p><p>For the KB-only models, we see the perfor- mance of each individual model F, E, and DIST- MULT. Model F was the best performing single model from ( <ref type="bibr" target="#b19">Riedel et al., 2013</ref>), but it does not perform well when textual mentions are not used.</p><p>In our implementation of model F, we created en- tity pair parameters only for entity pairs that co- occur in the text data ( <ref type="bibr" target="#b19">Riedel et al. (2013)</ref> also trained pairwise vectors for co-occuring entities only, but all of the training and test tuples in their study were co-occurring) <ref type="bibr">7</ref> . Without textual in- formation, model F is performing essentially ran- domly, because entity pairs in the test sets do not occur in training set relations (by construction of the dataset). Model E is able to do surprisingly well, given that it is making predictions for each object position of a relation without considering the given subject of the relation. DISTMULT is the best performing single model. Unlike model F, it is able to share parameters among entity pairs with common subject or object entities, and, un- like model E, it captures some dependencies be- tween the subject and object entities of a relation. The combination of models E+DISTMULT im- proves performance, but combining model F with the other two is not helpful.</p><p>The lower portion of <ref type="table" target="#tab_3">Table 3</ref> shows results when textual relations are added to the training knowl- edge graph. The basic models treat the textual re- lations as atomic and learn a separate latent feature vector for each textual relation. The CONV-mod- els use the compositional representations of tex-tual relations learned using the convolutional neu- ral network architecture shown in <ref type="figure">Figure 4</ref>. We show the performance of each individual model and its corresponding variant with a CONV pa- rameterization. For each model, we also show the optimal value of τ , the weight of the textual re- lations loss. Model F is able to benefit from tex- tual relations and its performance increases by 2.5 points in MRR, with the gain in performance be- ing particularly large on test triples with textual mentions. Model F is essentially limiting its space of considered argument fillers to ones that have co- occurred with the given subject entity. This gives it an advantage on test triples with textual mentions, but model F still does relatively very poorly over- all when taking into account the much more nu- merous test triples without textual mentions. The CONV parameterization performs slightly worse in MRR, but slightly better in HITS@10, com- pared to the atomic parameterization. For model E and its CONV variant, we see that text does not help as its performance using text is the same as that when not using text and the optimal weight of the text is zero. Model DISTMULT benefits from text, and its convolutional text variant CONV- DISTMULT outperforms the basic model, with the gain being larger on test triples with mentions.</p><p>The best model overall, as in the KB-only case, is E+DISTMULT. The basic model bene- fits from text slightly and the model with compo- sitional representations of textual patterns CONV- E+CONV-DISTMULT, improves the performance further, by 2.4 MRR overall, and by 5 MRR on triples with textual mentions. It is interesting that the text and the compositional representations helped most for this combined model. One hy- pothesis is that model E, which provides a prior over relation arguments, is needed in combination with DISTMULT to prevent the prediction of un- likely arguments based on noisy inference from textual patterns and their individual words and de- pendency links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter Sensitivity</head><p>To gain insight into the sensitivity of the model to hyper-parameters and initialization, we report on experiments starting with the best model CONV- E + CONV-DISTMULT from <ref type="table" target="#tab_3">Table 3</ref> and varying one parameter at a time. This model has weight of the textual relations loss τ = 0.25, weight of the L 2 penalty λ = 1, convolution window size of three, and is initialized randomly for the entity and KB relation vectors, and from pre-trained embed- dings for word vectors <ref type="bibr" target="#b26">(Turian et al., 2010</ref>). The overall MRR of the model is 40.4 on the validation set (test results are shown in the <ref type="table">Table)</ref>.</p><p>When the weight of τ is changed to 1 (i.e., equal contribution of textual and KB relations), the over- all MRR goes down to 39.6 from 40.4, indicat- ing the usefulness of weighting the two kinds of relations non-uniformly. When λ is reduced to 0.04, MRR is 40.0 and when λ is increased to 25, MRR goes down to 38.9. This indicates the L 2 penalty hyper-parameter has a large impact on performance. When we initialize the word embed- dings randomly instead of using pre-trained word vectors, performance drops only slightly to 40.3. If we initialize from a model trained using KB- only information, performance goes down sub- stantially to 38.7. This indicates that initialization is important and there is a small gain from using pre-trained word embeddings. There was a drop in performance to MRR 40.2 when using a win- dow size of one for the convolutional architecture in <ref type="figure">Figure 4</ref>, and an increase to 40.6 when using a window size of five.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>Here we explored an alternative representation of textual relations for latent feature models that learn to represent knowledge base and textual re- lations in the same vector space. We showed that given the large degree of sharing of sub-structure in the textual relations, it was beneficial to com- pose their continuous representations out of the representations of their component words and de- pendency arc links. We applied a convolutional neural network model and trained it jointly with a model mapping entities and knowledge base rela- tions to the same vector space, obtaining substan- tial improvements over an approach that treats the textual relations as atomic units having indepen- dent parameterization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A knowledge base fragment coupled with textual mentions of pairs of entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Textual relation extracted from an entity pair mention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The textual relations are represented as full lexi- calized dependency paths, as illustrated in Figure 2. An instance of the textual relation SUBJECT nsubj ← −− − president prep − − → of obj −→OBJECT connecting the entities BARACK OBAMA and UNITED STATES, is added to the knowledge graph based on this sentential occur- rence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Figure 3 :</head><label>13</label><figDesc>Figure 3: The continuous representations for model F, E and DISTMULT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(</head><label></label><figDesc>es,r,eo)∈T log p(e o |e s , r; Θ) − (es,r,eo)∈T log p(e s |e o , r; Θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>SUBJECT appos − −− →founder prep − − →of pobj − − →OBJECT 12 SUBJECT nsubj ← −− −co-founded dobj</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Textual patterns occurring with entity pairs in a person/organizations founded relationship. The count indicates the number of training set instances that have this KB relation, which co-occur with each textual pattern.</figDesc><table>SUBJECT 

appos 

! co-founder 

prep 

! 
of 

pobj 

! OBJECT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table shows the</head><label>shows</label><figDesc></figDesc><table>performance of models that have been trained us-
ing knowledge graphs including only knowledge # Relations # Entities # Triples in Train / Validation / Test 
KB 
237 
14,541 
272,115 / 17,535 / 20, 466 
Text 
2,740k 
13,937 
3,978k / 0 / 0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : The statistics of dataset FB15k-237.</head><label>2</label><figDesc></figDesc><table>Model 
Overall 
With mentions 
Without mentions 
MRR HITS@10 MRR HITS@10 MRR HITS@10 
KB only 
F 
16.9 
24.5 
26.4 
49.1 
13.3 
15.5 
E 
33.2 
47.6 
25.5 
37.8 
36.0 
51.2 
DISTMULT 
35.7 
52.3 
26.0 
39.0 
39.3 
57.2 
E+DISTMULT 
37.3 
55.2 
28.6 
42.9 
40.5 
59.8 
F+E+DISTMULT 
33.8 
50.1 
15.0 
26.1 
40.7 
59.0 
KB and text 
F (τ = 1) 
19.4 
27.9 
35.4 
61.6 
13.4 
15.5 
CONV-F (τ = 1) 
19.2 
28.4 
34.9 
63.7 
13.3 
15.4 
E (τ = 0) 
33.2 
47.6 
25.5 
37.8 
36.0 
51.2 
CONV-E (τ = 0) 
33.2 
47.6 
25.5 
37.8 
36.0 
51.2 
DISTMULT (τ = 0.01) 
36.1 
52.7 
26.5 
39.5 
39.6 
57.5 
CONV-DISTMULT (τ = 0.25) 
36.6 
53.5 
28.3 
43.4 
39.7 
57.2 
E + DISTMULT (τ = 0.01) 
37.7 
55.7 
28.9 
43.4 
40.9 
60.2 
CONV-E + CONV-DISTMULT (τ = 0.25) 
40.1 
58.1 
33.9 
49.9 
42.4 
61.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on FB15k-237 for KB only and KB+text inference, with basic models versus the pro-
posed CONV-augmented models. The values of the hyper-parameter τ (as shown in the </table></figure>

			<note place="foot" n="2"> Our experimental comparison focuses on predicting object entities only, but we consider both argument types in the training loss function. 3 Note that both margin-based and likelihood-based loss functions are susceptible to noise from potential selection of false negative examples. An empirical comparison of training loss functions would be interesting.</note>

			<note place="foot" n="4"> Check the first author&apos;s website for a release of the dataset.</note>

			<note place="foot" n="5"> The full set of 37 million textual patterns connecting the entity pairs of interest was pruned based on the count of patterns and their tri-grams, and their precision in indicating that entity pairs have KB relations.</note>

			<note place="foot" n="6"> Different models are combined by simply defining a combined scoring function which adds the scores from individual models. Combined models are trained jointly.</note>

			<note place="foot" n="7"> Learning entity pair parameters for all entity pairs would result in 2.2 billion parameters for vectors with dimensionality 10 for our dataset. This was infeasible and was also not found useful based on experiments with vectors of lower dimensionality.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous review-ers for their suggestions, and Jianfeng Gao, Scott Wen-tau Yih, and Wei Xu for useful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciaduran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Typed tensor decomposition of knowledge bases for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">FACC1: Freebase annotation of ClueWeb corpora, Version 1 (release date 2013-06-26</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>format version 1, correction level 0</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating vector space similarity in random walk inference over knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02419</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the limited memory bfgs method for large scale optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="503" to="528" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACLIJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs: From multi-relational link prediction to automated knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relation extraction with matrix factorization and universal schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A direct adaptive method for faster backpropagation learning: The rprop algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling missing data in distant supervision for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiinstance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP/CoNLL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
