<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
							<email>sebastian@ruder.io,ryan.cotterell@jhu.com,{yova|soegaard}@di.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="institution">Aylien Ltd</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">The Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<settlement>Copenhagen</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Insight Research Centre</orgName>
								<orgName type="institution">National University of Ireland</orgName>
								<address>
									<settlement>Galway</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Discriminative Latent-Variable Model for Bilingual Lexicon Induction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="458" to="468"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>458</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bi-partite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Is there a more fundamental bilingual linguistic re- source than a dictionary? The task of bilingual lexi- con induction seeks to create a dictionary in a data- driven manner directly from monolingual corpora in the respective languages and, perhaps, a small seed set of translations. From a practical point of view, bilingual dictionaries have found uses in a myriad of NLP tasks ranging from machine trans- lation ( <ref type="bibr" target="#b20">Klementiev et al., 2012</ref>) to cross-lingual named entity recognition ( <ref type="bibr" target="#b24">Mayhew et al., 2017)</ref>. In this work, we offer a probabilistic twist on the task, developing a novel discriminative latent-variable model that outperforms previous work.</p><p>Our proposed model is a bridge between cur- rent state-of-the-art methods in bilingual lexicon induction that take advantage of word embeddings, e.g., the embeddings induced by <ref type="bibr" target="#b27">Mikolov et al. (2013b)</ref>'s skip-gram objective, and older ideas in the literature that build explicit probabilistic models for the task. We propose a discriminative probabil- ity model, inspired by Irvine and Callison-Burch * The first two authors contributed equally. <ref type="bibr">1</ref> The code used to run the experiments is avail- able at https://github.com/sebastianruder/ latent-variable-vecmap.</p><p>(2013), infused with the bipartite matching dictio- nary prior of <ref type="bibr" target="#b15">Haghighi et al. (2008)</ref>. However, like more recent approaches ( <ref type="bibr" target="#b3">Artetxe et al., 2017)</ref>, our model operates directly over pretrained word em- beddings, induces a joint cross-lingual embedding space, and scales to large vocabulary sizes. To train our model, we derive a generalized expectation- maximization algorithm (EM; <ref type="bibr" target="#b29">Neal and Hinton, 1998)</ref> and employ an efficient matching algorithm.</p><p>Empirically, we experiment on three standard and three extremely low-resource language pairs. We evaluate intrinsically, comparing the quality of the induced bilingual dictionary, as well as an- alyzing the resulting bilingual word embeddings themselves. The latent-variable model yields gains over several previous approaches across language pairs. It also enables us to make implicit mod- eling assumptions explicit. To this end, we pro- vide a reinterpretation of <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> as a latent-variable model with an IBM Model 1-style ( <ref type="bibr" target="#b7">Brown et al., 1993</ref>) dictionary prior, which allows a clean side-by-side analytical comparison. Viewed in this light, the difference between our approach and <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>, the strongest baseline, is whether one-to-one alignments or one-to-many alignments are admitted between the words of the languages' respective lexicons. Thus, we conclude that our hard constraint on one-to-one alignments is primarily responsible for the improvements over <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Bilingual Lexicon Induction and Word Embeddings</head><p>Bilingual lexicon induction 2 is the task of finding word-level translations between the lexicons of two languages. For instance, the German word Hund and the English word dog are roughly semantically equivalent, so the pair Hund-dog should be an entry in a German-English bilingual lexicon. The task itself comes in a variety of flavors. We consider a version of the task that only relies on monolingual corpora in the tradition of <ref type="bibr" target="#b31">Rapp (1995)</ref> and <ref type="bibr" target="#b12">Fung (1995)</ref>. In other words, the goal is to produce a bilingual lexicon primarily from unannotated raw text in each of the respective languages. Impor- tantly, we avoid reliance on bitext, i.e. corpora with parallel sentences that are known translations of each other, e.g., <ref type="bibr">EuroParl (Koehn, 2005</ref>). The bi- text assumption is quite common in the literature; see  <ref type="table" target="#tab_2">, Table 2</ref>) for a survey. Ad- ditionally, we will assume the existence of a small seed set of word-level translations obtained from a dictionary; we also experiment with seed sets obtained from heuristics that do not rely on the existence of linguistic resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-Theoretic Formulation</head><p>To ease the later exposition, we will formulate the task graph-theoretically. Let src denote the source language and trg the target language. Suppose the source language src has n src word types in its lexicon V src and trg has n trg word types in its lexi- con V trg . We will write v src (i) for the i th word type in src and v trg (i) for the i th word type in trg . We can view the elements of V src and V trg as sets of vertices in a graph. Now consider the bipartite set of vertices V = V trg âª V src . In these terms, a bilin- gual lexicon is just a bipartite graph G = (E, V ) and, thus, the task of bilingual lexicon induction is a combinatorial problem: the search for a 'good' edge set E â V trg ÃV src . We depict such a bipartite graph in <ref type="figure" target="#fig_0">Figure 1</ref>. In Â§3, we will operationalize the notion of 'goodness' by assigning a weight w ij to each possible edge between V trg and V src .</p><p>When the edge set E takes the form of a match- ing, we will denote it as m. <ref type="bibr">3</ref> In general, we will be interested in partial matchings, where many ver- tices have no incident edges. We will write M for the set of all partial matchings on the bipar- tite graph G. The set of vertices in V trg (respec- tively V src ) with no incident edges will be termed u trg (respectively u src ). Note that for any matching m, we have the identity u trg = V trg \ {i : (i, j) â m}.</p><p>3 A matching is an edge set where none of the edges share common vertices <ref type="bibr" target="#b41">(West, 2000</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embeddings</head><p>Word embeddings will also play a key role in our model. For the remainder of the paper, we will as- sume we have access to d-dimensional embeddings for each language's lexicon-for example, those provided by a standard model such as skip-gram ( <ref type="bibr" target="#b27">Mikolov et al., 2013b</ref>). Notationally, we define the real matrices S â R dÃnsrc and T â R dÃntrg . Note that in this formulation s i â R d , the i th col- umn of S, is the word embedding corresponding to v src (i). Likewise, note that t i â R d , the i th col- umn of T , is the word embedding corresponding to v trg (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Latent-Variable Model</head><p>The primary contribution of this paper is a novel latent-variable model for bilingual lexicon induc- tion. The latent variable will be the edge set E, as discussed in Â§2.1. Given pretrained embeddings for the source and target languages, arranged into the matrices S and T , we define the density</p><formula xml:id="formula_0">p(T | S) := mâM p(T | S, m) Â· p(m) (1)</formula><p>where, recall from Â§2, M is the set of all bipar- tite matchings on the graph G and m â M is an individual matching. Note that, then, p(m) is a distribution over all bipartite matchings on G such as the matching shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We will take p(m) to be fixed as the uniform distribution for the remainder of the exposition, though more compli- cated distributions could be learned, of course. We further define the distribution</p><formula xml:id="formula_1">p Î¸ (T | S, m) := (i,j)âm p(t i | s j )Â· iâutrg p(t i ) (2)</formula><p>Recall we write (i, j) â m to denote an edge in the matching. Furthermore, for notational simplicity, we have dropped the dependence of u trg on m.</p><p>(Recall u trg = V trg \ {i : (i, j) â m}). Next, we define the two densities present in equation <ref type="formula">(2)</ref> as Gaussians:</p><formula xml:id="formula_2">p Î¸ (t | s) := N (â¦ s, I)<label>(3)</label></formula><formula xml:id="formula_3">â exp ||t â â¦ s|| 2 2 p Î¸ (t) := N (Âµ, I)<label>(4)</label></formula><p>Given a fixed matching m, we may create ma- trices S m â R dÃ|m| and T m â R dÃ|m| such that the rows correspond to word vectors of matched vertices (translations under the matching m). Now, after some algebra, we see that we can rewrite (i,j)âm p(t i | s i ) in matrix notation:</p><formula xml:id="formula_4">p Î¸ (T m | S m , m) = (i,j)âm p(t i | s j )<label>(5)</label></formula><formula xml:id="formula_5">â (i,j)âm exp ||t i â â¦ s j || 2 2 = exp (i,j)âm ||t i â â¦ s j || 2 2 = exp ||T m â â¦ S m || 2 F (6)</formula><p>where â¦ â R dÃd is an orthogonal matrix of param- eters to be learned. The result of this derivation, equation <ref type="formula">(6)</ref>, will become useful during the discus- sion of parameter estimation in Â§4. We define the model's parameters, to be opti- mized, as Î¸ = (â¦, Âµ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling Assumptions and their Limitations</head><p>In the previous section, we have formulated the induction of a bilingual lexicon as the search for an edge set E, which we treat as a latent variable that we marginalize out in equation <ref type="formula">(2)</ref>. Specifically, we assume that E is a partial matching. Thus, for every (i, j) â m, we have t i â¼ N (â¦ s j , I), that is, the embedding for v trg (i) is assumed to have been drawn from a Gaussian centered around the embed- ding for v src (j), after an orthogonal transformation. This gives rise to two modeling assumptions, which we make explicit: (i) There exists a single source for every word in the target lexicon and that source cannot be used more than once. <ref type="bibr">4</ref> (ii) There ex- ists an orthogonal transformation, after which the embedding spaces are more or less equivalent.</p><p>Assumption (i) may be true for related languages, but is likely false for morphologically rich lan- guages that have a many-to-many relationship be- tween the words in their respective lexicons. We propose to ameliorate this using a rank constraint that only considers the top n most frequent words in both lexicons for matching in Â§6. In addition, we experiment with priors that express different matchings in Â§7.</p><p>As for assumption (ii), previous work <ref type="bibr" target="#b42">(Xing et al., 2015;</ref><ref type="bibr" target="#b3">Artetxe et al., 2017)</ref> has achieved some success using an orthogonal transformation; recently, however, <ref type="bibr">SÃ¸gaard et al. (2018)</ref> demon- strated that monolingual embedding spaces are not approximately isomorphic and that there is a com- plex relationship between word form and meaning, which is only inadequately modeled by current ap- proaches, which for example cannot model poly- semy. Nevertheless, we will show that imbuing our model with these assumptions helps empirically in Â§6, giving them practical utility.</p><p>Why it Works: The Hubness Problem Why should we expect the bipartite matching prior to help, given that we know of cases when multi- ple source words should match a target word?</p><p>One answer is because the bipartite prior helps us obviate the hubness problem, a common issue in word-embedding-based bilingual lexicon induc- tion ( . The hubness problem is an intrinsic problem of high-dimensional vector spaces where certain vectors will be universal near- est neighbors, i.e. they will be the nearest neigh- bor to a disproportionate number of other vectors <ref type="bibr" target="#b30">(RadovanoviÂ´cRadovanoviÂ´c et al., 2010</ref>). Thus, if we allow one- to-many alignments, we will find the embeddings of certain elements of V src acting as hubs, i.e. the model will pick them to generate a disproportionate number of target embeddings, which reduces the quality of the embedding space. <ref type="bibr">5</ref> Another explanation for the positive effect of the Algorithm 1 Viterbi EM for our latent-variable model 1: repeat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>// Viterbi E-Step</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>m â argmax mâM log p Î¸ (m | S, T ) 4:</p><formula xml:id="formula_6">u trg â V trg \ {i : (i, j) â m } 5:</formula><p>// M-Step 6:</p><formula xml:id="formula_7">U Î£V â SVD T m S m 7: â¦ â U V 8: Âµ â 1 /|u trg | Â· iâu trg t i 9:</formula><p>Î¸ â (â¦ , Âµ ) 10: until converged one-to-one alignment prior is its connection to the Wasserstein distance and computational optimal transport <ref type="bibr" target="#b38">(Villani, 2008)</ref>. Concurrent work ( <ref type="bibr" target="#b14">Grave et al., 2018</ref>) similarly has found the one-to-one alignment prior to be beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parameter Estimation</head><p>We will conduct parameter estimation through Viterbi EM. We describe first the E-step, then the M-step. Viterbi EM estimates the parameters by al- ternating between the two steps until convergence. We give the complete pseudocode in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Viterbi E-Step</head><p>The E-step asks us to compute the posterior of la- tent bipartite matchings p(m | S, T ). Computation of this distribution, however, is intractable as it would require a sum over all bipartite matchings, which is #P-hard <ref type="bibr" target="#b37">(Valiant, 1979)</ref>. Tricks from com- binatorial optimization make it possible to max- imize over all bipartite matchings in polynomial time. Thus, we fall back on the Viterbi approxima- tion for the E-step ( <ref type="bibr" target="#b7">Brown et al., 1993;</ref><ref type="bibr" target="#b34">Samdani et al., 2012</ref>). The derivation will follow <ref type="bibr" target="#b15">Haghighi et al. (2008)</ref>. In order to compute</p><formula xml:id="formula_8">m = argmax mâM log p Î¸ (m | S, T )<label>(7)</label></formula><p>we construct a fully connected bipartite graph G = (E, V src âª V trg ), where E = V src Ã V trg . We weight each arc (i, j) â E with the weight between the projected source word and target word embeddings:</p><formula xml:id="formula_9">w ij = log p(t i | s j ) â log p(t i ) = ||t i â â¦ s j || 2 2 â ||t i âÂµ|| 2 2</formula><p>, where the normalizers of both Gaussians cancel as both have the same covariance matrix, i.e., I. Note that in the case where the t i and the s j are of length 1, that is, ||t i || 2 = ||s j || 2 = 1, and Âµ = 0, we recover cosine distance between the vectors up to an additive constant as orthogonal matrices preserve length (the constant is always -1 as ||t i || 2 = 1). We may ignore this constant during the E-step's combinatorial optimization. Note the optimal partial matching will contain no edges with weight w ij &lt; 0. For this reason, we remove such edges from the bipartite graph. To find the maximal partial bipartite matching on G to compute m , we employ an efficient algorithm as detailed in the next section.</p><p>Finding a Maximal Bipartite Matching We frame finding an optimal one-to-one alignment be- tween n src source and n trg words as a combina- torial optimization problem, specifically, a linear assignment problem (LAP; <ref type="bibr" target="#b5">Bertsimas and Tsitsiklis, 1997</ref>). In its original formulation, the LAP re- quires assigning a number of agents (source words) to a number of tasks (target words) at a cost that varies based on each assignment. An optimal solu- tion assigns each source word to exactly one target word and vice versa at minimum cost. The Hun- garian algorithm <ref type="bibr" target="#b22">(Kuhn, 1955</ref>) is one of the most well-known approaches for solving the LAP, but runs in O((n src + n trg ) 3 ). This works for smaller vocabulary sizes, 6 but is prohibitive for matching cross-lingual word embeddings with large vocabu- laries for real-world applications. <ref type="bibr">7</ref> For each source word, most target words, how- ever, are unlikely candidates for alignment. We thus propose to consider only the top k most similar target words for alignment with every source word. We sparsify the graph by weighting the edges for all other words with ââ. The remaining weights w ij are chosen as discussed above. We employ a version of the Jonker-Volgenant algorithm <ref type="bibr" target="#b18">(Jonker and Volgenant, 1987;</ref><ref type="bibr" target="#b39">Volgenant, 1996)</ref>, which has been optimized for LAP on sparse graphs, to find the maximum-weight matching m on G. <ref type="bibr">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">M-Step</head><p>Next, we will describe the M-step. Given an op- timal matching m computed in Â§4.1, we search for a matrix â¦ â R dÃd . We additionally enforce the constraint that â¦ is a real orthogonal matrix, i.e., â¦ â¦ = I. Previous work ( <ref type="bibr" target="#b42">Xing et al., 2015;</ref><ref type="bibr" target="#b3">Artetxe et al., 2017)</ref> found that the orthogonality constraint leads to noticeable improvements.</p><p>Our M-step optimizes two objectives indepen- dently. First, making use of the result in equa- tion (6), we optimize the following:</p><formula xml:id="formula_10">log p(T m | S m ,m )<label>(8)</label></formula><formula xml:id="formula_11">= ||T m â â¦ S m || 2 F + C</formula><p>with respect to â¦ subject to â¦ â¦ = I. (Note we may ignore the constant C during the optimization.) Second, we optimize the objective</p><formula xml:id="formula_12">log iâutrg p(t i ) = iâutrg ||t i â Âµ|| 2 2 + D (9)</formula><p>with respect to the mean parameter Âµ, which is simply an average. Note, again, we may ignore the constant D during optimization. Optimizing equation <ref type="formula" target="#formula_10">(8)</ref> with respect to â¦ is known as the orthogonal Procrustes prob- lem <ref type="bibr" target="#b35">(SchÃ¶nemann, 1966;</ref><ref type="bibr" target="#b13">Gower and Dijksterhuis, 2004</ref>) and has a closed form solution that ex- ploits the singular value decomposition <ref type="bibr" target="#b16">(Horn and Johnson, 2012</ref>). Namely, we compute U Î£V = T m S m . Then, we directly arrive at the optimum: â¦ = U V . Optimizing equation <ref type="formula">(9)</ref> can also been done in closed form; the point which min- imizes distance to the data points (thereby maxi- mizing the log-probability) is the centroid: Âµ = 1 /|utrg| Â· iâutrg t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Reinterpretation of Artetxe et al. (2017) as a Latent-Variable Model</head><p>The self-training method of <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>, our strongest baseline in Â§6, may also be inter- preted as a latent-variable model in the spirit of our exposition in Â§3. Indeed, we only need to change the edge-set prior p(m) to allow for edge sets other than those that are matchings. Specifically, a match- ing enforces a one-to-one alignment between types in the respective lexicons.  <ref type="formula" target="#formula_8">(2017)</ref>'s contribution as a latent-variable model, we lay down some more notation. Let A = {1, . . . , n src + 1} ntrg , where we define (n src + 1) to be none, a distinguished symbol indicating unalignment. The set A is to be interpreted as the set of all one-to-many alignments a on the bipartite vertex set V = V trg âª V src such that a i = j means the i th vertex in V trg is aligned to the j th vertex in V src . Note that a i = (n src + 1) = none means that the i th element of V trg is unaligned. Now, by analogy to our formulation in Â§3, we define</p><formula xml:id="formula_13">p(T | S) := aâA p(T | S, a) Â· p(a)<label>(10)</label></formula><formula xml:id="formula_14">= aâA ntrg i=1 p(t i | s a i , a i ) Â· p(a i ) (11) = ntrg i=1 nsrc+1 a i =1 p(t i | s a i , a i ) Â· p(a i ) (12)</formula><p>The move from equation <ref type="formula">(11)</ref> to equation <ref type="formula">(12)</ref> is the dynamic-programming trick introduced in <ref type="bibr" target="#b7">Brown et al. (1993)</ref>. This reduces the number of terms in the expression from exponentially many to polyno- mially many. We take p(a) to be a uniform distri- bution over all alignments with no parameters to be learned. log p(a i | S, T ) thus, we can simply find a component-wise as follows:</p><formula xml:id="formula_15">a i = argmax 1â¤a i â¤(nsrc+1) log p(a i | t i , s a i )<label>(13)</label></formula><p>Artetxe et al. <ref type="formula" target="#formula_8">(2017)</ref>'s M-step The M-step re- mains unchanged from the exposition in Â§3 with the exception that we fit â¦ given matrices S a and T a formed from a one-to-many alignment a, rather than a matching m.</p><p>Why a Reinterpretation? The reinterpretation of <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> as a probabilistic model yields a clear analytical comparison between our method and theirs. The only difference between the two is the constraint on the bilingual lexicon that the model is allowed to induce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We first conduct experiments on bilingual dictio- nary induction and cross-lingual word similarity on three standard language pairs, English-Italian, English-German, and English-Finnish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English-Italian</head><p>English-German English- <ref type="table" target="#tab_2">Finnish  5,000  25  num  iden 5,000  25  num  iden 5,000  25</ref> num iden <ref type="bibr" target="#b28">Mikolov et al. (2013c)</ref> 34.93 00.00 0.00 1.87 35.00 0.00 0.07 19.20 25.91 0.00 0.00 7.02 <ref type="bibr" target="#b42">Xing et al. (2015)</ref> 36.87 0.00 0. <ref type="bibr">13</ref>   English-German cross-lingual word similarity datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Details</head><p>Datasets For bilingual dictionary induction, we use the English-Italian dataset by  and the English-German and English-Finnish datasets by <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>. For cross-lingual word similarity, we use the RG-65 and WordSim- 353 cross-lingual datasets for English-German and the WordSim-353 cross-lingual dataset for English- Italian by <ref type="bibr" target="#b8">Camacho-Collados et al. (2015)</ref>.</p><p>Monolingual Embeddings We follow <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> and train monolingual embeddings with word2vec, CBOW, and negative sampling (Mikolov et al., 2013a) on a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC), a 1.6 billion word corpus for Italian (itWaC), a 0.9 bil- lion word corpus for German (SdeWaC), and a 2.8 billion word corpus for Finnish (Common Crawl). Implementation details Similar to <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>, we stop training when the improvement on the average cosine similarity for the induced dictionary is below 1 Ã 10 â6 between succeeding iterations. Unless stated otherwise, we induce a dictionary of 200,000 source and 200,000 target words as in previous work <ref type="bibr" target="#b28">(Mikolov et al., 2013c;</ref><ref type="bibr" target="#b2">Artetxe et al., 2016</ref>). For optimal 1:1 alignment, we have observed the best results by keeping the top k = 3 most similar target words. If using a rank constraint, we restrict the matching in the E- step to the top 40,000 words in both languages. <ref type="bibr">10</ref> Finding an optimal alignment on the 200,000 Ã 200,000 graph takes about 25 minutes on CPU; 11 with a rank constraint, matching takes around three minutes.</p><p>Baselines We compare our approach with and without the rank constraint to the original bilin- gual mapping approach by <ref type="bibr" target="#b28">Mikolov et al. (2013c)</ref>. In addition, we compare with <ref type="bibr" target="#b43">Zhang et al. (2016)</ref> and <ref type="bibr" target="#b42">Xing et al. (2015)</ref> who augment the former with an orthogonality constraint and normalization and an orthogonality constraint respectively.  <ref type="bibr" target="#b0">(2017)</ref> are special cases of our famework and com- parisons to these approaches thus act as an ablation study. Specifically, <ref type="bibr" target="#b28">Mikolov et al. (2013c)</ref> does not employ orthogonal Procrustes, but rather allows the learned matrix â¦ to range freely. Likewise, as discussed in Â§5, <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> make use of a Viterbi EM style algorithm with a different prior over edge sets. <ref type="bibr">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We show results for bilingual dictionary induc- tion in <ref type="table">Table 1</ref> and for cross-lingual word simi- larity in <ref type="table" target="#tab_2">Table 2</ref>. Our method with a 1:1 prior outperforms all baselines on English-German and English-Italian. <ref type="bibr">13</ref> Interestingly, the 1:1 prior by itself fails on English-Finnish with a 25 word and numerals seed lexicon. We hypothesize that the prior imposes too strong of a constraint to find a good solution for a distant language pair from a poor initialization. With a better-but still weakly supervised-starting point using identical strings, our approach finds a good solution. Alternatively, we can mitigate this deficiency effectively using a rank constraint, which allows our model to con- verge to good solutions even with a 25 word or numerals seed lexicon. The rank constraint gen- erally performs similarly or boosts performance, while being about 8 times faster. All approaches do better with identical strings compared to numerals, indicating that the former may be generally suitable as a default weakly-supervised seed lexicon.</p><p>On cross-lingual word similarity, our approach yields the best performance on WordSim-353 and RG-65 for English-German and is only outper- formed by <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> on English-Italian Wordsim-353.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Vocabulary sizes The beneficial contribution of the rank constraint demonstrates that in similar lan- guages, many frequent words will have one-to-one matchings, while it may be harder to find direct matches for infrequent words. We would thus ex- pect the latent variable model to perform better if we only learn dictionaries for the top n most fre- quent words in both languages. We show results for our approach in comparison to the baselines in <ref type="figure" target="#fig_4">Fig- ure 2</ref> for English-Italian using a 5,000 word seed lexicon across vocabularies consisting of different <ref type="bibr">12</ref> Other recent improvements such as symmetric reweight- ing ( <ref type="bibr" target="#b4">Artetxe et al., 2018</ref>) are orthogonal to our method, which is why we do not explicitly compare to them here. <ref type="bibr">13</ref> Note that results are not directly comparable to ( <ref type="bibr" target="#b10">Conneau et al., 2018</ref>) due to the use of embeddings trained on different monolingual corpora (WaCKy vs. Wikipedia). numbers n of the most frequent words 14 . The comparison approaches mostly perform sim- ilar, while our approach performs particularly well for the most frequent words in a language.</p><p>Different priors An advantage of having an ex- plicit prior as part of the model is that we can ex- periment with priors that satisfy different assump- tions. Besides the 1:1 prior, we experiment with a 2:2 prior and a 1:2 prior. For the 2:2 prior, we create copies of the source and target words V src and V trg and add these to our existing set of vertices V = (V trg +V trg , V src +V src ). We run the Viterbi E-step on this new graph G and merge matched pairs of words and their copies in the end. Similarly, for the 1:2 prior, which allows one source word to be matched to two target words, we augment the vertices with a copy of the source words V src and proceed as above. We show results for bilingual dictionary induction with different priors across different vocabulary sizes in <ref type="figure" target="#fig_5">Figure 3</ref>.</p><p>The 2:2 prior performs best for small vocabulary sizes. As solving the linear assignment problem for larger vocabularies becomes progressively more challenging, the differences between the priors be- come obscured and their performance converges.</p><p>Hubness problem We analyze empirically whether the prior helps with the hubness problem. Following , we define the hubness N k (y) at k of a target word y as follows:</p><formula xml:id="formula_16">N k (y) = |{x â Q | y â NN k (x, G)}| (14)</formula><p>where Q is a set of query source language words and NN k (x, G) denotes the k nearest neighbors  <ref type="formula" target="#formula_10">(18)</ref> -'hypocrisy' (13) jorge <ref type="formula" target="#formula_8">(17)</ref> ahmed <ref type="formula" target="#formula_2">(13)</ref> mohammed <ref type="formula" target="#formula_8">(17)</ref> ideologie -'ideology' (13) gewiÃ eduardo (13) -'certainly' (17) of x in the graph G. <ref type="bibr">15</ref> In accordance with , we set k = 20 and use the words in the evaluation dictionary as query terms. We show the target language words with the highest hubness using our method and <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> for English-German with a 5,000 seed lexicon and the full vocabulary in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="bibr">16</ref> Hubs are fewer and occur less often with our method, demonstrating that the prior-to some extent-aids with resolving hubness. Interestingly, compared to , hubs seem to occur less often and are more meaningful in current cross-lingual word embedding models. <ref type="bibr">17</ref> For in- stance, the neighbors of 'gleichgÃ¼ltigkeit' all relate to indifference and words appearing close to 'luis' or 'jorge' are Spanish names. This suggests that the prior might also be beneficial in other ways, e.g. by enforcing more reliable translation pairs for subsequent iterations. <ref type="bibr">15</ref> In other words, the hubness of a target word measures how often it occurs in the neighborhood of the query terms. <ref type="bibr">16</ref> We verified that hubs are mostly consistent across runs and similar across language pairs. 17  observed mostly rare words with N20 values of up to 50 and many with N20 &gt; 20.</p><p>en-tr en-bn en-hi et-fi  Low-resource languages Cross-lingual embed- dings are particularly promising for low-resource languages, where few labeled examples are typi- cally available, but are not adequately reflected in current benchmarks (besides the English-Finnish language pair). We perform experiments with our method with and without a rank constraint and <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> for three truly low- resource language pairs, English-{Turkish, Ben- gali, Hindi}. We additionally conduct an exper- iment for Estonian-Finnish, similarly to <ref type="bibr">SÃ¸gaard et al. (2018)</ref>. For all languages, we use fastText embeddings ( <ref type="bibr" target="#b6">Bojanowski et al., 2017</ref>) trained on Wikipedia, the evaluation dictionaries provided by <ref type="bibr" target="#b10">Conneau et al. (2018)</ref>, and a seed lexicon based on identical strings to reflect a realistic use case. We note that English does not share scripts with Ben- gali and Hindi, making this even more challenging. We show results in <ref type="table" target="#tab_5">Table 4</ref>. Surprisingly, the method by <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> is unable to leverage the weak supervision and fails to converge to a good solution for English-Bengali and English-Hindi. <ref type="bibr">18</ref> Our method without a rank constraint significantly outperforms <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>, while particularly for English-Hindi the rank constraint dramatically boosts performance.</p><p>Error analysis To illustrate the types of errors the model of <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> and our method with a rank constraint make, we query both of them with words from the test dictionary of Artetxe et al.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Partial lexicons of German and English shown as a bipartite graph. German is the target language and English is the source language. The ntrg = 7 German words are shown in blue and the nsrc = 6 English words are shown in green. A bipartite matching m between the two sets of vertices is also depicted. The German nodes in utrg are unmatched.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Artetxe et al. (2017)'s Viterbi E-Step In the context of Viterbi EM, it means the max over A will decompose additively s max aâA log p(a | S, T ) = ntrg i=1 max 1â¤a i â¤(nsrc+1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Seed dictionaries Following Artetxe et al. (2017), we use dictionaries of 5,000 words, 25 words, and a numeral dictionary consisting of words matching the [0-9]+ regular expression in both vocabularies. 9 In line with SÃ¸gaard et al. (2018), we additionally use a dictionary of identi- cally spelled strings in both vocabularies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fi- nally, we compare with Artetxe et al. (2016) who add dimension-wise mean centering to Xing et al. (2015), and Artetxe et al. (2017). Both Mikolov et al. (2013c) and Artetxe et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Bilingual dictionary induction results of our method and baselines for English-Italian with a 5,000 word seed lexicon across different vocabulary sizes.</figDesc><graphic url="image-1.png" coords="7,307.28,86.68,218.27,140.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Bilingual dictionary induction results of our method with different priors using a 5,000 word seed lexicon across different vocabulary sizes.</figDesc><graphic url="image-2.png" coords="8,72.00,62.81,156.80,100.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Spearman correlations on English-Italian and</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Hubs in English-German cross-lingual embedding space with degree of hubness. Non-name tokens are translated.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Bilingual dictionary induction results for English-

{Turkish, Bengali, Hindi} and Estonian-Finnish. 

</table></figure>

			<note place="foot" n="2"> For the purposes of this paper, we use bilingual lexicon and (bilingual) dictionary synonymously. On the other hand, unmodified lexicon always refers to a word list in a single language.</note>

			<note place="foot" n="4"> This is true by the definition of a matching. 5 In Â§5, we discuss the one-to-many alignment used in several of our baseline systems.</note>

			<note place="foot" n="6"> Haghighi et al. (2008) use the Hungarian algorithm to find a matching between 2000 source and target language words. 7 For reference, in Â§6, we learn bilingual lexicons between embeddings of 200,000 source and target language words. 8 After acceptance to EMNLP 2018, Edouard Grave pointed out that Sinkhorn propagation (Adams and Zemel, 2011; Mena et al., 2018) may have been a computationally more effective manner to deal with the latent matchings.</note>

			<note place="foot" n="9"> The resulting dictionaries contain 2772, 2148, and 2345 entries for English-{Italian, German, Finnish} respectively. 10 We validated both values with identical strings using the 5,000 word lexicon as validation set on English-Italian. 11 Training takes a similar amount of time as (Artetxe et al., 2017) due to faster convergence.</note>

			<note place="foot" n="14"> We only use the words in the 5,000 word seed lexicon that are contained in the n most frequent words. We do not show results for the 25 word seed lexicon and numerals as they are not contained in the smallest n of most frequent words.</note>

			<note place="foot" n="18"> One possible explanation is that Artetxe et al. (2017) particularly rely on numerals, which are normalized in the fastText embeddings.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors acknowledge Edouard Grave and Arya McCarthy, who, aside from being generally awesome, provided feedback post-submission on the ideas. Sebastian is supported by Irish Re-search Council Grant Number EBPPG/2014/30 and Science Foundation Ireland Grant Number SFI/12/RC/2289, co-funded by the European Re-gional Development Fund. Ryan is supported by an NDSEG fellowship and a Facebook fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query</head><p>Gold <ref type="bibr" target="#b3">Artetxe et al. (2017</ref> <ref type="table">) Ours   unregierbar  ungovernable  untenable  uninhabitable  nikolai  nikolaj  feodor  nikolai  memoranden  memorandums communiqus  memos  argentinier  argentinians  brazilians  argentines  trostloser  bleaker  dreary  dark-coloured  umverteilungen redistributions inequities  reforms  modischen  modish  trend-setting  modish  tranquilizer  tranquillizers  clonidine  opiates  sammelsurium hotchpotch  assortment  mishmash  demagogie  demagogy  opportunism  demagogy  andris  andris  rehn  viktor  dehnten  halmahera  overran  stretched  deregulieren</ref>   We show examples where nearest neighbours of the methods differ in <ref type="table">Table 5</ref>. Similar to <ref type="bibr" target="#b19">Kementchedjhieva et al. (2018)</ref>, we find that morphologically related words are often the source of mistakes. Other common sources of mistakes in this dataset are names that are trans- lated to different names and nearly synonymous words being predicted. Both of these sources indi- cate that while the learned alignment is generally good, it is often not sufficiently precise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related work</head><p>Cross-lingual embedding priors <ref type="bibr" target="#b15">Haghighi et al. (2008)</ref> first proposed an EM self-learning method for bilingual lexicon induction, representing words with orthographic and context features and using the Hungarian algorithm in the E-step to find an op- timal 1:1 matching. <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref> proposed a similar self-learning method that uses word em- beddings, with an implicit one-to-many alignment based on nearest neighbor queries. <ref type="bibr" target="#b40">VuliÂ´cVuliÂ´c and Korhonen (2016)</ref> proposed a more strict one-to-many alignment based on symmetric translation pairs, which is also used by <ref type="bibr" target="#b10">Conneau et al. (2018)</ref>. Our method bridges the gap between early latent vari- able and word embedding-based approaches and explicitly allows us to reason over its prior.</p><p>Hubness problem The hubness problem is an in- trinsic problem in high-dimensional vector spaces <ref type="bibr" target="#b30">(RadovanoviÂ´cRadovanoviÂ´c et al., 2010)</ref>.  first observed it for cross-lingual embedding spaces and proposed to address it by re-ranking neighbor lists.  proposed a max-marging ob- jective as a solution, while more recent approaches proposed to modify the nearest neighbor retrieval by inverting the softmax <ref type="bibr">(Smi, 2017)</ref> or scaling the similarity values ( <ref type="bibr" target="#b10">Conneau et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have presented a novel latent-variable model for bilingual lexicon induction, building on the work of <ref type="bibr" target="#b3">Artetxe et al. (2017)</ref>. Our model combines the prior over bipartite matchings inspired by <ref type="bibr" target="#b15">Haghighi et al. (2008)</ref> and the discriminative, rather than gen- erative, approach inspired by <ref type="bibr">Irvine and CallisonBurch (2013)</ref>. We show empirical gains on six language pairs and theoretically and empirically demonstrate the application of the bipartite match- ing prior to solving the hubness problem.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bilingual word vectors, orthogonal transformations and the inverted softmax</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ranking via Sinkhorn propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prescott</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.1925</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP16)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning bilingual word embeddings with (almost) no bilingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generalizing and Improving Bilingual Word Embedding Mappings with a Multi-Step Framework of Linear Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introduction to linear optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JosÃ©</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Word Translation Without Parallel Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">HervÃ©</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>JÃ©gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving Zero-Shot Learning by Mitigating the Hubness Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop track</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compiling bilingual lexicon entries from a non-parallel english-chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Procrustes problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garmt</forename><forename type="middle">B</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dijksterhuis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Berthet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11222</idno>
		<title level="m">Unsupervised Alignment of Embeddings with Wasserstein Procrustes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Bilingual Lexicons from Monolingual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2008</title>
		<meeting>ACL 2008</meeting>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="771" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Supervised bilingual lexicon induction with multiple monolingual signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A shortest augmenting path algorithm for dense and sparse linear assignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Jonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Volgenant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="340" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yova</forename><surname>Kementchedjhieva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ryan Cotterell, and Anders SÃ¸gaard</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward statistical machine translation without parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT Summit</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cheap translation for cross-lingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08665</idno>
		<title level="m">Learning latent permutations with Gumbel-Sinkhorn networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new view of the EM algorithm that justifies incremental, sparse and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<editor>M. I. Jordan</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hubs in space: Popular nearest neighbors in high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milos</forename><surname>RadovanoviÂ´cradovanoviÂ´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Nanopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjana</forename><surname>Ivanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Identifying word translations in non-parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd</title>
		<meeting>the 33rd</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="320" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of cross-lingual word embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>VuliÂ´cvuliÂ´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unified expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajhans</forename><surname>Samdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>MontrÃ©al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="688" to="698" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal Procrustes problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>SchÃ¶nemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On the Limitations of Unsupervised Bilingual Dictionary Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>SÃ¸gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>VuliÂ´cvuliÂ´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The complexity of computing the permanent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Optimal transport: Old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">CÃ©dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Linear and semi-assignment problems: a core oriented approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Volgenant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="917" to="932" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>VuliÂ´cvuliÂ´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Introduction to Graph Theory, 2 edition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">B</forename><surname>West</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ten Pairs to Tag Multilingual POS Tagging via Coarse Mapping between Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2016</title>
		<meeting>NAACL-HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1307" to="1317" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
