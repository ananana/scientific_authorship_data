<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-document Event Coreference Resolution based on Cross-media Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-document Event Coreference Resolution based on Cross-media Features</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we focus on a new problem of event coreference resolution across television news videos. Based on the observation that the contents from multiple data modalities are complementary, we develop a novel approach to jointly encode effective features from both closed captions and video key frames. Experiment results demonstrate that visual features provided 7.2% absolute F-score gain on state-of-the-art text based event extraction and coreference resolution.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>TV news is the medium that broadcasts events, stories and other information via television. The broadcast is conducted in programs with the name of "Newscast". Typically, newscasts require one or several anchors who are introducing stories and coordinating transition among topics, reporters or journalists who are presenting events in the fields and scenes that are captured by cameramen. Sim- ilar to newspapers, the same stories are often re- ported by multiple newscast agents. Moreover, in order to increase the impact on audience, the same stories and events are reported for mutliple times. TV audience passively receives redundant infor- mation, and often has difficulty in obtaining clear and useful digest of ongoing events. These proper- ties lead to needs for automatic methods to cluster information and remove redundancy. We propose a new research problem of event coreference reso- lution across multiple news videos.</p><p>To tackle this problem, a good starting point is processing the Closed Captions (CC) which is accompanying videos in newcasts. The CC is either generated by automatic speech recogni- tion (ASR) systems or transcribed by a human stenotype operator who inputs phonetics which are instantly and automatically translated into texts, where events can be extracted. There exist some previous event coreference resolution work such as <ref type="bibr" target="#b2">(Chen and Ji, 2009b;</ref><ref type="bibr" target="#b3">Chen et al., 2009;</ref><ref type="bibr" target="#b9">Lee et al., 2012;</ref><ref type="bibr" target="#b0">Bejan and Harabagiu, 2010)</ref>. However, they only focused on formally written newswire articles and utilized textual features. Such ap- proaches do not perform well on CC due to (1). the propagated errors from upper stream compo- nents (e.g., automatic speech/stenotype recogni- tion and event extraction); (2). the incomplete- ness of information. Different from written news, newscasts are often limited in time due to fixed TV program schedules, thus, anchors and journal- ists are trained and expected to organize reports which are comprehensively informative with com- plementary visual and CC descriptions within a short time. These two sides have minimal over- lapped information while they are inter-dependent. For example, anchors and reporters introduce the background story which are not presented in the videos, and thus the events extracted from CC of- ten lack information about participants.</p><p>For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, these two Conflict.Attack event mentions are coreferential. However, in the first event mention, a mistake in Closed Caption ("he was killed" → "it was killed") makes event extraction and text based coreference systems unable to detect and link "it" to the entity of "Jordanian pilot". Fortunately, videos often illustrate brief descriptions by vivid visual contents. Moreover, diverse anchors, re- porters and TV channels tend to use similar or identical video contents to describe the same story, even though they usually use different words and phrases. Therefore, the challenges in coreference resolution methods based on text information can be addressed by incorporating visual similarity. In this example, the visual similarity between the cor- responding video frames is high because both of them show the scene of the Jordanian pilot.</p><p>Similar work such as ( <ref type="bibr" target="#b7">Kong et al., 2014</ref>), <ref type="bibr" target="#b19">(Ramanathan et al., 2014)</ref>, <ref type="bibr" target="#b16">(Motwani and Mooney, 2012)</ref> and <ref type="bibr" target="#b18">(Ramanathan et al., 2013</ref>) have ex- plored methods of linking visual materials with texts. However, these methods mainly focus on connecting image concepts with entities in text mentions; and some of them do not clearly distin- guish entity and event in the documents since the definition of visual concepts often require both of them. Moreover, the aforementioned work mainly focuses on improving visual contents recognition by introducing text features while our work will take the opposite route, which takes advantage of visual information to improve event coreference resolution.</p><p>In this paper, we propose to jointly incorporate features from both speech (textual) and video (vi- sual) channels for the first time. We also build a newscast crawling system that can automatically accumulate video records and transcribe closed captions. With the crawler, we created a bench- mark dataset which is fully annotated with cross- document coreferential events 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Extraction</head><p>Given unstructured transcribed CC, we extract en- tities and events and present them in structured forms. We follow the terminologies used in ACE (Automatic Content Extraction) (NIST, 2005):</p><p>• Entity: an object or set of objects in the world, such as person, organization and facility.</p><p>• Entity mention: words or phrases in the texts that mention an entity.</p><p>• Event: a specific occurrence involving partici- pants.</p><p>• Event trigger: the word that most clearly ex- presses an event's occurrence.</p><p>• Event argument: an entity, or a temporal expres- sion or a value that has a certain role (e.g., Time- Within, Place) in an event.</p><p>• Event mention: a sentence (or a text span ex- tent) that mentions an event, including a distinct trigger and arguments involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Text based Event Coreference Resolution</head><p>Coreferential events are defined as the same spe- cific occurrence mentioned in different sentences, documents and transcript texts. Coreferential events should happen in the same place and within the same time period, and the entities involved and their roles should be identical. From the perspec- tive of extracted events, each specific attribute and argument from those events should match. How- ever, mentions for the same event may appear in forms of diverse words and phrases; and they do not always cover all arguments or attributes.</p><p>To tackle these challenges, we adopt a Maxi- mum Entropy (MaxEnt) model as in <ref type="bibr" target="#b2">(Chen and Ji, 2009b</ref>). We consider every pair of event men- tions which share the same event type as a can- didate and exploit features proposed in <ref type="bibr" target="#b2">(Chen and Ji, 2009b;</ref><ref type="bibr" target="#b3">Chen et al., 2009)</ref>. Note that the goal in <ref type="bibr" target="#b2">(Chen and Ji, 2009b;</ref><ref type="bibr" target="#b3">Chen et al., 2009</ref>) was to resolve event coreference within the same doc- ument, whereas our scenario yields to a cross- document/video transcript setting, so we remove some improper and invalid features. We also in- vestigated the approaches by ( <ref type="bibr" target="#b9">Lee et al., 2012)</ref> and <ref type="bibr" target="#b0">(Bejan and Harabagiu, 2010)</ref>, but the con- fidence estimation results from these alternative methods are not reliable. Moreover, the input of event coreference are automatic results from event extraction instead of gold standard, so the noise and errors significantly impact the corefer-ence performance, especially for unsupervised ap- proaches <ref type="bibr" target="#b0">(Bejan and Harabagiu, 2010)</ref>. Neverthe- less, we still incorporate features from the afore- mentioned methods. <ref type="table">Table 1</ref> shows the features that constitute the input of the MaxEnt model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual Similarity</head><p>Visual content provides useful cues complemen- tary with those used in text-based approach in event coreference resolution. For example, two coreferential events typically show similar or even duplicate scenes, objects, and activities in the vi- sual channel. Coherence of such visual content has been used in grouping multiple video shots into the same video story ( ), but it has not been used for event coreference res- olution. Recent work in computer vision has demonstrated tremendous progress in large-scale visual content recognition. In this work, we adopt the state-of-the-art techniques ( <ref type="bibr" target="#b8">Krizhevsky et al., 2012)</ref> and <ref type="bibr" target="#b20">(Simonyan and Zisserman, 2014</ref>) that train robust convolutional neural networks (CNN) over millions of web images to detect 20,000 se- mantic categories defined in ImageNet ( <ref type="bibr" target="#b4">Deng et al., 2009</ref>) from each image. The 2nd to the last layer features from such deep network can be considered as high-level visual representation that can be used to discriminate various seman- tic classes (scenes, objects, activity). It has been found effective in computing visual similarity be- tween images, by directly computing the L2 dis- tance of such features or through further met- ric learning. To compute the similarity between videos associated with two candidate event men- tions, we sample multiple frames from each video and aggregate the similarity scores of the few most similar image pairs between the videos. Let {f i 1 , f i 2 , ..., , f i l } be the key frames sampled from video V i and {f j 1 , f j 2 , ..., , f j l } be key frames sam- pled from video V j . All the frames are resized to a fixed resolution of 256 x 256 and fed into our pre- trained CNN model. We get the high-level visual representation F m = F C7(f m ) for each frame f m from the output of the 2nd to the last fully con- nected layer (FC7) of CNN model. F m is a 4096 dimension vector. The visual distance of frames f m and f n is defined by L 2 distance, which is</p><formula xml:id="formula_0">D mn = ||F C7(f i m ) − F C7(f j n )|| 2 .<label>(1)</label></formula><p>The distance of video pair (V i , V j ) is computed as</p><formula xml:id="formula_1">¯ D ij = 1 k * (fm,fn) D mn<label>(2)</label></formula><p>, where (f m , f n ) is the top k of most similar frame pairs. In our experiment, we use k = 3. Such aggregation method among the top matches is in- tended to capture similarity between videos that share only partially overlapped content. Each news video story typically starts with an introduction by an anchor person followed by news footages showing the visual scenes or activ- ities of the event. Therefore, when computing vi- sual similarity, it's important to exclude the anchor shot and focus on the story-related clips. Anchor frame detection ( ) is a well studied problem. In order to detect anchor frames auto- matically, a face detector is applied to all I-frames of a video. We can obtain the location and size of each detected face. After checking the tempo- ral consistency of the detected faces within each shot, we get a set of candidate anchor faces. The detected face regions are further extended to re- gions of interest that may include hair and upper body. All the candidate faces detected from the same video are clustered based on their HSV color histogram. It is reasonable to assume that the most frequent face cluster is the one corresponding to the anchor faces. Once the anchor frames are de- tected, they are excluded and only the non-anchor frames are used to compute the visual similarity between videos associated with event mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Joint Re-ranking</head><p>Using the visual distance calculated from Sec- tion 2.3, we can rerank the confidence values from Section 2.2 using the text-based MaxEnt model. We use the following empirical equation to adjust the confidence:</p><formula xml:id="formula_2">W ij = W ij * e − ¯ D ij α +1 ,<label>(3)</label></formula><p>where W ij denotes the original coreference con- fidence between event mentions i and j, D ij de- notes the visual distance between the correspond- ing video frames where the event mentions were spoken and α is a parameter which is used to ad- just the impact of visual distance. In the current implementation, we empirically set it as the aver- age of pair-wised visual distances between videos of all event coreference candidates. With this α</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>203</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Remarks (EM i : the first event mention, EM j : the sec- ond event mention)</p><p>Baseline type subtype pair of event type and subtype in EM i trigger pair trigger pair of EM i and EM j pos pair part-of-speech pair of triggers of EM i and EM j nominal 1 if the trigger of EM i is nominal nom number "plural" or "singular" if the trigger of EM i is nominal pronominal 1 if the trigger of EM i is pronominal exact match 1 if the trigger spelling in EM i matches that in EM j stem match 1 if the trigger stem in EM i matches that in EM j trigger sim the semantic similarity scores between triggers of EM i and EM j using WordNet <ref type="bibr" target="#b15">(Miller, 1995</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Setting</head><p>We establish a system that actively monitors over 100 U.S. major broadcast TV channels such as ABC, CNN and FOX, and crawls newscasts from these channels for more than two years ( <ref type="bibr" target="#b10">Li et al., 2013a)</ref>. With this crawler, we retrieve 100 videos and their correspondent transcribed CC with the topic of "ISIS" 2 . This system also tem- porally aligns the CC text with the transcribed text from automatic speech recognition following the methods in <ref type="figure" target="#fig_1">(Huang et al., 2003)</ref>. This provides ac- curate time alignment between the CC text and the video frames. As CC consists of capitalized let- ters, we apply the true-casing tool from Standford CoreNLP ( <ref type="bibr" target="#b13">Manning et al., 2014</ref>) on CC. Then we apply a state-of-the-art event extraction system ( <ref type="bibr" target="#b11">Li et al., 2013b;</ref>) to extract event men- tions from CC. We asked two human annotators to investigate all event pairs and annotate coref- erential pairs as the ground truth. Kappa coeffi- cient for measuring inter-annotator agreement is 2 abbreviation for Islamic State of Iraq and Syria 74.11%. In order to evaluate our system perfor- mance, we rank the confidence scores of all event mention pairs and present the results in Precision vs. Detection Depth curve. Finally we find the video frames corresponding to the event mentions, remove the anchor frames and calculate the visual similarity between the videos. Our final dataset consists of 85 videos, 207 events and 848 event pairs, where 47 pairs are considered coreferential.</p><p>We adopt the MaxEnt-based coreference reso- lution system from <ref type="bibr" target="#b2">(Chen and Ji, 2009b;</ref><ref type="bibr" target="#b3">Chen et al., 2009)</ref> as our baseline, and use ACE 2005 En- glish Corpus as the training set for the model. A 5-fold cross-validation is conducted on the train- ing set and the average f-score is 56%. It is lower than results from (Chen and Ji, 2009a) since we remove some features which are not available for the cross-document scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The peak F-score for the baseline system is 44.23% while our cross-media method boosts it to 51.43%. <ref type="figure" target="#fig_1">Figure 2</ref> shows the improvement af- ter incorporating the visual information. We adopt Wilcoxon signed-rank test to determine the signif- icance between the pairs of precision scores at the same depth. The z-ratio is 3.22, which shows the improvement is significant.</p><p>For example, the event pair "So why hasn't U.S. air strikes targeted Kobani within the city limits" and "Our strikes continue alongside our partners." was mistakenly considered coreferen- tial by text features. In fact, the former "strikes" mentions the airstrike and the latter refers to the war or battle, therefore, they are not coreferential.</p><p>The corresponding video shots demonstrate two different scenes: the former one shows bombing while the latter shows that the president is giving a speech about the strike. Thus the visual distance successfully corrected this error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Error Analysis</head><p>However, from <ref type="figure" target="#fig_1">Figure 2</ref> we can also notice that there are still some errors caused by the vi- sual features. One major error type resides in the negative pairs with both "relatively" high textual coreference confidence scores and "rela- tively" high visual similarity. From the text side, the event pair contains similar events, for exam- ple: "The Penn(tagon) says coalition air strikes in and around the Syrian city of Kobani have kill hundreds of ISIS fighters but more are stream- ing in even as the air campaign intensifies." and "Throughout the day, explosions from coalition air strikes sent plums of smoke towering into the sky.". They talk about two airstrikes during differ- ent time periods and are not coreferential, but the baseline system produces a high rank. Our current approach limits the image frames to those over- lapped with the speech of an event mention, and in this error, both videos show "battle" scene, yield- ing a small visual distance. The aforementioned assumption that anchors and journalists tend to use similar videos when describing the same events , which may introduce risk of error caused by sim- ilar text event mentions with similar video shots. For such errors, one potential solution is to expand the video frame windows to capture more events and concepts from videos. Expanding the detec- tion range to include visual events in the temporal neighborhood can also differentiate the events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>A systematic way of choosing α in Equation 3 will be useful. One idea is to adapt the α value for dif- ferent types of events, e.g., we expect some event types are more visually oriented than others and thus use a smaller α value.</p><p>We also notice the impact of the errors from the upstream event extraction system. According to ) the F-score of event trigger label- ing is 65.3%, and event argument labeling is 45%. Missing arguments in events is a main problem, thus the performance on automatically extracted event mentions is significantly worse. About 20 more coreferential pairs could be detected if events and arguments are perfectly extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions and Future Work</head><p>In this paper, we improved event coreference res- olution on newscast speech by incorporating vi- sual similarity. We also build a crawler that pro- vides a benchmark dataset of videos with aligned closed captions. This system can also help cre- ate more datasets to conduct research on video de- scription generation. In the future, we will focus on improving event extraction from texts by intro- ducing more fine-grained cross-media information such as object, concept and event detection results from videos. Moreover, joint detection of events from both sides is our ultimate goal, however, we need to explore the mapping among events from both text and visual sides, and automatic detection of a wide range of objects and events from news video itself is still challenging.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Similar visual contents improve detection of a coreferential event pair which has a low text-based confidence score. Closed Captions: "It 's not clear when it was killed."; "Jordan just executed two ISIS prisoners, direct retaliation for the capture of the killing Jordanian pilot."</figDesc><graphic url="image-2.png" coords="1,314.36,320.34,204.09,112.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance comparison between baseline and our cross-media method on top 150 pairs. Circles indicate the peak F-scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>1 if the attributes of EM i and EM j conflict Table 1: Features for Event Coreference Resolution we generally enhance the confidence of event pairs with small visual distances and penalize those with large ones. An alternative way for setting the alpha parameter is through cross validation over separate data partitions.</figDesc><table>) 
Arguments 
argument match 
1 if arguments holding the same roles in both EM i and 
EM j matches 

Attributes 
mod,pol,gen,ten 
four event attributes in EM i : modality, polarity, gener-
icity and tense 
mod conflict, 
pol conflict, gen conflict, 
ten conflict 

</table></figure>

			<note place="foot" n="1"> Dataset can be found at http://www.ee.columbia.edu/dvmm/newDownloads.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the U.S. DARPA DEFT Program No. FA8750-13-2-0041, ARL NS-CTA No. W911NF-09-2-0053, NSF CA-REER Award IIS-1523198, AFRL DREAM project, gift awards from IBM, Google, Disney and Bosch. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the of-ficial policies, either expressed or implied, of the U.S. Government. The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernment purposes notwithstanding any copyright notation here on.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised event coreference resolution with rich linguistic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Cosmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1412" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event coreference resolution: Algorithm, feature impact and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Events in Emerging Text Types (eETTs) Workshop, in conjunction with RANLP</title>
		<meeting>Events in Emerging Text Types (eETTs) Workshop, in conjunction with RANLP<address><addrLine>Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph-based event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing</title>
		<meeting>the 2009 Workshop on Graph-based Methods for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="54" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A pairwise event coreference model, feature impact and evaluation for event coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Events in Emerging Text Types, eETTs &apos;09</title>
		<meeting>the Workshop on Events in Emerging Text Types, eETTs &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovery and fusion of salient multimodal features toward news story segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndon</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giridharan</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Electronic Imaging</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="244" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Automatic closed caption alignment based on speech recognition transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin-Fu</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<pubPlace>Columbia</pubPlace>
		</imprint>
	</monogr>
	<note>Rapport technique</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>2014 IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint entity and event coreference resolution across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heeyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">News rover: Exploring topical structures and serendipity in heterogeneous multimedia news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jospeh</forename><forename type="middle">G</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morozoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia</title>
		<meeting>the 21st ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="449" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint event extraction via structured prediction with global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Constructing information networks using one single model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1846" to="1851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd</title>
		<meeting>52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<imprint>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
			<publisher>November</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving video activity recognition using object recognition and text mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tanvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th European Conference on Artificial Intelligence</title>
		<meeting>the 20th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="600" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nist</forename></persName>
		</author>
		<ptr target="http://www.itl.nist.gov/iad/mig/tests/ace/ace05/doc/ace05-evaplan.v3.pdf" />
		<title level="m">The ace 2005 evaluation plan</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video event understanding using natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2013 IEEE International Conference on Computer Vision</title>
		<meeting>2013 IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="905" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Linking people in videos with their names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Vignesh Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
