<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
							<email>aconneau@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>dkiela@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
							<email>schwenk@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıc</forename><surname>Lo¨ıc Barrault</surname></persName>
							<email>loic.barrault@univ-lemans.fr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>abordes@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Facebook AI Research</orgName>
								<orgName type="department" key="dep2">Facebook AI Research</orgName>
								<orgName type="department" key="dep3">Facebook AI Research</orgName>
								<orgName type="department" key="dep4">Facebook AI Research</orgName>
								<orgName type="institution" key="instit1">LIUM</orgName>
								<orgName type="institution" key="instit2">Université Le Mans</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="670" to="680"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many modern NLP systems rely on word embeddings, previously trained in an un-supervised manner on large corpora, as base features. Efforts to obtain embed-dings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsu-pervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsu-pervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features , which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words (or word em- beddings) ( <ref type="bibr" target="#b4">Bengio et al., 2003;</ref><ref type="bibr" target="#b8">Collobert et al., 2011;</ref><ref type="bibr" target="#b27">Mikolov et al., 2013;</ref><ref type="bibr" target="#b29">Pennington et al., 2014</ref>) have shown to provide useful features for various tasks in natural language processing and computer vision. While there seems to be a con- sensus concerning the usefulness of word embed- dings and how to learn them, this is not yet clear with regard to representations that carry the mean- ing of a full sentence. That is, how to capture the relationships among multiple words and phrases in a single vector remains an question to be solved.</p><p>In this paper, we study the task of learning uni- versal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks. Two questions need to be solved in order to build such an encoder, namely: what is the preferable neu- ral network architecture; and how and on what task should such a network be trained. Follow- ing existing work on learning word embeddings, most current approaches consider learning sen- tence encoders in an unsupervised manner like SkipThought ( ) or FastSent ( <ref type="bibr" target="#b12">Hill et al., 2016)</ref>. Here, we investigate whether su- pervised learning can be leveraged instead, tak- ing inspiration from previous results in computer vision, where many models are pretrained on the ImageNet ( <ref type="bibr" target="#b9">Deng et al., 2009</ref>) before being trans- ferred. We compare sentence embeddings trained on various supervised tasks, and show that sen- tence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy. We hypothesize that the suitability of NLI as a train- ing task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.</p><p>Unlike in computer vision, where convolutional neural networks are predominant, there are mul- tiple ways to encode a sentence using neural net- works. Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recur- rent and even simpler word composition schemes. Our experiments show that an encoder based on a bi-directional LSTM architecture with max pool- ing, trained on the Stanford Natural Language In- ference (SNLI) dataset <ref type="bibr" target="#b5">(Bowman et al., 2015)</ref>, yields state-of-the-art sentence embeddings com-pared to all existing alternative unsupervised ap- proaches like SkipThought or FastSent, while be- ing much faster to train. We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Transfer learning using supervised features has been successful in several computer vision appli- cations ( <ref type="bibr" target="#b30">Razavian et al., 2014</ref>). Striking examples include face recognition <ref type="bibr" target="#b33">(Taigman et al., 2014</ref>) and visual question answering ( <ref type="bibr" target="#b1">Antol et al., 2015)</ref>, where image features trained on ImageNet ( <ref type="bibr" target="#b9">Deng et al., 2009</ref>) and word embeddings trained on large unsupervised corpora are combined.</p><p>In contrast, most approaches for sentence repre- sentation learning are unsupervised, arguably be- cause the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence. Another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these bi- ases. Learning models on large unsupervised task makes it harder for the model to specialize. Lit- twin and <ref type="bibr" target="#b23">Wolf (2016)</ref> showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder. They pro- pose a loss that incorporates multiple orthogonal classifiers to counteract this effect.</p><p>Recent work on generating sentence embed- dings range from models that compose word em- beddings ( <ref type="bibr" target="#b20">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b2">Arora et al., 2017;</ref><ref type="bibr" target="#b36">Wieting et al., 2016b</ref>) to more complex neu- ral network architectures. SkipThought vectors (  propose an objective func- tion that adapts the skip-gram model for words <ref type="bibr" target="#b27">(Mikolov et al., 2013</ref>) to the sentence level. By en- coding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks. They further obtained better results using layer-norm regularization of their model in ( <ref type="bibr" target="#b3">Ba et al., 2016)</ref>. <ref type="bibr" target="#b12">Hill et al. (2016)</ref> showed that the task on which sentence embeddings are trained significantly impacts their quality. In ad- dition to unsupervised methods, they included su- pervised training in their comparison-namely, on machine translation data (using the WMT'14 En- glish/French and English/German pairs), dictio- nary definitions and image captioning data from the COCO dataset ( <ref type="bibr" target="#b21">Lin et al., 2014</ref>). These mod- els obtained significantly lower results compared to the unsupervised Skip-Thought approach.</p><p>Recent work has explored training sentence en- coders on the SNLI corpus and applying them on the SICK corpus ( <ref type="bibr" target="#b26">Marelli et al., 2014</ref>), either us- ing multi-task learning or pretraining ( <ref type="bibr" target="#b28">Mou et al., 2016;</ref><ref type="bibr" target="#b5">Bowman et al., 2015)</ref>. The results were in- conclusive and did not reach the same level as sim- pler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead ( <ref type="bibr" target="#b2">Arora et al., 2017)</ref>. To our knowledge, this work is the first attempt to fully exploit the SNLI cor- pus for building generic sentence encoders. As we show in our experiments, we are able to consis- tently outperform unsupervised approaches, even if our models are trained on much less (but human- annotated) data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>This work combines two research directions, which we describe in what follows. First, we ex- plain how the NLI task can be used to train univer- sal sentence encoding models using the SNLI task. We subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use. Specifically, we exam- ine standard recurrent models such as LSTMs and GRUs, for which we investigate mean and max- pooling over the hidden representations; a self- attentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Natural Language Inference task</head><p>The SNLI dataset consists of 570k human- generated English sentence pairs, manually la- beled with one of three categories: entailment, contradiction and neutral. It captures natural lan- guage inference, also known in previous incarna- tions as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality la- beled resources explicitly constructed in order to require understanding sentence semantics. We hy- pothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence embeddings in a supervised way. That is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sen- tence representations that capture universally use- ful features. Models can be trained on SNLI in two differ- ent ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).</p><p>Since our goal is to train a generic sentence en- coder, we adopt the first setting. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, a typical architecture of this kind uses a shared sentence encoder that outputs a representa- tion for the premise u and the hypothesis v. Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : (i) concatenation of the two representa- tions (u, v); (ii) element-wise product u * v; and (iii) absolute element-wise difference |u − v|. The resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fully- connected layers culminating in a softmax layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sentence encoder architectures</head><p>A wide variety of neural networks for encod- ing sentences into fixed-size representations ex- ists, and it is not yet clear which one best cap- tures generically useful information. We com- pare 7 different architectures: standard recurrent encoders with either Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), con- catenation of last hidden states of forward and backward GRU, Bi-directional LSTMs (BiLSTM) with either mean or max pooling, self-attentive network and hierarchical convolutional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">LSTM and GRU</head><p>Our first, and simplest, encoders apply recurrent neural networks using either LSTM <ref type="bibr" target="#b13">(Hochreiter and Schmidhuber, 1997</ref>) or GRU ( <ref type="bibr" target="#b6">Cho et al., 2014</ref>) modules, as in sequence to sequence en- coders ( <ref type="bibr" target="#b31">Sutskever et al., 2014</ref>). For a sequence of T words (w 1 , . . . , w T ), the network computes a set of T hidden representations h 1 , . . . , h T , with</p><formula xml:id="formula_0">h t = − −−− → LSTM(w 1 , .</formula><p>. . , w T ) (or using GRU units instead). A sentence is represented by the last hid- den vector, h T .</p><p>We also consider a model BiGRU-last that con- catenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">BiLSTM with mean/max pooling</head><p>For a sequence of T words {w t } t=1,...,T , a bidirec- tional LSTM computes a set of T vectors {h t } t . For t ∈ [1, . . . , T ], h t , is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions:</p><formula xml:id="formula_1">− → h t = − −−− → LSTM t (w 1 , . . . , w T ) ← − h t = ← −−− − LSTM t (w 1 , . . . , w T ) h t = [ − → h t , ← − h t ]</formula><p>We experiment with two ways of combining the varying number of {h t } t to form a fixed-size vec- tor, either by selecting the maximum value over each dimension of the hidden units (max pooling) <ref type="bibr" target="#b7">(Collobert and Weston, 2008)</ref> or by considering the average of the representations (mean pooling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Self-attentive network</head><p>The self-attentive sentence encoder ( <ref type="bibr" target="#b24">Liu et al., 2016;</ref><ref type="bibr" target="#b22">Lin et al., 2017</ref>) uses an attention mecha- nism over the hidden states of a BiLSTM to gen- erate a representation u of an input sentence. The attention mechanism is defined as :</p><formula xml:id="formula_2">¯ h i = tanh(W h i + b w ) α i = e ¯ h T i uw i e ¯ h T i uw u = t α i h i 672</formula><p>The movie was great</p><formula xml:id="formula_3">← − h 1 ← − h 2 ← − h 3 ← − h 4 − → h 4 − → h 3 − → h 2 − → h 1 w 1 w 2 w 3 w 4 x x x x x x x x max-pooling … … u :</formula><p>Figure 2: Bi-LSTM max-pooling network.</p><p>where {h 1 , . . . , h T } are the output hidden vec- tors of a BiLSTM. These are fed to an affine trans- formation (W , b w ) which outputs a set of keys</p><formula xml:id="formula_4">( ¯ h 1 , . . . , ¯ h T ).</formula><p>The {α i } represent the score of similarity between the keys and a learned con- text query vector u w . These weights are used to produce the final representation u, which is a weighted linear combination of the hidden vectors.</p><p>Following <ref type="bibr" target="#b22">Lin et al. (2017)</ref> we use a self- attentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task. Con- cretely, we have 4 context vectors u 1 w , u 2 w , u 3 w , u 4 w which generate 4 representations that are then con- catenated to obtain the sentence representation u. <ref type="figure">Figure 3</ref> illustrates this architecture.</p><p>The movie was great</p><formula xml:id="formula_5">u w ← − h 1 ← − h 2 ← − h 3 ← − h 4 − → h 4 − → h 3 − → h 2 − → h 1 α 1 α 2 α 3 α 4 u w 1 w 2 w 3 w 4</formula><p>Figure 3: Inner Attention network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Hierarchical ConvNet</head><p>One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent ( <ref type="bibr" target="#b38">Zhao et al., 2015</ref>), which con- catenates different representations of the sentences at different level of abstractions. Inspired by this architecture, we introduce a faster version consist- ing of 4 convolutional layers. At every layer, a representation u i is computed by a max-pooling operation over the feature maps (see <ref type="figure" target="#fig_1">Figure 4</ref>).  The final representation u = [u 1 , u 2 , u 3 , u 4 ] concatenates representations at different levels of the input sentence. The model thus captures hi- erarchical abstractions of an input sentence in a fixed-size representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training details</head><p>For all our models trained on SNLI, we use SGD with a learning rate of 0.1 and a weight decay of 0.99. At each epoch, we divide the learning rate by 5 if the dev accuracy decreases. We use mini- batches of size 64 and training is stopped when the learning rate goes under the threshold of 10 −5 . For the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units. We use open- source GloVe vectors trained on Common Crawl 840B 2 with 300 dimensions as fixed word embed- dings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation of sentence representations</head><p>Our aim is to obtain general-purpose sentence em- beddings that capture generic information that is name N task C examples MR 11k sentiment (movies) 2 "Too slow for a younger crowd , too shallow for an older one." (neg) CR 4k product reviews 2 "We tried it out christmas night and it worked great ." (pos) SUBJ 10k subjectivity/objectivity 2 "A movie that doesn't aim too high , but doesn't need to." (subj) MPQA 11k opinion polarity 2 "don't want"; "would like to tell"; (neg, pos) TREC 6k question-type 6 "What are the twin cities ?" (LOC:city) SST 70k sentiment (movies) 2 "Audrey Tautou has a knack for picking roles that magnify her [..]" (pos)  <ref type="bibr" target="#b17">and Ba, 2014</ref>) to fit a logistic regression classifier, with batch size 64.</p><p>Binary and multi-class classification We use a set of binary classification tasks (see <ref type="table" target="#tab_1">Table 1</ref>) that covers various types of sentence classifica- tion, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), sub- jectivity/objectivity (SUBJ) and opinion polarity (MPQA). We generate sentence vectors and train a logistic regression on top. A linear classifier re- quires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited. We tune the L2 penalty of the logistic regression with grid-search on the validation set.</p><p>Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R). We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint rep- resentation. For semantic relatedness evaluation, we follow the approach of ( <ref type="bibr" target="#b32">Tai et al., 2015</ref>) and learn to predict the probability distribution of re- latedness scores. We report Pearson correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS14 -Semantic Textual Similarity</head><p>While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 ( <ref type="bibr" target="#b0">Agirre et al., 2014</ref>). This dataset includes subsets of news articles, forum discussions, image descrip- tions and headlines from news articles contain- ing pairs of sentences (lower-cased), labeled with 3 https://www.github.com/ facebookresearch/SentEval a similarity score between 0 and 5. These tasks evaluate how the cosine distance between two sen- tences correlate with a human-labeled similarity score through Pearson and Spearman correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase detection The Microsoft Research</head><p>Paraphrase Corpus is composed of pairs of sen- tences which have been extracted from news sources on the Web. Sentence pairs have been human-annotated according to whether they cap- ture a paraphrase/semantic equivalence relation- ship. We use the same approach as with SICK-E, except that our classifier has only 2 classes.</p><p>Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models ( <ref type="bibr" target="#b14">Hodosh et al., 2013;</ref><ref type="bibr" target="#b21">Lin et al., 2014</ref>). The goal is either to rank a large collec- tion of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Caption Retrieval). We use a pairwise ranking- loss L cir (x, y):</p><formula xml:id="formula_6">y k max(0, α − s(V y, U x) + s(V y, U x k )) + x k max(0, α − s(U x, V y) + s(U x, V y k ))</formula><p>where (x, y) consists of an image y with one of its associated captions x, (y k ) k and (y k ) k are negative examples of the ranking loss, α is the margin and s corresponds to the cosine similarity. U and V are learned linear transformations that project the caption x and the image y to the same embedding space. We use a margin α = 0.2 and 30 contrastive terms. We use the same splits as in <ref type="bibr" target="#b16">(Karpathy and Fei-Fei, 2015</ref>), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test. For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and name task N premise hypothesis label SNLI NLI 560k "Two women are embracing while holding to go packages." "Two woman are holding packages." entailment</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SICK-E NLI 10k A man is typing on a machine used for stenography</head><p>The man isn't operating a steno- graph contradiction SICK-R STS 10k "A man is singing a song and play- ing the guitar" "A man is opening a package that contains headphones" 1.6 STS14 STS 4.5k "Liquid ammonia leak kills 15 in Shanghai" "Liquid ammonia leak kills at least 15 in Shanghai" 4.6   <ref type="figure" target="#fig_2">Figure 5</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical results</head><p>In this section, we refer to "micro" and "macro" averages of development set (dev) results on trans- fer tasks whose metrics is accuracy: we compute a "macro" aggregated score that corresponds to the classical average of dev accuracies, and the "mi- cro" score that is a sum of the dev accuracies, weighted by the number of dev samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architecture impact</head><p>Model We observe in <ref type="table" target="#tab_3">Table 3</ref> that different mod- els trained on the same NLI corpus lead to differ- ent transfer tasks results. The BiLSTM-4096 with the max-pooling operation performs best on both SNLI and transfer tasks. Looking at the micro and macro averages, we see that it performs signifi- cantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet. <ref type="table" target="#tab_3">Table 3</ref> also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when com- paring inner-attention and BiLSTM-Mean for in- stance.</p><p>We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose infor- mation of the input sentence. For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks. On the other hand, BiLSTM-Mean does not make sharp choices on which part of the sentence is more important than others. The difference be- tween the results seems to come from the different abilities of the models to incorporate general in- formation while not focusing too much on specific features useful for the task at hand. For a given model, the transfer quality is also sensitive to the optimization algorithm: when training with Adam instead of SGD, we observed that the BiLSTM-max converged faster on SNLI (5 epochs instead of 10), but obtained worse re- sults on the transfer tasks, most likely because of the model and classifier's increased capability to over-specialize on the training task.</p><p>Embedding size <ref type="figure" target="#fig_2">Figure 5</ref> compares the over- all performance of different architectures, showing the evolution of micro averaged performance with  <ref type="table">Table 4</ref>: Transfer test results for various architectures trained in different ways. Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way. † indicates methods that we trained, other transfer models have been extracted from ( <ref type="bibr" target="#b12">Hill et al., 2016</ref>). For best published supervised methods (no transfer), we consider AdaSent ( <ref type="bibr" target="#b38">Zhao et al., 2015)</ref>, TF-KLD ( <ref type="bibr" target="#b15">Ji and Eisenstein, 2013)</ref>, Tree-LSTM ( <ref type="bibr" target="#b32">Tai et al., 2015)</ref> and Illinois-LH system <ref type="bibr" target="#b19">(Lai and Hockenmaier, 2014</ref>). (*) Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods.</p><note type="other">Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-</note><note type="other">76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62 GloVe BOW † 78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding † 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9 .51/.54 BiLSTM-Max (untrained) † 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48 Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35 SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45 Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70 NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST) † (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI) † 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65 BiLSTM-Max (on AllNLI) † 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67 Supervised methods (directly trained for each task -no transfer) Naive Bayes -SVM 79.4 81.8 93.2 86.3 83.</note><formula xml:id="formula_7">1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - -</formula><p>regard to the embedding size.</p><p>Since it is easier to linearly separate in high di- mension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models. However, this is particularly true for some mod- els (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows. We hypothesize that such networks are able to incorporate infor- mation that is not directly relevant to the objective task (results on SNLI are relatively stable with re- gard to embedding size) but that can nevertheless be useful as features for transfer tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Task transfer</head><p>We report in <ref type="table">Table 4</ref> transfer tasks results for dif- ferent architectures trained in different ways. We group models by the nature of the data on which they were trained. The first group corresponds to models trained with unsupervised unordered sentences. This includes bag-of-words mod- els such as word2vec-SkipGram, the Unigram- TFIDF model, the Paragraph Vector model ( <ref type="bibr" target="#b20">Le and Mikolov, 2014)</ref>, the Sequential Denoising Auto-Encoder (SDAE) ( <ref type="bibr" target="#b12">Hill et al., 2016</ref>) and the SIF model ( <ref type="bibr" target="#b2">Arora et al., 2017)</ref>, all trained on the Toronto book corpus (   pervised ordered sentences such as FastSent and SkipThought (also trained on the Toronto book corpus). We also include the FastSent variant "FastSent+AE" and the SkipThought-LN version that uses layer normalization. We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for compar- ison with transfer learning approaches.</p><p>Comparison with SkipThought The best performing sentence encoder to date is the SkipThought-LN model, which was trained on a very large corpora of ordered sentences. With much less data (570k compared to 64M sentences) but with high-quality supervision from the SNLI dataset, we are able to consistently outperform the results obtained by SkipThought vectors. We train our model in less than a day on a single GPU compared to the best SkipThought-LN network trained for a month. Our BiLSTM-max trained on SNLI performs much better than released SkipThought vectors on MR, CR, MPQA, SST, MRPC-accuracy, SICK-R, SICK-E and STS14 (see <ref type="table">Table 4</ref>). Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA. We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST-LN). We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance (element-wise product and absolute difference) during training.</p><p>NLI as a supervised training set Our findings indicate that our model trained on SNLI obtains much better overall results than models trained on other supervised tasks such as COCO, dictio- nary definitions, NMT, PPDB ( <ref type="bibr" target="#b10">Ganitkevitch et al., 2013)</ref> and SST. For SST, we tried exactly the same models as for SNLI; it is worth noting that SST is smaller than NLI. Our representations constitute higher-quality features for both classification and similarity tasks. One explanation is that the natu- ral language inference task constrains the model to encode the semantic information of the input sen- tence, and that the information required to perform NLI is generally discriminative and informative.</p><p>Domain adaptation on SICK tasks Our trans- fer learning approach obtains better results than previous state-of-the-art on the SICK task -can be seen as an out-domain version of SNLI -for both entailment and relatedness. We obtain a pear- son score of 0.885 on SICK-R while <ref type="bibr" target="#b32">(Tai et al., 2015</ref>) obtained 0.868, and we obtain 86.3% test accuracy on SICK-E while previous best hand- engineered models <ref type="bibr" target="#b19">(Lai and Hockenmaier, 2014</ref>) obtained 84.5%. We also significantly outper- formed previous transfer learning approaches on SICK-E ( <ref type="bibr" target="#b5">Bowman et al., 2015</ref>) that used the pa- rameters of an LSTM model trained on SNLI to fine-tune on SICK (80.8% accuracy). We hypothe- size that our embeddings already contain the infor- mation learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task.</p><p>Image-caption retrieval results In <ref type="table" target="#tab_6">Table 5</ref>, we report results for the COCO image-caption re- trieval task. We report the mean recalls of 5 ran- dom splits of 1K test images. When trained with ResNet features and 30k more training data, the SkipThought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval R@1, and from 25.9 to 30.6 on image retrieval R@1. Our approach pushes the results even further, from 37.9 to 42.4 on cap- tion retrieval, and 30.6 to 33.2 on image retrieval. These results are comparable to previous approach of ( <ref type="bibr" target="#b25">Ma et al., 2015</ref>) that did not do transfer but di- rectly learned the sentence encoding on the image- caption retrieval task. This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task.</p><p>MultiGenre NLI The MultiNLI corpus ( <ref type="bibr" target="#b37">Williams et al., 2017)</ref> was recently released as a multi-genre version of SNLI. With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language. We augment <ref type="table">Table 4</ref> with our model trained on both SNLI and MultiNLI (AllNLI). We observe a significant boost in performance overall compared to the model trained only on SLNI. Our model even reaches AdaSent performance on CR, suggesting that having a larger coverage for the training task helps learn even better general representations. On semantic textual similarity STS14, we are also competitive with PPDB based paragram- phrase embeddings with a pearson score of 0.70. Interestingly, on caption-related transfer tasks such as the COCO image caption retrieval task, training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI (which contains mostly captions), which confirms the generalization power of our embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks. We showed that mod- els learned on NLI can perform better than mod- els trained in unsupervised conditions or on other supervised tasks. By exploring various architec- tures, we showed that a BiLSTM network with max pooling makes the best current universal sen- tence encoding methods, outperforming existing approaches like SkipThought vectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generic NLI training scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Hierarchical ConvNet architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Transfer performance w.r.t. embedding size using the micro aggregation method.</figDesc><graphic url="image-1.png" coords="6,72.00,463.99,218.24,113.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Classification tasks. C is the number of class and N is the number of samples. 

useful for a broad set of tasks. To evaluate the 
quality of these representations, we use them as 
features in 12 transfer tasks. We present our 
sentence-embedding evaluation procedure in this 
section. We constructed a sentence evaluation 
tool 3 to automate evaluation on all the tasks men-
tioned in this paper. The tool uses Adam (Kingma 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Natural Language Inference and Semantic Textual Similarity tasks. NLI labels are contra-
diction, neutral and entailment. STS labels are scores between 0 and 5. 

median (Med r) over the 5 splits. For fair compari-
son, we also report SkipThought results in our set-
ting, using 2048-dimensional pretrained ResNet-
101 (He et al., 2016) with 113k training images. 

Model 
NLI 
Transfer 
dim 
dev 
test 
micro macro 
LSTM 
2048 81.9 80.7 
79.5 
78.6 
GRU 
4096 82.4 81.8 
81.7 
80.9 
BiGRU-last 
4096 81.3 80.9 
82.9 
81.7 
BiLSTM-Mean 4096 79.0 78.2 
83.1 
81.7 
Inner-attention 
4096 82.3 82.5 
82.1 
81.0 
HConvNet 
4096 83.7 83.4 
82.0 
80.9 
BiLSTM-Max 
4096 85.0 84.5 
85.2 
83.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of sentence encoder ar-
chitectures on SNLI and (aggregated) transfer 
tasks. Dimensions of embeddings were selected 
according to best aggregated scores (see </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>). The sec- ond group consists of models trained with unsu-</figDesc><table>Caption Retrieval 

Image Retrieval 
Model 
R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r 
Direct supervision of sentence representations 
m-CNN 
(Ma et al., 2015) 
38.3 
-
81.0 
2 
27.4 
-
79.5 
3 
m-CNNENS 
(Ma et al., 2015) 
42.8 
-
84.1 
2 
32.6 
-
82.8 
3 
Order-embeddings 
(Vendrov et al., 2016) 
46.7 
-
88.9 
2 
37.9 
-
85.9 
2 
Pre-trained sentence representations 
SkipThought 
+ VGG19 (82k) 
33.8 
67.7 
82.1 
3 
25.9 
60.0 
74.6 
4 
SkipThought 
+ ResNet101 (113k) 
37.9 
72.2 
84.3 
2 
30.6 
66.2 
81.0 
3 
BiLSTM-Max (on SNLI) + ResNet101 (113k) 
42.4 
76.1 
87.0 
2 
33.2 
69.7 
83.6 
3 
BiLSTM-Max (on AllNLI) + ResNet101 (113k) 
42.6 
75.3 
87.3 
2 
33.9 
69.7 
83.8 
3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>COCO retrieval results. SkipThought is trained either using 82k training samples with VGG19 
features, or with 113k samples and ResNet-101 features (our setting). We report the average results on 5 
splits of 1k test images. 

</table></figure>

			<note place="foot" n="1"> https://www.github.com/ facebookresearch/InferSent</note>

			<note place="foot" n="2"> https://nlp.stanford.edu/projects/ glove/</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="91" />
		</imprint>
		<respStmt>
			<orgName>Computational Linguistics and Dublin City University</orgName>
		</respStmt>
	</monogr>
	<note>Association for</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 5th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rejean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PPDB: The paraphrase database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="758" to="764" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning distributed representations of sentences from unlabelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03483</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miach</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative improvements to distributional sentence similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Illinois-lh: A denotational and distributional approach to semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The multiverse loss for robust transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etai</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional lstm model and inner-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09090</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal convolutional neural networks for matching image and sentence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A sick cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 9th International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<title level="m">How transferable are neural networks in nlp applications? Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn features offthe-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Order-embeddings of images and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</title>
		<meeting>the 24th International Conference on Artificial Intelligence, IJCAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
