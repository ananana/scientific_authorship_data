<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Driven Grammatical Error Detection in Transcripts of Children&apos;s Speech</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Morley</surname></persName>
							<email>morleye@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Communicative Sciences and Disorders</orgName>
								<orgName type="institution" key="instit1">CSLU OHSU Portland</orgName>
								<orgName type="institution" key="instit2">New York University New York</orgName>
								<address>
									<postCode>97239, 10011</postCode>
									<region>OR, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Eva</forename><surname>Hallin</surname></persName>
							<email>ae.hallin@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Communicative Sciences and Disorders</orgName>
								<orgName type="institution" key="instit1">CSLU OHSU Portland</orgName>
								<orgName type="institution" key="instit2">New York University New York</orgName>
								<address>
									<postCode>97239, 10011</postCode>
									<region>OR, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
							<email>roarkbr@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Communicative Sciences and Disorders</orgName>
								<orgName type="institution" key="instit1">CSLU OHSU Portland</orgName>
								<orgName type="institution" key="instit2">New York University New York</orgName>
								<address>
									<postCode>97239, 10011</postCode>
									<region>OR, NY, NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Driven Grammatical Error Detection in Transcripts of Children&apos;s Speech</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="980" to="989"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate grammatical error detection in spoken language, and present a data-driven method to train a dependency parser to automatically identify and label grammatical errors. This method is agnostic to the label set used, and the only manual annotations needed for training are grammatical error labels. We find that the proposed system is robust to disfluencies, so that a separate stage to elide disfluen-cies is not required. The proposed system outperforms two baseline systems on two different corpora that use different sets of error tags. It is able to identify utterances with grammatical errors with an F1-score as high as 0.623, as compared to a baseline F1 of 0.350 on the same data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research into automatic grammatical error detec- tion has primarily been motivated by the task of providing feedback to writers, whether they be na- tive speakers of a language or second language learners. Grammatical error detection, however, is also useful in the clinical domain, for example, to assess a child's ability to produce grammatical lan- guage. At present, clinicians and researchers into child language must manually identify and clas- sify particular kinds of grammatical errors in tran- scripts of children's speech if they wish to assess particular aspects of the child's linguistic ability from a sample of spoken language. Such manual annotation, which is called language sample anal- ysis in the clinical field, is expensive, hindering its widespread adoption. Manual annotations may also be inconsistent, particularly between different research groups, which may be investigating dif- ferent phenomena. Automated grammatical error detection has the potential to address both of these issues, being both cheap and consistent.</p><p>Aside from performance, there are at least two key requirements for a grammatical error detector to be useful in a clinical setting: 1) it must be able to handle spoken language, and 2) it must be train- able. Clinical data typically consists of transcripts of spoken language, rather than formal written lan- guage. As a result, a system must be prepared to handle disfluencies, utterance fragments, and other phenomena that are entirely grammatical in speech, but not in writing. On the other hand, a system designed for transcripts of speech does not need to identify errors specific to written language such as punctuation or spelling mistakes. Further- more, a system designed for clinical data must be able to handle language produced by children who may have atypical language due to a developmen- tal disorder, and therefore may produce grammati- cal errors that would be unexpected in written lan- guage. A grammatical error detector appropriate for a clinical setting must also be trainable be- cause different groups of clinicians may wish to investigate different phenomena, and will there- fore prefer different annotation standards. This is quite different from grammatical error detectors for written language, which may have models for different domains, but which are not typically de- signed to enable the detection of novel error sets.</p><p>We examine two baseline techniques for gram- matical error detection, then present a simple data- driven technique to turn a dependency parser into a grammatical error detector. Interestingly, we find that the dependency parser-based approach mas- sively outperforms the baseline systems in terms of identifying ungrammatical utterances. Further- more, the proposed system is able to identify spe- cific error codes, which the baseline systems can- not do. We find that disfluencies do not degrade performance of the proposed detector, obviating the need (for this task) for explicit disfluency de- tection. We also analyze the output of our system to see which errors it finds, and which it misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>[EO] Overgeneralization errors He falled <ref type="bibr">[EO]</ref> .</p><p>[EW] Other word level errors He were <ref type="bibr">[EW]</ref> looking .</p><p>[EU] Utterance level errors And they came to stopped .</p><p>[OM] Omitted bound morpheme He go <ref type="bibr">[OM]</ref> .</p><p>[OW] Omitted word She <ref type="bibr">[OW]</ref> running . <ref type="table">Table 1</ref>: Error codes proposed in the SALT manual. Note that in SALT annotated transcripts, <ref type="bibr">[OM]</ref> and <ref type="bibr">[OW]</ref> are actually indicated by '*' followed by the morpheme or word hypothesized to be omitted. When treating codes (other than <ref type="bibr">[EU]</ref>) as tags, they are attached to the previous word in the string.</p><p>Finally, we evaluate our detector on a second set of data with a different label set and annotation standards. Although our proposed system does not perform as well on the second data set, it still out- performs both baseline systems. One interesting difference between the two data sets, which does appear to impact performance, is that the latter set more strictly follows SALT guidelines (see Sec- tion 2.1) to collapse multiple errors into a single label. This yields transcripts with a granularity of labeling somewhat less amenable to automation, to the extent that labels are fewer and can be re- liant on non-local context for aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Systematic Analysis of Language Transcripts (SALT)</head><p>The Systematic Analysis of Language Transcripts (SALT) is the de facto standard for clinicians look- ing to analyze samples of natural language. The SALT manual includes guidelines for transcrip- tion, as well as three types of annotations, of which two are relevant here: maze annotations, and error codes. 1 Mazes are similar to what is referred to as 'dis- fluencies' in the speech literature. The SALT manual defines mazes as "filled pauses, false starts, repetitions, reformulations, and interjec- tions" <ref type="bibr">(Miller et al., 2011, p. 6)</ref>, without defining any of these terms. Partial words, which are in- cluded and marked in SALT-annotated transcripts, are also included in mazes. Mazes are delimited by parentheses, and have no internal structure, un- like disfluencies annotated following the Switch- board guidelines <ref type="bibr" target="#b10">(Meteer et al., 1995)</ref>, which are commonly followed by the speech and language <ref type="bibr">1</ref> SALT also prescribes annotation of bound morphemes and clitics, for example -ed in past tense verbs. We preprocess all of the transcripts to remove bound morpheme and clitic annotations. processing communities. An example maze anno- tation would be: "He (can not) can not get up."</p><p>The SALT manual proposes the set of error codes shown (with examples) in <ref type="table">Table 1</ref>, but re- search groups may use a subset of these codes, or augment them with additional codes. For example, the SALT-annotated Edmonton Narrative Norms Instrument (ENNI) corpus ( <ref type="bibr" target="#b19">Schneider et al., 2006</ref>) rarely annotates omitted morphemes ( <ref type="bibr">[OM]</ref>), in- stead using the <ref type="bibr">[EW]</ref> code. Other SALT-annotated corpora include errors that are not described in the SALT manual. For example the CSLU ADOS cor- pus (Van Santen et al., 2010) includes the <ref type="bibr">[EX]</ref> tag for extraneous words, and the Narrative Story Retell corpus (SALT Software, 2014b) uses the code <ref type="bibr">[EP]</ref> to indicate pronominal errors (albeit inconsistently, as many such errors are coded as <ref type="bibr">[EW]</ref> in this corpus). We note that the definitions of certain SALT errors, notably <ref type="bibr">[EW]</ref> and <ref type="bibr">[EU]</ref>, are open to interpretation, and that these codes capture a wide variety of errors. For example, some of the errors captured by the <ref type="bibr">[EW]</ref> code are: pronominal case and gender errors; verb tense er- rors; confusing 'a' and 'an'; and using the wrong preposition.</p><p>The SALT guidelines specify as a general rule that annotators should not mark utterances with more than two omissions ( <ref type="bibr">[OM]</ref> or <ref type="bibr">[OW]</ref>) and/or word-level errors (ex <ref type="bibr">[EW]</ref>, <ref type="bibr">[EP]</ref>) (SALT Soft- ware, 2014a). Instead, annotators are instructed to code such utterances with an utterance-level er- ror ( <ref type="bibr">[EU]</ref>). How strictly annotators adhere to this rule affects the distribution of errors, reducing the number of word-level errors and increasing the number of utterance-level errors. Following this rule also increases the variety of errors captured by the <ref type="bibr">[EU]</ref> code. The annotations in different corpora, including ENNI and NSR, vary in how strictly they follow this rule, even though this is not mentioned in the the published descriptions of these corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Grammatical Error Detection</head><p>The most visible fruits of research into grammati- cal error detection are the spellchecking and gram- mar checking tools commonly included with word processors, for example Microsoft Word's gram- mar checker. Although developed for handling written language, many of the techniques used to address these tasks could still be applicable to transcripts of speech because many of the same errors can still occur. The earliest grammatical- ity tools simply performed pattern matching <ref type="bibr" target="#b9">(Macdonald et al., 1982)</ref>, but this approach is not robust enough to identify many types of errors, and pat- tern matching systems are not trainable, and there- fore cannot be adapted quickly to new label sets. Subsequent efforts to create grammaticality classi- fiers and detectors leveraged information extracted from parsers ( <ref type="bibr" target="#b7">Heidorn et al., 1982)</ref> and language models <ref type="bibr" target="#b0">(Atwell, 1987)</ref>. These systems, however, were developed for formal written English pro- duced by well-educated adults, as opposed to spo- ken English produced by young children, partic- ularly children with suspected developmental de- lays.</p><p>There have been a few investigations into tech- niques to automatically identify particular con- structions in transcripts of spoken English. Bow- den and <ref type="bibr" target="#b1">Fox (2002)</ref> proposed a rule-based sys- tem to classify many types of errors made by learners of English. Although their system could be used on either transcripts of speech, or on written English, they did not evaluate their sys- tem in any way. <ref type="bibr" target="#b3">Caines and Buttery (2010)</ref> use a logistic regression model to identify the zero- auxiliary construction (e.g., 'you going home?') with over 96% accuracy. Even though the zero- auxilliary construction is not necessarily ungram- matical, identifying such constructions may be useful as a preprocessing step to a grammatical- ity classifier. Caines and Buttery also demonstrate that their detector can be integrated into a sta- tistical parser yielding improved performance, al- though they are vague about the nature of the parse improvement (see <ref type="bibr">Caines and Buttery, 2010, p. 6</ref>).</p><p>Hassanali and Liu (2011) conducted the first in- vestigation into grammaticality detection and clas- sification in both speech of children, and speech of children with language impairments. They identi- fied 11 types of errors, and compared three types of systems designed to identify the presence of each type of error: 1) rule based systems; 2) deci- sion trees that use rules as features; and 3) naive Bayes classifiers that use a variety of features. They were able to identify all error types well (F1 &gt; 0.9 in all cases), and found that in general the statistical systems outperformed the rule based systems. Hassanali and Liu's system was designed for transcripts of spoken language collected from children with impaired language, and is able to detect the set of errors they defined very well. However, it cannot be straightforwardly adapted to novel error sets.</p><p>Morley et al. <ref type="formula">(2013)</ref> evaluated how well the detectors proposed by Hassanali and Liu could identify utterances with SALT error codes. They found that a simplified version of one of Has- sanali and Liu's detectors was the most effective at identifying utterances with any SALT error codes, although performance was very low (F1=0.18). Their system uses features extracted solely from part of speech tags with the Bernoulli Naive Bayes classifier in Scikit ( <ref type="bibr" target="#b14">Pedregosa et al., 2012</ref>). Their detector may be adaptable to other annotation standards, but it does not identify which errors are in each utterance; it only identifies which utter- ances have errors, and which do not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Redshift Parser</head><p>We perform our experiments with the redshift parser 2 , which is an arc-eager transition-based de- pendency parser. We selected redshift because of its ability to perform disfluency detection and de- pendency parsing jointly. <ref type="bibr" target="#b8">Honnibal and Johnson (2014)</ref> demonstrate that this system achieves state- of-the-art performance on disfluency detection, even compared to single purpose systems such as the one proposed by <ref type="bibr" target="#b15">Qian and Liu (2013)</ref>. <ref type="bibr" target="#b16">Rasooli and Tetreault (2014)</ref> have developed a sys- tem that performs disfluency detection and depen- dency parsing jointly, and with comparable perfor- mance to redshift, but it is not publicly available as of yet.</p><p>Redshift uses an averaged perceptron learner, and implements several feature sets. The first fea- ture set, which we will refer to as ZHANG is the one proposed by <ref type="bibr" target="#b21">Zhang and Nivre (2011)</ref>. It in- cludes 73 templates that capture various aspects of: the word at the top of the stack, along with its leftmost and rightmost children, parent and grand- parent; and the word on the buffer, along with its leftmost children; and the second and third words on the buffer. Redshift also includes fea- tures extracted from the Brown clustering algo- rithm ( <ref type="bibr" target="#b2">Brown et al., 1992)</ref>. Finally, redshift in- cludes features that are designed to help iden- tify disfluencies; these capture rough copies, ex- act copies, and whether neighboring words were marked as disfluent. We will refer to the feature set containing all of the features implemented in redshift as FULL. We refer the reader to Honnibal and Johnson (2014) for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data, Preprocessing, and Evaluation</head><p>Our investigation into using a dependency parser to identify and label grammatical errors requires training data with two types of annotations: de- pendency labels, and grammatical error labels. We are not aware of any corpora of speech with both of these annotations. Therefore, we use two dif- ferent sets of training data: the Switchboard cor- pus, which contains syntactic parses; and SALT annotated corpora, which have grammatical error annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Switchboard</head><p>The Switchboard treebank ( <ref type="bibr" target="#b5">Godfrey et al., 1992</ref>) is a corpus of transcribed conversations that have been manually parsed. These parses include EDITED nodes, which span disfluencies. We pre- process the Switchboard treebank by removing all partial words as well as all words dominated by EDITED nodes, and converting all words to lower- case. We then convert the phrase-structure trees to dependencies using the Stanford dependency con- verter <ref type="bibr" target="#b4">(De Marneffe et al., 2006</ref>) with the basic de- pendency scheme, which produces dependencies that are strictly projective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SALT Annotated Corpora</head><p>We perform two sets of experiments on the two SALT-annotated corpora described in <ref type="table" target="#tab_1">Table 2</ref>. We carry out the first set of experiments on on the Ed- monton Narrative Norms Instrument (ENNI) cor- pus, which contains 377 transcripts collected from children between the ages of 3 years 11 months and 10 years old. The children all lived in Edmon- ton, Alberta, Canada, were typically developing, and were native speakers of English.</p><p>After   <ref type="bibr">[EO]</ref>, and omitted morphemes instead of <ref type="bibr">[OM]</ref>. The <ref type="bibr">[EU]</ref> code is also far more frequent in ENNI than NSR. Finally, the NSR corpus includes an error code that does not appear in the ENNI corpus: <ref type="bibr">[EP]</ref>, which indicates a pronominal error, for example using the wrong person or case.</p><note type="other">ENNI NSR [EP] 0 20 [EO] 0 495 [EW] 4,916 1,506 [EU] 3,332 568 [OM] 10 297 [OW] 766 569 Total 9,024 3,455 (b) Error code counts</note><p>[EP], however, is rarely used. We preprocess the ENNI and NSR corpora to reconstruct surface forms from bound morpheme annotations (ex. 'go/3S' becomes 'goes'), partial words, and non-speech sounds. We also either ex- cise manually identified mazes or remove maze annotations, depending upon the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evaluation</head><p>Evaluating system performance in tagging tasks on manually annotated data is typically straight-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>983</head><p>Evaluation Level: ERROR UTTERANCE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Individual error codes Has error? Gold error codes: [EW] [EW]</head><p>Yes Predicted error codes: <ref type="bibr">[EW]</ref> [OW] Yes Evaluation: TP FN FP TP <ref type="figure">Figure 1</ref>: Illustration of UTTERANCE and ERROR level evaluation TP = true positive; FP = false positive; FN = false negative forward: we simply compare system output to the gold standard. Such evaluation assumes that the best system is the one that most faithfully repro- duces the gold standard. This is not necessarily the case with applying SALT error codes for three reasons, and each of these reasons suggests a dif- ferent form of evaluation. First, automatically detecting SALT error codes is an important task because it can aid clini- cal investigations. As <ref type="bibr" target="#b12">Morley et al. (2013)</ref> il- lustrated, even extremely coarse features derived from SALT annotations, for example a binary fea- ture for each utterance indicating the presence of any error codes, can be of immense utility for iden- tifying language impairments. Therefore, we will evaluate our system as a binary tagger: each ut- terance, both in the manually annotated data and system output either contains an error code, or it does not. We will label this form of evaluation as UTTERANCE level.</p><p>Second, clinicians are not only interested in how many utterances have an error, but also which particular errors appear in which utterances. To address this issue, we will compute precision, re- call, and F1 score from the counts of each er- ror code in each utterance. We will label this form of evaluation as ERROR level. <ref type="figure">Figure 1</ref> illus- trates both UTTERANCE and ERROR level evalua- tion. Note that the utterance level error code <ref type="bibr">[EU]</ref> is only allowed to appear once per utterance. As a result, we will ignore any predicted <ref type="bibr">[EU]</ref> codes beyond the first.</p><p>Third, the quality of the SALT annotations themselves is unknown, and therefore evaluation in which we treat the manually annotated data as a gold standard may not yield informative metrics. <ref type="bibr" target="#b13">Morley et al. (2014)</ref> found that there are likely inconsistencies in maze annotations both within and across corpora. In light of that finding, it is possible that error code annotations are somewhat inconsistent as well. Furthermore, our approach has a critical difference from manual annotation: we perform classification one utterance at a time, while manual annotators have access to the context of an utterance. Therefore certain types of errors, for example using a pronoun of the wrong gender, or responding ungrammatically to a question (ex. 'What are you doing?' 'Eat.') will appear gram- matical to our system, but not to a human anno- tator. We address both of these issues with an in- depth analysis of the output of one of our systems, which includes manually re-coding utterances out of context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detecting Errors in ENNI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We evaluate two existing systems to see how ef- fectively they can identify utterances with SALT error codes: 1) Microsoft Word 2010's gram- mar check, and 2) the simplified version of Has- sanali and Liu's grammaticality detector (2011) proposed by <ref type="bibr" target="#b12">Morley et al. (2013)</ref> (mentioned in Section 2.2). We configured Microsoft Word 2010's grammar check to look for the following classes of errors: negation, noun phrases, subject- verb agreement, and verb phrases (see http:// bit.ly/1kphUHa). Most error classes in gram- mar check are not relevant for transcribed speech, for example capitalization errors or confusing it's and its; we selected classes of errors that would typically be indicated by SALT error codes.</p><p>Note that these baseline systems can only give us an indication of whether there is an error in the utterance or not; they do not provide the spe- cific error tags that mimic the SALT guidelines. Hence we evaluate just the UTTERANCE level per- formance of the baseline systems on the ENNI de- velopment and test sets. These results are given in the top two rows of each section of <ref type="table" target="#tab_2">Table 3</ref>. We apply these systems to utterances in two condi- tions: with mazes (i.e., disfluencies) excised; and with unannotated mazes left in the utterances. As can be seen in <ref type="table" target="#tab_2">Table 3</ref>, the performance Microsoft Word's grammar checker degrades severely when <ref type="bibr">(a)</ref> Him <ref type="bibr">[EW]</ref> (can not) can not get up . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proposed System</head><p>Using the ENNI corpus, we now explore various configurations of a system for grammatical error code detection. All of our systems use redshift to learn grammars and to parse. First, we train an initial grammar G 0 on the Switchboard tree- bank ( <ref type="bibr" target="#b5">Godfrey et al., 1992</ref>) (preprocessed as de- scribed in Section 3.1). Redshift learns a model for part of speech tagging concurrently with G 0 . We use G 0 to parse the training portion of the ENNI corpus. Then, using the SALT annotations, we append error codes to the dependency arc labels in the parsed ENNI corpus, assigning each error code to the word it follows in the SALT annotated data. <ref type="figure" target="#fig_0">Figure 2</ref> shows a SALT annotated utterance, as well as its dependency parse augmented with error codes. Finally, we train a grammar G Err on the parse of the ENNI training fold that includes the augmented arc labels. We can now use G Err to automatically apply SALT error codes: they are simply encoded in the dependency labels. We also apply the <ref type="bibr">[EW]</ref> label to any word that is in a list of overgeneralization errors <ref type="bibr">3</ref> . We modify three variables in our initial trials on the ENNI development set. First, we change the proportion of utterances in the training data that contain an error by removing utterances. <ref type="bibr">4</ref> Doing so allows us to alter the operating point of our sys-tem in terms of precision and recall. Second, we again train and test on two versions of the ENNI corpus: one which has had mazes excised, and the other which has them present (but not annotated). Third, we evaluate two feature sets: ZHANG and FULL.</p><p>The plots in <ref type="figure" target="#fig_1">Figure 3</ref> show how the per- formances of our systems at different operating points vary, while <ref type="table" target="#tab_2">Table 3</ref> shows the performance of our best system configurations on the ENNI de- velopment and test sets. Surprisingly, we see that neither the choice of feature set, nor the presence of mazes has much of an effect on system per- formance. This is in strong contrast to Microsoft Word's grammar check, which is minimally effec- tive when mazes are included in the data. The <ref type="bibr" target="#b12">Morley et al. (2013)</ref> system is robust to mazes, but still performs substantially worse than our pro- posed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Error Analysis</head><p>We now examine the errors produced by our best performing system for data in which mazes are present. As shown in <ref type="table" target="#tab_2">Table 3</ref>, when we apply our system to ENNI-development, the UTTERANCE P/R/F1 is 0.831 / 0.502 / 0.626 and the ERROR P/R/F1is 0.759 / 0.434 / 0.552. This system's per- formance detecting specific error codes is shown in <ref type="table">Table 4</ref>. We see that the recall of <ref type="bibr">[EU]</ref> errors is quite low compared with the recall for <ref type="bibr">[EW]</ref> and <ref type="bibr">[OW]</ref> errors. This is not surprising, as human an- notators may need to leverage the context of an ut- terance to identify <ref type="bibr">[EU]</ref> errors, while our system makes predictions for each utterance in isolation.   <ref type="table">Table 4</ref>: ERROR level detection performance for each code (system trained on ENNI; 30% error utterances; ZHANG feature set; with mazes)</p><p>We randomly sampled 200 utterances from the development set that have a manually annotated error, are predicted by our system to have an er- ror, or both. A speech-language pathologist who has extensive experience with using SALT for re- search purposes in both clinical and typically de- veloping populations annotated the errors in each utterance. She annotated each utterance in isola- tion so as to ignore contextual errors. We compare our annotations to the original annotations, and system performance using our annotations and the original annotations as different gold standards. The results of this comparison are shown in <ref type="table" target="#tab_4">Table  5</ref>.</p><p>Comparing our manual annotations to the orig- inal annotations, we notice some disagreements. We suspect there are two reasons for this. First, unlike the original annotators, we annotate these utterances out of context. This may explain why we identify far fewer utterance level error <ref type="bibr">[EU]</ref> codes than the original annotators (20 compared with 67). Second, we may be using different cri- teria for each error code than the original anno- tators. This is an inevitable issue, as the SALT guidelines do not provide detailed definitions of the error codes, nor do individual groups of anno- tators. To illustrate, the "coding notes" section of   <ref type="table">Table 6</ref>: Error detection performance on NSR-development, mazes included the description of the ENNI corpus 5 only lists the error codes that were used consistently, but does not describe how to apply them. These findings illustrate the importance of having a rapidly train- able error code detector: research groups will be interested in different phenomena, and therefore will likely have different annotation standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Detecting Errors in NSR</head><p>We apply our system directly to the NSR corpus with mazes included. We use the same parameters set on the ENNI corpus in Section 4.2. We apply the model trained on ENNI to NSR, but find that it does not perform very well as illustrated in <ref type="table">Table  6</ref>. These results further underscore the need for a trainable error code detector in this domain, as opposed to the static error detectors that are more common in the grammatical error detection litera- ture.</p><p>We see in <ref type="table">Table 6</ref> that retraining our model on NSR data improves performance substantially (UTTERANCE F1 improves from 0.178 to 0.277), but not to the level we observed on the ENNI cor- pus. The <ref type="bibr" target="#b12">Morley et al. (2013)</ref> system also per- forms worse when trained and tested on NSR, as compared with ENNI. When mazes are included, the performance of Microsoft Word's grammar check is higher on NSR than on ENNI (F1=0.261 vs 0.084), but it it still yields the lowest perfor- mance of the three systems. We find that combin- ing our proposed system with either or both of the baseline systems further improves performance.</p><p>The NSR corpus differs from ENNI in several ways: it is smaller, contains fewer errors, and uses a different set of tags with a different distribution from the ENNI corpus, as shown in <ref type="table" target="#tab_1">Table 2</ref>. We found that the smaller amount of training data is not the only reason for the degradation in perfor- mance; we trained a model for ENNI with a set of training data that is the same size as the one for NSR, but did not observe a major drop in perfor- mance. We found that UTTERANCE F1 drops from 0.626 to 0.581, and ERROR F1 goes from 0.552 to 0.380, not nearly the magnitude drop in accuracy observed for NSR.</p><p>We believe that a major reason for why our sys- tem performs worse on NSR than ENNI may be that the ENNI annotations adhere less strictly to certain SALT recommendations than do the ones in NSR. The SALT guidelines suggest that utter- ances with two or more word-level <ref type="bibr">[EW]</ref> and/or omitted word <ref type="bibr">[OW]</ref> errors should only be tagged with an utterance-level <ref type="bibr">[EU]</ref> error <ref type="bibr" target="#b17">(SALT Software, 2014a</ref>). ENNI, however, has many utter-ances with multiple <ref type="bibr">[EW]</ref> and <ref type="bibr">[OW]</ref> error codes, along with utterances containing all three error codes. NSR has very few utterances with <ref type="bibr">[EU]</ref> and other codes, or multiple <ref type="bibr">[EW]</ref> and <ref type="bibr">[OW]</ref> codes. The finer grained annotations in ENNI may sim- ply be easier to learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Directions</head><p>We have proposed a very simple method to rapidly train a grammatical error detector and classifier. Our proposed system only requires training data with error code annotations, and is agnostic as to the nature of the specific error codes. Furthermore, our system's performance does not appear to be affected by disfluencies, which reduces the burden required to produce training data.</p><p>There are several key areas we plan to inves- tigate in the future. First, we would like to ex- plore different update functions for the parser; the predicted error codes are a byproduct of parsing, but we do not care what the parse itself looks like. At present, the parser is updated whenever it pro- duces a parse that diverges from the gold stan- dard. It may be better to update only when the error codes predicted for an utterance differ from the gold standard. Second, we hope to explore fea- tures that could be useful for identifying grammat- ical errors in multiple data sets. Finally, we plan to investigate why our system performed so much better on ENNI than on NSR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) SALT annotated utterance; mazes indicated by parentheses; (b) Dependency parse of same utterance parsed with a grammar trained on the Switchboard corpus and augmented dependency labels. We use a corpus of parses with augmented labels to train our grammaticality detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SALT error code detection performance at various operating points on ENNI development set Eval Mazes Excised Mazes Present System type P R F1 P R F1 Development MS Word UTT 0.843 0.245 0.380 0.127 0.063 0.084 Morley et al. (2013) UTT 0.407 0.349 0.376 0.343 0.321 0.332 Current paper UTT 0.943 0.470 0.627 0.831 0.502 0.626 ERR 0.895 0.412 0.564 0.759 0.434 0.552 Test MS Word UTT 0.824 0.209 0.334 0.513 0.219 0.307 Morley et al. (2013) UTT 0.375 0.328 0.350 0.349 0.252 0.293 Current Paper UTT 0.909 0.474 0.623 0.809 0.501 0.618 ERR 0.682 0.338 0.452 0.608 0.360 0.452</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Summary of ENNI and NSR Corpora. 
There can be multiple errors per utterance. Word 
counts include mazes. 

we evaluate how well our method works when it 
is applied to another corpus with different anno-
tation standards. Specifically, we train and test 
our technique on the Narrative Story Retell (NSR) 
corpus (SALT Software, 2014b), which contains 
496 transcripts collected from typically develop-
ing children living in Wisconsin and California 
who were between the ages of 4 years 4 months 
and 12 years 8 months old. The ENNI and NSR 
corpora were annotated by two different research 
groups, and as Table 2 illustrates, they contain 
a different distribution of errors. First, ENNI 
uses the [EW] (other word-level error) tag to code 
both overgeneralization errors instead of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Baseline and current paper systems' performance on ENNI. Evaluation is at the UTTERANCE 
(UTT) level except for the current paper's system, which also presents evaluation at the ERROR (ERR) 
level. 

Error Code 
P 
R 
F1 
EU 
0.639 0.193 0.297 
EW 
0.832 0.582 0.685 
OW 
0.680 0.548 0.607 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>986 Tag</head><label>986</label><figDesc></figDesc><table>Gold 
Gold Count Disagreement 
P 
R 
F1 
[EU] Original 
67 
52 
0.500 0.149 0.230 
Revised 
20 
0.450 0.333 0.383 
[EW] Original 
137 
27 
0.859 0.533 0.658 
Revised 
126 
0.800 0.540 0.645 
[OW] Original 
16 
13 
0.667 0.275 0.480 
Revised 
15 
0.444 0.267 0.333 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>System performance using ERROR level evaluation on 200 utterances selected from ENNI-dev 
using original and revised annotations as gold standard 

UTTERANCE level 
ERROR level 
System 
P 
R 
F1 
P 
R 
F1 
ENNI-trained 
0.310 0.124 0.178 0.157 0.057 0.084 
NSR-trained 
0.243 0.249 0.277 0.150 0.195 0.170 
MS Word 
0.561 0.171 0.261 
-
-
-
Morley et al. (2013) 
0.250 0.281 0.264 
-
-
-
NSR ∪ MS Word 
0.291 0.447 0.353 
-
-
-
NSR ∪ Morley et al. (2013) 0.297 0.387 0.336 
-
-
-
All 3 
0.330 0.498 0.397 
-
-
-

</table></figure>

			<note place="foot" n="2"> Redshift is available at https://github.com/ syllog1sm/redshift. We use the version in the experiment branch from May 15, 2014.</note>

			<note place="foot" n="3"> The list of overgeneralization errors was generously provided by Kyle Gorman 4 Of course, we never modify the development or test data.</note>

			<note place="foot" n="5"> http://www.saltsoftware.com/salt/ databases/ENNIRDBDoc.pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the following people for valuable input into this study: Joel Tetreault, Jan van Santen, Emily Prud'hommeaux, Kyle Gorman, Steven Bedrick, Alison Presmanes Hill and others in the CSLU Autism research group at OHSU. This material is based upon work supported by the National Institute on Deafness and Other Communication Disorders of the Na-tional Institutes of Health under award number R21DC010033. The content is solely the respon-sibility of the authors and does not necessarily rep-resent the official views of the National Institutes of Health.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to detect grammatical errors in a text without parsing it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Atwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<editor>Bente Maegaard, editor, EACL</editor>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-04" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A diagnostic approach to the detection of syntactic errors in english for non-native speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard K</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>The University of Texas-Pan American Department of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You talking to me?: A predictive model for zero auxiliary constructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Caines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula</forename><surname>Buttery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground</title>
		<meeting>the 2010 Workshop on NLP and Linguistics: Finding the Common Ground</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Switchboard: Telephone speech corpus for research and development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Holliman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="517" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring language development in early childhood education: a case study of grammar checking in child language transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Khairun-Nisa Hassanali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 6th Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="87" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The EPISTLE text-critiquing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Heidorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin S</forename><surname>Chodorow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="326" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint incremental disfluency detection and dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The writer&apos;s workbench: Computer aids for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">S</forename><surname>Lawrence T Frase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stacey A</forename><surname>Gingrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keenan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational psychologist</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="179" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dysfluency annotation stylebook for the switchboard corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">A</forename><surname>Marie W Meteer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rukmini</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Assessing language production using SALT software: A clinician&apos;s guide to language sample analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Andriacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Nockerts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>SALT Software, LLC</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The utility of manual and automatic linguistic error codes for identifying neurodevelopmental disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Van Santen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the Eighth Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Challenges in automating maze detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">Eva</forename><surname>Hallin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Linguistics and Clinical Psychology</title>
		<meeting>the First Workshop on Computational Linguistics and Clinical Psychology<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Duchesnay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Scikit-learn: Machine learning in python. CoRR, abs/1201.0490</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Disfluency detection using multi-step stacked learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lucy Vanderwende, Hal Daumé III, and Katrin Kirchhoff, editors, HLT-NAACL</title>
		<meeting><address><addrLine>Atlanta, Georgia, USA, June</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="820" to="825" />
		</imprint>
	</monogr>
	<note>The Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Non-monotonic parsing of fluent umm i mean disfluent sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<meeting><address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
	<note>Gosse Bouma and Yannick Parmentier</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llc</forename><surname>Salt Software</surname></persName>
		</author>
		<title level="m">Course 1306: Transcription-Conventions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Part 3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<ptr target="http://www.saltsoftware.com/salt/databases/NarStoryRetellRDBDoc.pdf" />
		<title level="m">LLC SALT Software. 2014b. Narrative Story Retell Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Storytelling from pictures using the edmonton narrative norms instrument</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phyllis</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denyse</forename><surname>Hayward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><forename type="middle">Vis</forename><surname>Dubé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Speech Language Pathology and Audiology</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">224</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computational prosodic markers for autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Jan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">T</forename><surname>Van Santen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lois</forename><forename type="middle">M</forename><surname>Prud&amp;apos;hommeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autism</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="236" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Short Papers)</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="188" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
