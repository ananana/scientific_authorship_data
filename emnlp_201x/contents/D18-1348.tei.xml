<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Zero-shot User Intent Detection via Capsule Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Yan</surname></persName>
							<email>yanxiaohui2@huawei.com, yichang@acm.org</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Technologies</orgName>
								<address>
									<postCode>95050</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Artificial Intelligence</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
								<address>
									<postCode>60607</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Zero-shot User Intent Detection via Capsule Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3090" to="3099"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3090</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users&apos; utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: INTENT-CAPSNET that extracts semantic features from utterances and aggregates them to discriminate existing intents, and INTENTCAPSNET-ZSL which gives INTENTCAPSNET the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the increasing complexity and accuracy of speech recognition technology, companies are striving to deliver intelligent conversation under- standing systems as people interact with soft- ware agents that run on speaker devices or smart phones via natural language interface <ref type="bibr" target="#b8">(Hoy, 2018)</ref>. Products like Apple's Siri, Amazon's Alexa and Google Assistant are able to interpret human speech and respond them via synthesized voices.</p><p>With recent developments in deep neural net- works, user intent detection models ( <ref type="bibr" target="#b9">Hu et al., 2009;</ref><ref type="bibr" target="#b24">Xu and Sarikaya, 2013;</ref><ref type="bibr" target="#b26">Zhang et al., 2016</ref>; <ref type="bibr" target="#b16">Liu and Lane, 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2016b</ref>) are pro- posed to classify user intents given their diversely * Indicates Equal Contribution expressed utterances in the natural language. The decent performances on intent detection usually come with deep neural network classifiers opti- mized on large-scale utterances which are human- labeled among existing predefined user intents.</p><p>As more features and skills are being added to devices which expand their capabilities to new programs, it is common for voice assistants to en- counter the scenario where no labeled utterance of an emerging user intent is available in the train- ing data, as illustrated in <ref type="figure">Figure 1</ref>. Current in- tent detection methods train classifiers in a super- vised fashion and they are good at discriminating existing intents such as Get Weather and Play Music whose labeled utterances are already avail- able. However, these models, by the nature of de- signs, are incapable to detect utterances of emerg- ing intents like AddToPlaylist and RateABook, since no labeled utterances are available. More- over, it's labor-intensive and time-consuming to annotate utterances of emerging intents and retrain the whole intent detection model. Thus, it is imperative to develop intent detection models with the zero-shot learning (ZSL) ability <ref type="bibr" target="#b14">(Lampert et al., 2014;</ref><ref type="bibr" target="#b20">Socher et al., 2013;</ref><ref type="bibr" target="#b0">Changpinyo et al., 2016)</ref>: the ability to expand classifiers and the intent detection space beyond the existing intents, of which we have labeled utterances dur- ing training, to emerging intents, of which no la- beled utterances are available.</p><p>The research on zero-shot intent detection is still in its infancy. Previous zero-shot learning methods for intent detection utilize external re- sources such as label ontologies <ref type="bibr">(Ferreira et al., 2015a,b)</ref> or manually defined attributes that de- scribe intents <ref type="bibr" target="#b25">(Yazdani and Henderson, 2015)</ref> to associate existing and emerging intents, which re- quire extra annotation. Compatibility-based meth- ods for zero-shot intent detection <ref type="bibr" target="#b1">(Chen et al., 2016a;</ref><ref type="bibr" target="#b12">Kumar et al., 2017</ref>) assume the capability</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• • How cold is it in Princeton Junction? • • Should I bring an umbrella today?</head><p>• • Put Sungmin into my summer playlist</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Existing Intents with Labeled Utterances</head><p>Get Weather</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Play Music</head><p>• • I want to hear any tune from twenties.</p><p>• Figure 1: Illustration of the proposed INTENTCAPSNET-ZSL model for zero-shot intent detection: labeled utterances with existing intents like GetWeather and PlayMusic are used to train an intent detection classifier among existing intents, in which SemanticCaps extract intepretable semantic features and DetectionCaps dynamically aggregate semantic features for intent detection using a novel routing-by-agreement mechanism. For emerging intents, INTENTCAPSNET-ZSL builds zero-shot DetectionCaps that utilize the (1) outputs of SemanticCaps, (2) the routing information on existing intents from DetectionCaps, and (3) similarities of the emerging intent label to existing intent labels to discriminate emerging intents like AddToPlayist from RateABook. Solid lines indicate the training process and dash lines indicate the zero-shot inference process.</p><p>of learning a high-quality mapping from the utter- ance to its intent directly, so that such mapping can be further capitalized to measure the compatibility of an utterance with emerging intents. However, the diverse semantic expressions may impede the learning of such mapping.</p><p>In this work, we make the very first attempt to tackle the zero-shot intent detection problem with a capsule-based <ref type="bibr" target="#b6">(Hinton et al., 2011;</ref><ref type="bibr" target="#b18">Sabour et al., 2017</ref>) model. A capsule houses a vector represen- tation of a group of neurons, and the orientation of the vector encodes properties of an object (like the shape/color of a face), while the length of the vec- tor reflects its probability of existence (how likely a face with certain properties exists). The capsule model learns a hierarchy of feature detectors via a routing-by-agreement mechanism: capsules for detecting low-level features (like nose/eyes) send their outputs to high-level capsules (such as faces) only when there is a strong agreement of their pre- dictions to high-level capsules.</p><p>The aforementioned properties of capsule mod- els could be quite appealing for text modeling, specifically in this case, modeling the user utter- ance for intent detection: low-level semantic fea- tures such as the get action, time and city name contribute to a more abstract intent (GetWeather) collectively. A semantic feature, which may be expressed quite differently among users, can con- tribute more to one intent than others. The dy- namic routing-by-agreement mechanism can be used to dynamically assign a proper contribution of each semantic and aggregate them to get an in- tent representation.</p><p>More importantly, we discover the potential of zero-shot learning ability on the capsule model, which is not yet widely recognized. It makes the capsule model even more suitable for text mod- eling when no labeled utterances are available for emerging intents. The ability to neglect the disagreed output of low-level semantics for cer- tain intents during routing-by-agreement encour- ages the learning of generalizable semantic fea- tures that can be adapted to emerging intents. For each emerging intent with no labeled utterances, a Zero-shot DetectionCaps is constructed explic- itly by using not only semantic features Seman- ticCaps extracted, but also existing routing agree- ments from DetectionCaps and similarities of an emerging intent label to existing intent labels. In summary, the contributions of this work are:</p><p>• Expanding capsule neural networks to text modeling, by extracting and aggregating seman- tics from utterances in a hierarchical manner;</p><p>• Proposing a novel and effective capsule-based model for zero-shot intent detection;</p><p>• Showing and interpreting the effectiveness of our model on two real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>In this section, we first define related concepts, and formally state the problem.</p><p>Intent. An intent is a purpose, or a goal that under- lies a user-generated utterance <ref type="bibr" target="#b23">(Watson Assistant, 2017</ref>). An utterance can be associated with one or multiple intents. We only consider the basic case that an utterance is with a single intent. However, utterances with multiple intents can be handled by segmenting them into single-intent snippets using sequential tagging tools like CRF ( <ref type="bibr" target="#b13">Lafferty et al., 2001</ref>), which we leave for future works. Intent Detection. Given a labeled training dataset where each sample has the following format: (x, y) where x is an utterance and y is its intent la- bel, each training example is associated with one of K existing intents y ∈ Y = {y 1 , y 2 , ..., y K }. The intent detection task tries to associate an ut- terance x existing with its correct intent category in the existing intent classes Y . Zero-shot Intent Detection. Given the labeled training set {(x, y)} where y∈Y , the zero-shot intent detection task aims to detect an utterance x emerging which belongs to one of L emerging in- tents z∈Z = {z 1 , z 2 , ..., z L } where Y ∩Z = ∅.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We propose two architectures based on cap- sule models: INTENTCAPSNET that is trained to discriminate among utterances with existing labels, e.g. existing intents for intent detec- tion; INTENTCAPSNET-ZSL that gives zero-shot learning ability to INTENTCAPSNET for discrim- inating unseen labels, i.e. emerging intents in this case. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the cores of the pro- posed architectures are three types of capsules: SemanticCaps that extract interpretable semantic features from the utterance, DetectionCaps that aggregate semantic features for intent detection, and Zero-shot DetectionCaps which discriminate emerging intents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SemanticCaps</head><p>In the original capsule model ( <ref type="bibr" target="#b18">Sabour et al., 2017)</ref>, convolution-based PrimaryCaps are intro- duced as the first layer to obtain different vector- ized features from the raw input image. While in this work, an intrinsically similar motivation is adopted to extract different semantic features from the raw utterance by a new type of capsule named SemanticCaps. Unlike the PrimaryCaps which use convolution operators with a large reception field to extract spacial-proximate features, the Seman- ticCaps is based on a bi-direction recurrent neural network with multiple self-attention heads, where each self-attention head focuses on certain part of the utterance and extracts a semantic feature that may not be expressed by words in proximity.</p><p>Given an input utterance x = (w 1 , w 2 , ..., w T ) of T words, each word is represented by a vector of dimension D W that can be pre-trained using a skip-gram language model (   INTENTCAPSNET-ZSL. During training, utterances with ex- isting intents are fed into the SemanticCaps which output vec- torized semantic features, i.e. semantic vectors. Then Detec- tionCaps combine these features into higher-level prediction vectors and output an activation vector for intent detection on each existing intent. During inference, emerging utterances take advantages of the SemanticCaps trained in INTENTCAP- SNET to extract semantic features from the utterance (shown in 1), then the vote vectors on the existing intents are trans- ferred to emerging intents (shown in 2) using similarities be- tween existing and emerging intents (shown in 3). The ob- tained activation vectors for emerging intents are used for zero-shot intent detection.</p><p>A recurrent neural network such as a bidirectional LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) is ap- plied to sequentially encode the utterance into hid- den states:</p><formula xml:id="formula_0">→ h t = LSTM f w (w t , ← h t−1 ), ← h t = LSTM bw (w t , ← h t+1 ).</formula><p>(1)</p><p>For each word w t , we concatenate each for- ward hidden state h t obtained from the forward LSTM f w with a backward hidden state ← h t from LSTM bw to obtain a hidden state h t for the word w t . The whole hidden state matrix can be defined as H = (h 1 , h 2 , ..., h T ) ∈ R T ×2D H , where D H is the number of hidden units in each LSTM.</p><p>Inspired by the success of self-attention mech- anisms ( <ref type="bibr" target="#b22">Vaswani et al., 2017;</ref><ref type="bibr" target="#b15">Lin et al., 2017)</ref> for sentence embedding, we adopt a multi-head self-attention framework where each self-attention head is encouraged to be attentive to a specific se- mantic feature of the utterance, such as certain sets of keywords or phrases in the utterance: one self- attention may be attentive for the "get" action in GetWeather, while another one may be attentive to city name in GetWeather: it decides for itself what semantics to be attentive to.</p><p>A self-attention weight matrix A is computed as:</p><formula xml:id="formula_1">A = softmax W s2 tanh W s1 H T ,<label>(2)</label></formula><p>where W s1 ∈ R D A ×2D H and W s2 ∈ R R×D A are weight matrices for the self-attention. D A is the hidden unit number of self-attention and R is the number of self-attention heads. The softmax func- tion makes sure for each self-attention head, the attentive scores on all the words sum to one. A total number of R semantic features are ex- tracted from the input utterance, each from a sep- arate self-attention head:</p><formula xml:id="formula_2">M = AH, where M = (m 1 , m 2 , ..., m R ) ∈ R R×2D H . Each m r is a 2D H −dimensional semantic vector.</formula><p>Each semantic vector will have a distinguish- able orientation when the objective is properly reg- ularized (details in Equation 6), as we want each attention to be attentive to a unique semantic fea- ture of the utterance. The vector representation adopted in capsules is suitable to portray the low- level semantic properties as well as high-level in- tents of the utterance, where the orientation of a vector represents semantic/intent properties that may slightly vary depending on the expressions. The capsule encourages the learning of general- izable semantic vectors: less informative seman- tic properties for one intent may not be penalized by their orientations: they simply possess small norms as they are less likely to exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DetectionCaps</head><p>The output of SemanticCaps are low-level vector representations of R different semantic features extracted from the utterances. To combine these features into higher-level representations, we build DetectionCaps that choose different semantic fea- tures dynamically so as to form an intent represen- tation for each intent via an unsupervised routing- by-agreement mechanism.</p><p>As a semantic feature may contribute differently in detecting different intents, the DetectionCaps first encode semantic features with respect to each intent:</p><formula xml:id="formula_3">p k|r = m r W k,r ,<label>(3)</label></formula><p>where k ∈ {1, 2, ..., K}, r ∈ {1, 2, ..., R}. W k,r ∈ R 2D H ×D P is the weight matrix of the De- tectionCaps, p k|r is the prediction vector of the r- th semantic feature of an existing intent k, and D P is the dimension of the prediction vector. Dynamic Routing-by-agreement. The predic- tion vectors obtained from SemanticCaps route dynamically to DetectionCaps. The Detection- Caps computes a weighted sum over all prediction vectors:</p><formula xml:id="formula_4">s k = R r c kr p k|r ,<label>(4)</label></formula><p>where c kr is the coupling coefficient that deter- mines how informative, or how much contribu- tion the r-th semantic feature is to the intent y k . c kr is calculated by an unsupervised, iterative dynamic routing-by-agreement algorithm ( <ref type="bibr" target="#b18">Sabour et al., 2017)</ref>, which is briefly recalled in Algorithm 1. As shown in this algorithm, b kr is the initial logit representing the log prior probability that a SemanticCap r is coupled to an DetectionCap k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dynamic routing algorithm</head><p>1: procedure DYNAMIC ROUTING(p k|r , iter)</p><formula xml:id="formula_5">2:</formula><p>for all semantic capsule r and intent capsule k: b kr ← 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>for iter iterations do Return v k 10: end procedure</p><p>The squashing function squash(·) is applied on s k to get an activation vector v k for each existing intent class k:</p><formula xml:id="formula_6">v k = s k 2 1 + s k 2 s k s k ,<label>(5)</label></formula><p>where the orientation of the activation vector v k represents intent properties while its norm indi- cates the activation probability. The dynamic routing-by-agreement mechanism assigns low c kr when there is inconsistency between p k|r and v k , which ensures the outputs of the SemanticCaps get sent to appropriate subsequent DetectionCaps. Max-margin Loss for Existing Intents. The loss function considers both the max-margin loss on each labeled utterance, as well as a regularization term that encourages each self-attention head to be attentive to a different semantic feature of the ut- terance:</p><formula xml:id="formula_7">L = K k=1 {[[y = y k ]] · max(0, m + − v k ) 2 + λ [[y = y k ]] · max(0, v k − m − ) 2 } + α||AA T − I|| 2 F ,<label>(6)</label></formula><p>where <ref type="bibr">[[]</ref>] is an indicator function, y is the ground truth intent label for the utterance x, λ is a down- weighting coefficient, m + and m − are margins. α is a non-negative trade-off coefficient that encour- ages the discrepancies among different attention heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Zero-shot DetectionCaps</head><p>To detect emerging intents effectively, Zero-shot DetectionCaps are designed to transfer knowledge from existing intents to emerging intents. Knowledge Transfer Strategies. As Semantic- Caps are trained to extract semantic features from utterances with various existing intents, a self- attention head which has similar extraction behav- ior among existing and emerging intents may help transfer knowledge. For example, a self-attention head that extracts the "play" action mentioned by turn on/I want to hear in the beginning of an utterance for PlayMusic is helpful if it is also attentive to expressions for the "add" action like add/I want to have in the beginning of an utterance with an emerging intent AddtoPlaylist. The coupling coefficient c kr learned by Detec- tionCaps in a totally unsupervised fashion embod- ies rich knowledge of how informative r-th seman- tic is to the existing intent k. We can capitalize on the existing routing information for emerging in- tents. For example, how the word play routes to GetWeather can be helpful in routing the word add to AddtoPlaylist.</p><p>The intent labels also contain knowledge of how two intents are similar with each other. For example, an emerging intent AddtoPlaylist can be closer to one existing intent PlayMusic than GetWeather due to the proximity of the embed- ding of Playlist to Play or Music, than Weather.</p><p>Build Vote Vectors. As the routing information and the semantic extraction behavior are strongly coupled (c kr is calculated by p k|r iteratively in Line 4-6 of Algorithm 1) and their products are summarized to get the activation vector v k for in- tent k (Line 5-6 of Algorithm 1), we denote vec- tors before summation as vote vectors:</p><formula xml:id="formula_8">g k,r = c kr p k|r ,<label>(7)</label></formula><p>where g k,r is the r-th vote vector for an existing intent k. Zero-shot Dynamic Routing. The zero-shot dy- namic routing utilizes vote vectors from existing intents to build intent representations for emerg- ing intents via a similarity metric between existing intents and emerging intents.</p><p>Since there are K existing intents and L emerg- ing intents, the similarities between existing and emerging intents form a matrix Q∈R L×K . Specif- ically, the similarity between an emerging intent z l ∈Z and an existing intent y k ∈Y is computed as:</p><formula xml:id="formula_9">q lk = exp {−d (e z l , e y k )} K k=1 exp {−d (e z l , e y k )} ,<label>(8)</label></formula><p>where</p><formula xml:id="formula_10">d (e z l , e y k ) = (e z l − e y k ) T Σ −1 (e z l − e y k ) . (9) e z l , e y k ∈ R D I ×1</formula><p>are intent embeddings com- puted by the sum of word embeddings of the in- tent label. Σ models the correlations among intent embedding dimensions and we use Σ = σ 2 I. σ is a hyper-parameter for scaling. The prediction vectors for emerging intents are thus computed as:</p><formula xml:id="formula_11">u l|r = K k=1 q lk g k,r .<label>(10)</label></formula><p>We feed the prediction vector n l to Algorithm 1 and derive activation vectors n l on emerging in- tents as the output. The final intent representa- tion n l for each emerging intent is updated toward the direction where it coincides with representa- tive votes vectors. We can easily classify the utterance of emerging intents by choosing the activation vector with the largest normˆznormˆ normˆz = arg max</p><formula xml:id="formula_12">z l ∈Z n l .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Setup</head><p>To demonstrate the effectiveness of our proposed models, we apply INTENTCAPSNET to detect existing intents in an intent detection task, and use INTENTCAPSNET-ZSL to detect emerging in- tents in a zero-shot intent detection task. Datasets. For each task, we evaluate our pro- posed models by applying it on two real-word   We also compare our proposed model INTENTCAPSNET-ZSL with different zero- shot learning strategies: 1) DeViSE ( <ref type="bibr" target="#b5">Frome et al., 2013</ref>) finds the most compatible emerging intent label for an utterance by learning a linear compatibility function between utterances and intents; 2) CMT ( <ref type="bibr" target="#b20">Socher et al., 2013</ref>) introduces non-linearity in the compatibility function; CMT and DeViSE are originally designed for zero-shot image classification based on pretrained CNN features. We use LSTM to encode the utterance and adopt their zero-shot learning strategies in our task; 3) CDSSM ( <ref type="bibr" target="#b1">Chen et al., 2016a</ref>) uses CNN to extract character-level sentence features, where the utterance encoder shares the weights with the label encoder; 4) Zero-shot DNN ( <ref type="bibr" target="#b12">Kumar et al., 2017</ref>) further improves the per- formance of CDSSM by using separate encoders for utterances and intent. The proposed model INTENTCAPSNET-ZSL can be seen as a hybrid model: it has the advantages of the compatibil- ity models to model the correlations between utterances and intents directly; it also explicitly derives intent representations for emerging intents without labeled utterances.  Implementation Details. The hyperparameters used for experiments are shown in <ref type="table" target="#tab_4">Table 3</ref>. We use three fold cross-validation to choose hyperpa-Model SNIPS-NLU (on 2 emerging intents) CVA (on 20 emerging intents) Accuracy Precision Recall F1 Accuracy Precision Recall F1 DeViSE ( <ref type="bibr" target="#b5">Frome et al., 2013)</ref> 0.7447 0.7448 0.7447 0.7446 0.7809 0.8060 0.7809 0.7617 CMT ( <ref type="bibr" target="#b20">Socher et al., 2013)</ref> 0.7396 0.8266 0.7396 0.7206 0.7721 0.7728 0.7721 0.7445 CDSSM ( <ref type="bibr" target="#b1">Chen et al., 2016a)</ref> 0.7588 0.7625 0.7588 0.7580 0.2140 0.4072 0.2140 0.1667 Zero-shot DNN ( <ref type="bibr" target="#b12">Kumar et al., 2017)</ref> 0  rameters. The dimension of the prediction vector D P is 10 for both datasets. D I = D W because we use the averaged word embeddings contained in the intent label as the intent embedding. An ad- ditional input dropout layer with a dropout keep rate 0.8 is applied to the SNIPS-NLU dataset. In the loss function, the down-weighting coefficient λ is 0.5, margins m + k and m − k are set to 0.9 and 0.1 for all the existing intents. The iteration num- ber iter used in the dynamic routing algorithm is 3. Adam optimizer ( <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) is used to minimize the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Quantitative Evaluation. The intention detection results on two datasets are reported in <ref type="table">Table 1</ref>, where the proposed capsule-based model INTENT- CAPSNET performs consistently better than bag- of-word classifiers using TF-IDF, as well as vari- ous neural network models designed for text clas- sification. These results demonstrate the novelty and effectiveness of the proposed capsule-based model INTENTCAPSNET in modeling text for in- tent detection.</p><p>Also, we report results on zero-shot inten- tion detection task in <ref type="table" target="#tab_6">Table 4</ref>, where our model INTENTCAPSNET-ZSL outperforms other base- lines that adopt different zero-shot learning strate- gies. CMT has higher precision but low ac- curacy and recall on the SNIPS-NLU dataset. CDSSM fails on CVA dataset, probabily because the character-level model is suitable for English corpus but not for CVA, which is in Chinese. Ablation Study. To study the contribution of different modules of INTENTCAPSNET-ZSL for zero-shot intent detection, we also report abla- tion test results in <ref type="table" target="#tab_6">Table 4</ref>. "w/o Self-attention" is the model without self-attention: the last for- ward/backward hidden states of the bi-LSTM re- current encoder are used; "w/o Bi-LSTM" uses the LSTM with only a forward pass; "w/o Reg- ularizer" does not encourage discrepancies among different self-attention heads: it adopts α = 0 in the loss function. Generally, from the lower part of <ref type="table" target="#tab_6">Table 4</ref> we can see that all modules contribute to the effectiveness of the model. On the SNIPS- NLU dataset, each of the three modules has a com- parable contribution to the whole model (around 2-3% improvement in F1 score). While on the CVA dataset, the self-attention plays the most im- portant role, which gives the model a 5.2% im- provement in F1 score. Discriminative Emerging Intent Representa- tions. Besides quantitative evidences supporting the effectiveness of the INTENTCAPSNET-ZSL, we visualize activation vectors of emerging intents in <ref type="figure" target="#fig_4">Figure 3</ref>. Since the activation vectors of utter- ances with emerging intents are of high dimension and we are interested in their orientations which indicate their intent properties, t-SNE is applied on the normal vector of the activation vectors to reduce the dimension to 2. We color the utterances according to their ground-truth emerging intent la- bels. As illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>, INTENTCAPSNET- ZSL has the ability to learn discriminative intent representations for emerging intents in zero-shot DetectionCaps, so that utterances with different intents naturally have different orientations. In the meanwhile, utterances of the same emerging in- tent but with nuances in expressions result in their proximity in the t-SNE space. However, we do observe less satisfied cases where the model mis- take an emerging intent DecreaseScreenBright- ness (No. 9) with ReduceFontSize (No. 10) and SetColdColor (No. 11). When we check activa- tion vectors of intents in <ref type="figure" target="#fig_4">Figure 3</ref> we also find that these three intents tend to have similar representa- tions around the area (15, -5). We think it is due to their inherent similarity as these three intents all try to tune display configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Interpretability</head><p>Capsule models try to bring more interpretability when compared with traditional deep neural net- works. We provide case studies here toward the intepretability of the proposed model in 1) extract- ing meaningful semantic features and 2) transfer- ring knowledge from existing intents to emerging intents. Extracting Meaningful Semantic Features. To show that SemanticCaps have the ability to extract meaningful semantic features from the utterance, we study the self-attention matrix A within the Se- manticCaps and visualize the attention scores of utterances on both existing and emerging intents.  From <ref type="table" target="#tab_7">Table 5</ref> we can see that each self-attention head almost always focuses on one unique seman- tic feature of the utterance. For example, in the in- tent of PlayMusic one self-attention head always focuses on the "play" action while another atten- tion focuses on musician names. We also observe that the learned attention adopts well to diverse ex- pressions. For example, the self-attention head in PlayMusic is attentive to various mentions of mu- sician names when they are followed by words like by, play and artist, even when named enti- ties are not tagged and given to the model. The self-attention head that extracts the "search" action in SearchCreativeWork is able to be attentive to various expressions such as find, looking for and show. Extraction-behavior Transfer by Semantic- Caps. More importantly, we observe appealing extraction behaviors of SemanticCaps on utter- ances of emerging intents as well, even if they are not trained to perform semantic extraction on ut- terances of emerging intents.  From <ref type="table" target="#tab_8">Table 6</ref> we observe that the same self- attention head that extracts "play" action in the ex- isting intent PlayMusic is also attentive to words or phrases referring to the "rate" action in an emerging intent RateABook: like rate, add the rating, and give. Other self-attention heads are almost always focusing on other aspects of the utterances such as the book name or the ac- tual rating score.</p><p>Such behavior not only shows that Seman- ticCaps have the capacity to learn an intent- independent semantic feature extractor, which ex- tracts generalizable semantic features that either existing or emerging intent representations are built upon, but also indicates that SemanticCaps has the ability to transfer extraction behaviors among utterances of different intents. Knowledge Transfer via Intent Similarity. Be- side extracting semantic features and utilizing existing routing information, we use similari- ties between intent embeddings to help trans-fer vote vectors from INTENTCAPSNET to INTENTCAPSNET-ZSL. We study the similarity distribution of each emerging intents to all exist- ing intents in <ref type="figure" target="#fig_7">Figure 4</ref>. The y axis is the zero-shot detection accuracy on each emerging intent in the CVA dataset. The x axis measures var(q l ), the variance of the simi- larity distribution of each emerging intent l to all the existing intents. If an emerging intent has a high variance in the similarity distribution, it means that some existing intents have higher sim- ilarities with this emerging intent than others: the model is more certain about which existing intent to transfer the similarity knowledge from, based on intent label similarities. In this case, 13 out of 20 emerging intents with high variances where var(q l ) &gt; 0.005 always have a decent perfor- mance (Accuracy0.83). While a low variance does not necessarily always lead to less satisfied performances as some intents can rely on existing intents more evenly together, but with less confi- dence on each, for knowledge transfer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of INTENTCAPSNET and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>4: for all SemanticCaps r: c r ← softmax(b r ) 5: for all DetectionCaps k: s k ← Σ r c kr p k|r 6: for all DetectionCaps k: v k = squash(s k ) 7: for all SemanticCaps r and DetectionCaps k: b kr ← b kr + p k|r · v k 8: end for 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE visualization of normal activation vectors of utterances with 20 emerging intents in CVA.</figDesc><graphic url="image-1.png" coords="7,307.28,543.48,218.26,128.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>play music by charlie adams from i want to hear any tune from twenties open up music on last fm • Musician Name i want to hear music by madeleine peyroux from on youtube play me a song by charles neidich use itunes to play artist ringo shiina track in heaven Existing Intent: SearchCreativeWork • Search Action find fields of sacrifice movie i m looking for music of nashville season saga show me television show children in need rocks • Creative Work Type please find me platinum box ii song ? show me a picture called heart like a hurricane where can i buy a photograph called feel love ?</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>i d rate this novel a five add the rating for this current series a four out of points i give ruled britannia a rating of five out of • Book Name give the televised morality series a one i want to give the coming of the terraphiles a rating of the chronicle charlie peace earns stars from me • Rating Score rate the grisly wife three points out of five i would give this current chronicle three points this saga deserves a score of four Emerging Intent: AddToPlaylist • Song/Artist Name add star light star bright to my jazz classics playlist i want a song by john schlitt in the bajo las estrellas playlist put sungmin into my summer playlist • Playlist Name add an album to my list la mejor msica dance can you add danny carey to my masters of metal playlist i want to put a copy of this tune into skatepark punks</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy vs. variance of the similarity distribution for 20 emerging intents in CVA dataset.</figDesc><graphic url="image-2.png" coords="9,72.00,130.35,218.26,146.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>• Play me a song by charles neidich</head><label></label><figDesc></figDesc><table>Add to Playlist 

Extracting interpretable 
semantic features 

Aggregating semantic 
features for intent detection 

loss 

Rate a Book 

Zero-shot Dynamic 
Routing 

Emerging Intents with 
Unlabeled Utterances 

DetectionCaps 

Zero-shot 
DetectionCaps 

… 

SemanticCaps 

Dynamic 
Routing 

0.82 

0.18 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>) . R × 2D H R × D P activation vectors 1× D I …… squash squashM W k ,r p k|r c kr v k m r activation vectors s k prediction vectors R × D P 1× D I squash squash u l|r n l Q lk,r = c kr p k|r</head><label>.</label><figDesc></figDesc><table>SemanticCaps 

DetectionCaps 

Multi-Head 
Self-attention 
Recurrent 
Encoder 

Play Music 

Get Weather 

prediction 
vectors 

Add to Playlist 
Zero-shot 
Intent 
Detection 

x existing 

x emerging 

Zero-shot DetectionCaps 

loss 

TrainINTENTCAPSNET on 
the existing intents for 
Intent Detection 

Inference emerging intents 
with INTENTCAPSNET-ZSL 
for Zero-shot Intent Detection 

• Put Sungmin into 
my summer playlist 

• Play me a song 
by charles neidich 

Rate a Book 

…… 

semantic 
vectors 

c kr 

vote 
vectors 

g k </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Dataset statistics.</head><label>2</label><figDesc></figDesc><table>Baselines. We first compare the proposed capsule-
based model INTENTCAPSNET with other text 
classification alternatives on the detection of ex-
isting intents: 1) TFIDF-LR/TFIDF-SVM: we use 
TF-IDF to represent the utterance and use logis-
tic regression/support vector machine as classi-
fiers. 2) CNN: a convolutional neural network 
(Kim, 2014) that uses convolution and pooling 
operations, which is popular for text classifica-
tion. 3) RNN/GRU/LSTM/BiLSTM: we adopt 
different types of recurrent neural networks: the 
vanilla recurrent neural network (RNN), gated 
recurrent unit (GRU) (Tang et al., 2015), long 
short-term memory networks (LSTM) (Hochre-
iter and Schmidhuber, 1997), and bi-directional 
long short-term memory (Bi-LSTM) (Schuster 
and Paliwal, 1997). Their last hidden states 

1 https://github.com/snipsco/nlu-benchmark/ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Hyperparameter settings.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Zero-shot intention detection results using INTENTCAPSNET-ZSL on two datasets. All the metrics (Accuray, Preci-

sion, Recall and F1) are reported using the average value weighted by their support on per class. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Attentions on utterances with existing intents on 

SNIPS-NLU. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Attentions on utterances with emerging intents on 

SNIPS-NLU. 

</table></figure>

			<note place="foot" n="7"> Conclusions In this paper, a capsule-based model, namely INTENTCAPSNET, is first introduced to harness the advantages of capsule models for text modeling in a hierarchical manner: semantic features are extracted from the utterances with selfattention, and aggregated via the dynamic routingby-agreement mechanism to obtain utterance-level intent representations. We believe that the inductive biases subsumed in such capsule-based hierarchical learning schema have broader applicability on various text modeling tasks, besides its evidenced performance on the intent detection task we studied in this paper. The proposed INTENTCAPSNET-ZSL model further introduces zero-shot learning ability to the capsule model via various means of knowledge transfer from existing intents for discriminating emerging intents where no labeled utterances or excessive external resources are available. Experiments on two real-world datasets show the effectiveness and intepretability of the proposed models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their valuable com-ments. This work is supported in part by NSF through grants IIS-1526499, IIS-1763325, and CNS-1626432, and NSFC 61672313. Xiaohui Yan's work is funded by the National Natural Sci-ence Foundation of China (NSFC) under Grant No. 61502447.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zeroshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot learning of intent embeddings for expansion by convolutional deep structured semantic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6045" to="6049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gökhan</forename><surname>Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3245" to="3249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Online adaptative zero-shot learning spoken language understanding using wordembedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefevre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5321" to="5325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Zero-shot semantic parser for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bassam</forename><surname>Jabaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1403" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Alexa, siri, cortana, and more: An introduction to voice assistants. Medical reference services quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding user&apos;s query intent with wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Lochovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="471" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zeroshot learning across heterogeneous overlapping domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjishnu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavankumar</forename><surname>Reddy Muddireddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Hoffmeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="2914" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute-based classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Christoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannes</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="453" to="465" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-based recurrent neural network models for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="685" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Defining intents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibm Watson Assistant</surname></persName>
		</author>
		<ptr target="https://console.bluemix.net/docs/services/conversation/intents.html#defining-intents" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular crf for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="78" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of zero-shot learning of spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mining user intentions from medical queries: A neural network based heterogeneous jointly modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1373" to="1384" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
