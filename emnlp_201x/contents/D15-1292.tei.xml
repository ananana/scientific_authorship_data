<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Navigating the Semantic Horizon using Relative Neighborhood Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaru</forename><surname>Cuba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyllensten</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavagai</forename><surname>Bondegatan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stockholm</forename><surname>Sweden</surname></persName>
						</author>
						<title level="a" type="main">Navigating the Semantic Horizon using Relative Neighborhood Graphs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces a novel way to navigate neighborhoods in distributional semantic models. The approach is based on relative neighborhood graphs, which uncover the topological structure of local neighborhoods in semantic space. This has the potential to overcome both the problem with selecting a proper k in k-NN search, and the problem that a ranked list of neighbors may conflate several different senses. We provide both qualitative and quantitative results that support the viability of the proposed method.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nearest neighbor search is a fundamental opera- tion in data mining, in which we are interested in finding the closest points to some given reference point. Formally, if we have a reference point r and a set of other points P in a metric space M with some distance function d, the nearest neigh- bor search task is to find the point p ∈ P that min- imizes d <ref type="bibr">(p, r)</ref>. In k-Nearest Neighbor search (k- NN), we want to find the k closest points to some given reference point. Nearest neighbor search is a well-studied task, and in particular the com- plexity of the task (a linear search has a running time of O(N i) where N is the cardinality of P and i the complexity of the distance function d) has generated a lot of research; suggestions for re- ducing the complexity of linear nearest neighbor searches include using various types of space par- titioning techniques like k-d trees <ref type="bibr" target="#b3">(Bentley, 1975)</ref>, or various techniques for doing approximate near- est neighbor search ( <ref type="bibr" target="#b2">Arya et al., 1998</ref>), of which one of the most well-known is locality-sensitive hashing <ref type="bibr" target="#b12">(Indyk and Motwani, 1998</ref>).</p><p>The problem we are concerned with in this paper is not the complexity of nearest neighbor search, but the question of how to identify the in- ternal structure of neighborhoods defined by the nearest neighbors. The problem with a normal k- NN is that the result -a sorted list of the k nearest neighbors -does not say anything about the inter- nal structure of the neighborhood. It is quite pos- sible for two neighborhoods with widely different internal structures to produce identical k-NN re- sults. In the context of Distributional Semantic Models (DSMs), which collect and represent co- occurrence statistics in high-dimensional vector spaces, such structural differences may carry sig- nificant semantic information, e.g. about the dif- ferent senses of terms. We argue that the inability of standard k-NN to account for structural prop- erties has been misinterpreted as a shortcoming of the distributional representation <ref type="bibr" target="#b11">(Erk and Padó, 2010</ref>).</p><p>We will demonstrate in this paper that this is not a shortcoming of the distributional represen- tation, but of the mode of querying the DSM. We argue that information about the different usages (i.e. senses) of a term is encoded in the structural properties of the nearest neighborhoods, and we propose the use of relative neighborhood graphs for identifying these structural properties. Relative neighborhood graphs may also be used for finding a relevant k for a given reference point, which we refer to as the horizon with respect to the reference point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Distributional Semantics and Nearest Neighbor Search</head><p>Collecting and comparing co-occurrence statis- tics for terms in language has become a stan- dard approach for computational semantics, and is now commonly referred to as distributional se- mantics. There are many different types of mod- els that can be used for this purpose, but their common objective is to represent terms as vec- tors that record (some function of) their distri-butional properties. The standard approach for generating such vectors is to collect distributional statistics in a co-occurrence matrix that records co-occurrence counts between terms and contexts. The co-occurrence matrix is then subject to var- ious types of transformations, ranging from the application of simple frequency filters or associ- ation measures to matrix factorization or regres- sion models. The resulting representations are re- ferred to as distributional vectors (or word embed- dings), which are used to compute similarity be- tween terms. Given a similarity -or distance -measure on such distributional vectors, we can perform a near- est neighbor search. This is a particularly impor- tant operation in distributional semantics, since it answers the question "which other terms are sim- ilar to this one?" and this is a central question in semantics; lexica and thesauri are built with the main purpose of answering this question. Conse- quently, nearest neighbor search in a DSM could be seen as a compilation step in a distributional lexicon.</p><p>The result of a nearest neighbor search in a DSM is often presented as a list of (the top k) neighbors, sorted by descending similarity with the target term. <ref type="table">Table 1</ref> illustrates typical sorted nearest neighbor lists produced with three dif- ferent DSMs: a standard model based on Point- wise Mutual Information (PMI) 1 that has been re- duced to 2,000 dimensions by applying a Gaus- sian random projection; GloVe, which uses regres- sion to find distributional vectors such that their dot product approximates their log probability of co-occurring ( <ref type="bibr" target="#b22">Pennington et al., 2014)</ref>; and the Skipgram model, which uses stochastic gradient descent and hierarchical softmax combined with negative sampling and subsampling to find dis- tributional vectors that maximize the probability of observed co-occurrence events ( <ref type="bibr" target="#b18">Mikolov et al., 2013)</ref>. We refer to the respective papers for de- tails regarding the various models. The similarity measure used is the cosine similarity: s(a, b) = a·b ab . <ref type="table">Table 1</ref> lists the 10 nearest neighbors to suit in these three different DSMs using the entire Wikipedia as data. As can be expected, there are both similarities and dissimilarities between <ref type="bibr">1</ref> For observations a and b, PMI(a, b)= log p(a,b)</p><formula xml:id="formula_0">p(a)p(b)</formula><p>. The probabilities are often replaced in DSMs by co-occurrence counts of a and b and their respective frequency counts. <ref type="table">Table 1</ref>: Sorted list of the nearest neighbors to "suit" in three different distributional models. <ref type="table">GloVe  Skipgram  suits  suits  suits  dress  lawsuit  lawsuit  jacket  filed  countersuit  wearing case  classaction  hat  wearing  doublebreasted  trousers laiming  skintight  costume lawsuits  necktie  shirt  alleging  wetsuit  pants  alleges  crossbone  lawsuit</ref> classaction lawsuits these neighborhoods; "suits" and "lawsuit" oc- cur among the 10 nearest neighbors to "suit" in all three models, whereas other terms are spe- cific for one particular model. What is com- mon between the three models is that they all feature neighbors that represent two different us- ages of "suit": the law-sense ("lawsuit") and the clothes-sense ("dress", "wearing", "double- breasted"). <ref type="bibr">2</ref> However, these distinction are not discernible by merely looking at the list of near- est neighbors; the only information it provides is the ranking of the nearest neighbors in descending order of similarity. It has been argued that DSMs that represent terms by a single vector cannot adequately handle polysemy, since they conflate several different us- age patterns in one and the same vector <ref type="bibr" target="#b29">(Véronis, 2004;</ref><ref type="bibr" target="#b11">Erk and Padó, 2010)</ref>. Examples like the one above is often cited as evidence. We argue that this critique is unfounded and misinformed, and that it is the mode of querying the DSM that can be susceptible to problems with polysemy. As the above example demonstrates, querying DSMs by k-NN conflates different usages of terms. The rea- son for this seems quite obvious: simply ranking the nearest neighbors by similarity (or distance) ignores any local structures of the neighborhood. If "suit" has as neighbors both "dress" and "law- suit", which represent two distinct types of usages of "suit", there will be a structural distinction in the neighborhood of "suit" between these differ- ent neighbors, since they will be mutually unre- lated (i.e. there is a similarity between "suit" and "dress" and between "suit" and "lawsuit", but not between "dress" and "lawsuit").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PMI</head><p>k-NN also gives rise to another problem re- lated to polysemy in DSMs. The problem is that the most frequent senses will populate the top of the nearest neighbor list, while the less fre- quent senses will not appear until further down the list, and if we set a too restrictive k, we will only see neighbors relating to the most frequent sense. As an example, consider the two differ- ent senses of "suit" above. The distributional vec- tor for "suit" can be thought of as a sum v suit = f suit|law v suit|law + f suit|clothes v suit|clothes , where v suit|law is an idealized notion of the true dis- tributional vector of "suit" in the law-sense, and f suit|law is the relative frequency of this sense. <ref type="bibr">3</ref> From there one can easily argue that a similar- ity such as s(v suit , v clothes ) is actually a weighted composite of the similarities s(v suit|law , v clothes ) and s(v suit|clothes , v clothes ). <ref type="bibr">4</ref> If "suit" occurs pre- dominantly in the law-sense in our corpus, the k- NN neighborhood of "suit" will be dominated by words pertaining to its law-sense, while the less frequent senses might not be present at all. A misguided k may thus obscure any other, less fre- quent, senses of a term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word-Sense Induction</head><p>Selecting a relevant k for a given term and group- ing the neighbors according to which senses they represent is an example of Word-Sense Induction (WSI). DSMs are well suited for this task, and there have been a number of different approaches suggested in the literature. One of the earliest ap- proaches is distributional clustering ( <ref type="bibr" target="#b23">Pereira et al., 1993)</ref>, which is based on a probabilistic decompo- sition model that uses maximum likelihood esti- mation to fit the model to observed data. Another example is Clustering By Committee (CBC) <ref type="bibr" target="#b21">(Pantel and Lin, 2002</ref>), which first uses average-link clustering to recursively cluster the nearest neigh- bors of a term into committees, which are then used to define clusters by iteratively adding com- mittees whose similarity to the term exceeds a cer- tain threshold, and that is not too similar to any other added committee. For each added commit- tee, its features are also removed from the distri- 3 Weighting schemes muddles this notion quite a bit, but we think the general intuition still holds. <ref type="bibr">4</ref> In the case of cosine similarity this follows nicely from the distributive property of dot products:</p><formula xml:id="formula_1">v = av1 + bv2, s(v, w) = v·w vw = a(v 1 ·w)+b(v 2 ·w)</formula><p>vw butional representation of the term. This last step ensures that the clusters do not become too similar, and that clusters representing less frequent senses can be discovered.</p><p>The idea of iteratively removing features from the distributional vector when a sense cluster as been formed is also present in <ref type="bibr" target="#b10">Dorow and Widdows (2003)</ref>, who use a graph-based clustering method. Another graph-based approach is the HyperLex algorithm <ref type="bibr">(Véronis, 2004</ref>), which con- structs a graph connecting all pairs of terms that co-occur in the context of an ambiguous term. The resulting graph contains highly connected compo- nents, which represent the different senses of the term. <ref type="bibr">Agirre et al. (2006) compare HyperLex to PageRank (Brin and</ref><ref type="bibr" target="#b5">Page, 1998)</ref> and demonstrates that the two methods perform similarly.</p><p>There have also been several attempts to use various types of matrix factorization for WSI. The idea is that the factorization uncovers a set of global senses in the form of the latent factors, and that the sense distribution for a given term can be described as a distribution over these la- tent factors. Examples of factorization methods that have been used include different versions of Latent Dirichlet Allocation (( <ref type="bibr" target="#b6">Brody and Lapata, 2009;</ref><ref type="bibr" target="#b25">Séaghdha and Korhonen, 2011;</ref><ref type="bibr" target="#b30">Yao and Van Durme, 2011</ref>; <ref type="bibr" target="#b16">Lau et al., 2012</ref>) and non- negative matrix factorization ( <ref type="bibr" target="#b9">Dinu and Lapata, 2010</ref>; Van de Cruys and Apidianaki, 2011). <ref type="bibr" target="#b26">Tomuro et al. (2007)</ref> argue that clustering ap- proaches like distributional clustering or CBC may produce clusters that are themselves polysemous, which may not be a desirable property of a WSI algorithm, and suggests using feature domain sim- ilarity to solve this problem. The idea is to incor- porate similarities between the features of items rather than the similarity between the items them- selves in a modified version of CBC that enables the algorithm to utilize feature similarities, which inhibit the formation of polysemous clusters.</p><p>Koptjevskaja <ref type="bibr" target="#b15">Tamm and Sahlgren (2014)</ref> also leverage on the idea of using feature similarity as the basis of sense clustering. The approach, called syntagmatically labeled partitioning, relies on a DSM that encodes sequential as well as sub- stitutable relations. The method essentially sorts the k nearest (substitutable) neighbors according to which sequential connections they share. The resulting partitioning of the nearest distributional neighbors does not only constitute a WSI, but it also provides labels for the induced senses in the form of the sequential connections the neighbors share.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neighborhood Graphs</head><p>Many of the previous WSI approaches operate at a global level, utilizing global structural proper- ties of the semantic spaces, e.g. by matrix fac- torization techniques. We believe this is as ill- advised as setting a global k or radius for the near- est neighbor search, since it is the local structures that are important when analyzing nearest neigh- bors. Other WSI approaches use various forms of clustering techniques. However, previous stud- ies of the intrinsic dimensionality of distributional semantic spaces using fractal dimensions indicate that neighborhoods in semantic space have a fila- mentary rather than clustered structure <ref type="bibr" target="#b14">(Karlgren et al., 2008)</ref>.</p><p>We therefore propose the use of topological models that take the local structure of neighbor- hoods in semantic space into account. The ap- proach discovers different word senses from the local structure of neighborhoods, given nothing but similarities between points. As such it is easy to test on widely different vector models, as long as there exists a well behaved similarity function. The proposed approach not only answers the ques- tion which other terms are similar to a given term, but also how are they similar.</p><p>Relative neighborhoods, first proposed in ( <ref type="bibr" target="#b27">Toussaint, 1980)</ref>, are examples of empty region graphs ( <ref type="bibr" target="#b7">Cardinal et al., 2009)</ref>, where points are neighbors if some region between them is empty. For Rela- tive Neighborhood Graphs (RNG) this region be- tween two points a and c is defined as the inter- section of the two spheres with centers in a and c with radius d(a, c). In other words, a point b lies between points a and c if it is closer to both a and c than a and c are to each other. If no such point b exists, a and c are neighbors. Illustrations of this can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. Such neighborhoods have been argued to better preserve local topology <ref type="bibr" target="#b4">(Bremer et al., 2014)</ref>, and be more robust to deformations of the data than k- NN neighborhoods <ref type="bibr" target="#b8">(Correa and Lindstrom, 2012)</ref> as they in some sense contain information about direction whereas k-NN neighborhoods only con- tain information about distance. Going back to the "suit" example, we can see that if "suit" in the law- sense is more similar to the composite "suit" than to its clothes-sense, and vice versa, then the com- posite v suit lies between v suit|law and v suit|clothes . This in turn means that out of those two points, both are relative neighbors to "suit", and neither of them lies between the other and "suit".</p><p>Formally, the set of points between two points a, c ∈ V can be characterized and computed in the following way:</p><formula xml:id="formula_2">btw(V, a, c) = {b|b ∈ V, b is between a and c} rng-nbh(V, a) = {c|c ∈ V, btw(V, a, c) = ∅} E rng (V ) = {(a, b)|a ∈ V, b ∈ rng-nbh(V, a)}</formula><p>where E rng is the undirected edge set of the RNG. The function btw(V, a, c) can be straightforwardly translated to an algorithm taking O(|V |) time, making the rng-nbh(V, a) function take O(|V | 2 ) time, which in turn makes the computation of the complete graph take O(|V | 3 ) time. <ref type="bibr">5</ref> Clearly un- feasible, but we have not found any alternatives that perform better in the high-dimensional case. <ref type="bibr">6</ref> Correa and Lindstrom (2012) note that the inter- section of the RNG and the k-NN graph is a more feasible alternative:</p><formula xml:id="formula_3">k-rng-nbh(V, a) = rng-nbh(V , a)</formula><p>where V = k nearest neighbors of a.</p><p>Given a precompiled k-NN lookup, the above takes O(k 2 ) time, so using a heap-based</p><formula xml:id="formula_4">O(|V | lg k) k-NN algorithm results in an algo- rithm taking O(k 2 + |V | lg k) time.</formula><p>The same idea can be used to build a tree struc- ture rooted in a reference word a in the following way:</p><formula xml:id="formula_5">rnbh-tree(V, a) = {(c, arg min b∈Bc d(b, c))|c ∈ V }</formula><p>where B c = {a} ∪ btw(V, a, c) <ref type="bibr">5</ref> Assuming a constant time distance function. <ref type="bibr">6</ref> It should be noted that there are more efficient algorithms for lower-dimensional situations. which can easily be restricted to the k-nearest neighbors of a in much the same way as above, with the same monotonic behavior.</p><p>Computing this for a point a produces a tree where the direct children of a are its relative neigh- bors, and the parent of a point c further down the tree is the point between a and c that is closest to c. This structure, while similar to a minimum spanning tree, differs in some crucial regards: the rnbh-tree(V, a) is rooted in a word a. The differ- ence between rnbh-tree(V, a) and rnbh-tree(V, b) is often quite significant. Furthermore, the re- stricted k-rnbh-tree is monotonic in k. That prop- erty does not hold for a minimum spanning tree of a local neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Examples of RNGs</head><p>To get an intuition of what these neighborhoods look like we present a few examples. The words have been chosen either because they are com- mon examples in similar work -e.g. "heart" and "suit" from Pantel and Lin (2002) -or because they represent different parts-of-speech ("above" is a preposition, "bad" is an adjective, and "ser- vice" is a noun) and disparate kinds of ambiguity ("orange" can be both a fruit and a color). <ref type="figure" target="#fig_1">Figure 2</ref> (next page) illustrates what an RNG looks like for the term "heart" and its 100 near- est neighbors in the PMI model. Note that the root "heart" (at the mid-left in the graph) only has two relative neighbors: "cardiac" and "soul," arguably representing a body-sense and a soul- sense of the term. One advantage of using this type of structure for the neighborhood is that it enables us to examine various depths of the tree. Depth one includes only the direct neigh- bors ("cardiac" and "soul"), while depth two in- cludes all neighbors two steps away in the graph: "disease," "coronary," "pulmonary," "cardiovascu- lar," "ventricular," and "failure," which are all chil- dren to "cardiac." This tree structure can be used to identify neighbors that are themselves polyse- mous (c.f. the critique mentioned in Section 3 of clustering-based approaches to word-sense induc- tion that they may produce polysemous clusters ). One example is the neighbor "disease" at depth two, which has six children that refer to different aspects of disease.</p><p>We argue that the RNG can be quite useful for WSI, since the branching structure indicates different usages, and the depth factor enables us to calibrate the granularity of the induced word senses. If we only consider direct neighbors (i.e. depth one), and set k = V (i.e. we do an exhaustive nearest neighbor search), we will ex- tract all terms that have a direct connection to the reference term. We refer to this neighborhood as the semantic horizon. At the most coarse level of analysis, this is the neighborhood that represents the main induced senses of a term. <ref type="table">Tables 2 and  3</ref> provide examples of 1,000-RNG neighborhoods of depth one. <ref type="table">Table 2</ref>: RNG for k = 1, 000 of the words "suit," "orange," and "heart" in three different semantic models. The numbers in parenthesis indicate the k-NN ranks of the neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PMI</head><p>GloVe Skipgram suit suits <ref type="formula">(1)</ref> suits <ref type="formula">(1)</ref> suits <ref type="formula">(1)</ref> dress <ref type="formula">(2)</ref> lawsuit <ref type="formula">(2)</ref> lawsuit <ref type="formula">(2)</ref> lawsuit <ref type="formula">(10)</ref> mobile <ref type="formula">(33)</ref> dinosaur <ref type="formula">(53)</ref> gundam <ref type="formula">(34)</ref> costly <ref type="formula">(60)</ref> trump <ref type="formula">(55)</ref> option <ref type="formula">(76)</ref> zoot <ref type="formula">(133)</ref> counterparts <ref type="formula">(99)</ref> rebid <ref type="formula">(423)</ref> predator <ref type="formula">(107)</ref> serenaders <ref type="formula">(458)</ref> trump <ref type="formula">(109)</ref> hev (987) . . . orange yellow <ref type="formula">(1)</ref> yellow <ref type="formula">(1)</ref> redorange (1) lemon <ref type="formula">(16)</ref> ktype <ref type="formula">(12)</ref> lemon <ref type="formula">(14)</ref> citrus <ref type="formula">(17)</ref> jersey <ref type="formula">(21)</ref> cherry <ref type="formula">(24)</ref> county <ref type="formula">(26)</ref> peel <ref type="formula">(42)</ref> jumpsuits <ref type="formula">(57)</ref> . . . heart cardiac <ref type="formula">(1)</ref> my <ref type="formula">(1)</ref> congestive <ref type="formula">(1)</ref> soul <ref type="formula">(22)</ref> blood <ref type="formula">(2)</ref> hearts <ref type="formula">(2)</ref> hearts <ref type="formula">(183)</ref> throbs <ref type="formula">(3)</ref> ashtray <ref type="formula">(641)</ref> suffering <ref type="formula">(4)</ref> rags <ref type="formula">(771)</ref> brain <ref type="formula">(6)</ref> cardiac <ref type="formula">(8)</ref> hearts <ref type="formula">(11)</ref> throb <ref type="formula">(17)</ref> lungs <ref type="formula">(22)</ref> . . .</p><p>These examples demonstrate some interesting similarities and differences between the three models. First of all, there are some direct neigh- bors that are present in all three models: "suit" has "suits" and "lawsuit" as direct neighbors in all three models, "heart" has "hearts," "service" has "services," and "above" has "below". Plu- ral forms are of course reasonable neighbors of their singular counterparts in a semantic model, but their usefulness for WSI can perhaps be ques- tioned. Taking "suits" to indicate the clothes-sense of "suit," all three models produce both a clothes- sense and a law-sense. For "orange," the Skipgram model only represents the color-sense, while the PMI and GloVe models also feature a fruit-sense. For "heart," all three models have a disease-sense (represented by the neighbors "cardiac" in the PMI and GloVe models, and the neighbor "congestive" in the Skipgram model), and an organ-sense (rep- resented by the plural form "hearts"). "Service" is a comparably vague term that has a number of different senses in the PMI and GloVe models, but only one in the Skipgram model. "Bad" pro- duces both a negativity-sense and a German spa town-sense in all three models, but only the GloVe and Skipgram models have a separate antonym- sense ("good" is not a direct neighbor in the PMI model). "Above" has both the antonym and direct neighbors relating to measurements in all three models.</p><p>It is interesting to note that GloVe produces a significant amount of sequential relations; "mo- bile suit gundam", "cheap suit serenaders", "or- ange peel", and "orange jumpsuit" are just some of many examples of sequential relations found in the relative neighborhood of terms in the GloVe model.</p><p>The PMI and GloVe models produce the struc- turally most similar RNGs in these examples, with on average a handful of direct neighbors, of which some can be very distant. The Skipgram model on the other hand produces very few direct neigh- bors. This led us to look further into the struc- tural properties of neighborhoods in the Skipgram model. An interesting observation -and possi- ble complication -is that the neighborhoods in the Skipgram model are highly asymmetric: the first neighbor of "information" is "informations", whereas "information" is the 1,829th neighbor of "informations." While such asymmetry occurs in all models, it seems much more prevalent in the Skipgram model.  <ref type="table">Table 3</ref>: k-RNG for k = 1, 000 of the words "ser- vice," "bad," and "above" in three different seman- tic models. The numbers in parenthesis indicate the k-NN ranks of the neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PMI GloVe</head><p>Skipgram service services <ref type="formula">(1)</ref> services <ref type="formula">(1)</ref> services <ref type="formula">(1)</ref> network <ref type="formula">(2)</ref> operated <ref type="formula">(3)</ref> operates <ref type="formula">(8)</ref> serving (6) launched <ref type="formula">(18)</ref> military (17) served <ref type="formula">(22)</ref> duty <ref type="formula">(20)</ref> intercity <ref type="formula">(34)</ref> passenger <ref type="formula">(21)</ref> dialaride <ref type="formula">(644)</ref> aftersales (759) limitedstop (802) bad terrible <ref type="formula">(1)</ref> good <ref type="formula">(1)</ref> nauheim <ref type="formula">(1)</ref> that <ref type="formula">(2)</ref> kissingen <ref type="formula">(2)</ref> good <ref type="formula">(2)</ref> luck <ref type="formula">(39)</ref> ugly <ref type="formula">(45)</ref> dreadful <ref type="formula">(5)</ref> unfortunate <ref type="formula">(70)</ref> nasty <ref type="formula">(48)</ref> stalling <ref type="formula">(276)</ref> dirty (106) donnersbergkreis omen (328) (860) rancid <ref type="formula">(980)</ref> conkers <ref type="formula">(360)</ref> karma (952) above below <ref type="formula">(1)</ref> below <ref type="formula">(1)</ref> below <ref type="formula">(1)</ref> around <ref type="formula">(2)</ref> level <ref type="formula">(2)</ref> 500ft (2) feet <ref type="formula">(5)</ref> height <ref type="formula">(3)</ref> measuring <ref type="formula">(29)</ref> just <ref type="formula">(4)</ref> beneath <ref type="formula">(36)</ref> stands <ref type="formula">(10)</ref> columns <ref type="formula">(62)</ref> lower <ref type="formula">(11)</ref> atop <ref type="formula">(102)</ref> beneath <ref type="formula">(12)</ref> rise <ref type="formula">(21)</ref> sea <ref type="formula">(30)</ref> . . .</p><p>shows that the local densities vary much more in the Skipgram model than in the others. This is not in itself undesirable, but wild differences in neigh- borhood reciprocity complicates the choice of k in the k-RNG algorithm, as observed by the particu- larly sparse neighborhoods of the Skipgram model above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">WSI Evaluation</head><p>The standard way to evaluate WSI algorithms is to use one the SemEval WSI test collections <ref type="bibr" target="#b0">(Agirre and Soroa, 2007;</ref><ref type="bibr" target="#b17">Manandhar et al., 2010;</ref><ref type="bibr" target="#b19">Navigli and Vannella, 2013;</ref><ref type="bibr" target="#b13">Jurgens and Klapaftis, 2013)</ref>, which are all designed similarly: systems are expected to first perform WSI and then to as- sign texts to the induced senses (i.e. in effect do- ing a word-sense disambiguation step). We con- sider this type of evaluation to be a less useful for our purposes, since the required disambigua- tion step is a highly non-trivial task in itself. The RNG method proposed in this paper is a pure WSI algorithm, and as such does not offer a solu- tion to the disambiguation problem. We therefore opted to focus solely on the hypothesis that rel- ative neighborhoods cover senses that k-NNs do not. In essence, we investigate whether k-RNG retrieval does a better job at covering different senses than k-NN retrieval. This was done using pseudowords.</p><p>Pseudowords are artificially ambiguous words, created by regarding different words as identi- cal. We can, for example, say that the pseu- doword &lt;deadeye&gt; is a composite of the two words marksman and loudspeaker. A corpus with the artificially ambiguous word &lt;deadeye&gt; in it can then be created by replacing all occurrences of the words marksman and loudspeaker with &lt;deadeye&gt;.</p><p>Using the pseudowords provided by Pilehvar and Navigli (2013) a corpus with 689 non- overlapping pseudowords was created, based on the BNC corpus. 7 Two models were then trained, one on the altered corpus, and one on the unal- tered one. To check whether the neighborhood of a pseudoword contains information about its under- lying senses we compared each underlying sense to the words in the neighborhood, taking the mini- mum of all senses' maximum similarity as a score, as demonstrated in <ref type="table" target="#tab_0">Table 4</ref>. The similarities were calculated using the model trained on the unaltered corpus, as the one based on the altered corpus will not contain the underlying senses of pseudowords.</p><p>Working through the example in <ref type="table" target="#tab_0">Table 4</ref>, the neighborhood of the pseudoword &lt;deadeye &gt; consists of the three words shooter, stereo, and sport. The pseudoword in itself is made up of the two underlying senses marksman and loud- speaker. The similarities between the words in the neighborhood of the model trained on the unal- 7 www.natcorp.ox.ac.uk tered data and the words of the underlying senses are as presented in <ref type="table" target="#tab_0">Table 4</ref>. The closest word to marksman is shooter, with a similarity score of 0.7. The closest word to loudspeaker is stereo, with a score of 0.3. So the scoring would, in total, be 0.3. It should be noted that the upper bound for this score is oftentimes significantly lower than 1: The neighborhood could not possibly contain the words marksman or loudspeaker, as those words are not present in the corpus. This means that the scores are bounded by the similarity of the least similar closest neighbor to the underlying senses. This score was chosen because of its simplic- ity and intuitive interpretation: a low score im- plies that at least one word sense was not repre- sented in the neighborhood whereas a high score means that all senses are represented in the neigh- borhood. One can then plot these scores for both relative neighborhoods and k-NN neighborhoods for each pseudoword as is done in <ref type="figure" target="#fig_3">Figure 4</ref>. Each point (x, y) represents a pseudoword, with x and y being the score of the k-NN neighborhood and the k-RNG neighborhood respectively. <ref type="figure" target="#fig_4">Figure 5</ref> shows an aggregate of <ref type="figure" target="#fig_3">Figure 4</ref>, plot- ting the distribution of y −x, i.e. the difference be- tween the scores achieved by the k-RNG and the k-NN. As seen in <ref type="figure" target="#fig_3">Figure 4</ref>, a lot of points lie on the line y = x, meaning both methods achieved the same score. However, when this is not the case, there is a clear bias for the k-RNG to out- perform the k-NN, as demonstrated in <ref type="figure" target="#fig_4">Figure 5</ref>. Here, using the BNC instead of Wikipedia as train- ing data, the GloVe and Skipgram models yielded sparse relative neighborhoods -both with an av- erage of about 8 neighbors -but the PMI model produced quite dense neighborhoods averaging 63 neighbors. Since the scoring function does not pe- nalize neighborhood size there is good reason to be skeptical of its viability, and specifically the performance of the PMI-model based on these fig- ures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This paper has discussed the question how to query semantic models, which is a question that has been long neglected in research on computa- tional semantics. Nearest neighbor search (or k- NN) is often treated as the only available option, which leads to misunderstandings regarding how semantic models represent and handle vagueness and polysemy. We have argued that the structure -or topology -of the local neighborhoods in se- mantic models carry useful semantic information regarding the different usages -or senses -of a term, and that such topological properties there- fore can be used to analyze polysemy and do WSI. We have introduced relative neighborhood graphs (RNG) as an alternative to standard k-NN, and we have exemplified k-RNG in three differ- ent well-known semantic models. The examples demonstrate that k-RNG manages to retrieve dis- parate and relevant neighbors in all three models, yet the kind of neighbors returned and the nature of the neighborhoods differ. Quantitatively, The k- RNG method consistently outperformed k-NN on underlying sense retrieval.</p><p>We have also illustrated how k-RNG can be used as a tool to gain insight into the topological properties of different models. The GloVe model, for example, makes no difference between sequen- tial and substitutable relations, leading to neigh- borhoods that contain n-grams instead of senses. This can clearly be seen in for example <ref type="table">Table 2</ref>. Skipgram uses more sophisticated tokenization, which alleviates this issue.</p><p>Another interesting result of the paper is that the RNG uncovers otherwise unseen differences be- tween the models, which manifest not as scoring differences but as properties of the word represen- tations themselves. One example is the differences in neighborhood reciprocity observed between the different models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of when point b is between point a and c (left), and when it is not (right).</figDesc><graphic url="image-1.png" coords="4,93.74,654.11,69.90,71.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RNG for "heart" in the PMI model, restricted to the 100 nearest neighbors.</figDesc><graphic url="image-3.png" coords="6,72.00,62.81,453.54,315.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 confirmsFigure 3 :</head><label>33</label><figDesc>Figure 3: Neighborhood reciprocity in the different models; PMI to the left, GloVe in the middle, and Skipgram to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of minmax pseudosense score for k-RNGs and k-NNs for k = 1, 000 and k = 10 respectively; PMI to the left, GloVe in the middle, and Skipgram to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of difference between scores for k-RNGs and k-NNs. Positive scores means that the k-RNG scored higher than the kNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Example scoring of a neighborhood of 
the word &lt;deadeye&gt;. 

&lt;deadeye&gt; shooter stereo sport max 
marksman 
0.7 
0.04 
0.4 
0.7 
loudspeaker 0.01 
0.3 
0.05 0.3 
min: 0.3 

</table></figure>

			<note place="foot" n="2"> The Skipgram model also features a manga-related sense of &quot;suit&quot; in the neighbor &quot;crossbone,&quot; which refers to the mange series &quot;Mobile Suit Crossbone Gundam.&quot;</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 02: Evaluating word sense induction and discrimination systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Two graph-based algorithms for state-of-the-art WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="585" to="593" />
		</imprint>
	</monogr>
	<note>Oier López de Lacalle, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An optimal algorithm for approximate nearest neighbor searching fixed dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="891" to="923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">Louis</forename><surname>Bentley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peer-Timo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingrid</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Hotz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Pascucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peikert</surname></persName>
		</author>
		<title level="m">Topological Methods in Data Analysis and Visualization III</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Empty region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Cardinal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Collette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Langerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational geometry</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="195" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Locally-scaled spectral clustering using empty region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">1330</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring distributional similarity in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1162" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering corpus-specific word senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beate</forename><surname>Dorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Widdows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="79" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exemplar-based models for word meaning in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: towards removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of STOC</title>
		<meeting>STOC</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval2013 task 13: Word sense induction for graded and non-graded senses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Klapaftis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="290" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Filaments of meaning in word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jussi</forename><surname>Karlgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Holst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR</title>
		<meeting>ECIR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="531" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Temperature in word space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Koptjevskaja Tamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aggregating dialectology, typology, and register analysis</title>
		<editor>Benedikt Szmrecsanyi and Bernhard Wälchli</editor>
		<imprint>
			<publisher>De Gruyter</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="231" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Word sense induction for novel sense detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="591" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 14: Word sense induction &amp; disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Klapaftis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Vannella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semeval-2013 task 11: Word sense induction and disambiguation within an end-user application</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributional clustering of english words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="183" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Paving the way to a large-scale pseudosenseannotated dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic models of similarity in syntactic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´o</forename><surname>Diarmuid´odiarmuid´</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1047" to="1057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Clustering using feature domain similarity to discover word senses for adjectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noriko</forename><surname>Tomuro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Lytinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoko</forename><surname>Kanzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSC</title>
		<meeting>ICSC</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The relative neighbourhood graph of a finite planar set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Godfried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="268" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent semantic word sense induction and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT</title>
		<meeting>HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">HyperLex: lexical cartography for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Véronis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="252" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian word sense induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TextGraphs</title>
		<meeting>TextGraphs</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="10" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
