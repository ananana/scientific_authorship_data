<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning how to Active Learn: A Deep Reinforcement Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
							<email>meng.fang@unimelb.edu.au, yuanl4@student.unimelb.edu.au, t.cohn@unimelb.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning how to Active Learn: A Deep Reinforcement Learning Approach</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="595" to="605"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Active learning aims to select a small subset of data for annotation such that a classi-fier learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by re-framing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for train- ing accurate models is a critical bottleneck. Thus active learning has been applied to NLP tasks to minimise the expense of annotating data <ref type="bibr" target="#b25">(Thompson et al., 1999;</ref><ref type="bibr" target="#b28">Tong and Koller, 2001;</ref><ref type="bibr" target="#b20">Settles and Craven, 2008)</ref>. Active learning aims to reduce cost by identifying a subset of unlabelled data for anno- tation, which is selected to maximise the accuracy of a supervised model trained on the data <ref type="bibr" target="#b19">(Settles, 2010)</ref>. There have been many successful appli- cations to NLP, e.g., <ref type="bibr" target="#b26">Tomanek et al. (2007)</ref> used an active learning algorithm for CoNLL corpus to get an F 1 score 84% with a reduction of annotation cost of about 48%. In prior work most active learn- ing algorithms are designed for English based on heuristics, such as using uncertainty or informa- tiveness. There has been comparatively little work done about how to learn the active learning strat- egy itself.</p><p>It is no doubt that active learning is extremely important for other languages, particularly low- resource languages, where annotation is typically difficult to obtain, and annotation budgets more modest ( <ref type="bibr" target="#b3">Garrette and Baldridge, 2013)</ref>. Such set- tings are a natural application for active learning, however there is little work to this end. A poten- tial reason is that most active learning algorithms require a substantial 'seed set' of data for learning a basic classifier, which can then be used for ac- tive data selection. However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible.</p><p>In this paper, <ref type="bibr">1</ref> we propose PAL, short for Pol- icy based Active Learning, a novel approach for learning a dynamic active learning strategy from data. This allows for the strategy to be applied in other data settings, such as cross-lingual applica- tions. Our algorithm does not use a fixed heuris- tic, but instead learns how to actively select data, formalised as a reinforcement learning (RL) prob- lem. An intelligent agent must decide whether or not to select data for annotation in a streaming set- ting, where the decision policy is learned using a deep Q-network ( . The policy is informed by observations including sentences' content information, the supervised model's clas- sifications and its confidence. Accordingly, a rich and dynamic policy can be learned for annotating new data based on the past sequence of annotation decisions.</p><p>Furthermore, in order to reduce the dependence on the data in the target language, which may be low resource, we first learn the policy of active learning on another language and then transfer it to the target language. It is easy to learn a policy on a high resource language, where there is plentiful data, such as English. We use cross-lingual word embeddings to learn compatible data representa- tions for both languages, such that the learned pol- icy can be easily ported into the other language.</p><p>Our work is different for prior work in active learning for NLP. Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language it- self. Another main difference is that many ac- tive learning algorithms use a fixed data selec- tion heuristic, such as uncertainty sampling <ref type="bibr" target="#b20">(Settles and Craven, 2008;</ref><ref type="bibr" target="#b24">Stratos and Collins, 2015;</ref><ref type="bibr" target="#b31">Zhang et al., 2016)</ref>. However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent.</p><p>The remainder of this paper is organised as fol- lows. In Section 2, we briefly review some related work. In Section 3, we present active learning al- gorithms, which cross multiple languages. The ex- perimental results are presented in Section 4. We conclude our work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for train- ing the best classifier. Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling ( <ref type="bibr" target="#b8">Lewis and Gale, 1994;</ref><ref type="bibr" target="#b28">Tong and Koller, 2001</ref>), which selects the data about which the current classi- fier is the most uncertain; 2) query by commit- tee ( <ref type="bibr">Seung et al., 1992)</ref>, which selects the data about which the "committee" disagree most; and 3) expected error reduction <ref type="bibr" target="#b17">(Roy and McCallum, 2001)</ref>, which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification <ref type="bibr" target="#b10">(McCallumzy and Nigamy, 1998;</ref><ref type="bibr" target="#b28">Tong and Koller, 2001</ref>), relation classification ( <ref type="bibr" target="#b16">Qian et al., 2014)</ref>, and structured prediction <ref type="bibr" target="#b22">(Shen et al., 2004;</ref><ref type="bibr" target="#b20">Settles and Craven, 2008;</ref><ref type="bibr" target="#b24">Stratos and Collins, 2015;</ref><ref type="bibr">Fang and Cohn, 2017)</ref>. Qian et al. used uncer- tainty sampling to jointly perform on English and Chinese. Stratos and Collins and Zhang et al. de- ployed uncertainty-based AL algorithms for lan- guages with the minimal supervision.</p><p>Deep reinforcement learning (DRL) is a general-purpose framework for decision mak- ing based on representation learning. Recently, there are some notable examples include deep Q- learning ( , deep visuomotor poli- cies ( <ref type="bibr" target="#b7">Levine et al., 2016)</ref>, attention with recur- rent networks ( <ref type="bibr" target="#b1">Ba et al., 2015)</ref>, and model predic- tive control with embeddings ( <ref type="bibr" target="#b30">Watter et al., 2015)</ref>. Other important works include massively parallel frameworks ( <ref type="bibr" target="#b12">Nair et al., 2015)</ref>, dueling architec- ture (  and expert move predic- tion in the game of <ref type="bibr">Go (Maddison et al., 2015)</ref>, which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search ( <ref type="bibr" target="#b23">Silver et al., 2016)</ref>. DRL has been also studied in NLP tasks. For example, recently, DRL has been studied for information extraction prob- lem ( <ref type="bibr" target="#b13">Narasimhan et al., 2016)</ref>. They designed a framework that can decide to acquire external ev- idence and the framework is under the reinforce- ment learning method. However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings.</p><p>Recent deep learning work has also looked at transfer learning <ref type="bibr" target="#b2">(Bengio, 2012)</ref>. More recent work in deep learning has also considered trans- ferring policies by reusing policy parameters be- tween environments ( <ref type="bibr" target="#b15">Parisotto et al., 2016;</ref><ref type="bibr">Rusu et al., 2016</ref>), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies be- tween languages with shared feature space in state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We now show how active learning can be for- malised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep rein- forcement learning. Later we introduce a method for transferring the policy between languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Active learning as a decision process</head><p>Active learning is a simple technique for labelling data, which involves first selecting some instances from an unlabelled dataset, which are then anno- tated by a human oracle, which is then repeated many times until a termination criterion is satis- fied, e.g., the annotation budget is exhausted. Most often the selection function is based on the pre-dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model's predictive uncertainty ( <ref type="bibr" target="#b8">Lewis and Gale, 1994)</ref>, or divergence in predictions over an ensemble <ref type="bibr">(Seung et al., 1992</ref>). The key idea of these meth- ods is to find the instances on which the model is most likely to make errors, such that after their la- belling and inclusion in the training set, the model becomes more robust to these types of errors on unseen data.</p><p>The steps in active learning can be viewed as a decision process, a means of formalising the ac- tive learning algorithm as a sequence of decisions, where the stages of active learning correspond to the state of the system. Accordingly, the state cor- responds to the selected data for labelling and their labels, and each step in the active learning algo- rithm corresponds to a selection action, wherein the heuristic selects the next items from a pool. This process terminates when the budget is ex- hausted.</p><p>Effectively the active learning heuristic is oper- ating as a decision policy, a form of function tak- ing as input the current state -comprising the la- belled data, from which a model is trained -and a candidate unlabelled data point -e.g., the model uncertainty. This raises the opportunity to con- sider general policy functions, based on the state and data point inputs, and resulting in a labelling decision, and, accordingly a mechanism for learn- ing such functions from data. We now elaborate on the components of this process, namely the for- mulation of the decision process, architecture of the policy function, and means of learning the de- cision policy automatically from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stream-based learning</head><p>For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream ( <ref type="bibr" target="#b8">Lewis and Gale, 1994)</ref>. <ref type="bibr">2</ref> As each instance arrives, an agent must decide the action to take, namely whether or not the instance should be man- ually annotated. This process is illustrated in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, which illustrates the space of decision se- quences for a small corpus. As part of this pro- cess, a separate model, p φ , is trained on the la- belled data, and updated accordingly as the la- belled dataset is expanded as new annotations ar-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3: Ms. Haag plays Elianti</head><p>1: Pierre Vinken will join the board 2: Mr. Vinken is chairman of Elsevier 4: There is no asbestos in our products rive. This model is central to the policy for choos- ing the labelling actions at each stage, and for de- termining the reward for a sequence of actions. This is a form of Markov Decision Process (MDP), which allows the learning of a policy that can dynamically select instances that are most in- formative. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> at each time, the agent observes the current state s i which in- cludes the sentence x i , and the learned model φ. The agent selects a binary action a i , denot- ing whether to label x i , according to the policy π. For a i = 1, the corresponding sentence is labelled and added to the labelled data, and the model p φ updated to include this new training point. The process then repeats, terminating when either the dataset is exhausted or a fixed annota- tion budget is reached. After termination a reward is computed based on the accuracy of the final model, φ. We represent the MDP framework as a tuple S, A, P r(s i+1 |s i , a), R, where S = {s} is the space of all possible states, A = {0, 1} is the set of actions, R(s, a) is the reward function, and P r(s i+1 |s i , a) is the transition function.</p><formula xml:id="formula_0">... label ~ (ɸ 0 , x 1 ) label ~ (ɸ 0 , x 2 ) train ɸ 1 | ɸ 0 x 1 y 1 yes; y 1 = PER PER O O O O no label ~ (ɸ 1 , x 2 ) label ~ (ɸ 1 , x 3 ) train ɸ 2 | ɸ 1 x 2 y 2 no label ~ (ɸ 0 , x 3 ) train ɸ' 1 | ɸ 0 x 2 y 2 no terminate ... ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">State</head><p>The state at time i comprises the candidate in- stance being considered for annotation and the la- belled dataset constructed in steps 1 . . . i. We rep- resent the state using a continuous vector, using the concatenation of the vector representation of x i , and outputs of the model p φ trained over the labelled data. These outputs use both the predic- tive marginal distributions of the model on the in- stance, and a representation of the model's confi- dence. We now elaborate on each component.</p><p>Content representation A key input to the agent is the content of the sentence, x i , which we encode using a convolutional neural network to ar- rive at a fixed sized vector representation, follow- ing <ref type="bibr" target="#b4">Kim (2014)</ref>. This involves embedding each of the n words in the sentence to produce a matrix X i = {x i,1 , x i,2 , · · · , x i,n }, after which a series of wide convolutional filters is applied, using mul- tiple filters with different gram sizes. Each filter uses a linear transformation with a rectified linear unit activation function. Finally the filter outputs are merged using a max-pooling operation to yield a hidden state h c , which is used to represent the sentence.</p><p>Representation of marginals The prediction outputs of the training model, p φ (y|x i ), are cen- tral to all active learning heuristics, and accord- ingly, we include this in our approach. In order to generalise existing techniques, we elect to use the predictive marginals directly, rather than only using statistics thereof, e.g., entropy. This gener- ality allows for different and more nuanced con- cepts to be learned, including patterns of proba- bilities that span several adjacent positions in the sentence (e.g., the uncertainty about the boundary of a named entity).</p><p>We use another convolutional neural network to process the predictive marginals, as shown in <ref type="figure" target="#fig_1">Fig- ure 2</ref>. The convolutional layer contains j filters with ReLU activation, based on a window of width 3 and height equal to the number of classes, and with a stride of one token. We use a wide convo- lution, by padding the input matrix to either size with vectors of zeros. These j feature maps are then subsampled with mean pooling, such that the network is easily able to capture the average un- certainty in each window. The final hidden layer h e is used to represent the predictive marginals. Confidence of sequential prediction The last component is a score C which indicates the con- fidence of the model prediction. This is de- fined based on the most probable label sequence under the model, e.g., using Viterbi algorithm with a CRF, and the probability of this se- quence is used to represent the confidence, C = n max y p φ (y|x i ), where n = |x i | is the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Action</head><p>We now turn to the action, which denotes whether the human oracle must annotate the current sen- tence. The agent selects either to annotate x i , in which case a i = 1, or not, with a i = 0, after which the agent proceeds to consider the next instance, x i+1 . When action a i = 1 is chosen, an oracle is requested to annotate the sentence, and the newly annotated sentence is added to the training data, and φ updated accordingly. A special 'terminate' option applies when no further data remains or the annotation budget is exhausted, which concludes the active learning run (referred to as an 'episode' or 'game' herein).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Reward</head><p>The training signal for learning the policy takes the form of a scalar 'reward', which provides feed- back on the quality of the actions made by the agent. The most obvious reward is to wait for a game to conclude, then measure the held-out per- formance of the model, which has been trained on the labelled data. However, this reward is de- layed, and is difficult to related to individual ac- tions after a long game. To compensate for this, we use reward shaping, whereby small interme- diate rewards are assigned which speeds up the learning process <ref type="bibr" target="#b14">(Ng, 2003;</ref><ref type="bibr" target="#b6">Lample and Chaplot, 2016)</ref>. At each step, the intermediate reward is defined as the change in held-out performance, i.e., R(s i−1 , a) = Acc(φ i ) − Acc(φ i−1 ), where Acc denotes predictive accuracy (here F1 score), and φ i is the trained model after action a has take place, which may include an additional training in- stance. Accordingly, when considering the aggre- gate reward over a game, the intermediate terms cancel, such that the total reward measures the performance improvement over the whole game. Note that the value of R(s, a) can be positive or negative, indicating a beneficial or detrimental ef- fect on the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Budget</head><p>There is a fixed budget B for the total number of instances annotated, which corresponds to the ter- minal state in the MDP. It is a predefined number and chosen according to time and cost constraints. A game is finished when the data is exhausted or the budget reached, and with the final result be- ing the dataset thus created, upon which the final model is trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Reinforcement learning</head><p>The remaining question is how the above compo- nents can be used to learn a good policy. Different policies make different data selections, and thus result in models with different performance. We adopt a reinforcement learning (RL) approach to learn a policy resulting a highly accurate model.</p><p>Having represented the problem as a MDP, episode as a sequence of transitions (s i , a, r, s i+1 ). One episode of active learning produces a finite sequence of states, actions and rewards. We use a deep Q-learning approach ( , which formalises the policy using function Q π (s, a) → R which determines the utility of tak- ing a from state s according to a policy π. In Q- learning, the agent iteratively updates Q(s, a) us- ing rewards obtained from each episode, with up- dates based on the recursive Bellman equation for the optimal Q:</p><formula xml:id="formula_1">Q π (s, a) = E[R i |s i = s, a i = a, π].<label>(1)</label></formula><p>Here, R i = T t=i γ t−i r t is the discounted fu- ture reward and γ ∈ [0, 1] is a factor discounting the value of future rewards and the expectation is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Learn an active learning policy</head><p>Input: data D, budget B Output: π 1: for episode = 1, 2, . . . , N do for i ∈ {0, 1, 2, . . . , |D|} do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Construct the state s i using x i 6:</p><p>The agent makes a decision according to a i = arg max Q π (s i , a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>if a i = 1 then 8:</p><p>Obtain the annotation y i 9:</p><formula xml:id="formula_2">D l ← D l + (x i , y i ) 10:</formula><p>Update model φ based on D l</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>end if 12:</p><p>Receive a reward r i using held-out set 13: Update policy π with θ</p><formula xml:id="formula_3">if |D l | = B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>21:</head><p>end for 22: end for 23: return the latest policy π taken over all transitions involving state s and ac- tion a.</p><p>Following Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the pa- rameters. We implement the Q-function using a single hidden layer neural network, taking as in- put the state representation (h c , h e , C) (defined in §3.2.1), and outputting two scalar values cor- responding to the values Q(s, a) for a ∈ {0, 1}. This network uses a rectified linear unit (ReLU) activation function in its hidden layer.</p><p>The parameters in the DQN are learnt using stochastic gradient descent, based on a regression objective to match the Q-values predicted by the DQN and the expected Q-values from the Bell- man equation, r i + γ max a Q(s i+1 , a; θ). Fol- lowing ( , we use an experi- ence replay memory M to store each transition (s, a, r, s ) as it is used in an episode, after which Algorithm 2 Active learning by policy transfer</p><formula xml:id="formula_4">Input: unlabelled data D, budget B, policy π Output: D l 1: D l ← ∅ 2: φ ← Random 3: for |D l | = B and D not empty do 4:</formula><p>Randomly sample x i from the data pool D and construct the state s i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>The agent chooses an action a i according to a i = arg max Q π (s i , a)</p><p>6:</p><formula xml:id="formula_5">if a i = 1 then 7:</formula><p>Obtain the annotation y i 8:</p><formula xml:id="formula_6">D l ← D l + (x i , y i ) 9:</formula><p>Update model φ based on D l 10:</p><p>end if 11:</p><formula xml:id="formula_7">D ← D\x i 12:</formula><p>Receive a reward r i using held-out set 13:</p><p>Update policy π 14: end for 15: return D l we sample a mini-batch of transitions from the memory and then minimize the loss function:</p><formula xml:id="formula_8">L(θ) = E s,a,r,s y i (r, s ) − Q(s, a; θ) 2 ,<label>(2)</label></formula><p>where y i (r, s ) = r + γ max a Q(s , a ; θ i−1 ) is the target Q-value, based on the current param- eters θ i−1 , and the expectation is over the mini- batch. Learning updates are made every training step, based on stochastic gradient descent to min- imise Eq. 2 w.r.t. parameters θ.</p><p>The algorithm for learning is summarised in Al- gorithm 1. We train the policy by running multi- ple active learning episodes over the training data, where each episode is a simulated active learning run. For each episode, we shuffle the data, and hide the known labels, which are revealed as re- quested during the run. A disjoint held-out set is used to compute the reward, i.e., model accu- racy, which is fixed over the episodes. Between each episode the model is reset to its initialisation condition, with the main changes being the differ- ent (random) data ordering and the evolving policy function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-lingual policy transfer</head><p>We now turn to the question of how the learned policy can be applied to another dataset. Given the extensive use of the training dataset, the policy application only makes sense when employed in a Algorithm 3 Active learning by policy and model transfer, for 'cold-start' scenario Input: unlabelled data D, budget B, policy π,</p><formula xml:id="formula_9">model φ Output: D l 1: D l ← ∅ 2: for |D l | = B and D not empty do 3:</formula><p>Randomly sample x i from the data pool D and construct the state s i 4:</p><p>The agent chooses an action a i according to a i = arg max Q π (s i , a)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if a i = 1 then 6:</p><formula xml:id="formula_10">D l ← D l + (x i , −) 7:</formula><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>D ← D\x i 9: end for 10: Obtain all the annotations for D l 11: return D l different data setting, e.g., where the domain, task or language is different. For this paper, we con- sider a cross-lingual application of the same task (NER), where we train a policy on a source lan- guage (e.g., English), and then transfer the learned policy to a different target language. Cross-lingual word embeddings provide a common shared rep- resentation to facilitate application of the policy to other languages.</p><p>We illustrate the policy transfer algorithm in Al- gorithm 2. This algorithm is broadly similar to Algorithm 1, but has two key differences. Firstly, Algorithm 2 makes only one pass over the data, rather than several passes, as befits an application to a low-resource language where oracle labelling is costly. Secondly, the algorithm also assumes an initial policy, π, which is fine tuned during the episode based on held-out performance such that the policy can adapt to the test scenario. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cold-start transfer</head><p>The above transfer algorithm has some limita- tions, which may not be realistic for low-resource settings: the requirement for held-out evaluation data and the embedding of the oracle annotator in- side the learning loop. The former implies more supervision than is ideal in a low-resource setting, while the latter places limitations on the commu- nication with annotator as well as a necessity for real-time processing, both which are unlikely in a field linguistics setting.</p><p>For this data and-communication-impoverished setting, denoted as cold-start, we allow only one chance to request labels for the target data, and, having no held-out data, do not allow policy up- dates. The agent needs to select a batch of unla- belled target instances for annotations, but cannot use these resulting annotations or any other feed- back to refine the selection. In this, more difficult cold-start setting, we bootstrap the process with an initial model, such that the agent can make infor- mative decisions in the absence of feedback.</p><p>The procedure is outlined in Algorithm 3. Us- ing the cross-lingual word embeddings, we trans- fer both a policy and a model into the target lan- guage. The model, φ, is trained on one source language, and the policy is learned on a different source language. Policy learning uses Alg 1, with the small change that in step 3 the model is ini- tialised using φ. Consequently the learned policy can exploit the knowledge from cross-lingual ini- tialisation, such that it can figure out which aspects that need to be corrected using target annotated data. Overall this allows for estimates and con- fidence values to be produced by the model, thus providing the agent with sufficient information for data selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language. We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target lan- guages we only permit a single episode, to mimic a language without existing resources.</p><p>We use NER corpora from CoNLL2002/2003 shared tasks, 4 which comprise NER annotated text in English (en), German (de), Spanish (es), and Dutch (nl), each annotated using the IOB1 la- belling scheme, which we convert to the IO label- ing scheme. We use the existing corpus partions, with train used for policy training, testb used</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Multilingual</head><p>Cold- <ref type="table">start  tgt src tgt  src  tgt src pre  de en  de en,nl,es de nl  en  nl  en  nl en,de,es nl de en  es  en  es en,de,nl es de en  - - - - de es  en  - - - - nl  es  en  - - - - es  nl  en   Table 1</ref>: Experimental configuration for the three settings, showing target language (tgt), source lan- guage (src) as used for policy learning, and lan- guage used for pre-training the model (pre).</p><p>as held-out for computing rewards, and final re- sults are reported on testa.</p><p>We consider three experimental conditions, as illustrated in <ref type="table">Table 1:</ref> bilingual where English is the source (used for policy learning) and we vary the target lan- guage;</p><p>multilingual where several source languages are the used in joint learning of the policy, and a separate language is used as target; and cold-start where a pretrained English NER tag- ger is used to initialise policy learning on a source language, and in cold-start application to a separate target language.</p><p>Configuration We now outline the parameter settings for the experimental runs. For learning an active learning policy, we run N = 10, 000 episodes with budget B = 200 sentences using Alg. 1. Content representations use three convo- lutional filters of size 3, 4 and 5, using 128 fil- ters for each size, while for predictive marginals, the convolutional filters are of width 3, using 20 filters. The size of the last hidden layer is 256. The discount factor is set to γ = 0.99. We used the ADAM algorithm with mini-batches of size 32 for training the neural network. To report perfor- mance, we apply the learned policy to the target training set (using Alg. 2 or 3, again with budget 200), <ref type="bibr">5</ref> after which we use the final trained model for which we report F 1 score.</p><p>For word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al., q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q Uncertainty sampling we use the total token en- tropy measure <ref type="bibr" target="#b20">(Settles and Craven, 2008)</ref>, which takes the instance x maximising |x| t=1 H(y t |x, φ), where H is the token en- tropy. We use the whole training set as the data pool, and select a single instance for labelling in each active learning step. This method was shown to achieve the best re- sult among model-independent active learn- ing methods on the CoNLL data.</p><p>Random sampling which randomly selects ex- amples from the unlabelled pool.</p><p>Results <ref type="figure" target="#fig_3">Figure 3</ref> shows results the bilingual case, where PAL b consistently outperforms the Random and Uncertainty baselines across the three target languages. Uncertainty sampling is in- effective, particularly towards the start of the run, as a consequence of its dependence on a high qual- ity model. The use of content information allows PAL b to make a stronger start, despite the poor ini- tial model. Also shown in <ref type="figure" target="#fig_3">Figure 3</ref> are results for multilin- gual policy learning, PAL m , which outperform all other approaches including PAL b . This illustrates that the additional training over several languages gives rise to a better policy, than only using one source language. The superior performance is par- ticularly marked in the early stages of the runs for Spanish and Dutch, which may indicate that the approach was better able to learn to exploit the sentence content information.</p><p>We evaluate the cold-start setting in <ref type="figure" target="#fig_5">Figure 4</ref>. Recall that in this setting there are no policy or model updates, as no heldout data is used, and all annotations arrive in a batch. The model, how- ever, is initialised with a NER tagger trained on a different language, which explains why the per- formance for all methods starts from around 40% rather than 0%. Even in this challenging eval- uation setting, our algorithm PAL c outperforms both baseline methods, showing that deep Q learn- ing allows for better exploitation of the pretrained classifier, alongside the sentence content.</p><p>Lastly, we report the results for all approaches in <ref type="table">Table 2</ref>, based on training on the full 200 la- belled sentences as selected under the different methods. It is clear that the PAL methods all out- perform the baselines, and among these the multi- lingual training of PAL m outperforms the bilingual setting in PAL b . Surprisingly, PAL c gives the over- all best results, despite using a static policy and model during target application, underscoring the importance of model pretraining. <ref type="table">Table 2</ref> also re-q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q es → nl nl → de nl → es  <ref type="table">Table 2</ref>: Results from active learning using the dif- ferent methods, where each approach constructs a training set of 200 sentences. The three target lan- guages are shown as columns, reporting in each F 1 score (%) and the relative cost reduction to match the stated performance of the Random strategy.</p><p>ports the cost reduction versus random sampling, showing that the PAL methods can reduce the an- notation burden to as low as 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have proposed a new active learn- ing algorithm capable of learning active learning strategies from data. We formalise active learn- ing under a Markov decision framework, whereby active learning corresponds to a sequence of bi- nary annotation decisions applied to a stream of data. Based on this, we design an active learning algorithm as a policy based on deep reinforcement learning. We show how these learned active learn- ing policies can be transferred between languages, which we empirically show provides consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling. This holds true even in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example illustrating sequential active learning as a Markov Decision process. Data arrives sequentially, and at each time the active learning policy, π, must decide whether it should be labelled or not, based on the state which includes a predictive model parameterised by φ, and an unlabelled data instance x. The process continues until termination, e.g., when the annotation budget is exhausted. The solid green path shows the maximum scoring decision sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture for representing predictive marginal distributions, p φ (y|x i ), as a fixed dimensional vector, to form part of the MDP state.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The performance of active learning methods on the bilingual and multilingual settings for three target languages, whereby the active learning policy is trained on only en, or all other languages excluding the target, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6</head><label></label><figDesc>http://128.2.220.95/multilingual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance of active learning methods on the cold-start setting, each showing different source → target configurations, in all cases pretraining in en.</figDesc></figure>

			<note place="foot" n="1"> Source code available at https://github.com/ mengf1/PAL</note>

			<note place="foot" n="2"> This is different to pool-based active learning, where one of several options is chosen for annotation. Our setup permits simpler learning, while remaining sufficiently general.</note>

			<note place="foot" n="3"> Moreover, the algorithm can be extended to a traditional batch setting by evaluating a batch of data instances and selectinag the best k instances for labelling under the policy. This could be applied in either the transfer step (Algorithm 2) or initial policy training (Algorithm 1), or both.</note>

			<note place="foot" n="4"> http://www.cnts.ua.ac.be/conll2002/ ner/, http://www.cnts.ua.ac.be/conll2003/ ner/</note>

			<note place="foot" n="5"> Although it is possible the policy may learn not to use the full budget, this does not occur in practise.</note>

			<note place="foot">Meng Fang and Trevor Cohn. 2017. Model transfer for tagging low-resource languages using a bilingual dictionary. In Proceedings of the 55th Annual Meeting on Association for Computational Linguistics (ACL).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a part-of-speech tagger from two hours of annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML)</title>
		<meeting>the Eighteenth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05521</idno>
		<title level="m">Playing FPS games with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 17th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Move evaluation in go using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Employing em and pool-based active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kachites Mccallumzy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Machine Learning (ICML)</title>
		<meeting>the 15th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="359" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Deep Learning</title>
		<meeting>ICML Workshop on Deep Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods on Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Shaping and Policy Search in Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Actor-mimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilingual active learning for relation classification via pseudo parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="582" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward optimal active learning through monte carlo estimation of error reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning (ICML)</title>
		<meeting>the 18th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. 2016. Progressive neural networks</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">11</biblScope>
			<pubPlace>Madison</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of active learning strategies for sequence labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Manfred Opper, and Haim Sompolinsky. 1992. Query by committee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sebastian Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th annual workshop on Computational Learning Theory</title>
		<meeting>the 5th annual workshop on Computational Learning Theory</meeting>
		<imprint>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-criteria-based active learning for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chew-Lim</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL)</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple semisupervised pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active learning for natural language parsing and information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Cynthia A Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Machine Learning (ICML)</title>
		<meeting>the 16th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wermter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<imprint>
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joschka</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Name tagging for low-resource incident languages based on expectation-driven learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Language Technologies (NAACL-HLT)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="249" to="259" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
