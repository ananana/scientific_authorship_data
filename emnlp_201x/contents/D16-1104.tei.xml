<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Are Word Embedding-based Features Useful for Sarcasm Detection?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IITB-Monash Research Academy</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhav</forename><surname>Tripathi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Carman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Are Word Embedding-based Features Useful for Sarcasm Detection?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1006" to="1011"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper makes a simple increment to state-of-the-art in sarcasm detection research. Existing approaches are unable to capture subtle forms of context incongruity which lies at the heart of sarcasm. We explore if prior work can be enhanced using semantic similarity/discordance between word embed-dings. We augment word embedding-based features to four feature sets reported in the past. We also experiment with four types of word embeddings. We observe an improvement in sarcasm detection, irrespective of the word embedding used or the original feature set to which our features are augmented. For example, this augmentation results in an improvement in F-score of around 4% for three out of these four feature sets, and a minor degradation in case of the fourth, when Word2Vec embeddings are used. Finally, a comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe, in terms of their benefit to sarcasm detection.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sarcasm is a form of verbal irony that is intended to ex- press contempt or ridicule. Linguistic studies show that the notion of context incongruity is at the heart of sar- casm ( <ref type="bibr" target="#b5">Ivanko and Pexman, 2003)</ref>. A popular trend in automatic sarcasm detection is semi-supervised extrac- tion of patterns that capture the underlying context in- congruity ( <ref type="bibr" target="#b18">Riloff et al., 2013)</ref>. However, techniques to extract these pat- terns rely on sentiment-bearing words and may not cap- ture nuanced forms of sarcasm. Consider the sentence 'With a sense of humor like that, you could make a liv- ing as a garbage man anywhere in the country. <ref type="bibr">1</ref> ' The speaker makes a subtle, contemptuous remark about the <ref type="bibr">1</ref> All examples in this paper are actual instances from our dataset. sense of humor of the listener. However, absence of sen- timent words makes the sarcasm in this sentence difficult to capture as features for a classifier.</p><p>In this paper, we explore use of word embeddings to capture context incongruity in the absence of sentiment words. The intuition is that word vector-based sim- ilarity/discordance is indicative of semantic similar- ity which in turn is a handle for context incongruity. In the case of the 'sense of humor' example above, the words 'sense of humor' and 'garbage man' are seman- tically dissimilar and their presence together in the sen- tence provides a clue to sarcasm. Hence, our set of fea- tures based on word embeddings aim to capture such se- mantic similarity/discordance. Since such semantic simi- larity is but one of the components of context incongruity and since existing feature sets rely on sentiment-based features to capture context incongruity, it is imperative that the two be combined for sarcasm detection. Thus, our paper deals with the question:</p><p>Can word embedding-based features when augmented to features reported in prior work improve the performance of sarcasm detection?</p><p>To the best of our knowledge, this is the first attempt that uses word embedding-based features to detect sar- casm. In this respect, the paper makes a simple increment to state-of-the-art but opens up a new direction in sarcasm detection research. We establish our hypothesis in case of four past works and four types of word embeddings, to show that the benefit of using word embedding-based features holds across multiple feature sets and word em- beddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In our literature survey of sarcasm detection ( <ref type="bibr" target="#b8">Joshi et al., 2016)</ref>, we observe that a popular trend is semi-supervised extraction of patterns with implicit sentiment. One such work is by <ref type="bibr" target="#b18">Riloff et al. (2013)</ref> who give a bootstrap- ping algorithm that discovers a set of positive verbs and negative/undesirable situations. However, this simplifi- cation (of representing sarcasm merely as positive verbs followed by negative situation) may not capture difficult forms of context incongruity. Consider the sarcastic sen- tence 'A woman needs a man like a fish needs bicycle' 2 . The sarcasm in this sentence is understood from the fact that a fish does not need bicycle -and hence, the sentence ridicules the target 'a man'. However, this sentence does not contain any sentiment-bearing word. Existing sar- casm detection systems relying on sentiment incongruity (as in the case of our past work reported as ) may not work well in such cases of sarcasm.</p><p>To address this, we use semantic similarity as a han- dle to context incongruity. To do so, we use word vector similarity scores. Consider similarity scores (as given by Word2Vec) between two pairs of words in the sentence above:</p><p>similarity(man,woman) = 0.766 similarity(fish,bicycle) = 0.131</p><p>Words in one part of this sentence ('man' and 'woman') are lot more similar than words in another part of the sen- tence <ref type="bibr">('fish' and 'bicycle')</ref>. This semantic discordance can be a clue to presence of context incongruity. Hence, we propose features based on similarity scores between word embeddings of words in a sentence. In general, we wish to capture the most similar and most dissimilar word pairs in the sentence, and use their scores as features for sarcasm detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: Features from prior work</head><p>We augment our word embedding-based features to the following four feature sets that have been reported:  . : In addition to unigrams, they use features based on implicit and explicit incongruity. Implicit incongruity features are patterns with im- plicit sentiment as extracted in a pre-processing step. Explicit incongruity features consist of number of sentiment flips, length of positive and negative sub- sequences and lexical polarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Embedding-based Features</head><p>In this section, we now describe our word embedding- based features. We reiterate that these features will be augmented to features from prior works (described in Section 3). As stated in Section 2, our word embedding-based fea- tures are based on similarity scores between word em- beddings. The similarity score is the cosine similarity between vectors of two words. To illustrate our features, we use our example 'A woman needs a man like a fish needs a bicycle'. The scores for all pairs of words in this sentence are given in <ref type="table" target="#tab_0">Table 1</ref>.  Using these similarity scores, we compute two sets of features:</p><p>1. Unweighted similarity features (S): We first com- pute similarity scores for all pairs of words (except stop words). We then return four feature values per sentence. 3 :</p><p>• Maximum score of most similar word pair • Minimum score of most similar word pair • Maximum score of most dissimilar word pair • Minimum score of most dissimilar word pair</p><p>For example, in case of the first feature, we consider the most similar word to every word in the sentence, and the corresponding similarity scores. These most similar word scores for each word are indicated in bold in <ref type="table" target="#tab_0">Table 1</ref>. Thus, the first feature in case of our example would have the value 0.766 derived from the man-woman pair and the second feature would take the value 0.078 due to the needs-man pair. The other features are computed in a similar manner.</p><p>2. Distance-weighted similarity features (WS): Like in the previous case, we first compute similarity scores for all pairs of words (excluding stop-words).</p><p>For all similarity scores, we divide them by square of distance between the two words. Thus, the simi- larity between terms that are close in the sentence is weighted higher than terms which are distant from one another. Thus, for all possible word pairs, we compute four features:</p><p>• Maximum distance-weighted score of most similar word pair • Minimum distance-weighted score of most similar word pair • Maximum distance-weighted score of most dissimilar word pair • Minimum distance-weighted score of most dis- similar word pair</p><p>These are computed similar to unweighted similarity features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment Setup</head><p>We create a dataset consisting of quotes on GoodReads 4 . GoodReads describes itself as 'the world's largest site for readers and book recommendations.' The website also allows users to post quotes from books. These quotes are snippets from books labeled by the user with tags of their choice. We download quotes with the tag 'sarcastic' as sarcastic quotes, and the ones with 'philosophy' as non- sarcastic quotes. Our labels are based on these tags given by users. We ensure that no quote has both these tags. This results in a dataset of 3629 quotes out of which 759 are labeled as sarcastic. This skew is similar to skews observed in datasets on which sarcasm detection experi- ments have been reported in the past ( <ref type="bibr" target="#b18">Riloff et al., 2013</ref>). We report five-fold cross-validation results on the above dataset. We use SV M perf by Joachims (2006) with c as 20, w as 3, and loss function as F-score opti- mization. This allows SVM to be learned while optimiz- ing the F-score.</p><p>As described above, we compare features given in prior work alongside the augmented versions. This means that for each of the four papers, we experiment with four con- figurations:</p><p>1. Features given in paper X  We experiment with four types of word embeddings:</p><p>1. LSA: This approach was reported in <ref type="bibr" target="#b11">Landauer and Dumais (1997)</ref>. We use pre-trained word em- beddings based on LSA 5 . The vocabulary size is 100,000.</p><p>2. GloVe: We use pre-trained vectors avaiable from the GloVe project <ref type="bibr">6</ref> . The vocabulary size in this case is 2,195,904.</p><p>3. Dependency Weights: We use pre-trained vectors <ref type="bibr">7</ref> weighted using dependency distance, as given in <ref type="bibr" target="#b12">Levy and Goldberg (2014</ref>  <ref type="bibr">Rehůřek and Sojka, 2010</ref>).</p><p>To interact with the first three pre-trained vectors, we use scikit library (Pedregosa et al., 2011).    <ref type="bibr">GonzálezIbánez et al. (2011a)</ref>, and compare them with augmented versions in <ref type="table">Table 3</ref>. <ref type="table">Table 3</ref> shows results for four kinds of word embed- dings. All entries in the tables are higher than the sim- ple unigrams baseline, i.e., F-score for each of the four is higher than unigrams -highlighting that these are bet- ter features for sarcasm detection than simple unigrams. Values in bold indicate the best F-score for a given prior work-embedding type combination. In case of <ref type="bibr" target="#b13">Liebrecht et al. (2013)</ref> for Word2Vec, the overall improvement in F-score is 4%. Precision increases by 8% while recall re- mains nearly unchanged. For features given in <ref type="bibr">GonzálezIbánez et al. (2011a)</ref>, there is a negligible degradation of 0.91% when word embedding-based features based on Word2Vec are used. For <ref type="bibr" target="#b0">Buschmeier et al. (2014)</ref> for Word2Vec, we observe an improvement in F-score from 76.61% to 78.09%. Precision remains nearly unchanged while recall increases. In case of  and Word2Vec, we observe a slight improvement of 0.20% when unweighted (S) features are used. This shows that word embedding-based features are useful, across four past works for Word2Vec. <ref type="table">Table 3</ref> also shows that the improvement holds across the four word embedding types as well. The maxi- mum improvement is observed in case of <ref type="bibr" target="#b13">Liebrecht et al. (2013)</ref>. It is around 4% in case of LSA, 5% in case of GloVe, 6% in case of Dependency weight-based and 4% in case of Word2Vec. These improvements are not directly comparable because the four embeddings have different vocabularies (since they are trained on different datasets) and vocabulary sizes, their results cannot be di- rectly compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Therefore, we take an intersection of the vocabulary (i.e., the subset of words present in all four embeddings) and repeat all our experiments using these intersection files. The vocabulary size of these intersection files (for all four embeddings) is 60,252. <ref type="table" target="#tab_7">Table 4</ref> shows the av- erage increase in F-score when a given word embed- ding and a word embedding-based feature is used, with the intersection file as described above. These gain val- ues are lower than in the previous case. This is be- cause these are the values in case of the intersection versions -which are subsets of the complete embed- dings. Each gain value is averaged over the four prior works. Thus, when unweighted similarity (+S) based features computed using LSA are augmented to fea- tures from prior work, an average increment of 0.835% is obtained over the four prior works. The values al- low us to compare the benefit of using these four kinds of embeddings. In case of unweighted similarity-based features, dependency-based weights give the maximum gain (0.978%). In case of weighted similarity-based features and '+S+WS', Word2Vec gives the maximum gain (1.411%).   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Error Analysis</head><p>Some categories of errors made by our system are:</p><p>1. Embedding issues due to incorrect senses: Be- cause words may have multiple senses, some em- beddings lead to error, as in 'Great. Relationship advice from one of America's most wanted.'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Contextual sarcasm: Consider the sarcastic quote 'Oh, and I suppose the apple ate the cheese'. The similarity score between 'apple' and 'cheese' is 0.4119. This comes up as the maximum similar pair. The most dissimilar pair is 'suppose' and 'apple' with similarity score of 0.1414. The sarcasm in this sentence can be understood only in context of the complete conversation that it is a part of.</p><p>3. Metaphors in non-sarcastic text: Figurative lan- guage may compare concepts that are not directly re- lated but still have low similarity. Consider the non- sarcastic quote 'Oh my love, I like to vanish in you like a ripple vanishes in an ocean -slowly, silently and endlessly'. Our system incorrectly predicts this as sarcastic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Early sarcasm detection research focused on speech <ref type="bibr" target="#b19">(Tepperman et al., 2006</ref>) and lexical features <ref type="bibr" target="#b10">(Kreuz and Caucci, 2007)</ref>. Several other features have been proposed <ref type="bibr" target="#b10">(Kreuz and Caucci, 2007;</ref><ref type="bibr" target="#b9">Khattri et al., 2015;</ref><ref type="bibr" target="#b13">Liebrecht et al., 2013;</ref><ref type="bibr" target="#b3">González-Ibánez et al., 2011a;</ref><ref type="bibr" target="#b15">Rakov and Rosenberg, 2013;</ref><ref type="bibr" target="#b23">Wallace, 2015;</ref><ref type="bibr" target="#b22">Wallace et al., 2014;</ref><ref type="bibr" target="#b21">Veale and Hao, 2010;</ref><ref type="bibr" target="#b4">González-Ibánez et al., 2011b;</ref><ref type="bibr" target="#b17">Reyes et al., 2012</ref>). Of particular relevance to our work are papers that aim to first extract patterns relevant to sarcasm detection.  use a semi-supervised approach that extracts sentiment-bearing patterns for sarcasm detection.  extract phrases corresponding to implicit incongruity i.e. the sit- uation where sentiment is expressed without use of sen- timent words. <ref type="bibr" target="#b18">Riloff et al. (2013)</ref> describe a bootstrap- ping algorithm that iteratively discovers a set of positive verbs and negative situation phrases, which are later used in a sarcasm detection algorithm.  also perform semi-supervised extraction of patterns for sar- casm detection. The only prior work which uses word embeddings for a related task of sarcasm detection is by <ref type="bibr" target="#b2">Ghosh et al. (2015)</ref>. They model sarcasm detection as a word sense disambiguation task, and use embeddings to identify whether a word is used in the sarcastic or non- sarcastic sense. Two sense vectors for every word are created: one for literal sense and one for sarcastic sense. The final sense is determined based on the similarity of these sense vectors with the sentence vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>This paper shows the benefit of features based on word embedding for sarcasm detection. We experiment with four past works in sarcasm detection, where we augment our word embedding-based features to their sets of fea- tures. Our features use the similarity score values re- turned by word embeddings, and are of two categories: similarity-based (where we consider maximum/minimum similarity score of most similar/dissimilar word pair re- spectively), and weighted similarity-based (where we weight the maximum/minimum similarity scores of most similar/dissimilar word pairs with the linear distance between the two words in the sentence). We experi- ment with four kinds of word embeddings: LSA, GloVe, Dependency-based and Word2Vec. In case of Word2Vec, for three of these past feature sets to which our features were augmented, we observe an improvement in F-score of at most 5%. Similar improvements are observed in case of other word embeddings. A comparison of the four embeddings shows that Word2Vec and dependency weight-based features outperform LSA and GloVe. This work opens up avenues for use of word embed- dings for sarcasm classification. Our word embedding- based features may work better if the similarity scores are computed for a subset of words in the sentence, or using weighting based on syntactic distance instead of linear distance as in the case of our weighted similarity-based features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>Liebrecht et al. (2013): They consider unigrams, bigrams and trigrams as features. 2. González-Ibánez et al. (2011a): They propose two sets of features: unigrams and dictionary-based. The latter are words from a lexical resource called LIWC. We use words from LIWC that have been annotated as emotion and psychological process words, as described in the original paper. 3. Buschmeier et al. (2014): In addition to uni- grams, they propose features such as: (a) Hy- perbole (captured by three positive or negative words in a row), (b) Quotation marks and ellipsis, (c) Positive/Negative Sentiment words followed by an exclamation mark or question mark, (d) Posi- tive/Negative Sentiment Scores followed by ellipsis (represented by a '...'), (e) Punctuation, (f) Interjec- tions, and (g) Laughter expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :</head><label>3</label><figDesc>Performance obtained on augmenting word embedding features to features from four prior works, for four word embeddings; L: Liebrecht et al. (2013), G: González-Ibánez et al. (2011a), B: Buschmeier et al. (2014) , J: Joshi et al. (2015) that word embedding-based features alone are not sufficient, and should be augmented with other fea- tures. Following this, we show performance using features presented in four prior works: Buschmeier et al. (2014), Liebrecht et al. (2013), Joshi et al. (2015) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Similarity scores for all pairs of content words in 'A woman needs a man like a fish needs bicycle'</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>2 .</head><label>2</label><figDesc></figDesc><table>Features given in paper X + unweighted similarity 
features (S) 

3. Features given in paper X + weighted similarity fea-
tures (WS) 

4. Features given in paper X + S+WS (i.e., weighted 
and unweighted similarity features) 

Features 
P 
R 
F 

Baseline 

Unigrams 
67.2 
78.8 
72.53 
S 
64.6 
75.2 
69.49 
WS 
67.6 
51.2 
58.26 
Both 
67 
52.8 
59.05 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of unigrams versus our similarity-based features 

using embeddings from Word2Vec 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>performance of sarcasm detection when 
our word embedding-based features are used on their own 
i.e, not as augmented features. The embedding in this 
case is Word2Vec. The four rows show baseline sets 
of features: unigrams, unweighted similarity using word 
embeddings (S), weighted similarity using word embed-
dings (WS) and both (i.e., unweighted plus weighted sim-
ilarities using word embeddings). Using only unigrams 
as features gives a F-score of 72.53%, while only un-
weighted and weighted features gives F-score of 69.49% 
and 58.26% respectively. This validates our intuition </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 averages these values over the</head><label>5</label><figDesc></figDesc><table>Word2Vec LSA 

GloVe 
Dep. 
Wt. 

+S 
0.835 
0.86 
0.918 
0.978 
+WS 
1.411 
0.255 
0.192 
1.372 
+S+WS 1.182 
0.24 
0.845 
0.795 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : Average gain in F-Scores obtained by using intersection of the</head><label>4</label><figDesc></figDesc><table>four word embeddings, for three word embedding feature-types, aug-

mented to four prior works; Dep. Wt. indicates vectors learned from 
dependency-based weights 

Word Embedding Average F-score Gain 

LSA 
0.452 
Glove 
0.651 
Dependency 
1.048 
Word2Vec 
1.143 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Average gain in F-scores for the four types of word embed-

dings; These values are computed for a subset of these embeddings 

consisting of words common to all four 

three types of word embedding-based features. Using 
Dependency-based and Word2Vec embeddings results in 
a higher improvement in F-score (1.048% and 1.143% 
respectively) as compared to others. 

</table></figure>

			<note place="foot" n="2"> This quote is attributed to Irina Dunn, an Australian writer (https://en.wikipedia.org/wiki/Irina_Dunn</note>

			<note place="foot" n="3"> These feature values consider all words in the sentence, i.e., the &apos;maximum&apos; is computed over all words</note>

			<note place="foot" n="4"> www.goodreads.com 5 http://www.lingexp.uni-tuebingen.de/z2/ LSAspaces/ 6 http://nlp.stanford.edu/projects/glove/ 7 https://levyomer.wordpress.com/2014/04/25/ dependency-based-word-embeddings/ 8 https://code.google.com/archive/p/Word2Vec/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An impact analysis of features in a classification approach to irony detection in product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Buschmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
		<meeting>the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised recognition of sarcastic sentences in twitter and amazon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sarcastic or not: Word embeddings to predict the literal or sarcastic meaning of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>González-Ibánez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>González-Ibánez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context incongruity and irony processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivanko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Penny M Pexman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discourse Processes</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="241" to="279" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Harnessing context incongruity for sarcasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinita</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="757" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Pushpak Bhattacharyya, and Mark James Carman</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03426</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Automatic sarcasm detection: A survey</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Your sentiment precedes you: Using an authors historical tweets to predict sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Khattri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">James</forename><surname>Carman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexical influences on the perception of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina</forename><forename type="middle">M</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on computational approaches to Figurative Language</title>
		<meeting>the Workshop on computational approaches to Figurative Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A solution to platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PSYCHOLOGICAL REVIEW</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="240" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dependency-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06-22" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The perfect solution for detecting sarcasm in tweets# not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Liebrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apj</forename><surname>Fa Kunneman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">sure, i did the right thing&quot;: a system for sarcasm detection in speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="842" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
		<ptr target="http://is.muni.cz/publication/884893/en" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From humor recognition to irony detection: The figurative language of social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sarcasm as contrast between a positive sentiment and negative situation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Surve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="704" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">yeah right&quot;: sarcasm recognition for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tepperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>David R Traum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<editor>INTERSPEECH. Citeseer</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Icwsma great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Tsur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Davidov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting ironic intent in creative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfen</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="765" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Humans require context to infer ironic intent (so computers probably do, too)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura Kertz Do Kook</forename><surname>Byron C Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse, contextually informed models for irony detection: Exploiting user communities,entities and sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
