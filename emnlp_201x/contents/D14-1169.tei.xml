<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiqiang</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">China Information Technology Security Evaluation Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjun</forename><surname>Cheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">China Information Technology Security Evaluation Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Science and Technology Dept. of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems National Laboratory for Information</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1614" to="1623"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Clustering aspect-related phrases in terms of product&apos;s property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis. Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts. In this paper, we explore a novel idea, sentiment distribution consistency, which states that different phrases (e.g. &quot;price&quot;, &quot;money&quot;, &quot;worth&quot;, and &quot;cost&quot;) of the same aspect tend to have consistent sentiment distribution. Through formalizing sentiment distribution consistency as soft constraint, we propose a novel unsu-pervised model in the framework of Posterior Regularization (PR) to cluster aspect-related phrases. Experiments demonstrate that our approach outperforms baselines remarkably.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-level sentiment analysis has become a cen- tral task in sentiment analysis because it can ag- gregate various opinions according to a product's properties, and provide much detailed, complete, and in-depth summaries of a large number of re- views. Aspect finding and clustering, a precursor process of aspect-level sentiment analysis, has at- tracted more and more attentions <ref type="bibr" target="#b12">(Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2013;</ref><ref type="bibr" target="#b17">Zhai et al., 2011a;</ref><ref type="bibr" target="#b16">Zhai et al., 2010)</ref>.</p><p>Aspect finding and clustering has never been a trivial task. People often use different words or phrases to refer to the same product property (also called product aspect or feature in the literature). Some terms are lexically dissimilar while seman- tically close, which makes the task more challeng- ing. For example, "price", "money" , "worth" and "cost" all refer to the aspect "price" in reviews. In order to present aspect-specific summaries of opinions, we first of all, have to cluster different aspect-related phrases. It is expensive and time- consuming to manually group hundreds of aspect- related phrases. In this paper, we assume that the aspect phrases have been extracted in advance and we keep focused on clustering domain synony- mous aspect-related phrases.</p><p>Existing studies addressing this problem are mainly based on the assumption that different phrases of the same aspect should have similar co- occurrence contexts. In addition to the traditional assumption, we develop a new angle to address the problem, which is based on sentiment distribution consistency assumption that different phrases of the same aspect should have consistent sentiment distribution, which will be detailed soon later. This new angle is inspired by this simple obser- vation (as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>): two phrases within the same cluster are not likely to be simultaneously placed in Pros and Cons of the same review. A straightforward way to use this information is to formulate cannot-link knowledge in clustering al- gorithms <ref type="bibr" target="#b5">(Chen et al., 2013;</ref><ref type="bibr" target="#b18">Zhai et al., 2011b</ref>). However, we have a particularly different manner to leverage the knowledge.</p><p>Due to the availability of large-scale semi- structured customer reviews (as exemplified in <ref type="figure" target="#fig_0">Fig. 1</ref>) that are supported by many web sites, we can easily get the estimation of sentiment dis- tribution for each aspect phrase by simply count- ing how many times a phrase appears in Pros and Cons respectively. As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, we can see that the estimated sentiment distribution of a phrase is close to that of its aspect. The above observation suggests the sentiment distri- bution consistency assumption: different phrases of the same aspect tend to have the same senti- ment distribution, or to have statistically close distributions. This assumption is also verified by our data: for most (above 91.3%) phrase with rela- tively reliable estimation (whose occurrence ≥50), the KL-divergence between the sentiment distri- bution of a phrase and that of its corresponding aspect is less than 0.05. It is worth noting that, the sentiment distribution of a phrase can be estimated accurately only when we obtain a sufficient number of reviews. When the number of reviews is limited, however, the es- timated sentiment distribution for each phrase is unreliable (as shown in <ref type="figure">Fig. 3)</ref>. A key issue, arisen here, is how to formulate this assumption in a statistically robust manner. The proposed model should be robust when only a limited number of reviews are available. <ref type="figure">Figure 3</ref>: The sentiment distribution of aspect "battery" and its related-phrases on nokia 3110c with a small mumber of reviews.</p><p>To deal with this issue, we model sentiment dis- tribution consistency as soft constraint, integrated into a probabilistic model that maximizes the data likelihood. We design the constraint to work in the following way: when we have sufficient ob- servations, the constraint becomes tighter, which plays a more important role in the learning pro- cess; when we have limited observations, the con- straint becomes very loose so that it will have less effect on the model. In this paper, we propose a novel unsupervised model, Sentiment Distribution Consistency Reg- ularized Multinomial Naive Bayes (SDC-MNB). The context part is modeled by Multinomial Naive Bayes in which aspect is treated as latent variable, and Sentiment distribution consistency is encoded as soft constraint within the framework of Poste- rior Regularization (PR) ( <ref type="bibr">Graca et al., 2008</ref>). The main contributions of this paper are summarized as follows:</p><p>• We study the problem of clustering phrases by integrating both context information and sentiment distribution of aspect-related phrases.</p><p>• We explore a novel concept, sentiment distri- bution consistency(SDC), and model it as soft constraint to guide the clustering process.</p><p>• Experiments show that our model outper- forms the state-of-art approaches for aspect clustering.</p><p>The rest of this paper is organized as follows. We introduce the SDC-MNB model in Section 2. We present experiment results in Section 3. In Section 4, we survey related work. We summarize the work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sentiment Distribution Consistency Regularized Multinomial Naive Bayes</head><p>In this section, we firstly introduce our assumption sentiment distribution consistency formally and show how to model the above assumption as soft constraint , which we term SDC-constraint. Sec- ondly, we show how to combine SDC-constraint with the probabilistic context model. Finally, we present the details for context and sentiment ex- traction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment Distribution Consistency</head><p>We define aspect as a set of phrases that refer to the same property of a product and each phrase is termed aspect-related phrase (or aspect phrase in short). For example, the aspect "battery" contains aspect phrases such as "battery", "battery life", "power", and so on. the aspect phrase set fj the j th aspect phrase yj the aspect for aspect phrase fj A the aspect set ai the i th aspect D the set of context documents dj the context document of fj V the word vocabulary wt the t th word in vocabulary V w d j ,k the k th word in dj Ntj the number of times word wt occurs in dj P the product set p k the k th product u ik the sentiment distribution parameter of aspect ai on p k ˆ s jk the estimated sentiment distribution parameter of phrase fj on p k n jk the occurrence times of aspect phrase fj on p k ˆ σ jk the sample standard deviation θ the model parameters p θ (ai|dj) the posterior distribution of ai given dj q(yj = ai) the projected posterior distribution of ai given dj Let us consider the sentiment distribution on a certain aspect a i . In a large review dataset, as- pect a i could receive many comments from differ- ent reviewers. For each comment, we assume that people either praise or complain about the aspect. So each comment on the aspect can be seen as a Bernoulli trial, where the aspect receives positive comments with probability p a i 1 . We introduce a random variable X a i to denote the sentiment on aspect a i , where X a i = 1 means that aspect a i receives positive comments, X a i = 0 means that aspect a i receives negative comments. Obviously, the sentiment on aspect a i follows the Bernoulli distribution,</p><formula xml:id="formula_0">P r(Xa i ) = p Xa i a i * (1 − pa i ) 1−Xa i , Xa i ∈ {0, 1}. (1)</formula><p>Or in short,</p><formula xml:id="formula_1">X a i ∼ Bernoulli(p a i )</formula><p>Let us see the case for aspect phrase f j , where f j ∈ aspect a i . Similarly, each comment on an as- pect phrase f j can also be seen as a Bernoulli trial. We introduce a random variable X f j to denote the sentiment on aspect phrase f j , where X f j = 1 means that aspect f j receives positive comments, X f j = 0 means that aspect f j receives negative comments. As just discussed, we assume that each aspect phrase follows the same distribution with 1 positive comment means that an aspect term is observed in Pros of a review. the corresponding aspect. This leads to the fol- lowing formal description:</p><p>• Sentiment Distribution Consistency : The sentiment distribution of aspect phrase is the same as that of the corresponding aspect. Formally, for all aspect phrase f j ∈ aspect a i , X f j ∼ Bernoulli(p a i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentiment Distribution Consistency Constraint</head><p>Assuming the sentiment distribution of aspect a i is given in advance, we need to judge whether an as- pect phrase f j belongs to the aspect a i with limited observations for f j . Let's consider the example in <ref type="figure" target="#fig_3">Fig. 4</ref>. For aspect phrase 3, we have no definite answer due to the limited number of observations. For aspect phrase 1, it seems that the sentiment distribution is consistent with that of the left as- pect. However, we can not say that the phrase be- longs to the aspect because the distribution may be the same for two different aspects. For aspect phrase 2, we are confident that its sentiment dis- tribution is different from that of the left aspect, given sufficient observations. To be concise, we judge an aspect phrase doesn't belong to certain aspect only when we are confident that they follow different sentiment dis- tributions.</p><p>Inspired by the intuition, we conduct interval parameter estimation for parameter p f j (sentiment distribution for phrase f j ) with limited observa- tions, and thus get a confidence interval for p f j . If p a i (sentiment distribution for aspect a i ) is not in the confidence interval of p f j , we then are con- fident that they follow different distributions. In other words, if aspect phrase f j ∈ aspect a i , we are confident that p a i is in the confidence interval of p f j .</p><p>More formally, we use u ik to denote the senti- ment distribution parameter of aspect a i on prod- uct p k , and assume that u ik is given in advance.</p><p>We want to know whether the sentiment distribu- tion on aspect phrase f j is the same as that of as- pect a i on product p k given a limited number of observations (samples). It's straightforward to cal- culate the confidence interval for parameter s jk in the Bernoulli distribution function. Let the sam- ple mean of n jk samples bê s jk , and the sample standard deviation bê σ jk . Since the sample size is small here, we use the Student-t distribution to calculate the confidence interval. According to our assumption, we are confident that u ik is in the con-</p><formula xml:id="formula_2">fidence interval if f j ∈ a i . ˆ s jk − C ˆ σ jk √ n jk ≤ u ik ≤ ˆ s jk + C ˆ σ jk √ n jk , ∀fj ∈ ai, ∀k. (2)</formula><p>where we look for t-table to find C corresponding to a certain confidence level(such as 95%) with the freedom of n jk − 1. For simplicity, we represent the above confidence interval by</p><formula xml:id="formula_3">[ˆ s jk − d jk , ˆ s jk + d jk ], where d jk = C ˆ σ jk √ n jk .</formula><p>We introduce an indicator variable z ij to repre- sent whether the aspect phrase f j belongs to aspect a i , as follows:</p><formula xml:id="formula_4">zji = { 1 ; if fj ∈ ai 0 ; otherwise (3)</formula><p>This leads to our SDC-constraint function.</p><formula xml:id="formula_5">ϕ = zji|u ik − ˆ s jk | ≤ d jk , ∀i, j, k<label>(4)</label></formula><p>SDC-constraint are flexible for modeling Senti- ment Distribution Consistency. The more obser- vations we have, the smaller d jk is. For frequent aspect phrase, the constraint can be very informa- tive because it can filter unrelated aspects for as- pect phrase f j . The less observations we have, the larger d jk is. For rare aspect phrases, the con- straint can be very loose, and will not have much effect on the clustering process for aspect phrase f j . In this way, the model can work very robustly.</p><p>SDC-constraints are data-driven constraints. Usually we have many reviews about hundreds of products in our dataset. For each aspect phrase, there are |A| * |P | constraints (the number of as- pects times the number of product). With thou- sands of constraints about which aspect it is not likely to belong to, the model learns to which as- pect a phrase f j should be assigned. Although most constraints may be loose because of the lim- ited observations, SDC-constraint can still play an important role in the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Sentiment Distribution Consistency</head><p>Regularized Multinomial Naive Bayes (SDC-MNB)</p><p>In this section, we present our probabilistic model which employs both context information and sen- timent distribution. First of all, we extract a context document d for each aspect phrase, which will be described in Section 2.5. In other word, a phrase is represented by its context document. Assuming that the doc- uments in D are independent and identically dis- tributed, the probability of generating D is then given by:</p><formula xml:id="formula_6">p θ (D) = |D| ∏ j=1 p θ (dj) = |D| ∏ j=1 ∑ y j ∈A p θ (dj, yj)<label>(5)</label></formula><p>where y j is a latent variable indicating the aspect label for aspect phrase f j , and θ is the model pa- rameter.</p><p>In our problem, we are actually more inter- ested in the posterior distribution over aspect, i.e., p θ (y j |d j ). Once the learned parameter θ is obtained, we can get our clustering result from p θ (y j |d j ), by assigning aspect a i with the largest posterior to phrase f j . We can also enforce SDC- constraint in expectation(on posterior p θ ). We use q(Y ) to denote the valid posterior distribution that satisfy our SDC-constraint, and Q to denote the valid posterior distribution space, as follows:</p><formula xml:id="formula_7">Q = {q(Y ) : Eq[zji|u ik − ˆ s jk |] ≤ d jk , ∀i, j, k}.<label>(6)</label></formula><p>Since posterior plays such an important role in joining the context model and SDC-constraint, we formulate our problem in the framework of Poste- rior Regularization (PR). PR is an efficient frame- work to inject constraints on the posteriors of la- tent variables. Instead of restricting p θ directly, which might not be feasible, PR penalizes the dis- tance of p θ to the constraint set Q. The posterior- regularized objective is termed as follows:</p><formula xml:id="formula_8">max θ {log p θ (D) − min q∈Q KL(q(Y )||p θ (Y |D))} (7)</formula><p>By trading off the data likelihood of the ob- served context documents (as defined in the first term), and the KL divergence of the posteriors to the valid posterior subspace defined by SDC- constraint (as defined in the second term), the ob- jective encourages models with both desired pos- terior distribution and data likelihood. In essence, the model attempts to maximize data likelihood of context subject (softly) to SDC-constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Multinomial Naive Bayes</head><p>In spirit to <ref type="bibr" target="#b17">(Zhai et al., 2011a</ref>), we use Multino- mial Naive Bayes (MNB) to model the context document. Let w d j ,k denotes the k th word in doc- ument d j , where each word is from the vocabulary V = {w 1 , w 2 , ..., w |V | }. For each aspect phrase f j , the probability of its latent aspect being a i and generating context document d i is</p><formula xml:id="formula_9">p θ (d j , y j = a i ) = p(a i ) |d j | ∏ k=1 p(w d j ,k |a i ) (8)</formula><p>where p(a i ) and p(w d j ,k |a i ) are parameters of this model. Each word w d j ,k is conditionally indepen- dent of all other words given the aspect a i .</p><p>Although MNB has been used in existing work for aspect clustering, all of the studies used it in a semi-supervised manner, with labeled data or pseudo-labeled data. In contrast, MNB proposed here is used in an unsupervised manner for aspect- related phrases clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">SDC-constraint</head><p>As mentioned above, the constraint posterior set Q is defined by</p><formula xml:id="formula_10">Q = {q(Y ) : q(yj = ai)|u ik − ˆ s jk | ≤ d jk , ∀i, j, k}.<label>(9)</label></formula><p>We can see that Q denotes a set of linear con- straints on the projected posterior distribution q. Note that we do not directly observe u ik , the sen- timent distribution of aspect a i on product p k . For aspect phrase f j that belongs to aspect a i , we es- timate u ik by counting all sentiment samples. We use the posterior p θ (a i |d j ) to approximately rep- resent how likely phrase f j belongs to aspect a i .</p><formula xml:id="formula_11">u ik = 1 ∑ |D| j=1 n jk p θ (ai|dj) |D| ∑ j=1 n jk p θ (ai|dj)ˆ s jk (10)</formula><p>where p θ (a i |d j ) is short for p θ (y j = a i |d j ), the probability that aspect phrase f j belongs to a i given the context document d j . We estimate u ik in this way because observations for aspect are rela- tively sufficient for a reliable estimation since ob- servations for an aspect are aggregated from those for all phrases belonging to that aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Optimization Algorithm</head><p>The optimization algorithm for the objective (see Eq. 7) is an EM-like two-stage iterative algorithm.</p><p>In E-step, we first calculate the posterior distri- bution p θ (a i |d j ), then project it onto the valid pos- terior distribution space Q. Given the parameters θ, the posterior distribution can be calculated by Eq. 11.</p><formula xml:id="formula_12">p θ (ai|dj) = p(ai) ∏ |d j | k=1 p(w d j ,k |ai) ∑ |A| r=1 p(ar) ∏ |d j | k=1 p(w d j ,k |ar)<label>(11)</label></formula><p>We use the above posterior distribution to update the sentiment parameter for each aspect by Eq. 10. The projected posterior distribution q is calculated by</p><formula xml:id="formula_13">q = argmin q∈Q KL(q(Y )||p θ (Y |D))<label>(12)</label></formula><p>For each instance, there are |A| * |P | constraints. However, we can prune a large number of useless constraints derived from limited observations. All constraints with d jk &gt; 1 can be pruned, due to the fact that the parameter u ik , ˆ s jk is within <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, and the difference can not be larger than 1. This optimization problem in Eq. 12 is easily solved via the dual form by the projected gradient algorithm ( <ref type="bibr" target="#b1">Boyd and Vandenberghe, 2004</ref>):</p><formula xml:id="formula_14">max λ≥0 ( − |A| ∑ i=1 |P | ∑ k=1 λ ik d jk − log |A| ∑ i=1 p θ (ai|dj )exp{− |P | ∑ k=1 λ ik |u ik − ˆ s jk |} − ϵ∥λ∥ )<label>(13)</label></formula><p>where ϵ controls the slack size for constraint. After solving the above optimization problem and ob- taining the optimal λ, we can calculate the pro- jected posterior distribution q by</p><formula xml:id="formula_15">q(yj = ai) = 1 Z p θ (ai|dj)exp{− |P | ∑ k=1 λ ik |u ik −ˆ s jk |} (14)</formula><p>where Z is the normalization factor. Note that sen- timent distribution consistency is actually modeled as instance-level constraint here, which makes it very efficient to solve. In M-step, the projected posteriors q(Y ) are then used to compute sufficient statistics and up- date the models parameters θ. Given the projected posteriors q(Y ), the parameters can be updated by Eq. 15,16.</p><formula xml:id="formula_16">p(ai) = 1 + ∑ |D| j=1 q(yj = ai) |A| + |D| (15) p(wt|ai) = 1 + ∑ |D| j=1 Ntiq(yj = ai) |V | + ∑ |V | m=1 ∑ |D| j=1 Nmjq(yj = ai)<label>(16)</label></formula><p>where N tj is the number of times that the word w t occurs in document d j .</p><p>The parameters are initialized randomly, and we repeat E-step and M-step until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Data Extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Context Extraction</head><p>In order to extract the context document d for each aspect phrase, we follow the approach in <ref type="bibr" target="#b17">Zhai et al. (2011a)</ref>. For each aspect phrase, we generate its context document by aggregating the surround- ing texts of the phrase in all reviews. The preced- ing and following t words of a phrase are taken as the context where we set t = 3 in this paper. Stop- words and other aspect phrases are removed. For example, the following review contains two aspect phrases, "screen" and "picture",</p><p>The LCD screen gives clear picture.</p><p>For "screen", the surrounding texts are {the, LCD, gives, clear, picture}. We remove stop- words "the", and the aspect term "picture", and the resultant context of "screen" in this review is context(screen) ={LCD, screen, gives, clear}.</p><p>Similarly, the context of "picture" in this review is context(picture) ={gives, clear}.</p><p>By aggregating the contexts of all the reviews that contain aspect phrase f j , we obtain the cor- responding context document d j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Sentiment Extraction</head><p>Since we use semi-structured reviews, we ob- tain the estimated sentiment distribution by sim- ply counting how many times each aspect phrase appears in Pros and Cons reviews for each prod- uct respectively. So for each aspect phrase f j , let n + jk denotes the times that f j appears in Pros of all reviews for product p k , and let n − jk denotes the times that f j appears in Cons of all reviews for product p k . So the total number of occurrence of a phrase is n jk = n + jk + n − jk . We have samples like (1,1,1,0,0) where 1 means a phrase occurs in Pros of a review, and 0 in Cons. Given a sequence of such observations, the sample mean is easily com- puted asˆsasˆ asˆs jk =</p><formula xml:id="formula_17">n + jk n + jk +n − jk .</formula><p>And the sample standard</p><formula xml:id="formula_18">deviation isˆσisˆ isˆσ jk = √ (1−ˆ s jk ) 2 * n + jk +(ˆ s jk ) 2 * n − jk n jk −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Preparation</head><p>The details of our review corpus are given in <ref type="table" target="#tab_2">Table 2</ref>. This corpus contains semi- structured customer reviews from four do- mains: Camera, Cellphone, Laptop, and MP3.</p><p>These reviews were crawled from the following web sites: www.amazon.cn, www.360buy.com, www.newegg.com.cn, and www.zol.com. The as- pect label of each aspect phrases is annotated by human curators.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camera</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Measures</head><p>We adapt three measures Purity, Entropy, and Rand Index for performance evaluation. These measures have been commonly used to evaluate clustering algorithms. Given a data set DS, suppose its gold-standard partition is G = {g 1 , ..., g j , ..., g k }, where k is the number of clusters. A clustering algo- rithm partitions DS into k disjoint subsets, say DS 1 , DS 2 , ..., DS k . Entropy: For each resulting cluster, we can mea- sure its entropy using Eq. 17, where P i (g j ) is the proportion of data points of class g j in DS i . The entropy of the entire clustering result is calculated by Eq. 18.</p><formula xml:id="formula_19">entropy(DSi) = − k ∑ j=1 Pi(gj)log2Pi(gj)<label>(17)</label></formula><formula xml:id="formula_20">entropy(DS) = k ∑ i=1 |DSi| |DS| entropy(DSi)<label>(18)</label></formula><p>Purity: Purity measures the extent that a cluster contains only data from one gold-standard parti- tion. The cluster purity is computed with Eq. 19. The total purity of the whole clustering result (all clusters) is computed with Eq. 20.  Positive) denotes number of pairs of elements in S that are in the same set in DS and in different sets in G. FN (False Negative) denotes number of pairs of elements that are in different sets in DS and in the same set in G. The Rand Index(RI) is computed with Eq. 21.</p><formula xml:id="formula_21">purity(DSi) = max j Pi(gj)<label>(19)</label></formula><formula xml:id="formula_22">purity(DS) = k ∑ i=1 |DSi| |DS| purity(DSi)<label>(20</label></formula><formula xml:id="formula_23">RI(DS) = T P + T N T P + T N + F P + F N<label>(21)</label></formula><p>3.3 Evaluation Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Comparison to unsupervised baselines</head><p>We compared our approach with several existing unsupervised methods. Some of the methods aug- mented unsupervised models by incorporating lex- ical similarity and other domain knowledge. All of them are context-based models. <ref type="bibr">2</ref> We list these models as follows.</p><p>• Kmeans: Kmeans is the most popular cluster- ing algorithm. Here we use the context distri- butional similarity (cosine similarity) as the similarity measure.</p><p>• L-EM: This is a state-of-the-art unsupervised method for clustering aspect phrases <ref type="bibr" target="#b17">(Zhai et al., 2011a</ref>). L-EM employed lexical knowl- edge to provide a better initialization for EM.</p><p>• LDA: LDA is a popular topic model( <ref type="bibr" target="#b0">Blei et al., 2003)</ref>. Given a set of documents, it out- puts groups of terms of different topics. In our case, each aspect phrase is processed as a term. <ref type="bibr">3</ref> Each sentence in a review is consid- ered as a document. Each aspect is consid- ered as a topic. In LDA, a term may belong to more than one topic/group, but we take the topic/group with the maximum probability.</p><p>• Constraint-LDA: Constraint-LDA ( <ref type="bibr" target="#b18">Zhai et al., 2011b</ref>) is a state-of-the-art LDA-based method that incorporates must-link and cannot-link constraints for this task. We set the damping factor λ = 0.3 and relaxation factor η = 0.9, as suggested in the original reference.</p><p>For all methods that depend on the random ini- tiation, we use the average results of 10 runs as the final result. For all LDA-based models, we choose α = 50/T , β = 0.1, and run 1000 iterations.</p><p>Experiment results are shown in <ref type="table" target="#tab_4">Table 3</ref>. We can see that our approach almost outperforms all unsupervised baseline methods by a large margin on all domains. In addition, we have the following observations:</p><p>• LDA and Kmeans perform poorly due to the fact that the two methods do not use any prior knowledge. It is also shown that only using the context distributional information is not sufficient for clustering aspect phrases.</p><p>• Constraint-LDA and L-EM that utilize prior knowledge perform better. We can see that Constraint-LDA outperforms LDA in terms of RI (Rand Index) on all domains. L-EM achieves the best results against the baselines. This demonstrates the effectiveness to incor- porate prior knowledge.</p><p>• SDC-MNB produces the optimal results among all models for clustering. Methods that use must-links and cannot-links may suf- fer from noisy links. For L-EM, we find that it is sensitive to noisy must-links. As L-EM assumes that must-link is transitive, several noisy must-links may totally misla- bel the softly annotated data. For Constraint- LDA, it is more robust than L-EM, because it doesn't assume the transitivity of must- link. However, it only promotes the RI (Rand Index) consistently by leveraging pair-wise prior knowledge, but sometimes it hurts the performance with respect to purity or en- tropy. Our method is consistently better on almost all domains, which shows the advan- tages of the proposed model.</p><p>• SDC-MNB is remarkably better than base- lines, particularly for the cellphone domain. We argue that this is because we have the largest number of reviews for each product in the cellphone domain. The larger dataset gives us more observations on each phrase, so that we obtain more reliable estimation of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Comparison to supervised baselines</head><p>We further compare our methods with two super- vised models. For each supervised model, we provide a proportion of manually labeled data for training, which is randomly selected from gold- standard annotations. However, we didn't use any labeled data for our approach.</p><p>• MNB: The labeled seeds are used to train a MNB classifier to classify all unlabeled as- pect phrases into different classes.</p><p>• L-Kmeans: In L-Kmeans, the clusters of the labeled seeds are fixed at the initiation and remain unchanged during iteration.  We experiment with several settings: taking 5%, 10% and 15% of the manually labeled aspect phrases for training, and the remainder as unla- beled data. Experiment results is shown in <ref type="table" target="#tab_6">Table  4</ref> (the results are averaged over 4 domains). We can see that our unsupervised approach is roughly as good as the supervised MNB with 10% labeled data. Our unsupervised approach is also slightly better than L-Kmeans with 15% labeled data. This result further demonstrates the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Purity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Influence of parameters</head><p>We vary the confidence level from 90% to 99.9% to see how it impacts on the performance of SDC- MNB. The results are presented in <ref type="figure" target="#fig_4">Fig. 5</ref> (the re- sults are averaged over 4 domains). We can see that the performance of clustering is fairly stable when changing the confidence level, which im- plies the robustness of our model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Analysis of SDC-constraint</head><p>As mentioned in Section 2.2, SDC-constraint is dependent on the number of observations. More observations we get, more informative the con- straint is, which means the constraint is tighter and d jk (see Eq.4) is smaller. For all k, we count how many d jk is less than 0.2 (and 1) on average for each aspect phrase f j . d jk is calculated with a confidence level of 99%. The statistics of con- straints is given in <ref type="table" target="#tab_8">Table 5</ref>. We can see that the cellphone domain has the most informative and largest constraint set, that may explain why SDC- MNB achieves the largest purity gain(over L-EM) in cellphone domain.  </p><formula xml:id="formula_24">#(d jk &lt; 0.2) #(0.2 &lt; d jk &lt; 1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work is related to two important research topics: aspect-level sentiment analysis, and constraint-driven learning. For aspect-level senti- ment analysis, aspect extraction and clustering are key tasks. For constraint-driven learning, a variety of frameworks and models for sentiment analysis have been studied extensively.</p><p>There have been many studies on clustering aspect-related phrases. Most existing studies are based on context information. Some works also encoded lexical similarity and synonyms as prior knowledge. <ref type="bibr" target="#b3">Carenini et al. (2005)</ref> proposed a method that was based on several similarity met- rics involving string similarity, synonyms, and lex- ical distances defined with WordNet. <ref type="bibr" target="#b9">Guo et al. (2009)</ref> proposed a multi-level latent semantic as- sociation model to capture expression-level and context-level topic structure. <ref type="bibr" target="#b16">Zhai et al. (2010)</ref> proposed an EM-based semi-supervised learning method to group aspect expressions into user- specified aspects. They employed lexical knowl- edge to provide a better initialization for EM. In <ref type="bibr" target="#b17">Zhai et al. (2011a)</ref>, an EM-based unsupervised version was proposed. The so-called L-EM model first generated softly labeled data by grouping fea- ture expressions that share words in common, and then merged the groups by lexical similarity. <ref type="bibr" target="#b18">Zhai et al. (2011b)</ref> proposed a LDA-based method that incorporates must-link and cannot-link con- straints.</p><p>Another line of work aimed to extract and clus- ter aspect words simultaneously using topic mod- eling. <ref type="bibr" target="#b15">Titov and McDonald (2008)</ref> proposed the multi-grain topic models to discover global and local aspects. <ref type="bibr" target="#b2">Branavan et al. (2008)</ref> proposed a method which first clustered the key-phrases in Pros and Cons into some aspect categories based on distributional similarity, then built a topic model modeling the topics or aspects. <ref type="bibr" target="#b19">Zhao et al. (2010)</ref> proposed the MaxEnt-LDA (a Maximum Entropy and LDA combination) hybrid model to jointly discover both aspect words and aspect- specific opinion words, which can leverage syn- tactic features to separate aspects and sentiment words. Mukherjee and Liu (2012) proposed a semi-supervised topic model which used user- provided seeds to discover aspects. <ref type="bibr" target="#b5">Chen et al. (2013)</ref> proposed a knowledge-based topic model to incorporate must-link and cannot-link informa- tion. Their model can adjust topic numbers auto- matically by leveraging cannot-link.</p><p>Our work is also related to general constraint- driven(or knowledge-driven) learning models. Several general frameworks have been proposed to fully utilize various prior knowledge in learning. Constraint-driven learning ( <ref type="bibr" target="#b4">Chang et al., 2008</ref>) (CODL) is an EM-like algorithm that incorpo- rates per-instance constraints into semi-supervised learning. Posterior regularization ( <ref type="bibr" target="#b8">Graca et al., 2007</ref>) (PR) is a modified EM algorithm in which the E-step is replaced by the projection of the model posterior distribution onto the set of dis- tributions that satisfy auxiliary expectation con- straints. Generalized expectation criteria <ref type="bibr" target="#b6">(Druck et al., 2008</ref>) (GE) is a framework for incorporating preferences about model expectations into param- eter estimation objective functions. <ref type="bibr" target="#b11">Liang et al. (2009)</ref> developed a Bayesian decision-theoretic framework to learn an exponential family model using general measurements on the unlabeled data. In this paper, we model our problem in the frame- work of posterior regularization.</p><p>Many works promoted the performance of sen- timent analysis by incorporating prior knowledge as weak supervision. <ref type="bibr" target="#b10">Li and Zhang (2009)</ref> in- jected lexical prior knowledge to non-negative ma- trix tri-factorization. <ref type="bibr" target="#b13">Shen and Li (2011)</ref> further extended the matrix factorization framework to model dual supervision from both document and word labels. Vikas Sindhwani (2008) proposed a general framework for incorporating lexical infor- mation as well as unlabeled data within standard regularized least squares for sentiment prediction tasks. <ref type="bibr" target="#b7">Fang (2013)</ref>proposed a structural learning model with a handful set of aspect signature terms that are encoded as weak supervision to extract la- tent sentiment explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Aspect finding and clustering is an important task for aspect-level sentiment analysis. In order to cluster aspect-related phrases, this paper has ex- plored a novel concept, sentiment distribution con- sistency. We formalize the concept as soft con- straint, integrate the constraint with a context- based probabilistic model, and solve the problem in the posterior regularization framework. The proposed model is also designed to be robust with both sufficient and insufficient observations. Ex- periments show that our approach outperforms state-of-the-art baselines consistently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A semi-structured Review.</figDesc><graphic url="image-1.png" coords="1,309.43,458.37,213.92,65.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The sentiment distribution of aspect "battery" and its related-phrases on nokia 5130 with a large amount of reviews.</figDesc><graphic url="image-2.png" coords="2,79.87,254.86,202.52,69.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sentiment distribution of an aspect, and observations on aspect phrases.</figDesc><graphic url="image-4.png" coords="3,310.73,409.24,211.33,73.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Influence of the confidence level on SDC-MNB.</figDesc><graphic url="image-5.png" coords="8,326.28,188.34,180.23,90.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Notations</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Statistics of the review corpus. # denotes 
the size. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Comparison to unsupervised baselines. (P is short for purity, E for entropy, and RI for random index.)</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison to supervised baselines. 
MNB-5% means MNB with 5% labeled data. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : Constraint statistics on different domains.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> In our method, we collect context document for each aspect phrase. This process is conducted for L-EM and Kmeans. But for LDA and Constraint-LDA, we take each sentence of reviews as a document. This setting for the LDA baselines is adapted from previous work. 3 Each aspect phrase is pre-processed as a single word (e.g., &quot;battery life&quot; is treated as battery-life). Other words are normally used in LDA.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convex Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning document-level semantic properties from free-text annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting knowledge from evaluative text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ed</forename><surname>Zwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Knowledge Capture, K-CAP &apos;05</title>
		<meeting>the 3rd International Conference on Knowledge Capture, K-CAP &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning and inference with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
		<meeting>the 23rd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1513" to="1518" />
		</imprint>
	</monogr>
	<note>AAAI&apos;08</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting domain knowledge in aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meichun</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mal</forename><surname>Castellanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riddhiman</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1655" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning from labeled features using generalized expectation criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Druck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08</title>
		<meeting>the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring weakly supervised latent sentiment explanations for aspect-level review analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<editor>Qi He, Arun Iyengar, Wolfgang Nejdl, Jian Pei, and Rajeev Rastogi</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1057" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Expectation maximization and posterior constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">V</forename><surname>Graca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lf</forename><surname>Inesc-Id</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename><forename type="middle">V</forename><surname>Graa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Inesc-Id</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Product feature categorization with multilevel latent semantic association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijia</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhili</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1087" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A nonnegative matrix tri-factorization approach to sentiment classification with lexical prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
	<note>ACL &apos;09</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from measurements in exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A non-negative matrix factorization based approach for active dual supervision from document and word labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="949" to="958" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document-word co-regularization for semisupervised sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Melville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1025" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on World Wide Web, WWW &apos;08</title>
		<meeting>the 17th International Conference on World Wide Web, WWW &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grouping product features using semi-supervised learning with soft-constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifa</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics, COLING &apos;10</title>
		<meeting>the 23rd International Conference on Computational Linguistics, COLING &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1272" to="1280" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering product features for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifa</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM &apos;11</title>
		<meeting>the Fourth ACM International Conference on Web Search and Data Mining, WSDM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constrained lda for grouping product features in opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwu</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peifa</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining-Volume Part I, PAKDD&apos;11</title>
		<meeting>the 15th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining-Volume Part I, PAKDD&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
