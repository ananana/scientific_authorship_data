<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:08+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human-in-the-Loop Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human-in-the-Loop Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2337" to="2342"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb &quot;pose,&quot; it could generate the question What would pose something? with candidate answers this paper and a parser. Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs. Experiments show that crowd workers can answer these questions quickly, accurately and cheaply. Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with a gain of 1.7 F1 on the 10% of sentences whose parses are changed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The size of labelled datasets has long been recog- nized as a bottleneck in the performance of nat- ural language processing systems <ref type="bibr" target="#b6">(Marcus et al., 1993;</ref><ref type="bibr" target="#b7">Petrov and McDonald, 2012)</ref>. Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even rela- tively large datasets, such as the Penn Treebank, are much smaller than required-as demonstrated by improvements from semi-supervised learning <ref type="bibr" target="#b9">(Søgaard and Rishøj, 2010;</ref><ref type="bibr" target="#b11">Weiss et al., 2015)</ref>.</p><p>We take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where Temple also said Sea Containers' plan raises numer- ous legal, regulatory, financial and fairness issues, but didn't elaborate. Q:</p><p>What didn't elaborate? <ref type="bibr">[1]</ref> **** Temple <ref type="bibr">[2]</ref> * Sea Containers' plan <ref type="bibr">[3]</ref> None of the above. non-experts improve parsing accuracy by answering questions automatically generated from the parser's output. We develop the approach for CCG parsing, leveraging the link between CCG syntax and seman- tics to convert uncertain attachment decisions into natural language questions. The answers are used as soft constraints when re-parsing the sentence.</p><p>Previous work used crowdsourcing for less struc- tured tasks such as named entity recognition <ref type="bibr" target="#b13">(Werling et al., 2015)</ref> and prepositional phrase attach- ment ( <ref type="bibr" target="#b4">Jha et al., 2010)</ref>. Our work is most related to that of <ref type="bibr" target="#b1">Duan et al. (2016)</ref>, which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowd- sourced judgments on two out-of-domain datasets. <ref type="bibr" target="#b0">Choe and McClosky (2015)</ref> improve a parser by cre- ating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly. Instead of using paraphrases, we build on the approach of QA-SRL ( <ref type="bibr" target="#b2">He et al., 2015)</ref>, which shows that untrained crowd workers can annotate predicate-argument structures by writing question-answer pairs.</p><p>Our experiments for newswire and biomedical text demonstrate improvements to parsing accuracy of 1.7 F1 on the sentences changed by re-parsing, while asking only less than 2 questions per sentence. The annotations we collected 1 are a representation- independent resource that could be used to develop new models or human-in-the-loop algorithms for related tasks, including semantic role labeling and syntactic parsing with other formalisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mapping CCG Parses to Queries</head><p>Our annotation task consists of multiple-choice what-questions that admit multiple answers. To gen- erate them, we produce question-answer (QA) pairs from each parse in the 100-best scored output of a CCG parser and aggregate the results together.</p><p>We designed the approach to generate queries with high question confidence-questions should be simple and grammatical, so annotators are more likely to answer them correctly-and high answer uncertainty-the parser should be uncertain about the answers, so there is potential for improvement.</p><p>Our questions only apply to core arguments of verbs where the argument phrase is an NP, which account for many of the parser's mistakes. Preposi- tional phrase attachment mistakes are also a large source of errors-we tried several approaches to generate questions for these, but the greater ambi- guity and inconsistency among both annotators and the gold parses made it difficult to extract meaning- ful signal from the crowd.</p><p>Generating Question-Answer Pairs <ref type="figure">Figure 1</ref> shows how we generate QA pairs. Each QA pair cor- responds to a dependency such that if the answer is correct, it indicates that the dependency is in the cor- rect parse. We determine a verb's set of arguments by the CCG supertag assigned to it in the parse (see <ref type="bibr" target="#b10">Steedman (2000)</ref> for an introduction to CCG). For example, in <ref type="figure">Figure 1</ref> the word put takes the category ((S\NP)/PP)/NP (not shown), indicating that it has a subject, a prepositional phrase argument, and an object. CCG parsing assigns dependencies to each argument position, even when the arguments are re- ordered (as with put → pizza) or span long distances (as with eat → I).  the pizza on →table What did something put something on? the table <ref type="figure">Figure 1</ref>: From a labeled dependency graph, we extract phrases corresponding to every argument of every verb using sim- ple heuristics. We then create questions about dependencies, adding a would modal to untensed verbs and placing arguments to the left or right of the verb based on its CCG category. We only generate QA pairs for subj, obj, and pobj dependencies.</p><p>To identify multiple answer options, we create QA pairs from all parses in the 100-best list and pool equivalent questions with different answers. See <ref type="table">Table 2</ref> for example queries.</p><p>To reduce the chance of parse errors causing non- sensical questions (for example, What did the pizza put something on?), we replace all noun phrases with something and delete unnecessary prepositional phrases. The exception to this is with copular predi- cates, where we include the span of the argument in the question (see Example 4 in <ref type="table">Table 2</ref>).</p><p>Grouping QA Pairs into Queries After generat- ing QA pairs for every parse in the 100-best output of the parser, we pool the QA pairs by the head of the dependency used to generate them, its CCG cat- egory, and their question strings. We also compute marginalized scores for each question and answer phrase by summing over the scores of all the parses that generated them. Each pool becomes a query, and for each unique dependency used to generate QA pairs in that pool, we add a candidate answer to the query by choosing the answer phrase that has the highest marginalized score for that dependency. For example, if some parses generated the answer phrase pizza for the dependency eat → pizza, but most of the high-scoring parses generated the answer phrase the pizza, then only the pizza appears as an answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Votes Answers (1) Structural Dynamics Research Corp. . . . said it introduced new technology in mechanical design automation that will improve mechanical engineering productivity.</p><p>What will improve something? 0 Structural Dynamics Research Corp 5 new technology 0 mechanical design automation (2) He said disciplinary proceedings are confidential and declined to comment on whether any are being held against Mr. Trudeau.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What would comment?</head><p>5 he 0 disciplinary proceedings (3) To avoid these costs, and a possible default, immediate action is imperative.</p><p>What would something avoid? 4 these costs 3 a possible default (4) The price is a new high for California Cabernet Sauvignon, but it is not the highest.</p><p>What is not the highest?</p><p>2 the price 3 it (5) Kalipharma is a New Jersey-based pharmaceuticals concern that sells products under the Purepac label. What sells something?</p><p>5 Kalipharma 0 a New Jersey-based pharma- ceuticals concern (6) Further, he said, the company doesn't have the capital needed to build the business over the next year or two.</p><p>What would build something?</p><p>4 the company 1 the capital (7) Timex had requested duty-free treatment for many types of watches, covered by 58 different U.S. tariff classifications.</p><p>What <ref type="table">Table 2</ref>: Example annotations from the CCGbank development set. Answers that agree with the gold parse are in bold. The answer choice None of the above was present for all examples, but we only show it when it was chosen by annotators.</p><note type="other">would be covered? 0 Timex 0 duty-free treatment 2 many types of watches 3 watches (8) You either believe Seymour can do it again or you do n't . What does? 3 you 0 Seymour 2 None of the above</note><p>From the resulting queries, we filter out questions and answers whose marginalized scores are below a certain threshold and queries that only have one an- swer choice. This way we only ask confident ques- tions with uncertain answer lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Crowdsourcing</head><p>We collected data on the crowdsourcing platform CrowdFlower. 2 Annotators were shown a sentence, a question, and a list of answer choices. Annota- tors could choose multiple answers, which was use- ful in case of coordination (see Example 3 in Ta- ble 2). There was also a None of the above option for when no answer was applicable or the question was nonsensical.</p><p>We instructed annotators to only choose options that explicitly and directly answer the question, to encourage their answers to closely mirror syn- tax. We also instructed them to ignore who/what and someone/something distinctions and overlook mistakes where the question was missing a nega- tion. The instructions included 6 example queries 2 www.crowdflower.com with answers and explanations. We used Crowd- Flower's quality control mechanism, displaying pre- annotated queries 20% of the time and requiring an- notators to maintain high accuracy. <ref type="table" target="#tab_3">Table 3</ref> shows how many sen- tences we asked questions for and the total number of queries annotated. We collected annotations for the development and test set for CCGbank <ref type="bibr" target="#b3">(Hockenmaier and Steedman, 2007)</ref> as in-domain data and the test set of the Bioinfer corpus ( <ref type="bibr" target="#b8">Pyysalo et al., 2007)</ref> as out-of-domain. The CCGbank devel- opment set was used for building question genera- tion heuristics and setting hyperparameters for re- parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Statistics</head><p>5 annotators answered each query; on CCGbank we required 85% accuracy on test questions and on Bioinfer we set the threshold at 80% because of the difficulty of the sentences. <ref type="table" target="#tab_4">Table 4</ref> shows inter-annotator agreement. Annotators unanimously chose the same set of answers for over 40% of the queries; an absolute majority is achieved for over 90% of the queries.    Qualitative Analysis <ref type="table">Table 2</ref> shows example queries from the CCGbank development set. Exam- ples 1 and 2 show that workers could annotate long- range dependencies and scoping decisions, which are challenging for existing parsers. However, there are some cases where annota- tors disagree with the gold syntax, mostly involv- ing semantic phenomena which are not reflected in the syntactic structure. Many cases involve co- reference, where annotators often prefer a proper noun referent over a pronoun or indefinite (see Ex- amples 4 and 5), even if it is not the syntactic ar- gument of the verb. Example 6 shows a complex control structure, where the gold CCGbank syntax does not recover the true agent of build. CCGbank also does not distinguish between subject and object control. For these cases, our method could be used to extend existing treebanks. Another common error case involved partitives and related constructions, where the correct attachment is subtle-as reflected by the annotators' split decision in Example 7.</p><p>Question Quality <ref type="table">Table 5</ref> shows the percentage of questions that are answered with None of the above (written N/A below) by at most k annotators. On all domains, about 80% of the queries are considered answerable by all 5 annotators. To have a better understanding of the quality of automatically gen- erated questions, we did a manual analysis on 50 questions for sentences from the CCGbank devel- opment set that are marked N/A by more than one annotator. Among the 50 questions, 31 of them are either generated from an incorrect supertag or unan- swerable given the candidates. So the N/A answer k-N/A CCG-Dev CCG-Test Bioinfer 0 77.6% 81.6% 79.3% ≤ 1 89.6% 92.6% 89.1% ≤ 2 93.8% 96.1% 92.8% <ref type="table">Table 5</ref>: The percentage of queries with at most k annotators choosing the None of the above (N/A) option.</p><p>can provide useful signal that the parses that gen- erated the question are likely incorrect. Common mistakes in question generation include: bad argu- ment span in a copula question (4 questions), bad modality/negation (3 questions), and missing argu- ment or particle (5 questions). Example 8 in <ref type="table">Table 2</ref> shows an example of a nonsensical question. While the parses agreed with the gold category S\NP, the question they generated omitted the negation and the verb phrase that was elided in the original sentence. In this case, 3 out of 5 annotators were able to an- swer with the correct dependency, but such mistakes can make re-parsing more challenging.</p><p>Cost and Speed We paid 6 cents for each answer. With 5 judgments per query, 20% test questions, and CrowdFlower's 20% service fee, the average cost per query was about 46 cents. On average, we col- lected about 1000 judgments per hour, so we were able to annotate all the queries generated from the CCGbank test set within 15 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Re-Parsing with QA Annotation</head><p>To improve the output of the parser, we re-parse each sentence with an augmented scoring function that penalizes parses for disagreeing with annota- tors' choices. If q is a question, a is an answer to q, d is the dependency that produced the QA pair q, a, and v(a) annotators chose a, we add re-parsing con- straints as follows:</p><p>• If v(None of the above) ≥ T + , penalize parses that agree with q's supertag on the verb by w t • If v(a) ≤ T − , penalize parses containing d by w − • If v(a) ≥ T + , penalize parses that do not contain d by w + where T + , T − , w t , w − , and w + are hyperparame- ters. We incorporate these penalties into the parsing model during decoding. By using soft constraints, we mitigate the risk of incorrect annotations wors- ening a high-confidence parse.  Some errors are predictable: for example, if a is a non-possessive pronoun and is closer to the verb than its referent a , annotators often choose a when a is correct (See Example 4 in <ref type="table">Table 2</ref>). If a is a sub- span of another answer a and their votes differ by at most one (See Example 7 in <ref type="table">Table 2</ref>), it is unlikely that both a and a are correct. In these cases we use disjunctive constraints, where the parse needs to have at least one of the desired dependencies.</p><p>Experimental Setup We use <ref type="bibr" target="#b5">Lewis et al. (2016)</ref>'s state-of-the-art CCG parser for our baseline. We chose the following set of hyperparameters based on performance on development data (CCG-Dev):</p><formula xml:id="formula_0">w + = 2.0, w − = 1.5, w t = 1.0, T + = 3, T − = 0.</formula><p>In the Bioinfer dataset, we found during develop- ment that the pronoun/subspan heuristics were not as useful, so we did not use them in re-parsing. <ref type="table" target="#tab_6">Table 6</ref> shows our end-to-end parsing re- sults. The larger improvement on out-of-domain sentences shows the potential for using our method for domain adaptation. There is a much smaller improvement on test data than development data, which may be related to the lower annotator agree- ment reported in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>There was much larger improvement (1.7 F1) on the subset of sentences that are changed after re- parsing, as shown in <ref type="table">Table 7</ref>. This suggests that our method could be effective for semi-supervised learn- ing or re-training parsers. Overall improvements on CCGbank are modest, due to only modifying 10% of sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>We introduced a human-in-the-loop framework for automatically correcting certain parsing mistakes. Our method identifies attachment uncertainty for core arguments of verbs and automatically generates Data L16 HITL Pct. CCG-Dev 83.9 87.1 12% CCG-Test 84.2 85.9 10% <ref type="table">Table 7</ref>: Improvements of CCG parsing accuracy on changed sentences for in-domain data. We achieved significant improve- ment over the 10%-12% (Pct.) sentences that were changed by re-parsing.</p><p>questions that can be answered by untrained annota- tors. These annotations improve performance, par- ticularly on out-of-domain data, demonstrating for the first time that untrained annotators can improve state-of-the-art parsers. Sentences modified by our framework show sub- stantial improvements in accuracy, but only 10% of sentences are changed, limiting the effect on overall accuracy. This work is a first step towards a com- plete approach to human-in-the-loop parsing.</p><p>Future work will explore the possibility of asking questions about other types of parsing uncertainties, such as nominal and adjectival argument structure, and a more thorough treatment of prepositional- phrase attachment, including distinctions between arguments and adjuncts. We hope to scale these methods to large unlabelled corpora or other lan- guages, to provide data for re-training parsers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>I</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>An automatically generated query from CCGbank. 

4 out of 5 annotators correctly answered Temple, providing a 

signal that can be used to improve parse predictions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>want to eat the pizza you put on the table</head><label></label><figDesc></figDesc><table>subj 
xcomp 

subj 
obj 
obj 

subj 
prep 
pobj 

subj Verb 
obj 
prep 
xcomp 

you put the pizza on the table 

I 
want 
to eat the pizza 

I 
eat the pizza 

Dependency Question 
Answer 
want →I 
What wants to eat something? 
I 
eat→I 
What would eat something? 
I 
eat→pizza What would something eat? 
the pizza 
put →you 
What put something? 
you 
put →pizza What did something put? 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Sentence coverage, number of queries annotated, and average number of queries per sentence (Q/S).</head><label>3</label><figDesc></figDesc><table>k-Agreed CCG-Dev CCG-Test Bioinfer 
5 
48.0% 
40.2% 
47.7% 
≥ 4 
76.6% 
68.0% 
75.0% 
≥ 3 
94.9% 
91.5% 
94.0% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>The percentage of queries with at least k annotators agreeing on the exact same set of answers.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : CCG parsing accuracy with human in the loop (HITL) versus the state-of-the-art baseline (L16) in terms of labeled F1</head><label>6</label><figDesc></figDesc><table>score. For both in-domain and out-domain, we have a modest 

gain over the entire corpus. 

</table></figure>

			<note place="foot" n="1"> Our code and data are available at https://github. com/luheng/hitl_parsing.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the NSF (IIS-1252835, IIS-1562364), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We are grateful to Chloé Kiddon for help-ful comments on the paper, and Kenton Lee for help with the CCG parser. We would also like to thank our workers on Crowdflower for their annotation and the anonymous reviewers for their valuable feed-back.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Parsing paraphrases with joint inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kook</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generating disambiguating paraphrases for struc2341 turally ambiguous sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjuan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Linguistic Annotation Workshop</title>
		<meeting>the 10th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Question-answer driven semantic role labeling: Using natural language to annotate natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Corpus creation for new genres: A crowdsourced approach to pp attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukund</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Thadani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</title>
		<meeting>the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon&apos;s Mechanical Turk</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lstm ccg parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<title level="m">Overview of the 2012 Shared Task on Parsing the Web. Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bioinfer: a corpus for information extraction in the biomedical domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Heimonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jari</forename><surname>Björne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Boberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouni</forename><surname>Järvinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semisupervised dependency parsing using generalized tritraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rishøj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The syntactic process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On-the-job learning with bayesian decision theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keenon</forename><surname>Werling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><forename type="middle">Tejasvi</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
