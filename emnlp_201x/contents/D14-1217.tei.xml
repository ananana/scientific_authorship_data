<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Spatial Knowledge for Text to 3D Scene Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Spatial Knowledge for Text to 3D Scene Generation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2028" to="2038"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We address the grounding of natural language to concrete spatial constraints, and inference of implicit pragmatics in 3D environments. We apply our approach to the task of text-to-3D scene generation. We present a representation for common sense spatial knowledge and an approach to extract it from 3D scene data. In text-to-3D scene generation, a user provides as input natural language text from which we extract explicit constraints on the objects that should appear in the scene. The main innovation of this work is to show how to augment these explicit constraints with learned spatial knowledge to infer missing objects and likely layouts for the objects in the scene. We demonstrate that spatial knowledge is useful for interpreting natural language and show examples of learned knowledge and generated 3D scenes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To understand language, we need an understanding of the world around us. Language describes the world and provides symbols with which we rep- resent meaning. Still, much knowledge about the world is so obvious that it is rarely explicitly stated. It is uncommon for people to state that chairs are usually on the floor and upright, and that you usu- ally eat a cake from a plate on a table. Knowledge of such common facts provides the context within which people communicate with language. There- fore, to create practical systems that can interact with the world and communicate with people, we need to leverage such knowledge to interpret lan- guage in context.</p><p>Spatial knowledge is an important aspect of the world and is often not expressed explicitly in nat- ural language. This is one of the biggest chal- <ref type="figure">Figure 1</ref>: Generated scene for "There is a room with a chair and a computer." Note that the system infers the presence of a desk and that the computer should be supported by the desk. lenges in grounding language and enabling natu- ral communication between people and intelligent systems. For instance, if we want a robot that can follow commands such as "bring me a piece of cake", it needs to be imparted with an understand- ing of likely locations for the cake in the kitchen and that the cake should be placed on a plate. The pioneering WordsEye system <ref type="bibr" target="#b2">(Coyne and Sproat, 2001</ref>) addressed the text-to-3D task and is an inspiration for our work. However, there are many remaining gaps in this broad area. Among them, there is a need for research into learning spa- tial knowledge representations from data, and for connecting them to language. Representing un- stated facts is a challenging problem unaddressed by prior work and the focus of our contribution. This problem is a counterpart to the image descrip- tion problem <ref type="bibr">(Kulkarni et al., 2011;</ref><ref type="bibr" target="#b17">Mitchell et al., 2012;</ref><ref type="bibr" target="#b5">Elliott and Keller, 2013)</ref>, which has so far remained largely unexplored by the community.</p><p>We present a representation for this form of spa- tial knowledge that we learn from 3D scene data and connect to natural language. We will show how this representation is useful for grounding language and for inferring unstated facts, i.e., the pragmatics of language describing physical envi- ronments. We demonstrate the use of this repre- sentation in the task of text-to-3D scene genera- <ref type="table">Table   Plate</ref> Cake <ref type="bibr">color(red)</ref> "There is a room with a  <ref type="figure">Figure 2</ref>: Overview of our spatial knowledge representation for text-to-3D scene generation. We parse input text into a scene template and infer implicit spatial constraints from learned priors. We then ground the template to a geometric scene, choose 3D models to instantiate and arrange them into a final 3D scene. tion, where the input is natural language and the desired output is a 3D scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Room</head><p>We focus on the text-to-3D task to demonstrate that extracting spatial knowledge is possible and beneficial in a challenging scenario: one requiring the grounding of natural language and inference of rarely mentioned implicit pragmatics based on spa- tial facts. <ref type="figure">Figure 1</ref> illustrates some of the inference challenges in generating 3D scenes from natural language: the desk was not explicitly mentioned in the input, but we need to infer that the computer is likely to be supported by a desk rather than di- rectly placed on the floor. Without this inference, the user would need to be much more verbose with text such as "There is a room with a chair, a com- puter, and a desk. The computer is on the desk, and the desk is on the floor. The chair is on the floor."</p><p>Contributions We present a spatial knowledge representation that can be learned from 3D scenes and captures the statistics of what objects occur in different scene types, and their spatial posi- tions relative to each other. In addition, we model spatial relations (left, on top of, etc.) and learn a mapping between language and the geometric con- straints that spatial terms imply. We show that using our learned spatial knowledge representa- tion, we can infer implicit constraints, and generate plausible scenes from concise natural text input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition and Overview</head><p>We define text-to-scene generation as the task of taking text that describes a scene as input, and gen- erating a plausible 3D scene described by that text as output. More concretely, based on the input text, we select objects from a dataset of 3D models and arrange them to generate output scenes.</p><p>The main challenge we address is in transform- ing a scene template into a physically realizable 3D scene. For this to be possible, the system must be able to automatically specify the objects present and their position and orientation with respect to each other as constraints in 3D space. To do so, we need to have a representation of scenes ( §3). We need good priors over the arrangements of objects in scenes ( §4) and we need to be able to ground textual relations into spatial constraints ( §5). We break down our task as follows (see <ref type="figure">Figure 2</ref>): Template Parsing ( §6.1): Parse the textual de- scription of a scene into a set of constraints on the objects present and spatial relations between them. Inference ( §6.2): Expand this set of constraints by accounting for implicit constraints not specified in the text using learned spatial priors. Grounding ( §6.3): Given the constraints and pri- ors on the spatial relations of objects, transform the scene template into a geometric 3D scene with a set of objects to be instantiated. Scene Layout ( §6.4): Arrange the objects and op- timize their placement based on priors on the rel- ative positions of objects and explicitly provided spatial constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scene Representation</head><p>To capture the objects present and their arrange- ment, we represent scenes as graphs where nodes are objects in the scene, and edges are semantic re- lationships between the objects.</p><p>We represent the semantics of a scene using a scene template and the geometric properties using a geometric scene. One critical property which is captured by our scene graph representation is that of a static support hierarchy, i.e., the order in which bigger objects physically support smaller ones: the floor supports tables, which support plates, which can support cakes. Static support and other con- straints on relationships between objects are rep- resented as edges in the scene graph. 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9 1 . 0 p ( S c e n e | K n i f e + T a b l e ) K i t c h e n Co u n t e r Di n i n g T a b l e L i v i n g Ro o m K i t c h e n <ref type="figure">Figure 3</ref>: Probabilities of different scene types given the presence of "knife" and "table".</p><p>Figure 4: Probabilities of support for some most likely child object categories given four different parent object categories, from top left clockwise: dining table, bookcase, room, desk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scene Template</head><p>A scene template T = (O, C, C s ) consists of a set of object descriptions O = {o 1 , . . . , o n } and constraints C = {c 1 , . . . , c k } on the relationships between the objects. A scene template also has a scene type C s . Each object o i , has properties associated with it such as category label, basic attributes such as color and material, and number of occurrences in the scene. For constraints, we focus on spatial re- lations between objects, expressed as predicates of the form supported_by(o i , o j ) or left(o i , o j ) where o i and o j are recognized objects. 1 <ref type="figure">Figure 2a</ref> shows an example scene template. From the scene tem- plate we instantiate concrete geometric 3D scenes. To infer implicit constraints on objects and spa- tial support we learn priors on object occurrences in 3D scenes ( §4.1) and their support hierarchies ( §4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Geometric Scene</head><p>We refer to the concrete geometric representation of a scene as a "geometric scene". It consists of a set of 3D model instances -one for each ob- ject -that capture the appearance of the object. A transformation matrix that represents the position, orientation, and scaling of the object in a scene is also necessary to exactly position the object. We generate a geometric scene from a scene template by selecting appropriate models from a 3D model database and determining transformations that op- 1 Our representation can also support other relationships such as larger(oi, oj).</p><p>timize their layout to satisfy spatial constraints. To inform geometric arrangement we learn priors on the types of support surfaces ( §4.2) and the relative positions of objects ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Spatial Knowledge</head><p>Our model of spatial knowledge relies on the idea of abstract scene types describing the occurrence and arrangement of different categories of objects within scenes of that type. For example, kitchens typically contain kitchen counters on which plates and cups are likely to be found. The type of scene and category of objects condition the spatial rela- tionships that can exist in a scene.</p><p>We learn spatial knowledge from 3D scene data, basing our approach on that of Fisher et al. <ref type="formula">(2012)</ref> and using their dataset of 133 small indoor scenes created with 1723 Trimble 3D Warehouse mod- els ( <ref type="bibr" target="#b6">Fisher et al., 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Occurrence Priors</head><p>We learn priors for object occurrence in different scene types (such as kitchens, offices, bedrooms).</p><formula xml:id="formula_0">P occ (C o |C s ) = count(C o in C s ) count(C s )</formula><p>This allows us to evaluate the probability of dif- ferent scene types given lists of object occurring in them (see <ref type="figure">Figure 3</ref>). For example given input of the form "there is a knife on the table" then we are likely to generate a scene with a dining table and other related objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Support Hierarchy Priors</head><p>We observe the static support relations of objects in existing scenes to establish a prior over what ob- jects go on top of what other objects. As an exam- ple, by observing plates and forks on tables most of the time, we establish that tables are more likely to support plates and forks than chairs. We esti- mate the probability of a parent category C p sup- porting a given child category C c as a simple con- ditional probability based on normalized observa- tion counts. 2</p><formula xml:id="formula_1">P support (C p |C c ) = count(C c on C p ) count(C c )</formula><p>We show a few of the priors we learn in <ref type="figure">Figure 4</ref> as likelihoods of categories of child objects being statically supported by a parent category object. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Support Surface Priors</head><p>To identify which surfaces on parent objects sup- port child objects, we first segment parent models into planar surfaces using a simple region-growing algorithm based on ( <ref type="bibr" target="#b9">Kalvin and Taylor, 1996)</ref>. We characterize support surfaces by the direction of their normal vector, limited to the six canonical directions: up, down, left, right, front, back. We learn a probability of supporting surface normal di- rection S n given child object category C c . For ex- ample, posters are typically found on walls so their support normal vectors are in the horizontal di- rections. Any unobserved child categories are as- sumed to have P surf (S n = up|C c ) = 1 since most things rest on a horizontal surface (e.g., floor).</p><formula xml:id="formula_2">P surf (S n |C c ) = count(C c on surface with S n ) count(C c )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relative Position Priors</head><p>We model the relative positions of objects based on their object categories and current scene type: i.e., the relative position of an object of category C obj is with respect to another object of category C ref and for a scene type C s . We condition on the relationship R between the two objects, whether they are siblings (R = Sibling) or child-parent (R = ChildP arent).</p><formula xml:id="formula_3">P relpos (x, y, θ|C obj , C ref , C s , R)</formula><p>When positioning objects, we restrict the search space to points on the selected support surface.</p><p>The position x, y is the centroid of the target ob- ject projected onto the support surface in the se- mantic frame of the reference object. The θ is the angle between the front of the two objects. We rep- resent these relative position and orientation pri- ors by performing kernel density estimation on the  observed samples. <ref type="figure" target="#fig_1">Figure 5</ref> shows predicted posi- tions of objects using the learned priors.</p><formula xml:id="formula_4">Relation P (relation) inside(A,B) V ol(A∩B) V ol(A) outside(A,B) 1 -V ol(A∩B) V ol(A) left_of(A,B) V ol(A∩ left_of (B)) V ol(A) right_of(A,B) V ol(A∩ right_of (B)) V ol(A) near(A,B) (dist(A, B) &lt; tnear) faces(A,B) cos(f ront(A), c(B) − c(A))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Spatial Relations</head><p>We define a set of formal spatial relations that we map to natural language terms ( §5.1). In addi- tion, we collect annotations of spatial relation de- scriptions from people, learn a mapping of spatial keywords to our formal spatial relations, and train a classifier that given two objects can predict the likelihood of a spatial relation holding ( §5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Predefined spatial relations</head><p>For spatial relations we use a set of predefined rela- tions: left_of, right_of, above, below, front, back, supported_by, supports, next_to, near, inside, out- side, faces, left_side, right_side. 3 These are mea- sured using axis-aligned bounding boxes from the viewer's perspective; the involved bounding boxes are compared to determine volume overlap or clos- est distance (for proximity relations; see <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Description delta(A, B)</head><p>3 Delta position (x, y, z) between the centroids of A and B dist(A, B)</p><p>1 Normalized distance (wrt B) between the centroids of A and B overlap(A, f (B)) 6 Fraction of A inside left/right/front/back/top/bottom regions wrt B: V ol(A∩f (B))</p><formula xml:id="formula_5">V ol(A) overlap(A, B) 2 V ol(A∩B) V ol(A)</formula><p>and V ol(A∩B)</p><formula xml:id="formula_6">V ol(B)</formula><p>support(A, B) 2 supported_by(A, B) and supports(A, B) <ref type="table">Table 3</ref>: Features for trained spatial relations predictor.</p><p>Figure 6: Our data collection task.</p><p>Since these spatial relations are resolved with re- spect to the view of the scene, they correspond to view-centric definitions of spatial concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning Spatial Relations</head><p>We collect a set of text descriptions of spatial rela- tionships between two objects in 3D scenes by run- ning an experiment on Amazon Mechanical Turk. We present a set of screenshots of scenes in our dataset that highlight particular pairs of objects and we ask people to fill in a spatial relationship of the form "The __ is __ the __" (see <ref type="figure">Fig 6)</ref>. We col- lected a total of 609 annotations over 131 object pairs in 17 scenes. We use this data to learn pri- ors on view-centric spatial relation terms and their concrete geometric interpretation.</p><p>For each response, we select one keyword from the text based on length. We learn a mapping of the top 15 keywords to our predefined set of spa- tial relations. We use our predefined relations on annotated spatial pairs of objects to create a binary indicator vector that is set to 1 if the spatial relation holds, or zero otherwise. We then create a simi- lar vector for whether the keyword appeared in the annotation for that spatial pair, and then compute the cosine similarity of the two vectors to obtain a score for mapping keywords to spatial relations. <ref type="table" target="#tab_2">Table 2</ref> shows the obtained mapping. Using just the top mapping, we are able to map 10 of the 15 keywords to an appropriate spatial relation. The 5 keywords that are not well mapped are proximity relations that are not well captured by our prede- fined spatial relations. Using the 15 keywords as our spatial relations, we train a log linear binary classifier for each key- word over features of the objects involved in that spatial relation (see <ref type="table">Table 3</ref>). We then use this model to predict the likelihood of that spatial re- lation in new scenes. <ref type="figure" target="#fig_2">Figure 7</ref> shows examples of predicted likeli- hoods for different spatial relations with respect to an anchor object in a scene. Note that the learned spatial relations are much stricter than our prede- fined relations. For instance, "above" is only used to referred to the area directly above the table, not to the region above and to the left or above and in front (which our predefined classifier will all con- sider to be above). In our results, we show we have more accurate scenes using the trained spatial re- lations than the predefined ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency Pattern Example Text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{tag:VBN}=verb &gt;nsubjpass {}=nsubj &gt;prep ({}=prep &gt;pobj {}=pobj)</head><p>The chair <ref type="bibr">[nsubj]</ref> is made <ref type="bibr">[verb]</ref> of <ref type="bibr">[prep]</ref> wood <ref type="bibr">[pobj]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>attribute(verb,pobj)(nsubj,pobj) material(chair,wood) {}=dobj &gt;cop {} &gt;nsubj {}=nsubj</head><p>The chair <ref type="bibr">[nsubj]</ref> is red <ref type="bibr">[dobj]</ref> .</p><p>attribute(dobj)(nsubj,dobj) color(chair,red) {}=dobj &gt;cop {} &gt;nsubj {}=nsubj &gt;prep ({}=prep &gt;pobj {}=pobj)</p><p>The table <ref type="bibr">[nsubj]</ref> is next <ref type="bibr">[dobj]</ref> to <ref type="bibr">[prep]</ref> the chair <ref type="bibr">[pobj]</ref> . spatial(dobj)(nsubj, pobj) next_to <ref type="table">(table,chair)</ref> {}=nsubj &gt;advmod ({}=advmod &gt;prep ({}=prep &gt;pobj {}=pobj))</p><p>There is a table <ref type="bibr">[nsubj]</ref> next <ref type="bibr">[advmod]</ref> to <ref type="bibr">[prep]</ref> a chair <ref type="bibr">[pobj]</ref> . spatial(advmod)(nsubj, pobj) next_to(table,chair) <ref type="table">Table 4</ref>: Example dependency patterns for extracting attributes and spatial relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Text to Scene generation</head><p>We generate 3D scenes from brief scene descrip- tions using our learned priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Scene Template Parsing</head><p>During scene template parsing we identify the scene type, the objects present in the scene, their attributes, and the relations between them. The input text is first processed using the Stanford CoreNLP pipeline ( ). The scene type is determined by matching the words in the utterance against a list of known scene types from the scene dataset.</p><p>To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet <ref type="bibr" target="#b16">(Miller, 1995)</ref> to determine which objects are visualizable (under the physical object synset, excluding locations). We use the Stanford coreference system to determine when the same object is being referred to.</p><p>To identify properties of the objects, we extract other adjectives and nouns in the noun phrase. We also match dependency patterns such as "X is made of Y" to extract additional attributes. Based on the object category and attributes, and other words in the noun phrase mentioning the object, we identify a set of associated keywords to be used later for querying the 3D model database.</p><p>Dependency patterns are also used to extract spatial relations between objects (see <ref type="table">Table 4</ref> for some example patterns). We use Semgrex patterns to match the input text to dependencies <ref type="bibr" target="#b1">(Chambers et al., 2007</ref>). The attribute types are deter- mined from a dictionary using the text express- ing the attribute (e.g., attribute(red)=color, at- tribute(round)=shape). Likewise, spatial relations are looked up using the learned map of keywords to spatial relations.</p><p>As an example, given the input "There is a room with a desk and a red chair. The chair is to the left of the desk." we extract the following objects and spatial relations: </p><formula xml:id="formula_7">Objects</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inferring Implicits</head><p>From the parsed scene template, we infer the pres- ence of additional objects and support constraints.</p><p>We can optionally infer the presence of addi- tional objects from object occurrences based on the scene type. If the scene type is unknown, we use the presence of known object categories to pre- dict the most likely scene type by using Bayes' rule on our object occurrence priors P occ to get P (C s |{C o }) ∝ P occ ({C o }|C s )P (C s ). Once we have a scene type C s , we sample P occ to find ob- jects that are likely to occur in the scene. We re- strict sampling to the top n = 4 object categories.</p><p>We can also use the support hierarchy priors P support to infer implicit objects. For instance, for each object o i we find the most likely supporting object category and add it to our scene if not al- ready present.</p><p>After inferring implicit objects, we infer the sup- port constraints. Using the learned text to prede- fined relation mapping from §5.2, we can map the keywords "on" and "top" to the supported_by re- lation. We infer the rest of the support hierarchy by selecting for each object o i the parent object o j that maximizes P support (C o j |C o i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Grounding Objects</head><p>Once we determine from the input text what ob- jects exist and their spatial relations, we select 3D models matching the objects and their associated properties. Each object in the scene template is grounded by querying a 3D models database with There is a desk and a keyboard and a monitor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Text Basic +Support Hierarchy +Relative Positions</head><p>There is a coffee  the appropriate category and keywords.</p><p>We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene syn- thesis and containing about 12490 mostly indoor objects ( <ref type="bibr" target="#b6">Fisher et al., 2012</ref>). These models have text associated with them in the form of names and tags. In addition, we semi-automatically annotated models with object category labels (roughly 270 classes). We used model tags to set these labels, and verified and augmented them manually.</p><p>In addition, we automatically rescale models so that they have physically plausible sizes and orient them so that they have a consistent up and front direction ( <ref type="bibr" target="#b18">Savva et al., 2014</ref>). We then indexed all models in a database that we query at run-time for retrieval based on category and tag labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Scene Layout</head><p>Once we have instantiated the objects in the scene by selecting models, we aim to optimize an over- all layout score L = λ obj L obj + λ rel L rel that is a weighted sum of object arrangement L obj score and constraint satisfaction L rel score:</p><formula xml:id="formula_8">L obj = ∑ o i P surf (S n |C o i ) ∑ o j ∈F (o i ) P relpos (·) L rel = ∑ c i P rel (c i )</formula><p>where F (o i ) are the sibling objects and parent ob- ject of o i . We use λ obj = 0.25 and λ rel = 0.75 for the results we present. We use a simple hill climbing strategy to find a reasonable layout. We first initialize the positions <ref type="figure">Figure 9</ref>: Generated scene for "There is a room with a desk and a lamp. There is a chair to the right of the desk." The inferred scene hierarchy is overlayed in the center.</p><p>of objects within the scene by traversing the sup- port hierarchy in depth-first order, positioning the children from largest to first and recursing. Child nodes are positioned by first selecting a supporting surface on a candidate parent object through sam- pling of P surf . After selecting a surface, we sam- ple a position on the surface based on P relpos . Fi- nally, we check whether collisions exist with other objects, rejecting layouts where collisions occur. We iterate by randomly jittering and repositioning objects. If there are any spatial constraints that are not satisfied, we also remove and randomly repo- sition the objects violating the constraints, and it- erate to improve the layout. The resulting scene is rendered and presented to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results and Discussion</head><p>We show examples of generated scenes, and com- pare against naive baselines to demonstrate learned priors are essential for scene generation. We also discuss interesting aspects of using spatial knowledge in view-based object referent resolu- tion ( §7.2) and in disambiguating geometric inter- pretations of "on" ( §7.3). <ref type="figure" target="#fig_3">Figure 8</ref> shows a compari- son of scenes generated by our model versus sev- eral simpler baselines. The top row shows the im- pact of modeling the support hierarchy and the rel- ative positions in the layout of the scene. The bot- tom row shows that the learned spatial relations can give a more accurate layout than the naive predefined spatial relations, since it captures prag- matic implicatures of language, e.g., left is only used for directly left and not top left or bottom left ( <ref type="bibr" target="#b20">Vogel et al., 2013</ref>). <ref type="figure">Figure 12</ref>: Left: chair is selected using "the chair to the right of the table" or "the object to the right of the table". Chair is not selected for "the cup to the right of the table". Right: Different view results in different chair being selected for the input "the chair to the right of the table".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Generated Scenes</head><p>Support Hierarchy <ref type="figure">Figure 9</ref> shows a generated scene along with the input text and support hier- archy. Even though the spatial relation between lamp and desk was not mentioned, we infer that the lamp is supported by the top surface of the desk.</p><p>Disambiguation <ref type="figure" target="#fig_4">Figure 10</ref> shows a generated scene for the input "There is a room with a poster bed and a poster". Note that the system differen- tiates between a "poster" and a "poster bed" -it correctly selects and places the bed on the floor, while the poster is placed on the wall.</p><p>Inferring objects for a scene type <ref type="figure">Figure 11</ref> shows an example of inferring all the objects present in a scene from the input "living room". Some of the placements are good, while others can clearly be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">View-centric object referent resolution</head><p>After a scene is generated, the user can refer to ob- jects with their categories and with spatial relations between them. Objects are disambiguated by both category and view-centric spatial relations. We use the WordNet hierarchy to resolve hyponym or hy- pernym referents to objects in the scene. In <ref type="figure">Fig- ure 12 (left)</ref>, the user can select a chair to the right of the table using the phrase "chair to the right of the table" or "object to the right of the table". The user can then change their viewpoint by rotating and moving around. Since spatial relations are re- solved with respect to the current viewpoint, a dif- ferent chair is selected for the same phrase from a different viewpoint in the right screenshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Disambiguating "on"</head><p>As shown in §5.2, the English preposition "on", when used as a spatial relation, corresponds strongly to the supported_by relation. In our trained model, the supported_by feature also has a high positive weight for "on".</p><p>Our model for supporting surfaces and hierar- chy allows interpreting the placement of "A on B" based on the categories of A and B. Fig- ure 13 demonstrates four different interpretations for "on". Given the input "There is a cup on the table" the system correctly places the cup on the top surface of the table. In contrast, given "There is a cup on the bookshelf", the cup is placed on a supporting surface of the bookshelf, but not nec- essarily the top one which would be fairly high. <ref type="figure">Figure 13</ref>: From top left clockwise: "There is a cup on the table", "There is a cup on the book- shelf", "There is a poster on the wall", "There is a hat on the chair". Note the different geometric interpretations of "on".</p><p>Given the input "There is a poster on the wall", a poster is pasted on the wall, while with the input "There is a hat on the chair" the hat is placed on the seat of the chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Limitations</head><p>While the system shows promise, there are still many challenges in text-to-scene generation. For one, we did not address the difficulties of resolving objects. A failure case of our system stems from using a fixed set of categories to identify visualiz- able objects. For example, the sense of "top" refer- ring to a spinning top, and other uncommon object types, are not handled by our system as concrete objects. Furthermore, complex phrases including object parts such as "there's a coat on the seat of the chair" are not handled. <ref type="figure">Figure 14</ref> shows some <ref type="figure">Figure 14</ref>: Left: A water bottle instead of wine bottle is selected for "There is a bottle of wine on the table in the kitchen". In addition, the selected table is inappropriate for a kitchen. Right: A floor lamp is incorrectly selected for the input "There is a lamp on the table". example cases where the context is important in selecting an appropriate object and the difficulties of interpreting noun phrases.</p><p>In addition, we rely on a few dependency pat- terns for extracting spatial relations so robustness to variations in spatial language is lacking. We only handle binary spatial relations (e.g., "left", "behind") ignoring more complex relations such as "around the table" or "in the middle of the room". Though simple binary relations are some of the most fundamental spatial expressions and a good first step, handling more complex expressions will do much to improve the system.</p><p>Another issue is that the interpretation of sen- tences such as "the desk is covered with paper", which entails many pieces of paper placed on the desk, is hard to resolve. With a more data-driven approach we can hope to link such expressions to concrete facts.</p><p>Finally, we use a traditional pipeline approach for text processing, so errors in initial stages can propagate downstream. Failures in depen- dency parsing, part of speech tagging, or coref- erence resolution can result in incorrect interpre- tations of the input language. For example, in the sentence "there is a desk with a chair in front of it", "it" is not identified as coreferent with "desk" so we fail to extract the spatial relation front_of(chair, desk).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>There is related prior work in the topics of mod- eling spatial relations, generating 3D scenes from text, and automatically laying out 3D scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Spatial knowledge and relations</head><p>Prior work that required modeling spatial knowl- edge has defined representations specific to the task addressed. Typically, such knowledge is man- ually provided or crowdsourced -not learned from data. For instance, WordsEye ( <ref type="bibr" target="#b3">Coyne et al., 2010</ref>) uses a set of manually specified relations. The NLP community has explored grounding text to physical attributes and relations <ref type="bibr" target="#b14">(Matuszek et al., 2012;</ref><ref type="bibr" target="#b10">Krishnamurthy and Kollar, 2013)</ref>, gener- ating text for referring to objects <ref type="bibr" target="#b7">(FitzGerald et al., 2013)</ref> and connecting language to spatial re- lationships ( <ref type="bibr" target="#b19">Vogel and Jurafsky, 2010;</ref><ref type="bibr" target="#b8">Golland et al., 2010;</ref>). Most of this work focuses on learning a mapping from text to formal representations, and does not model implicit spatial knowledge. Many priors on real world spatial facts are typically unstated in text and remain largely unaddressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Text to Scene Systems</head><p>Early work on the SHRDLU system <ref type="bibr" target="#b21">(Winograd, 1972)</ref> gives a good formalization of the linguis- tic manipulation of objects in 3D scenes. By re- stricting the discourse domain to a micro-world with simple geometric shapes, the SHRDLU sys- tem demonstrated parsing of natural language in- put for manipulating scenes. However, generaliza- tion to more complex objects and spatial relations is still very hard to attain.</p><p>More recently, a pioneering text-to-3D scene generation prototype system has been presented by <ref type="bibr">WordsEye (Coyne and Sproat, 2001</ref>). The authors demonstrated the promise of text to scene genera- tion systems but also pointed out some fundamen- tal issues which restrict the success of their system: much spatial knowledge is required which is hard to obtain. As a result, users have to use unnatural language (e.g., "the stool is 1 feet to the south of the table") to express their intent. Follow up work has attempted to collect spatial knowledge through crowd-sourcing ( <ref type="bibr" target="#b4">Coyne et al., 2012</ref>), but does not address the learning of spatial priors.</p><p>We address the challenge of handling natural language for scene generation, by learning spatial knowledge from 3D scene data, and using it to in- fer unstated implicit constraints. Our work is simi- lar in spirit to recent work on generating 2D clipart for sentences using probabilistic models learned from data ( <ref type="bibr" target="#b22">Zitnick et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Automatic Scene Layout</head><p>Work on scene layout has focused on determining good furniture layouts by optimizing energy func- tions that capture the quality of a proposed layout. These energy functions are encoded from design guidelines <ref type="bibr" target="#b15">(Merrell et al., 2011</ref>) or learned from scene data ( <ref type="bibr" target="#b6">Fisher et al., 2012</ref>). Knowledge of ob- ject co-occurrences and spatial relations is repre- sented by simple models such as mixtures of Gaus- sians on pairwise object positions and orientations. We leverage ideas from this work, but they do not focus on linking spatial knowledge to language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>We have demonstrated a representation of spatial knowledge that can be learned from 3D scene data and how it corresponds to natural language. We also showed that spatial inference and grounding is critical for achieving plausible results in the text- to-3D scene generation task. Spatial knowledge is critically useful not only in this task, but also in other domains which require an understanding of the pragmatics of physical environments.</p><p>We only presented a deterministic approach for mapping input text to the parsed scene template. An interesting avenue for future research is to automatically learn how to parse text describing scenes into formal representations by using more advanced semantic parsing methods.</p><p>We can also improve the representation used for spatial priors of objects in scenes. For instance, in this paper we represented support surfaces by their orientation. We can improve the representation by modeling whether a surface is an interior or exte- rior surface.</p><p>Another interesting line of future work would be to explore the influence of object identity in de- termining when people use ego-centric or object- centric spatial reference models, and to improve resolution of spatial terms that have different in- terpretations (e.g., "the chair to the left of John" vs "the chair to the left of the table").</p><p>Finally, a promising line of research is to explore using spatial priors for resolving ambiguities dur- ing parsing. For example, the attachment of "next to" in "Put a lamp on the table next to the book" can be readily disambiguated with spatial priors such as the ones we presented.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Predicted positions using learned relative position priors for chair given desk (top left), poster-room (top right), mouse-desk (bottom left), keyboard-desk (bottom right).</figDesc><graphic url="image-4.png" coords="4,72.00,62.73,218.23,123.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: High probability regions where the center of another object would occur for some spatial relations with respect to a table: above (top left), on (top right), left (mid left), right (mid right), in front (bottom left), behind (bottom right).</figDesc><graphic url="image-10.png" coords="5,284.82,259.34,147.84,110.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Top Generated scenes for randomly placing objects on the floor (Basic), with inferred Support Hierarchy, and with priors on Relative Positions. Bottom Generated scenes with no understanding of spatial relations (No Relations), scoring using Predefined Relations and Learned Relations.</figDesc><graphic url="image-17.png" coords="7,252.82,104.78,202.52,151.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Generated scene for "There is a room with a poster bed and a poster."</figDesc><graphic url="image-20.png" coords="8,72.00,216.26,218.30,108.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table and a cake. There is a red chair to the right of the table." a) Scene Template Input Text supports(o0,o1) supports(o0,o2) right(o2,o1) o3 cake c) 3D Scene o0 room o1 table o2 chair supports(o1,o4) supports(o4,o3) o4 plate Parse Infer Ground Layout b) Geometric Scene Render View Chair</head><label>and</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Definitions of spatial relation using 
bounding boxes. Note: dist(A, B) is normalized 
against the maximum extent of the bounding box 
of B. f ront(A) is the direction of the front vector 
of A and c(A) is the centroid of A. 

Keyword Top Relations and Scores 
behind 
(back_of, 0.46), (back_side, 0.33) 
adjacent 
(f ront_side, 0.27), (outside, 0.26) 
below 
(below, 0.59), (lower_side, 0.38) 
front 
(f ront_of, 0.41), (f ront_side, 0.40) 
left 
(lef t_side, 0.44), (lef t_of, 0.43) 
above 
(above, 0.37), (near, 0.30) 
opposite 
(outside, 0.31), (next_to, 0.30) 
on 
(supported_by, 0.86), (on_top_of, 0.76) 
near 
(outside, 0.66), (near, 0.66) 
next 
(outside, 0.49), (near, 0.48) 
under 
(supports, 0.62), (below, 0.53) 
top 
(supported_by, 0.65), (above, 0.61) 
inside 
(inside, 0.48), (supported_by, 0.35) 
right 
(right_of, 0.50), (lower_side, 0.38) 
beside 
(outside, 0.45), (right_of, 0.45) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Map of top keywords to spatial relations 
(appropriate mappings in bold). 

</table></figure>

			<note place="foot" n="2"> The support hierarchy is explicitly modeled in the scene dataset we use.</note>

			<note place="foot" n="3"> We distinguish left_of(A,B) as A being left of the left edge of the bounding box of B vs left_side(A,B) as A being left of the centroid of B.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their thoughtful comments. We gratefully acknowl-edge the support of the Defense Advanced Re-search Projects Agency (DARPA) Deep Explo-ration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of semantic parsers for mapping instructions to actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning alignments and leveraging natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mariecatherine De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WordsEye: an automatic text-to-scene conversion system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial relations in text-to-scene conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Spatial Language Interpretation, Workshop at Spatial Cognition</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Annotation tools and knowledge representation for a text-toscene system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Coyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Klapheke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoud</forename><surname>Rouhizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Technical Papers</title>
		<meeting>COLING 2012: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image description using visual dependency representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Example-based synthesis of 3D object arrangements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning distributions over logical forms for referring expression generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A game-theoretic approach to generating spatial descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Superfaces: Polygonal mesh simplification with bounded error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">D</forename><surname>Kalvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><forename type="middle">H</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="1996" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Visruth</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagnik</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating simple image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interactive furniture layout using interior design guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WordNet: a lexical database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Midge: Generating image descriptions from computer vision detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kota</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On being the right scale: Sizing large collections of 3D models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Hanrahan</surname></persName>
		</author>
		<idno>CSTR 2014-03</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Implicatures and nested beliefs in approximate Decentralized-POMDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intenational Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
