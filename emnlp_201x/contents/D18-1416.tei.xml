<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
						</author>
						<title level="a" type="main">Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3813" to="3823"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3813</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ&apos;s high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent&apos;s capability of adapting to a changing environment is tested. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are many virtual assistants commercially available today, such as Apple's Siri, Google's Home, Microsoft's Cortana, and Amazon's Echo. With a well-designed dialogue system as an intel- ligent assistant, people can accomplish tasks via natural language interactions. Recent advance in deep learning has also inspired many studies in neural dialogue systems <ref type="bibr" target="#b0">Bordes et al., 2017;</ref><ref type="bibr" target="#b2">Dhingra et al., 2017;</ref>.</p><p>A key component in such task-completion di- alogue systems is dialogue policy, which is of- ten formulated as a reinforcement learning (RL) problem ( <ref type="bibr" target="#b6">Levin et al., 1997;</ref><ref type="bibr" target="#b28">Young et al., 2013</ref>). However, learning dialogue policy via RL from the scratch in real-world systems is very challeng- ing, due to the inevitable dependency on the envi- ronment from which a learner acquires knowledge and receives rewards. In a dialogue scenario, real users act as the environment in the RL framework, and the system communicates with real users con- stantly to learn dialogue policy. Such process is very time-consuming and expensive for online learning.</p><p>One plausible strategy is to leverage user simulators trained on human conversational data ( <ref type="bibr" target="#b19">Schatzmann et al., 2007;</ref>, which allows the agent to learn dialogue pol- icy by interacting with the simulator instead of real users. The user simulator can provide infi- nite simulated experiences without additional cost, and the trained system can be deployed and then fine-tuned through interactions with real users ( <ref type="bibr" target="#b21">Su et al., 2016;</ref><ref type="bibr" target="#b29">Zhao and Eskenazi, 2016;</ref><ref type="bibr" target="#b27">Williams et al., 2017;</ref><ref type="bibr" target="#b2">Dhingra et al., 2017;</ref><ref type="bibr" target="#b10">Liu and Lane, 2017;</ref><ref type="bibr" target="#b16">Peng et al., 2017b;</ref><ref type="bibr" target="#b1">Budzianowski et al., 2017;</ref><ref type="bibr" target="#b14">Peng et al., 2017a;</ref><ref type="bibr" target="#b24">Tang et al., 2018)</ref>.</p><p>However, due to the complexity of real con- versations and biases in the design of user sim- ulators, there always exists the discrepancy be- tween real users and simulated users. Further- more, to the best of our knowledge, there is no uni- versally accepted metric for evaluating user sim- ulators for dialogue purpose <ref type="bibr" target="#b17">(Pietquin and Hastie, 2013)</ref>. Therefore, it remains controversial whether training task-completion dialogue agent via simu- lated users is a valid and effective approach.</p><p>A previous study, called Deep Dyna-Q (DDQ) ( <ref type="bibr" target="#b15">Peng et al., 2018)</ref>, proposed a new strat- egy to learn dialogue policies with real users by combining the Dyna-Q framework <ref type="bibr" target="#b22">(Sutton, 1990)</ref> with deep learning models. This framework in- corporates a learnable environment model (world model) into the dialogue policy learning pipeline, which simulates dynamics of the environment and generates simulated user behaviors to supplement the limited amount of real user experience. In DDQ, real user experiences play two pivotal roles: 1) directly improve the dialogue policy via RL; 2) improve the world model via supervised learning to make it behave more human-like. The former is referred to as direct reinforcement learning, and the latter world model learning. Respectively, the policy model is trained via real experiences collected by interacting with real users (direct reinforcement learning), and simulated experi- ences collected by interacting with the learned world model (planning or indirect reinforcement learning).</p><p>However, the effectiveness of DDQ depends upon the quality of simulated experiences used in planning. As pointed out in ( <ref type="bibr" target="#b15">Peng et al., 2018</ref>), although at the early stages of dialogue training it is helpful to perform planning aggressively with large amounts of simulated experiences regardless their quality, in the late stages when the dialogue agent has been significantly improved, low-quality simulated experiences often hurt the performance badly. Since there is no established method of evaluating the world model which generates sim- ulated experiences, <ref type="bibr" target="#b15">Peng et al. (2018)</ref> resorts to heuristics to mitigate the negative impact of low- quality simulated experiments, e.g., reducing the planning steps in the late stage of training. These heuristics need to be tweaked empirically, thus limit DDQ's applicability in real-world tasks.</p><p>To improve the effectiveness of planning with- out relying on heuristics, this paper proposes Dis- criminative Deep Dyna-Q (D3Q), a new frame- work inspired by generative adversarial network (GAN) that incorporates a discriminator into the planning process. The discriminator is trained to differentiate simulated experiences from real user experiences. As illustrated in <ref type="figure">Figure 1</ref>, all sim- ulated experiences generated by the world model need to be judged by the discriminator, only the high-quality ones, which cannot be easily detected by the discriminator as being simulated, are used for planning. During the course of dialogue train- ing, both the world model and discriminator are refined using the real experiences. So, the quality threshold held by the discriminator goes up with the world model and dialogue agent, especially in the late stage of training.</p><p>By employing the world model for planning and a discriminator for controlling the quality of simu- lated experiences, the proposed D3Q framework can be viewed as a model-based RL approach, which is generic and can be easily extended to other RL problems. In contrast, most model-based RL methods ( <ref type="bibr" target="#b23">Tamar et al., 2016;</ref><ref type="bibr" target="#b20">Silver et al., 2016;</ref><ref type="bibr" target="#b3">Gu et al., 2016;</ref><ref type="bibr">Racanì ere et al., 2017</ref>) are developed for simulation-based, synthetic prob- lems (e.g., games), not for real-world problems. In summary, our main contributions in this work are two-fold:</p><p>• The proposed Discriminative Deep Dyna-Q approach is capable of controlling the qual- ity of simulated experiences generated by the world model in the planning phase, which enables effective and robust dialogue policy learning.</p><p>• The proposed model is verified by experi- ments including simulation, human evalua- tion, and domain-extension settings, where all results show better sample efficiency over the DDQ baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discriminative Deep Dyna-Q (D3Q)</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, the D3Q frame- work consists of six modules: <ref type="formula" target="#formula_0">(1)</ref>   included in traditional framework of dialogue sys- tems. <ref type="figure">Figure 1</ref> illustrates the whole process: start- ing with an initial dialogue policy and an initial world model (both are trained with pre-collected human conversational data), D3Q training con- sists of four stages: (1) direct reinforcement learn- ing: the agent interacts with real users, collects real experiences and improves dialogue policy; (2) world model learning: the world model is learned and refined using real experience; (3) dis- criminator learning: the discriminator is learned and refined to differentiate simulated experience from real experience; and (4) controlled planning: the agent improves the dialogue policy using the high-quality simulated experience generated by the world model and the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Direct Reinforcement Learning</head><p>In this stage, we use the vanilla deep Q-network (DQN) method <ref type="bibr" target="#b12">(Mnih et al., 2015</ref>) to learn the di- alogue policy based on real experience. We con- sider task-completion dialogue as a Markov De- cision Process (MDP), where the agent interacts with a user through a sequence of actions to ac- complish a specific user goal.</p><p>At each step, the agent observes the dialogue state s, and chooses an action a to execute, us- ing an -greedy policy that selects a random action with probability or otherwise follows the greedy policy a = argmax a Q(s, a ; θ Q ). Q(s, a; θ Q ) which is the approximated value function, imple- mented as a Multi-Layer Perceptron (MLP) pa- rameterized by θ Q . The agent then receives re- ward r, observes next user response, and updates the state to s . Finally, we store the experience tu- ple (s, a, r, s ) in the replay buffer B u . This cycle continues until the dialogue terminates.</p><p>We improve the value function Q(s, a; θ Q ) by adjusting θ Q to minimize the mean-squared loss function as follows:</p><formula xml:id="formula_0">L(θ Q ) = E (s,a,r,s )∼B u [(y i − Q(s, a; θ Q )) 2 ], y i = r + γ max a Q (s , a ; θ Q ),<label>(1)</label></formula><p>where γ ∈ [0, 1] is a discount factor, and Q (.) is the target value function that is only periodi- cally updated (i.e., fixed-target). The dialogue pol- icy can be optimized through θ Q L(θ Q ) by mini- batch deep Q-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">World Model Learning</head><p>To enable planning, we use a world model to gen- erate simulated experiences that can be used to improve dialogue policy. In each turn of a dia- logue, the world model takes the current dialogue state s and the last system action a (represented as an one-hot vector) as the input, and generates the corresponding user response o, reward r, and a binary variable t (indicating if the dialogue ter- minates). The world model G(s, a; θ G ) is trained using a multi-task deep neural network ( <ref type="bibr" target="#b11">Liu et al., 2015)</ref> to generate the simulated experiences. The model contains two classification tasks for sim- ulating user responses o and generating terminal signals t, and one regression task for generating the reward r. The lower encoding layers are shared across all three tasks, while the upper layers are task-specific. G(s, a; θ G ) is optimized to mimic human behaviors by leveraging real experiences in the replay buffer B u . The model architecture is illustrated in the left part of <ref type="figure" target="#fig_1">Figure 3</ref>.</p><formula xml:id="formula_1">h = tanh(W h (s, a) + b h ), r = W r h + b r , o = softmax(W a h + b a ), t = sigmoid(W t h + b t ),</formula><p>where (s, a) is the concatenation of s and a, and all W and b are weight matrices and bias vectors, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discriminator Learning</head><p>The discriminator, denoted by D, is used to differ- entiate simulated experience from real experience. D is a neural network model with its architecture illustrated in the right part of <ref type="figure" target="#fig_1">Figure 3</ref>.  an LSTM to encode a dialogue as a feature vec- tor, and a Multi-Layer Perceptron (MLP) to map the vector to a probability indicating whether the dialogue looks like being generated by real users.</p><p>D is trained using the simulated experience gen- erated by the world model G and the collected real experience x. We use the objective function as</p><formula xml:id="formula_2">E real [log D(x)] + E simu [log(1 − D(G(.)))]. (2)</formula><p>Practically, we use the mini-batch training and the objective function can be rewritten as</p><formula xml:id="formula_3">1 m m i=1 [log D(x (i) ) + log(1 − D(G(.) (i) ))], (3)</formula><p>where m represents the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Controlled Planning</head><p>In this stage, we apply the world model G and the discriminator D to generate high-quality sim- ulated experience to improve dialogue policy. The D3Q method uses three replay buffers, B u for storing real experience, B s for simulated experi- ence generated by G, and B h for high-quality sim- ulated experience generated by G and D. Learn- ing and planning are implemented by the same DQN algorithm, operating on real experience in B u for learning and on simulated experience in B h for planning. Here we only describe how the high-quality simulated experience is generated.</p><p>At the beginning of each dialogue session, we uniformly draw a user goal (C, R) ( <ref type="bibr" target="#b19">Schatzmann et al., 2007)</ref>, where C is a set of constraints and R is a set of requests. For example, in movie-ticket booking dialogue, constraints are the slots with specified values, such as the name, the date of the movie and the number of tickets to buy. And requests can contain slots which the user plans to acquire the values for, such as the start time of the movie. The first user action o 1 can be either a request or an inform di- alogue act. A request dialogue act consists of a request slot, multiple constraint slots and the corresponding values, uniformly sampled from R and C. For example, request(theater; moviename=avergers3). An inform dia- logue act contains constraint-slots only. Semantic frames can also be transformed into natural lan- guage via NLG component, e.g., "which theater will play the movie avergers3?"</p><p>For each dialogue episode with a sampled user goal, the agent interacts with world model G(s, a; θ G ) to generate a simulated dialogue ses- sion, which is a sequence of simulated experi- ence tuples (s, a, r, s ). We always store the G- generated session in B s , but only store it in B h if it is selected by discriminator D. We repeat the process until K simulated dialogue sessions are added in B h , where K is a pre-defined planning step size. This can be viewed as a sampling pro- cess. In theory if the world model G is not well- trained this process could take forever to gener- ate K high-quality samples accepted by D. For- tunately, this never happened in our experiments because D is trained using the simulated experi- ence generated by G and D is updated whenever G is refined. Now, we compare controlled planning in D3Q with the planning process in the original DDQ ( <ref type="bibr" target="#b15">Peng et al., 2018)</ref>. In DDQ, after each step of di- rect reinforcement learning, the agent improves its policy via K steps of planning. A larger planning step means that more simulated experiences gen- erated by G are used for planning. Theoretically, larger amounts of high-quality simulated experi- ences can boost the performance of the dialogue policy more quickly. However, the world model by no means perfectly reflects real human behav- ior, and the generated experiences, if of low qual- ity, can have negative impact on dialogue policy learning. Prior work resorts to heuristics to miti- gate the impact. For example, <ref type="bibr" target="#b15">Peng et al. (2018)</ref> proposed to reduce planning steps at the late stage of policy learning, thus forcing all DDQ agents to converge to the same one trained with a small number of planning steps. <ref type="figure" target="#fig_2">Figure 4</ref> shows the performance of DDQ agents with different planning steps without heuristics. It is observable that the performance is unstable, es- pecially for larger planning steps, which indicates that the quality of simulated experience is becom- ing more pivotal as the number of planning steps increases. D3Q resolves this issue by introducing a dis- criminator and allows only high-quality simulated experience, judged by the discriminator, to be used for planning. In the next section, we will show that D3Q does not suffer from the problem of DDQ and the D3Q training is quite stable even with large sizes of planning steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate D3Q on the movie-ticket booking task with both simulated users and real users in two set- tings: full domain and domain extension.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Raw conversational data in a movie-ticket book- ing scenario was collected via Amazon Mechani- cal Turk. The dataset has been manually labeled based on a schema defined by domain experts, as shown in <ref type="table">Table 1</ref>, consisting of 11 intents and 16 slots in the full domain setting, while there are 18 slots in the domain extension setting. Most of these slots can be both "inform slots" and "re- quest slots", except for a few. For example, the slot number of people is categorized as an in- form slot but not a request slot, because arguably the user always knows how many tickets she/he wants. In total, the dataset contains 280 annotated dialogues, the average length of which is approxi- mately 11 turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>To verify the effectiveness of D3Q, we devel- oped different versions of task-completion dia- logue agents as baselines to compare with.</p><p>• A DQN agent is implemented with only di- rect reinforcement learning in each episode.</p><p>• The DQN(K) has K times more real expe- riences than the DQN agent. The perfor- mance of DQN(K) can be viewed as the up- per bound of DDQ(K) and D3Q(K) with the same number of planning steps (K − 1), as these models have the same training settings and the same amount of training samples dur- ing the entire learning process.</p><p>• The DDQ(K) agents are learned with an ini- tial world model pre-trained on human con- versational data, with (K − 1) as the number of planning steps. These agents store the sim- ulated experience without being judged by the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed D3Q</head><p>• The D3Q(K) agents are learned through the process described in Section 2.4.</p><p>• The D3Q(K, fixed θ D ) agents are learned as described in Section 2.4 without training dis- criminator. The D3Q(K, fixed θ D ) agents are only evaluated in the simulation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>Settings and Hyper-parameters -greedy is al- ways applied for exploration. We set the discount factor γ = 0.9. The buffer size of B u and B h is set to 2000 and 2000 ×K planning steps, respec- tively. The batch size is 16, and the learning rate is 0.001. To prevent gradient explosion, we applied gradient clipping on all the model parameters to maximum norm = 1. All the NN models are ran- domly initialized. The high-quality simulated ex- perience buffer B h and the simulated experience buffer B s are initialized as empty. The target net- work is updated at the beginning of each training episode. The optimizer for all the neural networks is RMSProp ( <ref type="bibr" target="#b5">Hinton et al., 2012</ref>). The maximum length of a simulated dialogue is 40. If exceed- ing the maximum length, the dialogue fails. To make dialogue training efficient, we also applied a variant of imitation learning, called Reply Buffer Spiking (RBS) ( , by building a simple and straightforward rule-based agent based on human conversational dataset. We then pre- filled the real experience replay buffer B u with experiences of 50 dialogues, before training for all the variants of models. The batch size for collect- ing experiences is 10, which means if the running agent is DDQ/D3Q(K), 10 real experience tuples and 10 × (K − 1) simulated experience tuples are stored into the buffers at every episode.</p><p>Agents For all the models (DQN, DDQ, and D3Q) and their variants, the value networks Q(.) are MLPs with one hidden layer of size 80 and ReLU activation.</p><p>World Model For all the models (DDQ and D3Q) and their variants, the world models M (.) are MLPs with one shared hidden layer of size 160, hyperbolic-tangent activation, and one en- coding layer of hidden size 80 for each state and action input. Discriminator In the proposed D3Q frame- work, the LSTM cell is utilized, the hidden size is 128. The encoding layer for the current state and output layer are MLPs with single hidden layer of size 80. The threshold interval is set to range between 0.45 and 0.55, i.e., only when 0.45 ≤ D(x) ≤ 0.55 that x would be stored into the buffer B h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Simulation Evaluation</head><p>In this setting, the dialogue agents are optimized by interacting with the user simulators instead of with real users. In another word, the world model is trained to mimic user simulators. In spite of the discrepancy between simulators and real users, this setting endows us with the flexibility to per- form a detailed analysis of models without much cost, and to reproduce experimental results easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User Simulator</head><p>We used an open-sourced task- oriented user simulator ( ) in our simulated evaluation experiments (Appendix A for more details). The simulator provides the agent with a simulated user response in each dialogue turn along with a reward signal at the end of the dialogue. A dialogue is considered successful if and only if a movie ticket is booked successfully, and the information provided by the agent satisfies all the constraints of the sampled user goal. At the end of each dialogue, the agent receives a positive reward 2 * L for success, or a negative reward −L for failure, where L is the maximum number of turns in each dialogue, and is set to 40 in our ex- periments. Furthermore, in each turn, a reward −1 is provided to encourage shorter dialogues.  <ref type="figure" target="#fig_4">(5, fixed θD)</ref> .    <ref type="formula">(5)</ref>, and has simi- lar training efficiency to DQN(5). Note that here the planning steps of D3Q is 4, which means D3Q (pink) and DDQ(5) (purple) use the same amount of training samples (both real and sim- ulated experiences) to update the agent through- out the whole training process. The difference be- tween these two agents is that D3Q employs a dis- criminator as a quality judge. The experimental result shows that our proposed framework could boost the learning efficiency even without any pre- training on the discriminator. Furthermore, D3Q (pink) uses the same amount of training samples as DQN <ref type="formula">(5)</ref>  ficacy and feasibility of D3Q is hereby justly veri- fied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Agent Epoch = 100 Epoch = 200 Epoch = 300 Success Reward Turns Success Reward Turns Success Reward Turns</head><note type="other">6800 33.86 17.48 .7000 36.57 16.85 .6933 35.67 17.06 DQN(5) .7400 42.19 15.23 .8533 57.76 11.28 .7667 46.56 12.88 DDQ(10) .5733 24.00 11.60 .5533 19.89 15.01 .4800 10.04 17.12 DDQ(10, rand-init θG) .5000 12.79 16.41 .5333 17.71 14.57 .6000 24.98 16.04 DDQ(10, fixed θG) .3467 -10.25 25.69 .2400 -23.38 26.36 .0000 -55.53 33.07 D3Q(10) .6333 28.99 16.01 .7000 37.24 15.52 .6667 33.09 15.83 D3Q(10, fixed θD) .7133 36.36 20.48 .8400 54.87 20.48 .7400 42.89 13.81 DQN(10) .8333 55.5 11.</note><p>As mentioned in the previous section, a large number of planning steps means leveraging a large amount of simulated experience to train the agents. The experimental result <ref type="figure" target="#fig_2">(Figure 4)</ref> shows that the DDQ agents are highly sensitive to the quality of simulated experience. In contrast, the proposed D3Q framework demonstrates robustness to the number of planning steps ( <ref type="figure" target="#fig_5">Figure 6</ref>). <ref type="figure" target="#fig_6">Figure 7</ref> shows that D3Q also outperforms DDQ original setting ( <ref type="bibr" target="#b15">Peng et al., 2018</ref>) and D3Q without train- ing discriminator. The performance detail includ- ing success rate, reward, an number of turns is shown in <ref type="table" target="#tab_5">Table 2</ref>. From the table, with fewer sim- ulated experiences, the difference between DDQ and D3Q may not be significant, where DDQ agents achieve about 50%-60% success rate and D3Q agents achieve higher than 68% success rate after 100 epochs. However, when the number of planning steps increases, more fake experiences significantly degrade the performance for DDQ agents, where DDQ(10, fixed θ G ) suffers from bad simulated experiences after 300 epochs and achieves 0% success rate.</p><p>Domain Extension In the domain extension experiments, more complicated user goals are adopted. Moreover, we narrow down the action space into a small subspace instead of that used in full-domain setting, and gradually introduce more complex user goals and expand the action space as the training proceeds. Specifically, we start from a set of necessary slots and actions to accomplish most of the user goals, and then extend the ac- tion space and complexity of user goals once every 20 epoch (after epoch 50). Note that the domain will keep extending and become full-domain after epoch 130. Such experimental setting makes the training environment more complicated and unsta- ble than the previous full-domain one.</p><p>The results summarized in <ref type="figure" target="#fig_7">Figure 8</ref> show that D3Q significantly outperforms the baseline meth- ods, demonstrating its robustness. Furthermore, D3Q shows remarkable learning efficiency while extending the domain, which even outperforms DQN(5). A potential reason might be that the world model could improve exploration in such unstable and noisy environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Human Evaluation</head><p>In the human evaluation experiments, real users interact with different models without knowing which agent is behind the system. At the begin- ning of each dialogue session, one of the agents was randomly picked to converse with the user.  The user was instructed to converse with the agent to complete a task given a user goal sampled from the corpus. The user can abandon the task and ter- minate the dialogue at any time, if she or he be- lieves that the dialogue was unlikely to succeed, or simply because the dialogue drags on for too many turns. In such cases, the dialogue session is considered as failure.</p><p>Full Domain Three agents <ref type="bibr">(DQN, DDQ(5)</ref>, and D3Q) trained in the full domain setting ( <ref type="figure" target="#fig_4">Figure 5</ref>) at epoch 100 are selected for testing. As illustrated in <ref type="figure" target="#fig_8">Figure 9</ref>, the results of human evaluation are consistent with those in the simulation evaluation <ref type="bibr">(Section 3.4)</ref>, and the proposed D3Q significantly outperforms other agents.</p><p>Domain Extension To test the adaptation capa- bility of the agents to the complicated, dynam- ically changing environment, we selected three trained agents <ref type="bibr">(DQN, DDQ(5)</ref>, and D3Q) at epoch 100 before the environment extends to full do- main, and another three agents trained at epoch 200 after the environment becomes full domain. <ref type="figure" target="#fig_9">Figure 10</ref> shows that the results are consistent with those in the simulation evaluation <ref type="figure" target="#fig_7">(Figure 8</ref>), and the proposed D3Q significantly outperforms other agents in both stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper proposes a new framework, Discrimi- native Deep Dyna-Q (D3Q), for task-completion dialogue policy learning. With a discriminator as judge, the proposed approach is capable of con- trolling the quality of simulated experience gener- ated in the planning phase, which enables efficient and robust dialogue policy learning. Furthermore, D3Q can be viewed as a generic model-based RL approach easily-extensible to other RL problems.</p><p>We validate the D3Q-trained dialogue agent on a movie-ticket-booking task in the simulation, hu- man evaluation, and domain-extension settings. Our results show that the D3Q agent significantly outperforms the agents trained using other state- of-the-art methods including DQN and DDQ.</p><p>We generated the user goals from the labeled dataset using two mechanisms. One mechanism is to extract all the slots (known and unknown) from the first user turns (excluding the greeting user turn) in the data, since usually the first turn contains some or all the required information from user. The other mechanism is to extract all the slots (known and unknown) that first appear in all the user turns, and then aggregate them into one user goal. We dump these user goals into a file as the user-goal database. Every time when running a dialogue, we randomly sample one user goal from this user goal database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed D3Q dialogue system framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The model architectures of the world model and the discriminator for controlled planning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The learning curves of DDQ(K) agents where (K − 1) is the number of planning steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Full</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The learning curves of agents (DQN, DDQ, and D3Q) under the full domain setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The learning curves of D3Q(K) agents which (K-1) is the number of planning steps (K = 2, 3, 5, 10, 15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The learning curves of D3Q, DDQ(5), DDQ(5) (Peng et al., 2018), and D3Q fixed θ D agents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The learning curves of agents (DQN, DDQ, and D3Q) under the domain extension setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The human evaluation results of D3Q, DDQ(5), and D3Q in the full domain setting, the number of test dialogues indicated on each bar, and the pvalues from a two-sided permutation test (difference in mean is significant with p &lt; 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The human evaluation results of DQN, DDQ(5), and D3Q in the domain extension setting, the number of test dialogues indicated on each bar. The prefix 'b-' implies that the trained models are picked before the environment extends to full domain, while the prefix 'a-' indicates that the trained models are picked after the environment becomes full domain (difference in mean is significant with p &lt; 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Domain &amp; Domain Extension request, inform, deny, confirm question, Intent confirm answer, greeting, closing, not sure, multiple choice, thanks, welcome Full Domain</figDesc><table>Slot 

city, closing, date, distanceconstraints, 
greeting, moviename, numberofpeople, 
price, starttime, state, taskcomplete, theater, 
theater chain, ticket, video format, zip 
Domain Extension 

Slot 

city, closing, date, distanceconstraints, 
greeting, moviename, numberofpeople, 
price, starttime, state, taskcomplete, theater, 
theater chain, ticket, video format, zip, 
genre, other 

Table 1: The data schema for full domain and domain 
extension settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results of different agents at training epoch = {100, 200, 300}. Each number is averaged over 3 runs, 
each run tested on 50 dialogues. (Success: success rate, Reward: Average Reward, Turns: Average Turns) 

</table></figure>

			<note place="foot" n="1"> The source code is available at https://github. com/MiuLab/D3Q.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful feedback on the work. We would like to acknowledge the volunteers from Microsoft for participating the human evaluation experiments. Shang-Yu Su and Yun-Nung Chen are supported by the Ministry of Science and Technology of Tai-wan and MediaTek Inc.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A User Simulator</head><p>In the task-completion dialogue setting, the entire conversation is around a user goal implicitly, but the agent knows nothing about the user goal ex- plicitly and its objective is to help the user to ac- complish this goal. Generally, the definition of user goal contains two parts:</p><p>• inform slots contain a number of slot-value pairs which serve as constraints from the user.</p><p>• request slots contain a set of slots that user has no information about the values, but wants to get the values from the agent dur- ing the conversation. ticket is a default slot which always appears in the request slots part of user goal. To make the user goal more realistic, we add some constraints in the user goal: slots are split into two groups. Some of slots must appear in the user goal, we called these elements as Required slots. In the movie-booking scenario, it includes moviename, theater, starttime, date, num- berofpeople; the rest slots are Optional slots, for example, theater chain, video format etc.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning end-to-end goal-oriented dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Subdomain modelling for dialogue management with hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawel</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Casanueva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06210</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards end-to-end reinforcement learning of dialogue agents for information access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="484" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Continuous deep q-learning with model-based acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2829" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning dialogue strategies within the markov decision process framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esther</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Pieraccini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Eckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
<note type="report_type">Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end taskcompletion neural dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 8th International Joint Conference on Natural Language Processing</title>
		<meeting>The 8th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A user simulator for task-completion dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05081</idno>
		<title level="m">Efficient exploration for dialogue policy learning with bbq networks &amp; replay buffer spiking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative policy learning in end-to-end trainable task-oriented neural dialog models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
		<meeting>2017 IEEE Workshop on Automatic Speech Recognition and Understanding</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Representation learning using multi-task deep neural networks for semantic classification and information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-Yi</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1777</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial advantage actor-critic model for taskcompletion dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11277</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06176</idno>
		<title level="m">Integrating planning for task-completion dialogue policy learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2221" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A survey on metrics for the evaluation of user simulations. The knowledge engineering review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Hastie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagination-augmented agents for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théophane</forename><surname>Sébastienracanì Ere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrì</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puigdomènech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5694" to="5705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agenda-based user simulation for bootstrapping a pomdp dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Weilhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2007; Companion Volume, Short Papers</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hado Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dulacarnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barreto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08810</idno>
		<title level="m">The predictron: Endto-end learning and planning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Rojasbarahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsunghsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02689</idno>
		<title level="m">Continuously learning neural dialogue management</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard S Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on machine learning</title>
		<meeting>the seventh international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2154" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Subgoal discovery for hierarchical dialogue policy learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07855</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A networkbased end-to-end trainable task-oriented dialogue system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei-Hao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ultes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="438" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semantically conditioned LSTM-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hybrid code networks: Practical and efficient end-to-end dialog control with supervised and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavosh</forename><surname>Jason D Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pomdp-based statistical spoken dialog systems: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Gaši´gaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1160" to="1179" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxine</forename><surname>Eskenazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02560</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
