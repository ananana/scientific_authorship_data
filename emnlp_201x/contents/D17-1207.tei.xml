<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Earth Mover&apos;s Distance Minimization for Unsupervised Bilingual Lexicon Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
							<email>luanhuanbo@gmail.com, sms@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Jiangsu Collaborative Innovation Center for Language Competence</orgName>
								<address>
									<settlement>Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Earth Mover&apos;s Distance Minimization for Unsupervised Bilingual Lexicon Induction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1934" to="1945"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cross-lingual natural language processing hinges on the premise that there exists in-variance across languages. At the word level, researchers have identified such in-variance in the word embedding semantic spaces of different languages. However , in order to connect the separate spaces, cross-lingual supervision encoded in parallel data is typically required. In this paper, we attempt to establish the cross-lingual connection without relying on any cross-lingual supervision. By viewing word embedding spaces as distributions , we propose to minimize their earth mover&apos;s distance, a measure of divergence between distributions. We demonstrate the success on the unsupervised bilingual lexicon induction task. In addition , we reveal an interesting finding that the earth mover&apos;s distance shows potential as a measure of language difference.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite tremendous variation and diversity, lan- guages are believed to share something in com- mon. Indeed, this belief forms the underlying ba- sis of computational approaches to cross-lingual transfer <ref type="bibr">(Täckström et al., 2013</ref>, inter alia), other- wise it would be inconceivable for the transfer to successfully generalize.</p><p>Linguistic universals manifest themselves at various levels of linguistic units. At the word level, there is evidence that different languages represent concepts with similar structure <ref type="bibr" target="#b65">(Youn et al., 2016)</ref>. Interestingly, as computational mod- els of word semantics, monolingual word embed- dings also exhibit isomorphism across languages ( <ref type="bibr" target="#b43">Mikolov et al., 2013a</ref>). This finding opens up the possibility to use a simple transformation, e.g. a linear map, to connect separately trained word em- beddings cross-lingually. Learning such a trans- formation typically calls for cross-lingual supervi- sion from parallel data <ref type="bibr" target="#b23">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b38">Lu et al., 2015;</ref><ref type="bibr" target="#b56">Smith et al., 2017)</ref>.</p><p>In this paper, we ask the question: Can we un- cover the transformation without any cross-lingual supervision? At first sight, this task appears formidable, as it would imply that a bilingual se- mantic space can be constructed by using mono- lingual corpora only. On the other hand, the ex- istence of structural isomorphism across mono- lingual embedding spaces points to the feasibility of this task: The transformation exists right there only to be discovered by the right tool.</p><p>We propose such a tool to answer the above question in the affirmative. The key insight is to view embedding spaces as distributions, and the desired transformation should make the two dis- tributions close. This naturally calls for a measure of distribution closeness, for which we introduce the earth mover's distance. Therefore, our task can be formulated as the minimization of the earth mover's distance between the transformed source embedding distribution and the target one with re- spect to the transformation. Importantly, the mini- mization is performed at the distribution level, and hence no word-level supervision is required.</p><p>We demonstrate that the earth mover's distance minimization successfully uncovers the transfor- mation for cross-lingual connection, as evidenced by experiments on the bilingual lexicon induction task. In fact, as an unsupervised approach, its per- formance turns out to be highly competitive with supervised methods. Moreover, as an interesting byproduct, the earth mover's distance provides a distance measure that may quantify a facet of lan- guage difference.  <ref type="figure">Figure 1</ref>: An illustration of our earth mover's distance minimization formulation. The subplots on the left schematically visualize Chinese and English embeddings. Due to isomorphism, there exists a simple transformation G that aligns the two embedding spaces well, as shown on the right. We expect to find the transformation G by minimizing the earth mover's distance without the need for cross-lingual word-level supervision, because the earth mover's distance holistically measures the closeness between two sets of weighted points. It computes the minimal cost of transporting one set of points to the other, whose weights are indicated by the sizes of squares and dots. We show the transport scheme in the right subplot with arrows, which can be interpreted as word translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aligning Isomorphic Embeddings</head><p>As discovered by previous work <ref type="bibr" target="#b43">(Mikolov et al., 2013a</ref>), monolingual word embeddings exhibit isomorphism across languages, i.e., they appear similar in structure. However, as they are trained independently, the specific "orientation" of each embedding space is arbitrary, as illustrated in the left part of <ref type="figure">Figure 1</ref>. In order to connect the sep- arate embedding spaces, we can try to transform the source embeddings so that they align well with target ones. Naturally, we need a measure for the quality of the alignment to guide our search for the transformation.</p><p>As we aim to eliminate the need for cross- lingual supervision from word translation pairs, the measure cannot be defined at the word level as in previous work <ref type="bibr" target="#b43">(Mikolov et al., 2013a</ref>). Rather, it should quantify the difference between the entire distributions of embeddings. With this in mind, we find the earth mover's distance to be a suit- able choice ( <ref type="bibr" target="#b67">Zhang et al., 2016b</ref>). Its workings are illustrated in the right part of <ref type="figure">Figure 1</ref>. We can think of target embeddings as piles of earth, and transformed source embeddings as holes to be filled. Then the earth mover's distance computes the minimal cost of moving the earth to fill the holes. Clearly, if the two sets of embeddings align well, the earth mover's distance will be small. Therefore, we can try to find the transformation that minimizes the earth mover's distance.</p><p>Another desirable feature of the earth mover's distance is that the computed transport scheme can be readily interpreted as translations. Moreover, this interpretation naturally handles multiple al- ternative translations. For example, the Chinese word "mao" can be translated to "cat" or "kitten", as shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Form of the Transformation</head><p>The approximate isomorphism across embedding spaces inspires researchers to use a simple form of transformation. For example, <ref type="bibr" target="#b43">Mikolov et al. (2013a)</ref> chose to use a linear transformation, i.e. the transformation G parametrized by a matrix. Later, proposals for using an orthogonal trans- formation are supported empirically ( <ref type="bibr" target="#b64">Xing et al., 2015;</ref><ref type="bibr" target="#b69">Zhang et al., 2016c;</ref><ref type="bibr" target="#b4">Artetxe et al., 2016</ref>) and theoretically ( <ref type="bibr" target="#b56">Smith et al., 2017)</ref>. Indeed, an orthogonal transformation has desirable prop- erties in this setting. If G is an orthogonal matrix that transforms the source embeddings into the tar- get space, then its transpose (also its inverse) G performs transformation in the reverse direction. In that case, any word embedding a can be re- covered by transforming back and forth because G Ga = a. Moreover, computing the cosine sim- ilarity between a source embedding a and a target embedding b will be independent of the semantic space in which the similarity is measured, because</p><formula xml:id="formula_0">b Ga/ Ga b = a G b/ a G b</formula><p>. There- fore we are inclined to use an orthogonal transfor- mation for our task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Earth Mover's Distance</head><p>The earth mover's distance (EMD) is a powerful tool widely used in computer vision and natural language processing <ref type="bibr" target="#b52">(Rubner et al., 1998;</ref><ref type="bibr" target="#b36">Kusner et al., 2015;</ref><ref type="bibr" target="#b33">Huang et al., 2016;</ref><ref type="bibr">Zhang et al., 2016b,a)</ref>. Mathematically speaking, the EMD de- fines a distance between probability distributions. In the discrete case, a probability distribution can be represented by a sum of Dirac delta functions. For a pair of discrete distributions P 1 = i u i δ x i and P 2 = j v j δ y j , the EMD is defined as</p><formula xml:id="formula_1">EMD (P 1 , P 2 ) = min T ∈U (u,v) i j T ij c (x i , y j ) ,</formula><p>(1) where c (x i , y j ) gives the ground distance between x i and y j , and U (u, v) is known as the transport polytope, defined as</p><formula xml:id="formula_2">   T |T ij ≥ 0, j T ij = u i , i T ij = v j , ∀i, j    .</formula><p>(2) After solving the minimization program (1), the transport matrix T stores information of the trans- port scheme: A non-zero T ij indicates the amount of probability mass transported from y j to x i . For our task, this can be interpreted as evidence for word translation ( <ref type="bibr" target="#b67">Zhang et al., 2016b</ref>), as indi- cated by arrows in the right part of <ref type="figure">Figure 1</ref>.</p><p>The EMD is closely related to the Wasserstein distance in mathematics, defined as</p><formula xml:id="formula_3">W (P 1 , P 2 ) = inf γ∈Γ(P 1 ,P 2 ) E (x,y)∼γ [c (x, y)] ,<label>(3)</label></formula><p>where Γ (P 1 , P 2 ) denotes the set of all joint distri- butions γ (x, y) with marginals P 1 and P 2 on the first and second factors respectively. As we can see, the Wasserstein distance generalizes the EMD to allow continuous distributions. In our context, we will use both terms interchangeably. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>In our task, we are interested in a pair of distri- butions of word embeddings, one for the source language and the other for the target language. A source word embedding w S s is a d-dimensional column vector that represents the s-th source word in the V S -sized source language vocabulary. Its distribution is characterized by a positive vector of frequencies f S satisfying</p><formula xml:id="formula_4">V S s=1 f S s = 1, i.e. P w S s = f S s .</formula><p>Notations are similar for the target side. We assume the embeddings are normalized to have unit L 2 norm, which makes no difference to the result as we use cosine to measure semantic similarity.</p><p>Under this setting, we develop two approaches to our EMD minimization idea, called WGAN (Section 3.1) and EMDOT (Section 3.2) respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wasserstein GAN (WGAN)</head><p>Generative adversarial nets (GANs) are originally proposed to generate natural images ( <ref type="bibr" target="#b26">Goodfellow et al., 2014</ref>). They can generate sharp images if trained well, but they are notoriously difficult to train. Therefore, a lot of research efforts have been dedicated to the investigation into stabler train- ing ( <ref type="bibr" target="#b50">Radford et al., 2015;</ref><ref type="bibr" target="#b53">Salimans et al., 2016;</ref><ref type="bibr" target="#b46">Nowozin et al., 2016;</ref><ref type="bibr" target="#b42">Metz et al., 2016;</ref><ref type="bibr" target="#b42">Poole et al., 2016;</ref><ref type="bibr" target="#b2">Arjovsky and Bottou, 2017)</ref>, and the recently proposed Wasserstein GAN ( ) is a promising technique along this line of research.</p><p>While the original GAN is formulated as an ad- versarial game (hence its name), the Wasserstein GAN can be directly understood as minimizing the Wasserstein distance (3). <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the concept in the context of our unsupervised bilingual lexicon induction task. The generator G takes source word embeddings and transforms them, with the goal that the transformed source distribution P G(S) and the target distribution P T should be close as measured by the Wasserstein distance. The critic D takes both transformed source word embeddings and target word embed- dings and attempts to accurately estimate their Wasserstein distance, which will guide the gener- ator during training. The overall objective is</p><formula xml:id="formula_5">min G∈R d×d W P G(S) , P T ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">P G(S) = V S s=1 f S s δ Gw S s and P T = V T t=1 f T t δ w T t</formula><p>are the distributions of transformed source word embeddings and target word embed- dings. Here we do not impose the orthogonal con- straint on G to facilitate the use of a gradient-based optimizer. With the ground distance c being Eu- clidean distance L 2 , the Kantorovich-Rubinstein duality <ref type="bibr" target="#b59">(Villani, 2009)</ref> gives</p><formula xml:id="formula_7">W P G(S) , P T = 1 K sup f L ≤K E y∼P T [f (y)] − E y∼P G(S) [f (y)] ,<label>(5)</label></formula><p>where the supremum is over all K-Lipschitz func- tions f . As neural networks are universal function approximators <ref type="bibr" target="#b32">(Hornik, 1991)</ref>, we can attempt to approximate f with a neural network, called the critic D, with weight clipping to ensure the func- tion family is K-Lipschitz. Therefore the objec- tive of the critic is</p><formula xml:id="formula_8">max D E y∼P T [f D (y)] − E x∼P S [f D (Gx)] . (6)</formula><p>Conceptually, the critic D assigns scores f D to real target embeddings and fake ones generated by the generator G. When the objective (6) is trained until optimality, the difference of the scores will approximate the Wasserstein distance up to a mul- tiplicative constant. The generator G then aims to minimize the approximate distance, which leads to</p><formula xml:id="formula_9">min G∈R d×d −E x∼P S [f D (Gx)] .<label>(7)</label></formula><p>3.2 EMD Minimization Under Orthogonal Transformation (EMDOT)</p><p>Alternative to minimizing the Wasserstein dis- tance by duality, the primal program with the or- thogonal constraint can be formalized as</p><formula xml:id="formula_10">min G∈O(d) EMD P G(S) , P T ,<label>(8)</label></formula><p>where O (d) is the orthogonal group in dimen- sion d. The exact solution to this minimization program is NP-hard ( <ref type="bibr" target="#b15">Ding and Xu, 2016)</ref>. For- tunately, an alternating minimization procedure is guaranteed to converge to a local minimum <ref type="bibr" target="#b9">(Cohen and Guibas, 1999</ref>). Starting from an initial matrix G (0) , we alternate between the following subprograms repeatedly:</p><formula xml:id="formula_11">T (k) = arg min T ∈U (f S ,f T ) V S s=1 V T t=1 T st c G (k) w S s , w T t ,<label>(9)</label></formula><formula xml:id="formula_12">G (k+1) = arg min G∈O(d) V S s=1 V T t=1 T (k) st c Gw S s , w T t .</formula><p>(10) The minimization in <ref type="formula" target="#formula_11">(9)</ref> is the EMD program (1), with existing solvers available. For better scal- ability, we choose an approximate solver <ref type="bibr" target="#b11">(Cuturi, 2013)</ref>.</p><p>The minimization in (10) aims to find the trans- formation G (k+1) with cross-lingual connection provided in T (k) . This is exactly the supervised scenario, and previous works typically resort to gradient-based solvers ( <ref type="bibr" target="#b43">Mikolov et al., 2013a</ref>). But they can be cumbersome especially as we impose the orthogonal constraint on G. Fortu- nately, if we choose the ground distance c to be the squared Euclidean distance L 2 2 , the program (10) is an extension of the orthogonal Procrustes prob- lem <ref type="bibr" target="#b54">(Schönemann, 1966)</ref>, which admits a closed- form solution:</p><formula xml:id="formula_13">G (k+1) = U V ,<label>(11)</label></formula><p>where U and V are obtained from a singular value decomposition (SVD):</p><formula xml:id="formula_14">V S s=1 V T t=1 T (k) st w T t w S s = U SV .<label>(12)</label></formula><p>Note that the SVD is efficient because it is per- formed on a d × d matrix, which is typically low- dimensional. Choosing c = L 2 2 is also motivated by its equivalence to the cosine dissimilarity, as proved in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>Starting from the idea of earth mover's distance minimization, we have developed two approaches towards the goal. They employ different optimiza- tion techniques, which in turn lead to different practical choices. For example, we choose c = L 2 2 for the EMDOT approach to obtain a closed-form solution to the subprogram (10), otherwise we would have to use gradient-based solvers. In con- trast, the WGAN approach calls for c = L 2 be- cause the Kantorovich-Rubinstein duality takes a simple form only in this case.</p><p>The EMDOT approach is attractive for sev- eral reasons: It is consistent for training and test- ing (the equivalence between the ground distance c = L 2 2 and cosine dissimilarity), compatible with the orthogonal constraint, mathematically sound (without much assumption and approximation), guaranteed to converge, almost hyperparameter free, and fast in speed (the alternating subpro- grams have either effective approximate solvers or closed-form solutions). However, it suffers from a serious limitation: The alternating minimization procedure only converges to local minima, and they often turn out to be rather poor in practice.</p><p>Although the WGAN approach employs a stochastic-gradient-based optimizer (RMSProp) and does not guarantee global optima either, it works reasonably well in practice. It seems better at exploring the parameter space and finally land- ing in a neighborhood of a good optimum. Like other success stories of using stochastic-gradient- based optimizers to train neural networks, theoret- ical understanding of the behavior remains elusive.</p><p>We can enjoy the best of both worlds by incor- porating the merits of both approaches: First the WGAN approach locates a good neighborhood of the parameter space, and then, starting from a rea- sonable initialization, the EMDOT approach effi- ciently explores the neighborhood to achieve en- hanced performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We first investigate the learning behavior of our WGAN approach, and then present experiments on the bilingual lexicon induction task, followed by a showcase of the earth mover's distance as a language distance measure. Details of the data sets and hyperparameters are described in Appendices B and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Behavior of WGAN</head><p>We analyze the learning behavior of WGAN by looking at a typical training trajectory on Chinese- English. During training, we save 100 mod- els, translate based on the nearest neighbor, and record their accuracy as the bilingual lexicon in- duction performance indicator at these training checkpoints. In theory, the critic objective (6) pro- vides an estimate of the Wasserstein distance up to a multiplicative constant, and a smaller Wasser- stein distance should mean the transformed source embedding space and the target embedding space align better, which should in turn result in a better bilingual lexicon. This is validated in <ref type="figure" target="#fig_2">Figure 3</ref> by the correlation between Wasserstein estimate and accuracy. Therefore, the Wasserstein estimate can serve as an indicator for the bilingual lexicon in- duction performance, and we can save the model with the lowest value during training as the final model. In <ref type="figure" target="#fig_2">Figure 3</ref>, we also plot the value of G G − I F , which indicates the degree of or- thogonality of the transformation matrix G. Inter- estingly, this also correlates nicely with the other curves, even though our WGAN formulation does not encourage G towards orthogonality. This find- ing confirms that a good transformation matrix is indeed close to orthogonality, and empirically jus- tifies the orthogonal constraint for the EMDOT formulation.</p><p>Finally, we observe that the curves in <ref type="figure" target="#fig_2">Fig- ure 3</ref> are not very smooth. This means that al- though WGAN does well in exploring the param- eter space and locating a reasonable transforma-method # seeds zh-en es-en it-en ja-zh tr-en   <ref type="table">Table 1</ref>: F 1 scores for bilingual lexicon induction on Chinese-English, Spanish-English, Italian-English, Japanese-Chinese, and Turkish-English. The supervised methods TM and IA require seeds to train, and are listed for reference. Our EMDOT approach is initialized with the transformation found by WGAN, and consistently improves on it, reaching competitive performance with supervised methods. tion matrix, it cannot stably refine the transforma- tion. Fortunately, this is where EMDOT thrives, and hence combining them enjoys the benefits of both approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Bilingual Lexicon Induction Performance</head><p>We test the quality of the cross-lingual transfor- mation by evaluating on the bilingual lexicon induction task for five language pairs: Chinese- English, Spanish-English, Italian-English, Japanese-Chinese, and Turkish-English.</p><p>As the EMD automatically handles multiple al- ternative translations, we follow ( <ref type="bibr" target="#b67">Zhang et al., 2016b</ref>,a) to use F 1 score as the preferred evalu- ation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>Our formulation is based on the isomorphism found across monolingual word embeddings. This idea has led to previous supervised methods:</p><p>• Although they need seed word translation pairs to train and thus not directly comparable to our sys- tem, we nonetheless report their results using {50, 100, 200, 500} seeds for a ballpark range of ex- pected performance on this task, and skip the set of 500 seeds when testing all systems. We en- sure the same input embeddings for these meth- ods and ours. Their seeds are obtained through Google Translate (details in Appendix B.2). We apply the EMD as a postprocessing step ( <ref type="bibr" target="#b67">Zhang et al., 2016b</ref>) to allow them to handle multiple al- ternative translations. This is also done for our WGAN approach, as it does not produce the trans- port scheme to interpret as translation due to its duality formulation. <ref type="table">Table 1</ref> shows the F 1 scores on the five language pairs. As we can see, WGAN successfully finds a transformation that produces reasonable word translations. On top of that, EMDOT consider- ably improves the performance, which indicates that EMDOT refines the transformation found by WGAN. Similar behavior across language pairs proves the generality of our approaches, as they build on embeddings learned from monolingual corpora without language-specific engineering. The qual- ity of the embeddings, thus, will have an impor- tant effect on the performance, which may explain the lower scores on Turkish-English, as this low- resource setting may lack sufficient data to pro- duce reliable embeddings. Higher noise levels in the preprocessing and ground truth for this lan-zh-en es-en it-en ja-zh tr-en EMD 0.650 0.445 0.559 0.599 0.788 typology dissimilarity 0.467 0.342 0.259 0.433 0.541 geographical distance (km) 8161 1246 1464 2095 2854 <ref type="table">Table 2</ref>: The earth mover's distance (EMD), typology dissimilarity, and geographical distance for Chinese-English, Spanish-English, Italian-English, Japanese-Chinese, and Turkish-English. The EMD shows correlation with both factors of linguistic difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>guage pair (cf. the supplemental material), as well as the morphological richness of Turkish, may also be contributing factors to the relatively low scores.</p><p>Concerning the supervised methods TM and IA, they attain better performance with more supervi- sion from seeds, as expected. For TM in particu- lar, hundreds of seeds are needed for generaliza- tion, in line with the finding in <ref type="bibr" target="#b60">(Vuli´cVuli´c and Korhonen, 2016)</ref>. Below that threshold, its performance drops dramatically, and this is when IA fares bet- ter with the orthogonal constraint. This indicates the importance of orthogonality when the seeds are few, or even zero as faced by our system. As the number of seeds increases, the performance of the supervised methods converges to a level com- parable to our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The EMD as Language Distance</head><p>As our system minimizes the earth mover's dis- tance between embeddings of two languages, we show here the final EMD can indicate the degree of difference between languages, serving as a proxy for language distance. <ref type="table">Table 2</ref> lists the EMD for the five language pairs considered in this paper, as well as their typology dissimilarity and geograph- ical distance. The typology dissimilarity is com- puted from features in the WALS database <ref type="bibr" target="#b20">(Dryer and Haspelmath, 2013)</ref>. It is defined as one minus relative Hamming similarity, which is in turn de- fined as the number of agreeing features divided by the number of total features available for the language pair <ref type="bibr" target="#b0">(Albu, 2006;</ref><ref type="bibr" target="#b14">Cysouw, 2013b)</ref>. As a rough approximation, the geographical distance is measured by the distance between the capital cities of the countries where the considered lan- guages are spoken ( <ref type="bibr" target="#b22">Eger et al., 2016)</ref>.</p><p>The typology dissimilarity reflects genealogical influence on the divergence between languages, while the geographical distance indicates the ef- fect of language contact. Both play important roles in shaping the languages we perceive today, and they also correlate with each other <ref type="bibr" target="#b13">(Cysouw, 2013a</ref>). As we analyze <ref type="table">Table 2</ref>, we find the EMD may be explained by both factors. Spanish- English and Italian-English are close both ge- nealogically and geographically, and their EMD values are the lowest. English, Chinese, and Japanese belong to different language families, but the geographical proximity of the latter two en- ables intensive language contact, especially for the vocabularies, causing relatively smaller EMD. Fi- nally, Turkish and English are distant in both as- pects, and the EMD between them is large. Note that, however, the large EMD may also be caused by the relatively poor quality of monolingual em- beddings due to low resource, and this should be a caveat of using the EMD to measure language distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bilingual Lexicon Induction</head><p>Bilingual lexicon induction is a long-standing re- search task in cross-lingual natural language pro- cessing. Traditional methods build statistical mod- els for monolingual word co-occurrence, and com- bine cross-lingual supervision to solve the task. As word alignment for parallel sentences can produce fairly good bilingual lexica <ref type="bibr" target="#b47">(Och and Ney, 2003)</ref>, these methods focus on non-parallel data with a seed lexicon as cross-lingual supervision <ref type="bibr" target="#b51">(Rapp, 1999;</ref><ref type="bibr" target="#b25">Gaussier et al., 2004</ref>).</p><p>An exception that does not rely on cross-lingual supervision is the decipherment approach ( <ref type="bibr">Knight, 2012, 2013;</ref><ref type="bibr" target="#b19">Dou et al., 2015)</ref>. It views the source language as a cipher for the target lan- guage, and solves a statistical model that attempts to decipher the source language.</p><p>Following the popularity of monolingual word embeddings, cross-lingual word representation learning has also attracted significant attention in recent years. Building bilingual lexica from the learned cross-lingual embeddings is often consid- ered an evaluative tool. Most methods rely on su- pervision encoded in parallel data, at the document level <ref type="bibr" target="#b62">(Vuli´cVuli´c and Moens, 2015)</ref>, the sentence level ( <ref type="bibr" target="#b70">Zou et al., 2013;</ref><ref type="bibr" target="#b8">Chandar A P et al., 2014;</ref><ref type="bibr" target="#b35">Hermann and Blunsom, 2014;</ref><ref type="bibr" target="#b35">Kočisk´Kočisk´y et al., 2014;</ref><ref type="bibr" target="#b39">Luong et al., 2015;</ref><ref type="bibr" target="#b10">Coulmance et al., 2015;</ref><ref type="bibr" target="#b48">Oshikiri et al., 2016)</ref>, or the word level (i.e. in the form of seed lexicon) ( <ref type="bibr" target="#b28">Gouws and Søgaard, 2015;</ref><ref type="bibr" target="#b63">Wick et al., 2016;</ref><ref type="bibr" target="#b21">Duong et al., 2016;</ref><ref type="bibr" target="#b55">Shi et al., 2015;</ref><ref type="bibr" target="#b43">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b23">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b38">Lu et al., 2015;</ref><ref type="bibr" target="#b1">Ammar et al., 2016;</ref><ref type="bibr" target="#b66">Zhang et al., 2016a</ref><ref type="bibr" target="#b68">Zhang et al., , 2017</ref><ref type="bibr" target="#b56">Smith et al., 2017)</ref>.</p><p>There is a recent work that aims to remove the need for cross-lingual supervision <ref type="bibr" target="#b7">(Cao et al., 2016)</ref>. Similar to ours, the underlying idea is to match cross-lingually at the level of distribution rather than word. However, the distributions con- sidered in that work are the hidden states of neu- ral embedding models during the course of train- ing. They are assumed to be Gaussian, so that the matching of distributions reduces to matching their means and variances, but this assumption is hard to justify and interpret. In contrast, our pro- posal does not make any assumption on the dis- tributions, and directly matches the transformed source embedding distribution with the target dis- tribution by minimizing their earth mover's dis- tance.</p><p>Another attempt to learn cross-lingual em- bedding transformation without supervision is <ref type="bibr" target="#b6">(Barone, 2016)</ref>.</p><p>Architectures of generative adversarial nets and adversarial autoencoders <ref type="bibr" target="#b40">(Makhzani et al., 2015</ref>) are experimented, but the reported results are not positive. We tried the publicly available code on our data and ob- tained negative results as well. This outcome is likely caused by the training difficulty pointed out by <ref type="bibr" target="#b2">(Arjovsky and Bottou, 2017)</ref>, as traditional GAN training minimizes Jensen-Shannon diver- gence between distributions, which can provide pathological gradient to the generator and ham- per its learning. The use of Wasserstein GAN ad- dresses this problem and allows our simple archi- tecture to be trained successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Language Distance</head><p>Quantifying language difference is an open ques- tion with on-going efforts that put forward better measures based on manually compiled data <ref type="bibr" target="#b0">(Albu, 2006;</ref><ref type="bibr" target="#b30">Hammarström and O'Connor, 2013)</ref>. Re- searchers in computational linguistics also try to contribute corpus-based approaches to this ques- tion. Parallel data is typically exploited, and ideas range from information-theoretic <ref type="bibr" target="#b34">(Juola, 1998)</ref>, statistical <ref type="bibr" target="#b41">(Mayer and Cysouw, 2012)</ref>, to graph- based ( <ref type="bibr" target="#b22">Eger et al., 2016;</ref><ref type="bibr" target="#b5">Asgari and Mofrad, 2016)</ref>. To our knowledge, the earth mover's dis- tance is proposed for language distance for the first time, with the distinctive feature of relying on non- parallel data only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Earth Mover's Distance</head><p>First introduced into computer vision <ref type="bibr" target="#b52">(Rubner et al., 1998)</ref>, the earth mover's distance also finds application in natural language processing <ref type="bibr" target="#b36">(Kusner et al., 2015;</ref><ref type="bibr" target="#b33">Huang et al., 2016)</ref>, in- cluding bilingual lexicon induction ( <ref type="bibr">Zhang et al., 2016b,a)</ref>. <ref type="bibr" target="#b67">Zhang et al. (2016b)</ref> build upon bilin- gual word embeddings and apply the EMD pro- gram as a postprocessing step to automatically produce multiple alternative translations. Later, <ref type="bibr" target="#b66">Zhang et al. (2016a)</ref> introduce the EMD into the training objective of bilingual word embeddings as a regularizer. These previous works rely on cross- lingual supervision, and do not approach the task from the view of embedding transformation, while our work formulates the task as EMD minimiza- tion to allow zero supervision.</p><p>Apart from the usage as a regularizer (Zhang et al., 2016a), the EMD can also play other roles in optimization programs designed for various appli- cations <ref type="bibr" target="#b12">(Cuturi and Doucet, 2014;</ref><ref type="bibr" target="#b24">Frogner et al., 2015;</ref><ref type="bibr" target="#b45">Montavon et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work, we attack the problem of find- ing cross-lingual transformation between mono- lingual word embeddings in a purely unsuper- vised setting. We introduce earth mover's dis- tance minimization to tackle this task by exploit- ing its distribution-level matching to sidestep the requirement for word-level cross-lingual supervi- sion. Even though zero supervision poses a clear challenge, our system attains competitive perfor- mance with supervised methods for bilingual lex- icon induction. In addition, the earth mover's dis- tance provides a natural measure that may prove helpful for quantifying language difference.</p><p>We have implemented the earth mover's dis- tance minimization framework from two paths, and their combination has worked well, but both can be potentially improved by recent advances in optimization techniques ( <ref type="bibr" target="#b29">Gulrajani et al., 2017;</ref><ref type="bibr" target="#b15">Ding and Xu, 2016)</ref>. Future work should also eval- uate the earth mover's distance between more lan- guages to assess its quality as language distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof</head><p>The following proof shows that using squared Eu- clidean distance as the ground distance (c = L 2 2 ) is equivalent to using cosine dissimilarity when min- imizing Equation (10).</p><formula xml:id="formula_15">min G∈O(d) V S s=1 V T t=1 T (k) st Gw S s − w T t 2 = min G∈O(d) V S s=1 V T t=1 T (k) st w S s 2 + w T t 2 − 2w T t Gw S s = min G∈O(d) −2 V S s=1 V T t=1 T (k) st cos Gw S s , w T t + const = min G∈O(d) − V S s=1 V T t=1 T (k) st cos Gw S s , w T t .<label>(13)</label></formula><p>B Data Preparation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Non-Parallel Corpora for Training Embeddings</head><p>The data for training monolingual word embed- dings comes from Wikipedia comparable cor- pora. <ref type="bibr">2</ref> Following <ref type="bibr" target="#b61">(Vuli´cVuli´c and Moens, 2013)</ref>, we retain only nouns with at least 1,000 occurrences except for Turkish-English, whose frequency cut- off threshold is 100, as the amount of data is rela- tively small in this low-resource setting. For the Chinese side, we first use OpenCC 3 to normal- ize characters to be simplified, and then perform Chinese word segmentation and POS tagging with THULAC. <ref type="bibr">4</ref> The preprocessing of the English side involves tokenization, POS tagging, lemmatiza- tion, and lowercasing, which we carry out with the NLTK toolkit <ref type="bibr">5</ref>    <ref type="bibr" target="#b57">Strassel and Tracey, 2016)</ref>, and its English side is preprocessed by NLTK. The statistics of the preprocessed corpora is given in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Seed Word Translation Pairs</head><p>The seed word translation pairs for the transla- tion matrix (TM) and isometric alignment (IA) approaches are obtained as follows. First, we ask Google Translate 8 to translate the source lan- guage vocabulary. Then the target translations are queried again and translated back to the source language, and those that do not match the original source words are discarded. This helps to ensure the translation quality. Finally, the translations are discarded if they fall out of our target language vo- cabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Ground Truth</head><p>As the ground truth bilingual lexicon for evalu- ation, we use Chinese-English Translation Lexi- con Version 3.0 (LDC2002L27) for the Chinese- English pair. For Spanish-English and Italian- English, we access Open Multilingual WordNet 9 through NLTK. For Japanese-Chinese, we use an in-house lexicon. For Turkish-English, we build a set of ground truth translation pairs in the same way as how we obtain seed word translation pairs from Google Translate, described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 WGAN</head><p>We parametrize the critic D as a feed-forward neu- ral network with one hidden layer of 500 neurons. The generator G is initialized with a random or- thogonal matrix. The expectations in critic and generator objectives (6)(7) are approximated by minibatches of 1024 samples. We train for 10 7 minibatches. Most other hyperparameters follow from ( ) except the learning rates, for which larger values of 0.05 and 0.0005 are used for the generator and the critic respec- tively for faster convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EMDOT</head><p>The approximate EMD solver <ref type="bibr" target="#b11">(Cuturi, 2013)</ref> gives fairly accurate approximation with orders of mag- nitude speedup. However, it makes the transport matrix T no longer sparse. This is problematic, as we rely on interpreting a non-zero T st as evidence to translate the s-th source word to the t-th target word ( <ref type="bibr" target="#b67">Zhang et al., 2016b</ref>). We therefore retain the largest pV S elements of T , where p encodes our belief of the expected number of translations a source word can have. We set p = 1.3. The alternating minimization procedure con- verges very fast. We run 10 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Monolingual Word Embeddings</head><p>As input monolingual word embeddings to the tested systems, we train the CBOW model ( <ref type="bibr" target="#b44">Mikolov et al., 2013b</ref>) with default hyperparam- eters in word2vec 10 . The embedding dimension d is 50.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Wasserstein GAN for unsupervised bilingual lexicon induction. The generator G transforms the source word embeddings into the target space. The critic D takes both sets of embeddings and tries to estimate their Wasserstein distance, and this information will be passed to the generator G during training to guide it towards minimizing the Wasserstein estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A typical training trajectory of WGAN. The three curves all correlate well. The Wasserstein estimate is rescaled because its magnitude is irrelevant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>TM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of the non-parallel corpora 
for training monolingual word embeddings. Lan-
guage codes: zh = Chinese, en = English, es = 
Spanish, it = Italian, ja = Japanese, tr = Turkish. 

and Moens, 2013). For the Japanese corpus, we 
use MeCab 7 for word segmentation and POS tag-
ging. For Turkish, we utilize the preprocessing 
tools (tokenization and POS tagging) provided in 
LORELEI Language Packs (</table></figure>

			<note place="foot" n="2"> http://linguatools.org/tools/corpora/wikipediacomparable-corpora 3 https://github.com/BYVoid/OpenCC 4 http://thulac.thunlp.org 5 http://www.nltk.org 6 http://www.cis.uni-muenchen.de/˜schmid/tools/ TreeTagger</note>

			<note place="foot" n="7"> http://taku910.github.io/mecab 8 https://translate.google.com 9 http://compling.hss.ntu.edu.sg/omw</note>

			<note place="foot" n="10"> https://code.google.com/archive/p/word2vec</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments. This work is supported by the National Natural Science Foundation of China (No. 61522204), the 973 Program (2014CB340501), and the National Natural Sci-ence Foundation of China (No. 61331013). This research is also supported by the Singapore Na-tional Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantitative analyses of typological data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Albu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Univ. Leipzig</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively Multilingual Word Embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards Principled Methods For Training Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. 2017. Wasserstein GAN</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparing Fifty Natural Languages and Twelve Genetic Languages Using Word Embedding Language Divergence (WELD) as a Quantitative Measure of Language Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP</title>
		<meeting>the Workshop on Multilingual and Cross-lingual Methods in NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards crosslingual distributed representations without parallel text trained with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio Valerio Miceli</forename><surname>Barone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Distribution-based Model to Learn Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Autoencoder Approach to Learning Bilingual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance Under Transformation Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transgram, Fast Cross-lingual Word-embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jocelyn</forename><surname>Coulmance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amine</forename><surname>Benhalloum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In EMNLP</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sinkhorn Distances: Lightspeed Computation of Optimal Transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast Computation of Wasserstein Barycenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Disentangling geography from genealogy. Space in language and linguistics: Geographical, interactional, and cognitive perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Predicting language learning difficulty. Approaches to measuring linguistic differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FPTAS for Minimizing the Earth Mover&apos;s Distance Under Rigid Transformations and Related Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Algorithmica</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving Zero-Shot Learning by Mitigating the Hubness Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large scale decipherment for out-of-domain machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DependencyBased Decipherment for Resource-Limited Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unifying Bayesian Inference and Vector Space Models for Improved Decipherment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">WALS Online. Max Planck Institute for Evolutionary Anthropology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Haspelmath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Leipzig</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Crosslingual Word Embeddings without Bilingual Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language classification from bilingual word embedding graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Hoenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving Vector Space Word Representations Using Multilingual Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning with a Wasserstein Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matveeva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dejean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Nets. In NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple task-specific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
	</analytic>
	<monogr>
		<title level="j">Improved Training of Wasserstein GANs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dependency-sensitive typological distance. Approaches to measuring linguistic differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hammarström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loretta O&amp;apos;</forename><surname>Connor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multilingual Distributed Representations without Word Alignment</title>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Karl Moritz Hermann and Phil Blunsom</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised Word Mover&apos;s Distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cross-Entropy and Linguistic Typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Juola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Bilingual Word Representations by Marginalizing Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From Word Embeddings To Document Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep Multilingual Correlation for Improved Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<editor>NAACLHLT</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bilingual Word Representations with Monolingual Quality in Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial Autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Language comparison through sparse multilingual word alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cysouw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</title>
		<meeting>the EACL 2012 Joint Workshop of LINGVIS &amp; UNCLH</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohldickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Exploiting Similarities among Languages for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wasserstein Training of Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00709</idno>
		<title level="m">Training Generative Neural Samplers using Variational Divergence Minimization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cross-Lingual Word Representations via Spectral Graph Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takamasa</forename><surname>Oshikiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02780</idno>
		<idno>ArXiv: 1612.02780</idno>
		<title level="m">Jascha SohlDickstein, and Anelia Angelova. 2016. Improved generator objectives for GANs</title>
		<imprint/>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automatic Identification of Word Translations from Unrelated English and German Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Metric for Distributions with Applications to Image Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A generalized solution of the orthogonal procrustes problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schönemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning Cross-lingual Word Embeddings via Matrix Co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">LORELEI Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Optimal Transport: Old and New</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">CrossLingual Semantic Similarity of Words as the Similarity of Their Semantic Word Responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings from Non-Parallel DocumentAligned Data Applied to Bilingual Lexicon Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Minimally-Constrained Multilingual Embeddings via Artificial Code-Switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallika</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pocock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">On the universal structure of human lexical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyejin</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">F</forename><surname>Wilkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Maddieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bhattacharya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Proceedings of the National Academy of Sciences</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Inducing Bilingual Lexica From Non-Parallel Data With Earth Mover&apos;s Distance Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Building Earth Mover&apos;s Distance on Bilingual Word Embeddings for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Izuha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Bilingual Lexicon Induction From Non-Parallel Data With Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Ten Pairs to Tag-Multilingual POS Tagging via Coarse Mapping between Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gaddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings for Phrase-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
