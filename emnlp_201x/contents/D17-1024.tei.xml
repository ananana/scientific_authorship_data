<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dict2vec : Learning Word Embeddings using Lexical Dictionaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Tissier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab Hubert Curien</orgName>
								<orgName type="laboratory" key="lab2">UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ. Lyon</orgName>
								<orgName type="institution" key="instit2">UJM Saint-Etienne CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab Hubert Curien</orgName>
								<orgName type="laboratory" key="lab2">UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ. Lyon</orgName>
								<orgName type="institution" key="instit2">UJM Saint-Etienne CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Lab Hubert Curien</orgName>
								<orgName type="laboratory" key="lab2">UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ. Lyon</orgName>
								<orgName type="institution" key="instit2">UJM Saint-Etienne CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dict2vec : Learning Word Embeddings using Lexical Dictionaries</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="254" to="263"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Learning word embeddings on large unla-beled corpus has been shown to be successful in improving many natural language tasks. The most efficient and popular approaches learn or retrofit such representations using additional external data. Resulting embeddings are generally better than their corpus-only counterparts, although such resources cover a fraction of words in the vocabulary. In this paper, we propose a new approach, Dict2vec, based on one of the largest yet refined datasource for describing words-natural language dictionaries. Dict2vec builds new word pairs from dictionary entries so that semantically-related words are moved closer, and negative sampling filters out pairs whose words are unrelated in dictionaries. We evaluate the word representations obtained using Dict2vec on eleven datasets for the word similarity task and on four datasets for a text classification task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning word embeddings usually relies on the distributional hypothesis -words appearing in similar contexts must have similar meanings, and thus close representations. Finding such represen- tations for words and sentences has been one hot topic over the last few years in Natural Language Processing (NLP) ( <ref type="bibr" target="#b19">Mikolov et al., 2013;</ref><ref type="bibr" target="#b23">Pennington et al., 2014</ref>) and has led to many improvements in core NLP tasks such as Word Sense Disam- biguation ( <ref type="bibr" target="#b14">Iacobacci et al., 2016)</ref>, Machine Trans- lation ( <ref type="bibr" target="#b6">Devlin et al., 2014</ref>), Machine Comprehen- sion ( <ref type="bibr" target="#b13">Hewlett et al., 2016)</ref>, and Semantic Role La- beling ( <ref type="bibr" target="#b33">Zhou and Xu, 2015;</ref><ref type="bibr" target="#b5">Collobert et al., 2011</ref>) -to name a few.</p><p>These methods suffer from a classic drawback of unsupervised learning: the lack of supervision between a word and those appearing in the associ- ated contexts. Indeed, it is likely that some terms of the context are not related to the considered word. On the other hand, the fact that two words do not appear together -or more likely, not often enough together -in any context of the training corpora is not a guarantee that these words are not semantically related. Recent approaches have pro- posed to tackle this issue using an attentive model for context selection ( <ref type="bibr" target="#b17">Ling et al., 2015)</ref>, or by us- ing external sources -like knowledge graphs - in order to improve the embeddings ( ). Similarities derived from such resources are part of the objective function during the learn- ing phase ( <ref type="bibr" target="#b32">Yu and Dredze, 2014;</ref><ref type="bibr" target="#b16">Kiela et al., 2015)</ref> or used in a retrofitting scheme <ref type="bibr" target="#b7">(Faruqui et al., 2015)</ref>. These approaches tend to specialize the embeddings to the resource used and its asso- ciated similarity measures -while the construction and maintenance of these resources are a set of complex, time-consuming, and error-prone tasks.</p><p>In this paper, we propose a novel word em- bedding learning strategy, called Dict2vec, that leverages existing online natural language dictionaries. We assume that dictionary entries (a definition of a word) contain latent word similar- ity and relatedness information that can improve language representations. Such entries provide, in essence, an additional context that conveys general semantic coverage for most words. Dict2vec adds new co-occurrences information based on the terms occurring in the definitions of a word. This information introduces weak supervi- sion that can be used to improve the embeddings. We can indeed distinguish word pairs for which each word appears in the definition of the other (strong pairs) and pairs where only one appears in the definition of the other (weak pairs) -each having their own weight as two hyperparameters. Not only this information is useful at learning time to control words vectors to be close for such word pairs, but also it becomes possible to devise a controlled negative sampling. Controlled negative sampling as introduced in Dict2vec consists in filtering out random negative examples in conventional negative sampling that forms a (strong or weak) pair with the target word -they are obviously non-negative examples. Processing online dictionaries in Dict2vec does not require a human-in-the-loop -it is fully automated. The neural network architecture from Dict2vec (Section 3) extends Word2vec ( <ref type="bibr" target="#b19">Mikolov et al., 2013</ref>) approach which uses a Skip-gram model with negative sampling.</p><p>Our main results are as follows :</p><p>• Dict2vec exhibits a statistically significant improvement around 12.5% against state-of- the-art solutions on eleven most common evaluation datasets for the word similarity task when embeddings are learned using the full Wikipedia dump.</p><p>• This edge is even more significant for small training datasets (50 millions first tokens of Wikipedia) than using the full dataset, as the average improvement reaches 30%.</p><p>• Since Dict2vec does significantly better than competitors for small dimensions (in the <ref type="bibr">[20; 100]</ref> range) for small corpus, it can yield smaller yet efficient embeddings -even when trained on smaller corpus -which is one of the utmost practical interest for the working natural language processing practitioners.</p><p>• We also show that the embeddings learned by Dict2vec perform similarly to other base- lines on an extrinsic text classification task.</p><p>Dict2vec software is an extension and an opti- mization from the original Word2vec framework leading to a more efficient learning. Source code to fetch dictionaries, train Dict2vec models and evaluate word embeddings are publicly availabe 1 and can be used by the community as a seed for future works.</p><p>The paper is organized as follows. Section 2 presents related works, along with a special fo- cus on Word2vec, which we later derive in our 1 https://github.com/tca19/dict2vec approach presented in Section 3. Our experimen- tal setup and evaluation settings are introduced in Section 4 and we discuss the results in Section 5. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning Word Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Neural Network Approach</head><p>In the original model from <ref type="bibr" target="#b4">Collobert and Weston (2008)</ref>, a window approach was used to feed a neural network and learn word embeddings. Since there are long-range relations between words, the window-based approach was later extended to a sentence-based approach <ref type="bibr" target="#b5">(Collobert et al., 2011</ref>) leading to capture more semantic similarities into word vectors. Recurrent neural networks are another way to exploit the context of a word by considering the sequence of words preceding it ( <ref type="bibr" target="#b20">Mikolov et al., 2010;</ref><ref type="bibr" target="#b27">Sutskever et al., 2011</ref>). Each neuron receives the current window as an in- put, but also its own output from the previous step. <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref> introduced the Skip-gram architecture built on a single hidden layer neural network to learn efficiently a vector representa- tion for each word w of a vocabulary V from a large corpora of size C. Skip-gram iterates over all (target, context) pairs (w t ,w c ) from every win- dow of the corpus and tries to predict w c knowing w t . The objective function is therefore to maxi- mize the log-likelihood :</p><formula xml:id="formula_0">C t=1 n k=−n log p(w t+k |w t )<label>(1)</label></formula><p>where n represents the size of the window (com- posed of n words around the central word w t ) and the probability can be expressed as :</p><formula xml:id="formula_1">p(w t+k |w t ) = e v t+k ·vt w∈V e v·vt<label>(2)</label></formula><p>with v t+k (resp. v t ) the vector associated to w t+k (resp. w t ). This model relies on the principle "You shall know a word by the company it keeps" -Firth <ref type="bibr">(1957)</ref>. Thus, words that are frequent within the context of the target word will tend to have close representations, as the model will update their vectors so that they will be closer. Two main drawbacks can be said about this approach. First, words within the same window are not al- ways related. Consider the sentence "Turing is widely considered to be the father of theoretical computer science and artificial intelligence." 2 , the words (Turing,widely) and (father,theoretical) will be moved closer while they are not semantically related. Second, strong semantic relations be- tween words (like synonymy or meronymy) hap- pens rarely within the same window, so these rela- tions will not be well embedded into vectors. fastText introduced in  uses internal additional information from the cor- pus to solve the latter drawback. They train a Skip- gram architecture to predict a word w c given the central word w t and all the n-grams G wt (subwords of 3 up to 6 letters) of w t . The objective function becomes :</p><formula xml:id="formula_2">C t=1 n k=−n w∈Gw t log p(w t+k |w)<label>(3)</label></formula><p>Along learning one vector per word, fastText also learns one vector per n-gram. fastText is able to extract more semantic relations between words that share common n-gram(s) (like fish and fish- ing) which can also help to provide good embed- dings for rare words since we can obtain a vector by summing vectors of its n-grams.</p><p>In what follows, we report related works that leverage external resources in order to address the two raised issues about the window approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using External Resources</head><p>Even with larger and larger text data available on the Web, extracting and encoding every linguis- tic relations into word embeddings directly from corpora is a difficult task. One way to add more relations into embeddings is to use external data. Lexical databases like WordNet or sets of syn- onyms like MyThes thesaurus can be used during learning or in a post-processing step to specialize word embeddings. For example, <ref type="bibr" target="#b32">Yu and Dredze (2014)</ref> include prior knowledge about synonyms from WordNet and the Paraphrase Database in a joint model built upon Word2vec. <ref type="bibr" target="#b7">Faruqui et al. (2015)</ref> introduce a graph-based retrofitting method where they post-process learned vectors with re- spect to semantic relationships extracted from ad- ditional lexical resources. <ref type="bibr" target="#b16">Kiela et al. (2015)</ref> propose to specialize the embeddings either on similarity or relatedness relations in a Skip-gram joint learning approach by adding new contexts from external thesaurus or from a norm associa- tion base in the function to optimize.  combine several sources (syllables, POS tags, antonyms/synonyms, Freebase relations) and incorporate them into a CBOW model. These ap- proaches have generally the objective to improve tasks such as document classification, synonym detection or word similarity. They rely on ad- ditional resources whose construction is a time- consuming and error-prone task and tend gener- ally to specialize the embeddings to the external corpus used. Moreover, lexical databases contain less information than dictionaries (117k entries in WordNet, 200k in a dictionary) and less accurate content (some different words in WordNet belong to the same synset thus have the same definition).</p><p>Another type of external resources are knowl- edge bases, containing triplets. Each triplet links two entities with a relation, for example Paris - is capital of -France. Several methods ( <ref type="bibr" target="#b29">Weston et al., 2013;</ref><ref type="bibr" target="#b30">Xu et al., 2014</ref>) have been proposed to use the information from knowledge base to improve semantic relations in word embeddings, and extract more easily rela- tional facts from text. These approaches are fo- cused on knowledge base dependent task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dict2vec</head><p>The definition of a word is a group of words or sentences explaining its meaning. A dictionary is a set of tuples (word, definition) for several words. For example, one may find in a dictionary : car: A road vehicle, typically with four wheels, powered by an internal combus- tion engine and able to carry a small number of people. <ref type="bibr">3</ref> The presence of words like "vehicle", "road" or "engine" in the definition of "car" illustrates the relevance of using word definitions for obtaining weak supervision allowing us to get semantically related pairs of words.</p><p>Dict2vec models this information by build- ing strong and weak pairs of words ( §3.1), in order to provide both a novel positive sampling objective ( §3.2) and a novel controlled nega- tive sampling objective ( §3.3). These objectives participate to the global objective function of Dict2vec ( §3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Strong pairs, weak pairs</head><p>In a definition, each word does not have the same semantic relevance. In the definition of "car", the words "internal" or "number" are less relevant than "vehicle". We introduce the concept of strong and weak pairs in order to capture this relevance. If the word w a is in the definition of the word w b and w b is in the definition of w a , they form a strong pair, as well as the K closest words to w a (resp. w b ) form a strong pair with w b (resp. w a ). If the word w a is in the definition of w b but w b is not in the definition of w a , they form a weak pair.</p><p>The word "vehicle" is in the definition of "car" and "car" is in the definition of "vehicle". Hence, (car-vehicle) is a strong pair. The word "road" is in the definition of "car", but "car" is not in the definition of "road". Therefore, (car-road) is a weak pair.</p><p>Some weak pairs can be promoted as strong pairs if the two words are among the K closest neighbours of each other. We chose the K clos- est words according to the cosine distance from a pretrained word embedding and find that using K = 5 is a good trade-off between semantic and syntactic extracted information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Positive sampling</head><p>We introduce the concept of positive sampling based on strong and weak pairs. We move closer vectors of words forming either a strong or a weak pair in addition to moving vectors of words co- occurring within the same window.</p><p>Let S(w) be the set of all words forming a strong pair with the word w and W(w) be the set of all words forming a weak pair with w. For each target w t from the corpus, we build V s (w t ) a random set of n s words drawn with replacement from S(w t ) and V w (w t ) a random set of n w words drawn with replacement from W(w t ). We com- pute the cost of positive sampling J pos for each target as follows:</p><formula xml:id="formula_3">J pos (w t ) = β s w i ∈Vs(wt) (v t · v i ) + β w w j ∈Vw(wt) (v t · v j )<label>(4)</label></formula><p>where is the logistic loss function defined by : x → log(1 + e −x ) and v t (resp. v i and v j ) is the vector associated to w t (resp. w i and w j ).</p><p>The objective is to minimize this cost for all tar- gets, thus moving closer words forming a strong or a weak pair.</p><p>The coefficients β s and β w , as well as the num- ber of drawn pairs n s and n w , tune the importance of strong and weak pairs during the learning phase. We discuss the choice of these hyperparameters in Section 5. When β s = 0 and β w = 0, our model is the Skip-gram model of <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Controlled negative sampling</head><p>Negative sampling consists in considering two random words from the vocabulary V to be unre- lated. For each word w t from the vocabulary, we generate a set F(w t ) of k randomly selected words from the vocabulary :</p><formula xml:id="formula_4">F(w t ) = {w i } k , w i ∈ V \ {w t }<label>(5)</label></formula><p>The model aims at separating the vectors of words from F(w t ) and the vector of w t . More for- mally, this is equivalent to minimize the cost J neg for each target word w t as follows:</p><formula xml:id="formula_5">J neg (w t ) = w i ∈F (wt) (−v t · v i )<label>(6)</label></formula><p>where the notation , v t and v i are the same as described in previous subsection. However, there is a non-zero probability that w i and w t are related. Therefore, the model will move their vectors further instead of mov- ing them closer. With strong/weak word pairs in Dict2vec, it becomes possible to better ensure that this is less likely to occur: we prevent a neg- ative example to be a word that forms a weak or strong pair with with w t . The negative sampling objective from Equation 6 becomes :</p><formula xml:id="formula_6">J neg (w t ) = w i ∈F (wt) w i / ∈S(wt) w i / ∈W(wt) (−v t · v i )<label>(7)</label></formula><p>In our experiments, we noticed this method dis- cards around 2% of generated negative pairs. The influence on evaluation depends on the nature of the corpus and is discussed at Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Global objective function</head><p>Our objective function is derived from the noise- contrastive estimation which is a more efficient objective function than the log-likelihood in Equa- tion 1 according to <ref type="bibr" target="#b19">Mikolov et al. (2013)</ref>. We add the positive sampling and the controlled neg- ative sampling described before and compute the cost for each (target,context) pair (w t , w c ) from the corpus as follows:</p><formula xml:id="formula_7">J(w t , w c ) = (v t · v c ) + J pos (w t ) + J neg (w t )<label>(8)</label></formula><p>The global objective is obtained by summing every pair's cost over the entire corpus :</p><formula xml:id="formula_8">J = C t=1 n c=−n J(w t , w t+c )<label>(9)</label></formula><p>4 Experimental setup</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fetching online definitions</head><p>We extract all unique words with more than 5 oc- currences from a full Wikipedia dump, represent- ing around 2.2M words. Since there is no dic- tionary that contains a definition for all existing words (the word w might be in the dictionary D i but not in D j ), we combine several dictionaries to get a definition for almost all of these words (some words are too rare to have a definition anyway). We use the English version of Cambridge, Oxford, Collins and dictionary.com. For each word, we download the 4 different webpages, and use regex to extract the definitions from the HTML template specific to each website, making the process fully accurate. Our approach does not focus on poly- semy, so we concatenate all definitions for each word. Then we concatenate results from all dic- tionaries, remove stop words and punctuation and lowercase all words. For our illustrative example in Section 3, we obtain :</p><p>car: road vehicle engine wheels seats small [...] platform lift.</p><p>Among the 2.2M unique words, only 200K does have a definition. We generate strong and weak pairs from the downloaded definitions accord- ing to the rule described in subsection 3.1 lead- ing to 417K strong pairs (when the parameter K from 3.1 is set to 5) and 3.9M weak pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training settings</head><p>We train our model with the generated pairs from subsection 4.1 and the November 2016 English dump from Wikipedia 4 . After removing all XML tags and converting all words to lowercase (with the help of Mahoney's script 5 ), we separate the corpus into 3 files containing respectively the first 50M tokens, the first 200M tokens, and the full dump. Our model uses additional knowledge during training. For a fair comparison against other frameworks, we also incorporate this infor- mation into the training data and create two ver- sions for each file : one containing only data from Wikipedia (corpus A) and one with data from Wikipedia concatenated with the definitions ex- tracted (corpus B). We use the same hyperparameters we usually find in the literature for all models. We use 5 neg- atives samples, 5 epochs, a window size of 5, a vector size of 100 (resp. 200 and 300) for the 50M file (resp. 200M and full dump) and we remove the words with less than 5 occurrences. We fol- low the same evaluation protocol as Word2vec and fastText to provide the fairest comparison against competitors, so every other hyperparameters (K, β s , β w , n s , n w ) are tuned using a grid search to maximize the weighted average score. For n s and n w , we go from 0 to 10 with a step of 1 and find the optimal values to be n s = 4 and n w = 5. For β s and β w we go from 0 to 2 with a step of 0.05 and find β s = 0.8 and β w = 0.45 to be the best val- ues for our model. <ref type="table">Table 1</ref> reports training times for the three models (all experiments were run on a E3-1246 v3 processor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>50M</head><p>200M <ref type="table" target="#tab_4">Full   Word2vec 15m30 86m 2600m  fastText  8m44  66m 1870m  Dict2vec  4m09  26m  642m   Table 1</ref>: Training time (in min) of Word2vec, fast- Text and Dict2vec models for several corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Word similarity evaluation</head><p>We follow the standard method for word similar- ity evaluation by computing the Spearman's rank correlation coefficient <ref type="bibr" target="#b26">(Spearman, 1904</ref>  <ref type="bibr" target="#b25">(Rubenstein and Goodenough, 1965)</ref>, RW ( <ref type="bibr" target="#b18">Luong et al., 2013</ref>), SimVerb-3500 ( <ref type="bibr" target="#b11">Gerz et al., 2016)</ref>, WordSim-353 ( <ref type="bibr" target="#b9">Finkelstein et al., 2001</ref>) and YP-130 ( <ref type="bibr" target="#b31">Yang and Powers, 2006</ref>) clas- sic datasets. We follow the same protocol used by Word2vec and fastText by discarding pairs which contain a word that is not in our embedding. Since all models are trained with the same corpora, the embeddings have the same words, therefore all competitors share the same OOV rates.</p><p>We run each experiment 3 times and report in <ref type="table">Table 2</ref> the average score to minimize the effect of the neural network random initialization. We compute the average by weighting each score by the number of pairs evaluated in its dataset in the same way as <ref type="bibr" target="#b14">Iacobacci et al. (2016)</ref>. We multiply each score by 1, 000 to improve readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Text classification evaluation</head><p>Our text classification task follows the same setup as the one for fastText in . We train a neural network composed of a single hidden layer where the input layer corresponds to the bag of words of a document and the output layer is the probability to belong to each label. The weights between the input and the hidden layer are initial- ized with the generated embeddings and are fixed during training, so that the evaluation score solely depends on the embedding. We update the weights of the neural network classifier with gradient de- scent. We use the datasets AG-News 6 , DBpe- dia ( <ref type="bibr" target="#b0">Auer et al., 2007)</ref> and Yelp reviews (polarity and full) <ref type="bibr">7</ref> . We split each datasets into a training and a test file. We use the same training and test files for all models and report the classification ac- curacy obtained on the test file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Baselines</head><p>We train Word2vec 8 and fastText 9 on the same 3 files and their 2 respective versions (A and B) described in 4.2 and use the same hyperparam- eters also described in 4.2 for all models. We train Word2vec with the Skip-gram model since our method is based on the Skip-gram model. We also train GloVe with their respective hyperparam- eters described in <ref type="bibr" target="#b23">Pennington et al. (2014)</ref>, but the results are lower than all other baselines (weighted average on word similarity task is 350 on the 50M file, 389 on the 200M file and 454 on the full dump) so we do no report GloVe's results.</p><p>We also retrofit the learned embeddings on cor- pus A with the Faruqui's method to compare an- other method using additional resources. The retrofitting introduces external knowledge from the WordNet semantic lexicon <ref type="bibr" target="#b21">(Miller, 1995)</ref>. We use the Faruqui's Retrofitting 10 with the W N all semantic lexicon from WordNet and 10 iterations as advised in the paper of <ref type="bibr" target="#b7">Faruqui et al. (2015)</ref>. Furthermore, we compare the performance of our method when using WordNet additional resources instead of dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and model analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic similarity</head><p>Table 2 (top) reports the Spearman's rank correla- tion scores obtained with the method described in subsection 4.3. We observe that our model outper- forms state-of-the-art approaches for most of the datasets on the 50M and 200M tokens files, and almost all datasets on the full dump (this is signif- icant according to a two-sided Wilcoxon signed- rank test with α = 0.05). With the weighted av- erage score, our model improves fastText's perfor- mance on raw corpus (column A) by 28.3% on the 50M file, by 17.7% on the 200M and by 12.8% on the full dump. Even when we train fastText with the same additional knowledge as ours (column B), our model improves performance by 2.9% on the 50M file, by 5.1% in the 200M and by 11.9% on the full dump.</p><p>We notice the column B (corpus composed of Wikipedia and definitions) has better results than the column A for the 50M (+24% on average) and the 200M file (+12% on average). This demon- strates the strong semantic relations one can find in definitions, and that simply incorporating defi- nitions in small training file can boost the perfor- mance of the embeddings. Moreover, when the training file is large (full dump), our supervised method with pairs is more efficient, as the boost brought by the concatenation of definitions is in- significant (+1.5% on average).</p><p>We also note that the number of strong and weak pairs drawn must be set according to the size of the training file. For the 50M and 200M to- kens files, we train our model with hyperparame- ters n s = 4 and n w = 5. For the full dump (20  <ref type="table" target="#tab_4">w2v  FT  our  w2v  FT  our  w2v  FT  our   oov  A  B  A  B  A  B  oov  A  B  A  B  A  B  oov A  B  A  B  A  B   MC-30  0% 697 847 722 823</ref>   <ref type="table">Table 2</ref>: Spearman's rank correlation coefficients between vectors' cosine similarity and human judge- ment for several datasets (top) and accuracies on text classification task (bottom). We train and evaluate each model 3 times and report the average score for each dataset, as well as the weighted average for all word similarity datasets.  <ref type="table">Table 3</ref>: Percentage changes of word similarity scores for several datasets after the Faruqui's retrofitting method is applied. We compare each model to their own non-retrofitted version (vs self) and our non- retrofitted version (vs our). A positive percentage indicates the level of improvement of the retrofitting approach, while a negative percentage shows that the compared method is better without retrofitting. As an illustration: the +13.9% at the top left means that retrofitting Word2vec's vectors improves the initial vectors output by 13.9%, while the -7.3% below indicates that our approach without retrofitting is better than the retrofitted Word2vec's vectors.</p><formula xml:id="formula_9">50M 200M Full w2v R FT R our R w2v R FT R our R w2v R FT R our R MC-30 vs self +13.9% +9.2% +1.3% +5.8% +4.8% +3.0% +5.2% +2.9% +1.2% vs our -7.3% -4.4% − -3.6% -2.4% − -1.0% -0.6% − MEN-TR-3k vs self +0.9% -0.7% -0.1% +0.7% -1.9% +0.4% +1.4% -2.8% +1.6% vs our -4.2% -7.4% − -1.3% -1.6% − -1.7% -3.3% − MTurk-287 vs self +1.4% +0.2% +3.5% -2.9% -3.3% +3.0% -0.9% -5.</formula><p>times larger than the 200M tokens file), the num- ber of windows in the corpus is largely increased, so is the number of (target,context) pairs. There- fore, we need to adjust the influence of strong and weak pairs and decrease n s and n w . We set n s = 2, n w = 3 to train on the full dump. The Faruqui's retrofitting method improves the word similarity scores on all frameworks for all datasets, except on RW and WS353 <ref type="table">(Table 3)</ref>. But even when Word2vec and fastText are retrofitted, their scores are still worse than our non-retrofitted model (every percentage on the vs our line are neg- ative). We also notice that our model is compati- ble with a retrofitting improvement method as our scores are also increased with Faruqui's method.</p><p>We also observe that, although our model is su- perior on each corpus size, our model trained on the 50M tokens file outperforms the other mod- els trained on the full dump (an improvement of 17% compared to the results of fastText, our best competitor, trained on the full dump). This means considering strong and weak pairs is more efficient than increasing the corpus size and that using dic- tionaries is a good way to improve the quality of the embeddings when the training file is small.</p><p>The models based on knowledge bases cited in §2.2 do not provide word similarity scores on all the datasets we used. However, for the reported scores, Dict2vec outperforms these models : <ref type="bibr" target="#b16">Kiela et al. (2015)</ref> achieves a correlation of 0.72 on the MEN dataset (vs. 0.756); <ref type="bibr" target="#b30">Xu et al. (2014)</ref> achieves 0.683 on the WS353-ALL dataset (vs. 0.758). <ref type="table">Table 2</ref> (bottom) reports the classification ac- curacy for the considered datasets. Our model achieves the same performances as Word2vec and fastText on the 50M file and slightly improves re- sults on the 200M file and the full dump. Us- ing supervision with pairs during training does not make our model specific to the word similarity task which shows that our embeddings can also be used in downstream extrinsic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Text classification accuracy</head><p>Note that for this experiment, the embeddings were fixed and not updated during learning (we only learned the classifier parameters) since our objective was rather to evaluate the capability of the embeddings to be used for another task rather than obtaining the best possible models. It is any- way possible to obtain better results by updating the embeddings and the classifier parameters with respect to the supervised information to adapt the embeddings to the classification task at hand as done in .    We also trained Dict2vec with pairs from Word- Net as well as no additional pairs during train- ing (in this case, this is the Skip-gram model from Word2vec). Results are reported in <ref type="table" target="#tab_6">Table 5</ref>. Training with WordNet pairs increases the scores, showing that the supervision brought by positive sampling is beneficial to the model, but lags be- hind the training using dictionary pairs demon- strating once again that dictionaries contain more semantic information than WordNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dictionaries vs. WordNet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Positive and negative sampling</head><p>For the positive sampling, an empirical grid search shows that a 1 2 ratio between β s and β w is a good rule-of-thumb for tuning these hyperparameters. We also notice that when these coefficients are too low (β s ≤ 0.5 and β w ≤ 0.2), results get worse because the model does not take into account the information from the strong and weak pairs. On the other side, when they are too high (β s ≥ 1.2 and β w ≥ 0.6), the model discards too much the information from the context in favor of the infor- mation from the pairs. This behaviour is similar when the number of strong and weak pairs is too low or too high (n s , n w ≤ 2 or n s , n w ≥ 5).</p><p>For the negative sampling, we notice that the control brought by the pairs increases the aver- age weighted score by 0.7% compared to the un- controlled version. We also observe that increas- ing the number of negative samples does not sig- nificantly improve the results except for the RW dataset where using 25 negative samples can boost performances by 10%. Indeed, this dataset is mostly composed of rare words so the embeddings must learn to differentiate unrelated words rather than moving closer related ones. In <ref type="figure" target="#fig_1">Fig. 1</ref>, we observe that our model is still able to outperform state-of-the-art approaches when we reduce the dimension of the embeddings to 20 or 40. We also notice that increasing the vector size does increase the performance, but only until a di- mension around 100, which is the common dimen- sion used when training on the 50M tokens file for related approaches reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Vector size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we presented Dict2vec, a new ap- proach for learning word embeddings using lexi- cal dictionaries. It is based on a Skip-gram model where the objective function is extended by lever- aging word pairs extracted from the definitions weighted differently with respect to the strength of the pairs. Our approach shows better results than state-of-the-art word embeddings methods for the word similarity task, including methods based on a retrofitting from external sources. We also provide the full source code to reproduce the experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spearman's rank correlation coefficient for RW-STANFORD (RW) and WS-353ALL (WS) on the fastText model (FT) and our, with different vector size. Training is done on the corpus A of 50M tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Weighted average Spearman correla-
tion score of raw vectors and after retrofitting 
with WordNet pairs (R W N ) and dictionary pairs 
(R dict ). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 reports</head><label>4</label><figDesc></figDesc><table>the Spearman's rank correlation 
score for vectors obtained after training (Raw col-
umn) and the scores after we retrofit those vectors 
with pairs from WordNet (R W N ) and extracted 
pairs from dictionaries (R dict ). Retrofitting with 
dictionaries outperforms retrofitting with Word-
Net lexicons, meaning that data from dictionaries 
are better suited to improve embeddings toward 
semantic similarities when retrofitting. 

50M 200M full 

No pairs 
453 
471 
488 
With WordNet pairs 564 
566 
559 
With dictionary pairs 569 
569 
571 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Weighted average Spearman correlation 
score of Dict2vec vectors when trained without 
pairs and with WordNet or dictionary pairs. 

</table></figure>

			<note place="foot" n="2"> https://en.wikipedia.org/wiki/Alan_Turing</note>

			<note place="foot" n="3"> Definition from Oxford dictionary.</note>

			<note place="foot" n="4"> https://dumps.wikimedia.org/enwiki/20161101/</note>

			<note place="foot" n="5"> http://mattmahoney.net/dc/textdata#appendixa</note>

			<note place="foot" n="6"> https://www.di.unipi.it/ ˜ gulli/AG_corpus_of_ news_articles.html 7 https://www.yelp.com/dataset_challenge 8 https://github.com/dav/word2vec 9 https://github.com/facebookresearch/fastText</note>

			<note place="foot" n="10"> https://github.com/mfaruqui/retrofitting</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data. The semantic web pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="page" from="49" to="50" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujay</forename><surname>Kumar Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North</title>
		<meeting>the 2015 Conference of the North</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">American</forename><surname>Chapter</surname></persName>
		</author>
		<title level="m">the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1606" to="1615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rupert Firth</surname></persName>
		</author>
		<title level="m">Papers in Linguistics 19341951: Repr</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Simverb-3500: A largescale evaluation set of verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00869</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fandrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03542</idno>
		<title level="m">Wikireading: A novel large-scale language understanding task over wikipedia. arXiv preprint</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Embeddings for word sense disambiguation: An evaluation study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chu-Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1367" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual correlates of semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and cognitive processes</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American journal of psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="101" />
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Citeseer</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Connecting language and knowledge bases with embedding models for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1307.7973</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rcnet: A general framework for incorporating knowledge into word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1219" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Verb similarity on the taxonomy of WordNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Masaryk University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improving lexical embeddings with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="545" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end learning of semantic role labeling using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1127" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
