<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCFG Induction for Unsupervised Parsing and Language Modelling</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Scicluna</surname></persName>
							<email>james.scicluna@univ-nantes.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR6241</orgName>
								<orgName type="institution" key="instit1">Université de Nantes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-44000</postCode>
									<region>LINA</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>De La Higuera</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Nantes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LINA</orgName>
								<address>
									<postCode>UMR6241, F-44000</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PCFG Induction for Unsupervised Parsing and Language Modelling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1353" to="1362"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The task of unsupervised induction of probabilistic context-free grammars (PCFGs) has attracted a lot of attention in the field of computational linguistics. Although it is a difficult task, work in this area is still very much in demand since it can contribute to the advancement of language parsing and modelling. In this work, we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact artificial natural language grammars and typical context-free grammars. Moreover, this algorithm can work on large grammars and datasets and infers correctly even from small samples. Our analysis shows that the type of grammars induced by our algorithm are, in theory, capable of modelling natural language. One of our experiments shows that our algorithm can potentially outperform the state-of-the-art in unsupervised parsing on the WSJ10 corpus.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of unsupervised induction of PCFGs has attracted a lot of attention in the field of compu- tational linguistics. This task can take the form of either parameter search or structure learning. In parameter search, a CFG is fixed and the fo- cus is on assigning probabilities to this grammar using Bayesian methods <ref type="bibr">(Johnson et al., 2007)</ref> or maximum likelihood estimation <ref type="bibr" target="#b21">(Lari and Young, 1990</ref>). In structure learning, the focus is on build- ing the right grammar rules from scratch. We take the latter approach.</p><p>Unsupervised structure learning of (P)CFGs is a notoriously difficult task (de la <ref type="bibr" target="#b12">Higuera, 2010;</ref><ref type="bibr" target="#b9">Clark and Lappin, 2010)</ref>, with theoretical results showing that in general it is either impossible to achieve <ref type="bibr" target="#b13">(Gold, 1967;</ref><ref type="bibr" target="#b11">de la Higuera, 1997)</ref> or requires impractical resources <ref type="bibr" target="#b14">(Horning, 1969;</ref><ref type="bibr" target="#b35">Yang, 2012)</ref>. At the same time, it is well known that context-free structures are needed for better language parsing and modelling, since less expres- sive models (such as HMMs) are not good enough <ref type="bibr" target="#b23">(Manning and Schütze, 2001;</ref><ref type="bibr" target="#b17">Jurafsky and Martin, 2008)</ref>. Moreover, the trend is towards unsu- pervised (rather than supervised) learning meth- ods due to the lack in most languages of annotated data and the applicability in wider domains ( <ref type="bibr" target="#b24">Merlo et al., 2010</ref>). Thus, despite its difficulty, unsuper- vised PCFG grammar induction (or induction of other similarly expressive models) is still an im- portant task in computational linguistics.</p><p>In this paper, we describe a new algorithm for PCFG induction based on a principled approach and capable of inducing accurate yet compact grammars. Moreover, this algorithm can work on large grammars and datasets and infers correctly even from small samples. We show that our algo- rithm is capable of achieving competitive results in both unsupervised parsing and language mod- elling of typical context-free languages and arti- ficial natural language grammars. We also show that the type of grammars we propose to learn are, in theory, capable of modelling natural language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Grammars and Languages</head><p>A context-free grammar (CFG) is a 4-tuple N, Σ, P, I, where N is the set of non-terminals, Σ the set of terminals, P the set of production rules and I a set of starting non-terminals (i.e. multi- ple starting non-terminals are possible). The lan- guage generated from a particular non-terminal A is L(A) = {w|A * ⇒ w} and the language gen- erated by a grammar G is L(G) = S∈I L(S). A CFG is in Chomsky Normal Form (CNF) if ev-ery production rule is of the form N → N N or N → Σ.</p><p>A probabilistic context-free grammar (PCFG) is a CFG with a probability value assigned to every rule and every starting non-terminal. The prob- ability of a leftmost derivation from a PCFG is the product of the starting non-terminal probabil- ity and the production probabilities used in the derivation. The probability of a string generated by a PCFG is the sum of all its leftmost deriva- tions' probabilities. The stochastic language gen- erated from a PCFG G is (L(G), φ G ), where φ G is the distribution over Σ * defined by the probabil- ities assigned to the strings by G. For a PCFG to be consistent, the probabilities of the strings in its stochastic language must add up to 1 <ref type="bibr" target="#b34">(Wetherell, 1980)</ref>. Any PCFG mentioned from now onwards is assumed to be consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Congruence Relations</head><p>A congruence relation ∼ on Σ * is any equivalence relation on Σ * that respects the following condi- tion: if u ∼ v and x ∼ y then ux ∼ vy. The con- gruence classes of a congruence relation are sim- ply its equivalence classes. The congruence class of w ∈ Σ * w.r.t. a congruence relation ∼ is de- noted by <ref type="bibr">[w]</ref> ∼ . The set of contexts of a substring w with respect to a language L, denoted Con(w, L), is {(l, r) ∈ Σ * × Σ * | lwr ∈ L}. Two strings u and v are syntactically congruent with respect to</p><formula xml:id="formula_0">L, written u ≡ L v, if Con(u, L) = Con(v, L). This is a congruence relation on Σ * . The con- text distribution of a substring w w.r.t. a stochastic language (L, φ), denoted C (L,φ) w</formula><p>, is a distribution whose support is all the possible contexts over al- phabet Σ (i.e. Σ * × Σ * ) and is defined as follows:</p><formula xml:id="formula_1">C (L,φ) w (l, r) = φ(lwr) l ,r ∈Σ * φ(l wr ) Two strings u and v are stochastically congru- ent with respect to (L, φ), written u ∼ = (L,φ) v, if C (L,φ) u is equal to C (L,φ) v</formula><p>. This is a congruence relation on Σ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Congruential Grammars</head><p>Clark (2010a) defines Congruential CFGs (C- CFGs) as being all the CFGs G which, for any non</p><formula xml:id="formula_2">-terminal A, if u ∈ L(A) then L(A) ⊆ [u] ≡ L(G) (where [u] ≡ L(G)</formula><p>is the syntactic congru- ence class of u w.r.t. the language of G). This class of grammars was defined with learnability in mind. Since these grammars have a direct relationship between congruence classes and the non-terminals, their learnability is reduced to that of finding the correct congruence classes <ref type="bibr">(Clark, 2010a)</ref>.</p><p>This class of grammars is closely related to the class of NTS-grammars ( <ref type="bibr" target="#b2">Boasson and Sénizergues, 1985)</ref>. Any C-CFG is an NTS- grammar but not vice-versa. However, it is not known whether languages generated by C-CFGs are all NTS-languages <ref type="bibr">(Clark, 2010a)</ref>. Note that NTS-languages are a subclass of deterministic context-free languages and contain the regular languages, the substitutable <ref type="bibr" target="#b8">(Clark and Eyraud, 2007)</ref> and k-l-substitutable context-free languages <ref type="bibr" target="#b36">(Yoshinaka, 2008)</ref>, the very simple languages and other CFLs such as the Dyck language <ref type="bibr" target="#b2">(Boasson and Sénizergues, 1985)</ref>.</p><p>We define a slightly more restrictive class of grammars, which we shall call Strongly Congru- ential CFGs (SC-CFGs). A CFG G is a SC- CFG if, for any non</p><formula xml:id="formula_3">-terminal A, if u ∈ L(A) then L(A) = [u] ≡ L(G) .</formula><p>The probabilistic equiv- alent of this is the class of Strongly Congruential PCFGs (SC-PCFGs), defined as all the PCFGs G which, for any non</p><formula xml:id="formula_4">-terminal A, if u ∈ L(A) then L(A) = [u]∼ = (L(G),φ)</formula><p>. In other words, the non- terminals (i.e. syntactic categories in natural lan- guage) of these grammars directly correspond to classes of substitutable strings (i.e. substitutable words and phrases in NL). One may ask whether this is too strict a restriction for natural language grammars. We argue that it is not, for the follow- ing reasons.</p><p>First of all, this restriction complies with the ap- proach taken by American structural linguists for the identification of syntactic categories, as shown by <ref type="bibr" target="#b26">Rauh (2010)</ref>: "[Zellig and Fries] identified syntactic categories as distribution classes, em- ploying substitution tests and excluding semantic properties of the items analysed. Both describe syntactic categories exclusively on the basis of their syntactic environments and independently of any inherent properties of the members of these categories".</p><p>Secondly, we know that such grammars are ca- pable of describing languages generated by gram- mars that contain typical natural language gram- matical structures (see Section 4.1; artificial natu- ral language grammars NL1-NL7, taken from var- ious sources, generate languages which can be de- scribed by SC-PCFGs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head><p>COMINO (our algorithm) induces SC-PCFGs from a positive sample S. The steps involved are:</p><p>1. Inducing the stochastically congruent classes of all the substrings of S 2. Selecting which of the induced classes are non-terminals and subsequently building a CFG.</p><p>3. Assigning probabilities to the induced CFG.</p><p>The approach we take is very different from tra- ditional grammar induction approaches, in which grouping of substitutable substrings is done incre- mentally as the same groups are chosen to rep- resent non-terminals. We separate these two task so that learning takes place in the grouping phase whilst selection of non-terminals is done indepen- dently by solving a combinatorial problem. For the last step, the standard EM-algorithm for PCFGs ( <ref type="bibr" target="#b21">Lari and Young, 1990</ref>) is used. In Sec- tions 3.1 and 3.2, the first and second steps of the algorithm are described in detail. We analyse our algorithm in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Inducing the Congruence Classes</head><p>We describe in Algorithm 1 how the congruence classes are induced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Learn Congruence Classes</head><p>Input: A multiset S; parameters: n, d, i; distance function dist on local contexts of size k Output: The congruence classes CC over the substrings of S 1 Subs ← Set of all substrings of S ;</p><formula xml:id="formula_5">2 CC ← {{w} | w ∈ Subs} ; 3 while True do 4 P airs ← {(x, y) | x, y ∈ CC, x = y, |S| x ≥ n , |S| y ≥ n} ; 5 if |P airs| = 0 then exitloop ; 6</formula><p>Order P airs based on dist k ;</p><formula xml:id="formula_6">7 (x, y) ← P airs[0] ; 8 init = {[w] CC | w ∈ S} ; 9 if dist k (x, y) ≥ d and |init| ≤ i then exitloop ; 10 CC ← Merge(x, y, CC) ; 11 end 12 return CC ;</formula><p>At the beginning, each substring (or phrase for natural language) in the sample is assigned its own congruence class (line 2). Then, pairs of frequent congruence classes are merged together depend- ing on the distance between their empirical con- text distribution, which is calculated on local con- texts. The following points explain each keyword:</p><p>• The empirical context distribution of a sub- string w is simply a probability distribution over all the contexts of w, where the prob- ability for a context (l, r) is the number of occurrences of lwr in the sample divided by the number of occurrences of w. This is ex- tended to congruence classes by treating each substring in the class as one substring (i.e. the sum of occurrences of lw i r, for all w i in the class, divided by the sum of occurrences of all w i ).</p><p>• Due to the problem of sparsity with contexts (in any reasonably sized corpus of natural language, very few phrases will have more than one occurrence of the same context), only local contexts are considered. The lo- cal contexts of w are the pairs of first k sym- bols (or words for natural language) preced- ing and following w. The lower k is, the less sparsity is a problem, but the empirical con- text distribution is less accurate. For natural language corpora, k is normally set to 1 or 2.</p><p>• A frequent congruence class is one whose substring occurrences in the sample add up to more than a pre-defined threshold n. In- frequent congruence classes are ignored due to their unreliable empirical context dis- tribution. However, as more merges are made, more substrings are added to infre- quent classes, thus increasing their frequency and eventually they might be considered as frequent classes.</p><p>• A distance function dist between samples of distributions over contexts is needed by the algorithm to decide which is the closest pair of congruence classes, so that they are merged together. We used L1-Distance and Pearson's chi-squared test for experiments in Sections 4.1 and 4.2 respectively.</p><p>• After each merge, other merges are logically deduced so as to ensure that the relation re-mains a congruence 1 . In practice, the vast majority of the merges undertaken are logi- cally deduced ones. This clearly relieves the algorithm from taking unnecessary decisions (thus reducing the chance of erroneous de- cisions). On the downside, one bad merge can have a disastrous ripple effect. Thus, to minimize as much as possible the chance of this happening, every merge undertaken is the best possible one at that point in time (w.r.t. the distance function used). The same idea is used in DFA learning ( <ref type="bibr" target="#b20">Lang et al., 1998</ref>).</p><p>This process is repeated until either 1) no pairs of frequent congruence classes are left to merge (line 5) or 2) the smallest distance between the candidate pairs is bigger or equal to a pre-defined threshold d and the number of congruence classes containing strings from the sample is smaller or equal to a pre-defined threshold i (line 9).</p><p>The first condition of point 2 ensures that con- gruence classes which are sufficiently close to each other are merged together. The second con- dition of point 2 ensures that the hypothesized congruence classes are generalized enough (i.e. to avoid undergeneralization). For natural language examples, one would expect that a considerable number of sentences are grouped into the same class because of their similar structure. Obviously, one can make use of only one of these conditions by assigning the other a parameter value which makes it trivially true from the outset (0 for d and |Subs| for i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Building the Context-Free Grammar</head><p>Deciding which substrings are constituents (in our case, this translates into choosing which congru- ence classes correspond to non-terminals) is a problematic issue and is considered a harder task than the previous step <ref type="bibr" target="#b19">(Klein, 2004)</ref>. A path fol- lowed by a number of authors consists in using an Ockham's razor or Minimal Description Length principle approach <ref type="bibr" target="#b30">(Stolcke, 1994;</ref><ref type="bibr" target="#b5">Clark, 2001;</ref><ref type="bibr" target="#b25">Petasis et al., 2004</ref>). This generally leads to choos- ing as best hypothesis the one which best com- presses the data. Applying this principle in our case would mean that the non-terminals should be assigned in such a way that the grammar built is the smallest possible one (in terms of the number of non-terminals and/or production rules) consis- tent with the congruence classes. To our knowl- edge, only local greedy search is used by systems in the literature which try to follow this approach.</p><p>We propose a new method for tackling this problem. We show that all the possible SC-CFGs in CNF consistent with the congruence classes di- rectly correspond to all the solutions of a boolean formula built upon the congruence classes, where the variables of this formula correspond to non- terminals (and, with some minor adjustments, pro- duction rules as well). Thus, finding the smallest possible grammar directly translates into finding a solution which has the smallest possible amount of true variables. Finding a minimal solution for this type of formula is a known NP-Hard problem ( <ref type="bibr" target="#b18">Khanna et al., 2000</ref>). However, sophisticated lin- ear programming solvers ( <ref type="bibr" target="#b1">Berkelaar et al., 2008)</ref> can take care of this problem. For small examples (e.g. all the examples in <ref type="table" target="#tab_0">Table 1</ref>), these solvers are able to find an exact solution in a few sec- onds. Moreover, these solvers are capable of find- ing good approximate solutions to larger formulas containing a few million variables.</p><p>The formula contains one variable per congru- ence class. All variables corresponding to congru- ence classes containing strings from the sample are assigned the value True (since there must be a starting non-terminal that generates these strings). All variables corresponding to congruence classes containing symbols from Σ are assigned the value True (since for every a ∈ Σ, there must be a rule A → a). Finally, and most importantly, for every congruence class <ref type="bibr">[w]</ref> and for every string w in <ref type="bibr">[w]</ref> (|w| = n), the following conditional statement is added to the formula:</p><formula xml:id="formula_7">v(w) ⇒ (v(w 1,1 ) ∧ v(w 2,n )) ∨ (v(w 1,2 ) ∧ v(w 3,n )) ∨ . . . ∨ (v(w 1,n−1 ) ∧ v(w n,n ))</formula><p>where v(x) is the variable corresponding to the congruence class <ref type="bibr">[x]</ref> and w i,j is the substring of w from the i th to the j th symbol of w. This statement is representing the fact that if a congruence class <ref type="bibr">[w]</ref> is chosen as a non-terminal then for each string in w ∈ [w], there must be at least one CNF rule A → BC that generates w and thus there must be at least one division of w into w 1,k w k+1,n such that B corresponds to <ref type="bibr">[w 1,k ]</ref> and C corresponds to [w k+1,n ].</p><p>The grammar extracted from the solution of this formula is made up of all the possible CNF pro- duction rules built from the chosen non-terminals. The starting non-terminals are those which corre- spond to congruence classes that contain at least one string from the sample.</p><p>The following is a run of the whole process on a simple example: </p><formula xml:id="formula_8">Sample</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boolean Formula</head><p>There is one conditional statement per sub- string. For example, X 6 ⇒ (X 1 ∧X 3 )∨(X 4 ∧ X 2 ) represents the two possible ways aab in congruence class 6 can be split (a|ab , aa|b).</p><p>Variables X 1 , X 2 and X 3 are true.</p><formula xml:id="formula_9">X 3 ⇒ (X 1 ∧ X 2 ) X 3 ⇒ (X 1 ∧ X 7 ) ∨ (X 4 ∧ X 5 ) ∨ (X 6 ∧ X 2 ) X 3 ⇒ (X 1 ∧ X 7 ) ∨ (X 4 ∧ X 11 ) ∨ (X 8 ∧ X 9 )∨ (X 10 ∧ X 5 ) ∨ (X 6 ∧ X 2 ) X 4 ⇒ (X 1 ∧ X 1 ) X 5 ⇒ (X 2 ∧ X 2 ) X 6 ⇒ (X 1 ∧ X 3 ) ∨ (X 4 ∧ X 2 ) X 6 ⇒ (X 1 ∧ X 3 ) ∨ (X 4 ∧ X 7 ) ∨ (X 8 ∧ X 5 )∨ (X 10 ∧ X 2 ) X 7 ⇒ (X 1 ∧ X 5 ) ∨ (X 3 ∧ X 2 ) X 7 ⇒ (X 1 ∧ X 11 ) ∨ (X 4 ∧ X 9 ) ∨ (X 6 ∧ X 5 )∨ (X 3 ∧ X 2 ) X 8 ⇒ (X 1 ∧ X 4 ) ∨ (X 4 ∧ X 1 ) X 9 ⇒ (X 2 ∧ X 5 ) ∨ (X 5 ∧ X 2 ) X 10 ⇒ (X 1 ∧ X 6 ) ∨ (X 4 ∧ X 3 ) ∨ (X 8 ∧ X 2 ) X 11 ⇒ (X 1 ∧ X 9 ) ∨ (X 3 ∧ X 5 ) ∨ (X 7 ∧ X 2 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Solution</head><p>Running the solver on this formula will re- turn the following true variables that make up a minimal solution: X 1 , X 2 , X 3 and X 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grammar</head><p>For every statement x ⇒ . . . ∨ (y ∧ z) ∨ . . . where x,y and z are true, a production rule x → yz is added. So, the following grammar is built: X 3 is the starting non-terminal</p><formula xml:id="formula_10">X 3 → X 1 X 7 | X 1 X 2 X 7 → X 3 X 2 X 1 → a X 2 → b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>In the first phase of the algorithm, we are group- ing all the substrings of the sample S according to the congruence relation ∼ = <ref type="bibr">(L,φ)</ref> , where (L, φ) is the target stochastic language (for natural language, this is the language model). To do so, we are as- suming that S was i.i.d. generated from (L, φ). In the second phase, we are representing the space of all CFGs consistent with the classes obtained in phase one as different solutions to a boolean formula. Here we introduce our bias in favour of smaller grammars by finding a minimal solution to the formula. In the last phase, probabilities are as- signed to the grammar obtained in phase two using the standard MLE algorithm for PCFGs.</p><p>Unlike many other systems, in our case the hy- pothesis space of grammars is well-defined. This allows us to analyse our algorithm in a theoreti- cal framework and obtain theoretical learnability results. Moreover, this gives us an idea on the types of syntactical features our system is capable of learning.</p><p>Assuming our algorithm always takes correct merge decisions, the sample required for identifi- cation needs only to be structurally complete w.r.t. the target grammar (i.e. every production rules is used at least once in the generation of the sample). This means that, in theory, our algorithm can work with very small samples (polynomial size w.r.t. the number of rules in the target grammar).</p><p>Some approaches in the literature assume that whenever a particular substring is a constituent in some sentence, then it is automatically a con- stituent in all other sentences (whenever it does not overlap with previously chosen constituents) <ref type="bibr" target="#b31">(van Zaanen, 2001;</ref><ref type="bibr" target="#b5">Clark, 2001;</ref><ref type="bibr" target="#b0">Adriaans et al., 2000</ref>). In reality, this is clearly not the case. A simple experiment on the WSJ10 corpus reveals that only 16 of the most frequent 1009 POS sequences (oc- curring 10 or more times in the sample) which are at least once constituents, are in fact always con- stituents. This assumption does not hold for am- biguous grammars in our class.</p><p>The approach we take to solve the smallest grammar problem can be extended to other classes of grammars. A similar formula can be built for grammars whose non-terminals have a one-to-one correspondence with congruence classes contain- ing features of their language <ref type="bibr" target="#b7">(Clark, 2010b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Artificial Data</head><p>We tested our system on 11 typical context-free languages and 9 artificial natural language gram- mars taken from 4 different sources <ref type="bibr" target="#b30">(Stolcke, 1994;</ref><ref type="bibr">Langley and Stromsten, 2000;</ref><ref type="bibr" target="#b0">Adriaans et al., 2000;</ref><ref type="bibr" target="#b29">Solan et al., 2005</ref>). The 11 CFLs in- clude 7 described by unambiguous grammars: UC1: a n b n UC2: a n b n c m d m UC3: a n b m n ≥ m UC4: a p b q , p = q UC5: Palindromes over alpha- bet {a, b} with a central marker UC6:Palindromes over alphabet {a, b} without a central marker  <ref type="table">Table 2</ref> in <ref type="bibr">(Langley and Stromsten, 2000</ref>) NL2: Grammar 'b', <ref type="table">Table 2</ref> in <ref type="bibr">(Langley and Stromsten, 2000</ref>) NL3: Lexical categories and constituency, pg 96 in <ref type="bibr" target="#b30">(Stolcke, 1994)</ref> NL4: Recursive embedding of constituents, pg 97 in <ref type="bibr" target="#b30">(Stolcke, 1994)</ref> NL5: Agreement, pg 98 in <ref type="bibr" target="#b30">(Stolcke, 1994)</ref> NL6: Singular/plural NPs and number agreement, pg 99 in <ref type="bibr" target="#b30">(Stolcke, 1994)</ref> NL7: Experiment 3.1 grammar in ( <ref type="bibr" target="#b0">Adriaans et al., 2000</ref>) NL8:Grammar in <ref type="table" target="#tab_0">Table 10</ref> ( <ref type="bibr" target="#b0">Adriaans et al., 2000</ref>) NL9: TA1 grammar in ( <ref type="bibr" target="#b29">Solan et al., 2005</ref>).</p><p>The quality of the learned model depends on its capacity to predict the correct structure (parse trees) on the one hand and to predict the correct sentence probabilities on the other (i.e. assigns a probability distribution close to the target one). To evaluate parse trees, we follow suggestions given by van <ref type="bibr" target="#b32">Zaanen and Geertzen (2008)</ref> and use micro-precision and micro-recall over all the non- trivial brackets. We take the harmonic mean of these two values to obtain the Unlabelled brack- ets F 1 score (UF 1 ). The learned distribution can be evaluated using perplexity (when the target dis- tribution is not known) or some similarity metric between distributions (when the target distribution is known). In our case, the target distribution is  <ref type="table">Table 2</ref>: Relative Entropy and UF 1 results of our system COMINO vs ADIOS and ABL respec- tively. Best results are highlighted, close results (i.e. with a difference of at most 0.1 for relative entropy and 1% for UF 1 ) are both highlighted known. We chose relative entropy 2 as a good mea- sure of distance between distributions. Our UF 1 results over test sets of one thousand strings were compared to results obtained by ABL <ref type="bibr" target="#b31">(van Zaanen, 2001</ref>), which is a system whose primary aim is that of finding good parse trees (rather than identifying the target language). Al- though ABL does not obtain state-of-the-art re- sults on natural language corpora, it proved to be the best system (for which an implementation is readily available) for unsupervised parsing of sen- tences generated by artificial grammars. Results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ex. |Σ| |N | |P |</head><note type="other">UC1 2 3 4 UC2 4 7 9 UC3 2 3 5 UC4 2 5 9 UC5 2 3 5 UC6 2 3 8 UC7 2 2 3 AC1 2 4 9 AC2 2 5 11 AC3 2 3 5 AC4 7 8 13 NL1 9 8 15 NL2 8 8 13 NL3 12 10 18 NL4 13 11 22 NL5 16 12 23 NL6 19 17 32 NL7 12 3 9 NL8 30 10 35 NL9 50 45 81</note><p>We calculated the relative entropy on a test set of one million strings generated from the target grammar. We compared our results with ADIOS ( <ref type="bibr" target="#b29">Solan et al., 2005</ref>), a system which obtains com- petitive results on language modelling ( <ref type="bibr" target="#b33">Waterfall et al., 2010)</ref> and whose primary aim is of correctly identifying the target language (rather than finding good parse trees). Results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>For the tests in the first section of <ref type="table" target="#tab_0">Table 1</ref> (i.e. above the first dashed line), our algorithm was ca- pable of exactly identifying the structure of the tar- get grammar. Notwithstanding this, the bracketing results for these tests did not always yield perfect scores. This happened whenever the target gram- mar was ambiguous, in which case the most prob- able parse trees of the target and learned grammar can be different, thus leading to incorrect bracket- ing. For the tests in the second section of <ref type="table" target="#tab_0">Table 1</ref> (i.e. between the two dashed lines), our algorithm was capable of exactly identifying the target lan- guage (but not the grammar). In all of these cases, the induced grammar was slightly smaller than the target one. For the remaining tests, our algorithm did not identify the target language. In fact, it al- ways overgeneralised. The 3 typical CFLs UC3, UC4 and UC6 are not identified because they are not contained in our subclass of CFLs. Inspite of this, the relative entropy results obtained are still relatively good. Overall, it is fair to say that the results obtained by our system, for both language modelling and unsupervised parsing on artificial data, are competitive with the results obtained by other methods.</p><p>bution D is defined as</p><formula xml:id="formula_11">w∈Σ * ln D(w) D (w) D(w). Add-one</formula><p>smoothing is used to solve the problem of zero probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Natural Language Experiments</head><p>We also experimented on natural language cor- pora. For unsupervised parsing, we tested our system on the WSJ10 corpus, using POS tagged sentences as input. Due to time efficiency, we changed the algorithm for finding congruence classes. Instead of always choosing the best pos- sible merge w.r.t. the distance function, a distance threshold is set and all congruence classes whose distance is smaller than the threshold are merged. Also, we changed the distance function from L1- Distance to Pearson's χ 2 test.</p><p>In a first experiment (vaguely similar to the one done by <ref type="bibr" target="#b22">Luque and López (2010)</ref>), we constructed the best possible SC-CFG consistent with the merges done in the first phase and assigned prob- abilities to this grammar using Inside-Outside. In other words, we ran the second phase of our system in a supervised fashion by using the treebank to decide which are the best congru- ence classes to choose as non-terminals. The CNF grammar we obtained from this experiment (COMINO-UBOUND) gives very good parsing results which outperform results from state-of-the- art systems DMV+CCM <ref type="bibr" target="#b19">(Klein, 2004</ref>), U-DOP <ref type="bibr" target="#b3">(Bod, 2006a)</ref>, UML-DOP <ref type="bibr" target="#b4">(Bod, 2006b</ref>) and In- cremental <ref type="bibr" target="#b27">(Seginer, 2007)</ref> as shown in <ref type="table">Table 2</ref>. Moreover, the results obtained are very close to the best results one can ever hope to obtain from any CNF grammar on WSJ10 (CNF-UBOUND) <ref type="bibr" target="#b19">(Klein, 2004)</ref>. However, the grammar we obtain does not generalise enough and does not describe a good language model. In a second experiment, we ran the complete COMINO system. The grammar obtained from this experiment did not give com- petitive parsing results.</p><p>The first experiment shows that the merge deci- sions taken in the first phase do not hinder the pos- sibility of finding a very good grammar for pars- ing. This means that the merge decisions taken by our system are good in general. Manual anal- ysis on some of the merges taken confirms this. This experiment also shows that there exists a non- trivial PCFG in our restrictive class of grammars that is capable of achieving very good parsing re- sults. This is a positive sign for the question of how adequate SC-PCFGs are for modelling natu- ral languages. However, the real test remains that of finding SC-PCFGs that generate good bracket- ings and good language models. The second ex- periment shows that the second phase of our al-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UP</head><p>UR UF 1 State-of-the-art DMV+CCM 69.3 88.0 77.6 U-DOP 70.8 88.2 78.5 UML-DOP - - 82.9 Incremental 75.6 76.2 75.9 Upper bounds COMINO-UBOUND 75.8 96.9 85.1 CNF-UBOUND 78.8 100.0 88.1 <ref type="table">Table 3</ref>: Parsing results on WSJ10. Note that In- cremental is the only system listed as state-of-the- art which parses from plain text and can generate non-binary trees gorithm is not giving good results. This means that the smallest possible grammar might not be the best grammar for parsing. Therefore, other cri- teria alongside the grammar size are needed when choosing a grammar consistent with the merges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion and Future Work</head><p>In order to improve our system, we think that our algorithm has to take a less conservative merging strategy in the first phase. Although the merges being taken are mostly correct, our analysis shows that not enough merging is being done. The prob- lematic case is that of taking merge decisions on (the many) infrequent long phrases. Although many logically deduced merges involve infrequent phrases and also help in increasing the frequency of some long phrases, this proved to be not enough to mitigate this problem. As for future work, we think that clustering techniques can be used to help solve this problem. A problem faced by the system is that, in cer- tain cases, the statistical evidence on which merge decisions are taken does not point to the intuitively expected merges. As an example, consider the two POS sequences "DT NN" and "DT JJ NN" in the WSJ corpus. Any linguist would agree that these sequences are substitutable (in fact, they have lots of local contexts in common). However, statisti- cal evidence points otherwise, since their context distributions are not close enough. This happens because, in certain positions of a sentence, "DT NN" is far more likely to occur than "DT JJ NN" (w.r.t. the ratio of their total frequencies) and in other positions, "DT JJ NN" occurs more than ex- pected. The following table shows the frequencies of these two POS sequences over the whole WSJ corpus and their frequencies in contexts (#,VBD) and (IN,#) (the symbol # represents the end or beginning of a sentence): It is clear that the ratios do not match, thus lead- ing to context distributions which are not close enough. Thus, this shows that basic sequences such as "DT NN" and "DT JJ NN", which lin- guists would group into the same concept NP, are statistically derived from different sub-concepts of NP. Our algorithm is finding these sub-concepts, but it is being evaluated on concepts (such as NP) found in the treebank (created by linguists).</p><p>From the experiments we did on artificial nat- ural language grammars, it resulted that the tar- get grammar was always slightly bigger than the learned grammar. Although in these cases we still managed to identify the target language or have a good relative entropy result, the bracketing re- sults were in general not good. This and our sec- ond experiment on the WSJ10 corpus show that the smallest possible grammar might not be the best grammar for bracketing. To not rely solely on finding the smallest grammar, a bias can be added in favour of congruence classes which, according to constituency tests (like the Mutual Information criterion in <ref type="bibr" target="#b5">Clark (2001)</ref>), are more likely to con- tain substrings that are constituents. This can be done by giving different weights to the congruence class variables in the formula and finding the so- lution with the smallest sum of weights of its true variables.</p><p>The use of POS tags as input can also have its problems. Although we solve the lexical spar- sity problem with POS tags, at the same time we lose a lot of information. In certain cases, one POS sequence can include raw phrases which ide- ally are not grouped into the same congruence class. To mitigate this problem, we can use POS tags only for rare words and subdivide or ignore POS tags for frequent words such as determinants and prepositions. This will reduce the number of raw phrases represented by POS sequences whilst keeping lexical sparsity low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We defined a new class of PCFGs that adequately models natural language syntax. We described a learning algorithm for this class which scales well to large examples and is even capable of learning from small samples. The grammars induced by this algorithm are compact and perform well on unsupervised parsing and language modelling of typical CFLs and artificial natural language gram- mars.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>{ab, aabb, aaabbb} Congruence Classes 1 : [a], 2 : [b], 3 : [ab, aabb, aaabbb], 4 : [aa], 5 : [bb], 6 : [aab, aaabb], 7 : [abb, aabbb], 8 : [aaa], 9 : [bbb], 10 : [aaab], 11 : [abbb]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>UC7: Lukasiewicz language (S → aSS|b) and 4 described by ambiguous grammars: AC1: |w| a = |w| b AC2: 2|w| a = |w| b AC3: Dyck language AC4: Regular expressions. The 9 artificial natural language grammars are: NL1: Grammar 'a',</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Size of the alphabet, number of non- terminals and productions rules of the grammars.</figDesc><table>Relative Entropy 
UF 1 
Ex. 
|S| COMINO ADIOS COMINO ABL 
UC1 
10 
0.029 
1.876 
100 
100 
UC2 
50 
0.0 
1.799 
100 
100 
UC5 
10 
0.111 
7.706 
100 
100 
UC7 
10 
0.014 
1.257 
100 
27.86 
AC1 
50 
0.014 
4.526 
52.36 
35.51 
AC2 
50 
0.098 
6.139 
46.95 
14.25 
AC3 
50 
0.057 
1.934 
99.74 
47.48 
AC4 100 
0.124 
1.727 
83.63 
14.58 
NL7 100 
0.0 
0.124 
100 
100 
NL1 100 
0.202 
1.646 
24.08 
24.38 
NL2 200 
0.333 
0.963 
45.90 
45.80 
NL3 100 
0.227 
1.491 
36.34 
75.95 
NL5 100 
0.111 
1.692 
88.15 
79.16 
NL6 400 
0.227 
0.138 
36.28 
100 
UC3 100 
0.411 
0.864 
61.13 
100 
UC4 100 
0.872 
2.480 
42.84 
100 
UC6 100 
1.449 
1.0 
20.14 
8.36 
NL4 500 
1.886 
2.918 
65.88 
52.87 
NL8 1000 
1.496 
1.531 
57.77 
50.04 
NL9 800 
1.701 
1.227 
12.49 
28.53 

</table></figure>

			<note place="foot" n="1"> for example, if a congruence class contains the phrases &quot;the big&quot; and &quot;that small&quot;, and another class contains &quot;dog barked&quot; and &quot;cat meowed&quot;, it can be logically deduced that the phrases &quot;the big dog barked&quot;,&quot;the big cat meowed&quot;, &quot;that small dog barked&quot; and &quot;that small cat meowed&quot; should be in the same class.</note>

			<note place="foot" n="2"> The relative entropy (or Kullback-Leibler divergence) between a target distribution D and a hypothesized distri</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors acknowledge partial support by the Région des Pays de la Loire.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards High Speed Grammar Induction on Large Text Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><forename type="middle">W</forename><surname>Adriaans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marten</forename><surname>Trautwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Vervoort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOFSEM</title>
		<editor>Václav Hlavác, Keith G. Jeffery, and Jirí Wiedermann</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1963</biblScope>
			<biblScope unit="page" from="173" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Berkelaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
	<note>Interface to Lp solve v. 5.5 to solve linear/integer programs. R package version</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Boasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraud</forename><surname>Sénizergues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTS Languages Are Deterministic and Congruential. J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="332" to="342" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised Parsing with U-DOP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An All-Subtrees Approach to Unsupervised Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="865" to="872" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised Language Acquisition: Theory and Practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Sussex</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional Learning of Some Context-Free Languages with a Minimally Adequate Teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sempere and García</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="24" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards General Algorithms for Grammatical Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Marcus Hutter, Frank Stephan, Vladimir Vovk, and Thomas Zeugmann</editor>
		<imprint>
			<biblScope unit="volume">6331</biblScope>
			<biblScope unit="page" from="11" to="30" />
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Polynomial Identification in the Limit of Substitutable Context-free Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Eyraud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1725" to="1745" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised Learning and Grammar Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalom</forename><surname>Lappin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Computational Linguistics and Natural Language Processing</title>
		<editor>Alexander Clark, Chris Fox, and Shalom Lappin</editor>
		<imprint>
			<publisher>Wiley-Blackwell</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="197" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Coste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Miclet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grammatical Inference: Algorithms and Applications, 9th International Colloquium, ICGI 2008</title>
		<meeting><address><addrLine>Saint-Malo, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">5278</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Characteristic Sets for Polynomial Grammatical Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Higuera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Grammatical Inference: Learning Automata and Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Higuera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Language Identification in the Limit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Mark</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="447" to="474" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A Study of Grammatical Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James Jay</forename><surname>Horning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bayesian Inference for PCFGs via Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<editor>Candace L</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanja</forename><surname>Sidner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stone</surname></persName>
		</author>
		<title level="m">HLT-NAACL</title>
		<editor>ChengXiang Zhai</editor>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<title level="m">Prentice Hall Series in Artificial Intelligence)</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Speech and Language Processing. 2 edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Approximability of Constraint Satisfaction Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhu</forename><surname>Sudan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Trevisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1863" to="1920" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Unsupervised Learning of Natural Language Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Results of the Abbadingo One DFA Learning Competition and a New Evidence-Driven State Merging Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>Ramon López de Mántaras and Enric Plaza</editor>
		<imprint>
			<biblScope unit="volume">1433</biblScope>
			<biblScope unit="page" from="220" to="228" />
			<date type="published" when="1998" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Lecture Notes in Computer Science</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Estimation of Stochastic Context-Free Grammars using the Inside-Outside Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Lari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><forename type="middle">J</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bounding the Maximal Parsing Performance of Non-Terminally Separated Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">G Infante</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sempere and García</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Current Trends in Parsing Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paola</forename><surname>Merlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Bunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trends in Parsing Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">eg-GRIDS: Context-Free Grammatical Inference from Positive Examples Using Genetic Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Petasis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><forename type="middle">D</forename><surname>Spyropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Halatsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Georgios Paliouras and Yasubumi Sakakibara</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3264</biblScope>
			<biblScope unit="page" from="223" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Syntactic Categories: Their Identification and Description in Linguistic Theories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gisa</forename><surname>Rauh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Surveys in Syntax &amp; Morphology</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2010" />
			<publisher>OUP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast Unsupervised Incremental Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Seginer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<editor>John A. Carroll, Antal van den Bosch, and Annie Zaenen</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Sempere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grammatical Inference: Theoretical Results and Applications, 10th International Colloquium, ICGI 2010</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6339</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="11629" to="11634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bayesian learning of probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bootstrapping Structure into Language: Alignment-Based Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Menno Van Zaanen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Leeds</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Problems with Evaluation of Unsupervised Empirical Grammatical Inference Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeroen</forename><surname>Menno Van Zaanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geertzen</surname></persName>
		</author>
		<editor>Clark et al.</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="301" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Empirical Generative Framework for Computational Modeling of Language Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidi</forename><forename type="middle">R</forename><surname>Waterfall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Sandbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Onnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Edelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Language</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="671" to="703" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic Languages: A Review and Some Open Questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">S</forename><surname>Wetherell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="361" to="379" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computational Models of Syntactic Acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="213" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Identification in the Limit of k, lSubstitutable Context-Free Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Yoshinaka</surname></persName>
		</author>
		<editor>Clark et al.</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="266" to="279" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
