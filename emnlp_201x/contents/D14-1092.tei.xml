<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Joint Model for Unsupervised Chinese Word Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohong</forename><surname>Chen</surname></persName>
							<email>miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ministry of Education School of Electronics Engineering and Computer Science</orgName>
								<orgName type="laboratory">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University Beijing</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Joint Model for Unsupervised Chinese Word Segmentation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="854" to="863"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we propose a joint model for unsupervised Chinese word segmentation (CWS). Inspired by the &quot;products of ex-perts&quot; idea, our joint model firstly combines two generative models, which are word-based hierarchical Dirichlet process model and character-based hidden Markov model, by simply multiplying their probabilities together. Gibbs sampling is used for model inference. In order to further combine the strength of goodness-based model, we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler. We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff. Test results on these two datasets show that the joint model achieves much better results than all of its component models. Statistical significance tests also show that it is significantly better than state-of-the-art systems, achieving the highest F-scores. Finally, analysis indicates that compared with nVBE and HDP, the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unlike English and many other western languages, there are no explicit word boundaries in Chinese sentences. Therefore, word segmentation is a cru- cial first step for many Chinese language process- ing tasks such as syntactic parsing, information re- trieval and machine translation. A great deal of su- pervised methods have been proposed for Chinese word segmentation. While successful, they re- quire manually labeled resources and often suffer from issues like poor domain adaptability. Thus, unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora.</p><p>Previous unsupervised approaches to word seg- mentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) <ref type="bibr" target="#b7">(Kit and Wilks, 1999</ref>), accessor variety (AV) ), boundary entropy (BE) <ref type="bibr" target="#b6">(Jin and Tanaka-Ishii, 2006</ref>) and normalized vari- ation of branching entropy (nVBE) <ref type="bibr" target="#b8">(Magistry and Sagot, 2012)</ref> etc. Goodness measure based model is not segmentation model in a very strict mean- ing and is actually strong in generating word list without supervision. It inherently lacks capabil- ity to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation.</p><p>The second type focuses on designing sophis- ticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical mod- els includes Hierarchical Dirichlet process (HDP) model <ref type="bibr" target="#b3">(Goldwater et al., 2009</ref>), Nested Pitman- Yor process (NPY) model ( <ref type="bibr" target="#b9">Mochihashi et al., 2009</ref>) etc, which are actually nonparametric lan- guage models and therefor can be categorized as word-based model. Word-based model makes de- cision on wordhood of a candidate character se- quence mainly based on information outside the sequence, namely, the wordhood of character se- quences being adjacent to the concerned sequence.</p><p>Inspired by the success of character-based model in supervised word segmentation, we pro- pose a Bayesian HMM model for unsupervised Chinese word segmentation. With the Bayesian HMM model, we formulate the unsupervised seg- mentation tasks as procedure of tagging positional tags to characters. Different from word-based model, character-based model like HMM-based model as we propose make decisions on word- hood of a candidate character sequence based on information inside the sequence, namely, ability of characters to form words. Although the Bayesian HMM model alone does not produce competi- tive results, it contributes substantially to the joint model as proposed in this paper.</p><p>Our joint model takes advantage from three dif- ferent models: namely, a character-based model (HMM-based), a word-based model (HDP-based) and a goodness measure based model (nVBE model). The combination of HDP-based model and HMM-based model enables to utilize infor- mation of both word-level and character-level. We also show that using nVBE model as initialization model could further improve the performance to outperform the state-of-the-art systems and leads to improvement in both wordhood judgment and disambiguation ability.</p><p>Word segmentation systems are usually eval- uated with metrics like precision, recall and F- Score, regardless of supervised or unsupervised. Following normal practice, we evaluate our model and compare it with state-of-the-art systems us- ing F-Score. However, we argue that the ability to solve segmentation ambiguities is also impor- tant when evaluating different types of unsuper- vised word segmentation systems. This paper is organized as follows. In Section 2, we will introduce several related systems for unsupervised word segmentation. Then our joint model is presented in Section 3. Section 4 shows our experiment results on the benchmark datasets and Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Unsupervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian meth- ods.</p><p>There have been a plenty of work that is based on a specific goodness measure. <ref type="bibr" target="#b17">Zhao and Kit (2008)</ref> compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as De- scription Length Gain (DLG) proposed by <ref type="bibr" target="#b7">Kit and Wilks (1999)</ref>, Accessor Variety (AV) proposed by  and Boundary Entropy <ref type="bibr" target="#b6">(Jin and Tanaka-Ishii, 2006</ref>). A notable goodness-based method is ESA: "Evaluation, Selection, Adjust- ment", which is proposed by <ref type="bibr" target="#b15">Wang et al. (2011)</ref> for unsupervised Mandarin Chinese word segmen- tation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disad- vantage of ESA is that it needs to iterate the pro- cess several times on the corpus to get good perfor- mance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper ex- ponent). Another notable work is nVBE: Mag- istry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi decoding, they improve performance over <ref type="bibr" target="#b6">Jin and Tanaka-Ishii (2006)</ref> and remove most of the parameters and thresholds from the model. Nonparametric Bayesian models also achieved state-of-the-art performance in unsupervised word segmentation. <ref type="bibr" target="#b3">Goldwater et al. (2009)</ref> introduced a unigram and a bigram model for unsupervised word segmentation, which are based on Dirichlet process and hierarchical Dirichlet process ( <ref type="bibr" target="#b11">Teh et al., 2006</ref>) respectively. The main drawback is that it needs almost 20,000 iterations before the Gibbs sampler converges. <ref type="bibr" target="#b9">Mochihashi et al. (2009)</ref> ex- tended this method by introducing a nested charac- ter model and an efficient blocked Gibbs sampler. Their method is based on what they called nested Pitman-Yor language model. One disadvantage of goodness measure based methods is that they do not have any disambigua- tion ability in theory in spite of their competitive performances. This is because once the goodness measure is given, the decoding algorithm will seg- ment any ambiguous strings into the same word sequences, no matter what their context is. In contrast, nonparametric Bayesian language mod- els aim to segment character string into a "reason- able" sentence according to the posterior probabil- ity. Thus, theoretically, this method should have better ability to solve ambiguities over goodness measure based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Combining HDP and HMM</head><p>In supervised Chinese word segmentation lit- erature, word-based approaches and character- based approaches often have complementary ad- vantages ( <ref type="bibr" target="#b14">Wang et al., 2010)</ref>.Since the two types of model try to solve the problem from different perspectives and by utilizing different levels of in- formation (word level and character level). In un- supervised Chinese word segmentation literature, the HDP-base model can be viewed as a typi- cal word-based method. And we can also build a character-based unsupervised model by using a hidden Markov model. We believe that the HDP- based model and the HMM-based model are also complementary with each other, and a combina- tion of them will take advantage of both and thus capture different levels of information. Now the problem we are facing is how to com- bine these two models. To keep the joint model simple and involve as little extra parameters as possible, we combine the two baseline models by just multiplying their probabilities together and then renormalizing it. Let C = c 1 c 2 · · · c |C| be a string of characters and W = w 1 w 2 · · · w |W | is the corresponding segmented words sequence. Then the conditional probability of the segmentation W given the character string C in our joint model is defined as:</p><formula xml:id="formula_0">P J (W |C) = 1 Z(C) P D (W |C)P M (W |C) (1)</formula><p>where P D (W |C) is the probability from the HDP model as given in Equation 6 and P M (W |C) is the probability given by the Bayesian HMM model as given in Equation 2. Z(C) is a nor- malization term to make sure that P J (W |C) is a probability distribution. The combining method is inspired by Hinton (1999), which proved that it is possible to combine many individual expert mod- els by multiplying the probabilities and then renor- malizing it. They called it "product of experts". We can see that combining models in this way does not involve any extra parameters and Gibbs sampling can be easily used for model inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bayesian HMM</head><p>The dominant method for supervised Chinese word segmentation is character-based model which was first proposed by <ref type="bibr" target="#b16">Xue (2003)</ref>. This method treats word segmentation as a tagging problem, each tag indicates the position of a char- acter within a word. The most commonly used tag set is {Single, Begin, Middle, End}. Specifi- cally, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the charac- ter is in the middle of the word. Existing models are trained on manually annotated data in a super- vised way based on discriminative models such as Conditional Random Fields ( <ref type="bibr" target="#b12">Peng et al., 2004;</ref><ref type="bibr" target="#b13">Tseng et al., 2005</ref>). Supervised character-based methods make full use of character level informa- tion and thus have been very successful in the last decade. However, no unsupervised model has uti- lized character level information in the way as su- pervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation:</p><formula xml:id="formula_1">P M (W |C) = |C| i=1 P t (t i |t i−1 )P e (c i |t i ) (2)</formula><p>where C and W have the same meaning as be- fore. P t (t i |t i−1 ) is the transition probability of tag t i given its former tag t i−1 and P e (c i |t i ) is the emission probability of character c i given its tag t i . This model can be easily trained with Maximum Likelihood Estimation (MLE) on annotated data or with Expectation Maximization (EM) on raw texts. But using any of this methods will make it difficult to combine it with the HDP-based model. Instead, we propose a Bayesian HMM for unsu- pervised word segmentation. The Bayesian HMM model is defined as follows:</p><formula xml:id="formula_2">t i |t i−1 = t, p t ∼ M ult(p t ) c i |t i = t, e t ∼ M ult(e t ) p t |θ ∼ Dirichlet(θ) e t |σ ∼ Dirichlet(σ)</formula><p>where p t and e t are transition and emission dis- tributions, θ and σ are the symmetric parameters of Dirichlet distributions. Now suppose we have observed tagged text h, then the conditional prob- ability P M (w i |w i−1 = l, h) can be obtained:</p><formula xml:id="formula_3">P M (w i |w i−1 = l, h) = |w i | j=1 P t (t j |t j−1 , h)P e (c j |t j , h) (3)</formula><p>where &lt; w i−1 , w i &gt; is a word bigram, l is the in- dex of word w i−1 , c j is the jth character in word w i and t j is the corresponding tag.P t (t j |t j−1 , h) and P e (c j |t j , h) are the posterior probabilities, they are given as:</p><formula xml:id="formula_4">P t (t j |t j−1 , h) = n &lt;t j−1 ,t j &gt; + θ n &lt;t j−1 , * &gt; + T θ (4) P e (c j |t j , h) = n &lt;t j ,c j &gt; + σ n &lt;t j , * &gt; + V σ<label>(5)</label></formula><p>where n &lt;t j−1 ,t j &gt; is the tag bigram count of &lt; t j−1 , t j &gt; in h, n &lt;t j ,c j &gt; denotes the number of oc- currences of tag t j and character c j , and * means a sum operation. T and V are the size of character tag set (we follow the commonly used {SBME} tag set and thus T = 4 in this case) and character vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">HDP Model</head><p>Goldwater et al. <ref type="formula" target="#formula_10">(2009)</ref> proposed a nonparametric Bayesian model for unsupervised word segmenta- tion which is based on HDP ( <ref type="bibr" target="#b11">Teh et al., 2006</ref>). In this model, the conditional probability of the seg- mentation W given the character string C is de- fined as:</p><formula xml:id="formula_5">P D (W |C) = |W | i=0 P D (w i |w i−1 )<label>(6)</label></formula><p>where w i is the ith word in W . This is actually a nonparametric bigram language model. This bi- gram model assumes that each different word has a different distribution over words following it, but all these different distributions are linked through a HDP model:</p><formula xml:id="formula_6">w i |w i−1 = l ∼ G l G l ∼ DP (α 1 , G 0 ) G 0 ∼ DP (α, H)</formula><p>where DP denotes a Dirichlet process. Suppose we have observed segmentation re- sult h, then we can get the posterior probability</p><formula xml:id="formula_7">P D (w i |w i−1 = l, h) by integrating out G l : P D (w i |w i−1 = l, h) = n &lt;w i−1 ,w i &gt; + α 1 P D (w i |h) n &lt;w i−1 , * &gt; + α 1<label>(7)</label></formula><p>where n &lt;w i−1 ,w i &gt; denotes the total number of oc- currences of the bigram &lt; w i−1 , w i &gt; in the ob- servation h. And P D (w i |h) can be got by integrat- ing out G 0 :</p><formula xml:id="formula_8">P D (w i |h) = t w i + αH(w i ) t + α<label>(8)</label></formula><p>where t w i denotes the number of tables associ- ated with w i in the Chinese Restaurant Franchise metaphor ( <ref type="bibr" target="#b11">Teh et al., 2006</ref>), t is the total number of tables and H(w i ) is the base measure of G 0 . In fact, H(w i ) is the prior distribution over words, so prior knowledge can be injected in this distribution to enhance the performance. In <ref type="bibr" target="#b3">Goldwater et al. (2009)</ref>'s work, the base measure H(w i ) are defined as a character unigram model:</p><formula xml:id="formula_9">H(w i ) = (1 − p s ) |w i |−1 p s j P (c ij )</formula><p>where, p s is the probability of generating a word boundary. P (c ij ) is the probability of the jth char- acter c ij in word w i , this probability can be esti- mated from the training data using maximum like- lihood estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Initializing with nVBE</head><p>Among various goodness measure based models, we choose nVBE <ref type="bibr" target="#b8">(Magistry and Sagot, 2012</ref>) to initialize our Gibbs sampler with its segmentation results. nVBE achieved a relatively high perfo- mance over other goodness measure based meth- ods. And it's very simple as well as efficient.</p><p>Theoretically, the Gibbs sampler may be initial- ized at random or using any other methods. Initial- ization does not make a difference since the Gibbs sampler will eventually converge to the posterior distribution if it iterates as much as possible. This is an essential attribute of Gibbs sampling. How- ever, we believe that initializing the Gibbs sam- pler with the result of nVBE will benefit us in two ways. On one hand, in consideration of its combination of nonparametric Bayesian method and goodness-based method, it will improve the overall performance as well as solve more seg- mentation ambiguities with the help of HDP-based model. On the other hand, it makes the conver- gence of Gibbs sampling faster. In practice, ran- dom initialization often leads to extremely slow convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inference with Gibbs Sampling</head><p>In our proposed joint model, Gibbs sam- pling (Casella and George, 1992) can be easily used to identify the highest probability segmen- tation from among all possibilities. Following <ref type="bibr" target="#b3">Goldwater et al. (2009)</ref>, we can repeatedly sample from potential word boundaries. Each boundary variable can only take on two possible values, cor- responding to a word boundary or not word bound- ary.</p><p>For instance, suppose we have obtained a seg- mentation result β|c i−2 c i−1 c i c i+1 c i+2 |γ, where β and γ are the words sequences to the left and</p><note type="other">right and c i−2 c i−1 c i c i+1 c i+2 are characters be- tween them. Now we are sampling at location i to decide whether there is a word boundary be- tween c i and c i+1 . Denote h 1 as the hypothesis that it forms a word boundary (the correspond- ing result is βw 1 w 2 γ where w 1 = c i−2 c i−1 c i and w 2 = c i+1 c i+2 ), and h 2 as the opposite hypoth- esis (then the corresponding result is βwγ where w = c i−2 c i−1 c i c i+1 c i+2 ). The posterior probabil- ity for these two hypotheses would be:</note><formula xml:id="formula_10">P (h 1 |h − ) ∝ P D (h 1 |h − )P M (h 1 |h − )<label>(9)</label></formula><formula xml:id="formula_11">P (h 2 |h − ) ∝ P D (h 2 |h − )P M (h 2 |h − )<label>(10)</label></formula><p>where P D (h|h − ) and P M (h|h − ) are the pos- terior probabilities in HDP-based model and in HMM-based model, and h − denotes the current segmentation results for all observed data except c i−2 c i−1 c i c i+1 c i+2 . Note that the normalization term Z(C) can be ignored during inference. The posterior probabilities for these two hypotheses in the HDP-based model is given as:</p><formula xml:id="formula_12">P D (h 1 |h − ) = P D (w 1 |w l , h − ) × P D (w 2 |w 1 , h − )P D (w r |w 2 , h − ) (11) P D (h 2 |h − ) = P D (w|w l , h − ) × P D (w r |w, h − )<label>(12)</label></formula><p>where w l (w r ) is the first word to the left (right) of w. And the posterior probabilities for the Bayesian HMM model is given as:</p><formula xml:id="formula_13">P M (h 1 |h − ) ∝ i+2 j=i−2 P t (t j |t j−1 , h − )P e (c j |t j , h − ) (13) P M (h 2 |h − ) ∝ i+2 j=i−2 P t (t j |t j−1 , h − )P e (c j |t j , h − ) (14)</formula><p>where P t (t j |t j−1 , h − ) and P e (c j |t j , h − ) are given in Equation 4 and 5. The difference is that un- der hypothesis h 1 , c i−2 c i−1 c i c i+1 c i+2 are tagged as "BMEBE" and under hypothesis h 2 as "BM- MME".</p><p>Once the Gibbs sampler is converged, a natu- ral way to is to treat the result of last iteration as the final segmentation result, since each set of as- signments to the boundary variables uniquely de- termines a segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we test our joint model on PKU and MSRA datesets provided by the Second Seg- mentation Bake-off <ref type="bibr">(SIGHAN 2005)</ref>  <ref type="bibr" target="#b1">(Emerson, 2005</ref>). Most previous works reported their results on these two datasets, this will make it convenient to directly compare our joint model with theirs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setting</head><p>The second SIGHAN Bakeoff provides several large-scale labeled data for evaluating the per- formance of Chinese word segmentation systems. Two of the four datasets are used in our exper- iments. Both of the dataset contains only sim- plified Chinese. <ref type="table">Table 1</ref> shows the statistics of the two selected corpus. For development set, we randomly select a small subset (about 10%) of the training data. Specifically, 2000 sentences are selected for PKU corpus and 8000 sentences for MSRA corpus. The rest training data plus the test set is then combined for segmentation but only test data is used for evaluation. The development set is used to tune parameters of the HDP-based model and HMM-based model separately. Since our joint model does not involve any additional parameters, we reuse the parameters of the HDP-based model and HMM-based model in the joint model. Specif- ically, we set α 1 = 1000.0, α = 10.0, p s = 0.5 for the HDP-based model and set θ = 1.0, σ = 0.01 for the HMM-based model. For evaluation, we use standard F-Score on words for all following experiments. F-Score is the harmonic mean of the word precision and re- call. Precision is given as:</p><formula xml:id="formula_14">P =</formula><p>#correct words in result #total words in result and recall is given as: R = #correct words in result #total words in gold corpus then F-Score is calculated as: <ref type="table">Table 1</ref>: Statistics of training and testing data <ref type="bibr" target="#b5">Huang and Zhao (2007)</ref> provided an empirical method to estimate the consistency between the four different segmentation standards involved in the Bakeoff-3. A lowest consistency rate 84.8% is found among the four standards. <ref type="bibr" target="#b17">Zhao and Kit (2008)</ref> considered this figure as the upper bound for any unsupervised Chinese word segmentation systems. We also use it as the topline in our com- parison.</p><formula xml:id="formula_15">F = 2 × R × F R + F Corpus TrainingSize (words) TestSize (words) PKU 1.1M 104K MSRA 2.37M 107K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prior Knowledge Used</head><p>When it comes to the evaluation and compari- son for unsupervised word segmentation systems, an important issue is what kind of pre-processing steps and prior knowledge are needed. To be fully unsupervised, any prior knowledge such as punc- tuation information, encoding scheme and word length could not be used in principle. Neverthe- less, information like punctuation can be easily in- jected to most existing systems and significantly enhance the performance. The problem we are faced with is that we don't know for sure what kind of prior information are used in other sys- tems. One may use a small punctuation set to segment a long sentence into shorter ones, while another may write simple regular expressions to identify dates and numbers. Lot of work we com- pare to don't even mention this subject.</p><p>Fortunately, we notice that <ref type="bibr" target="#b15">Wang et al. (2011)</ref> provided four kinds of preprocessings (they call settings). In their settings 1 and 2, punctuation and other encoding information are not used. In setting 3, punctuation is used to segment charac- ter sequences into sentences, and both punctuation and other encoding information are used in setting 4. Then the results reported in Magistry and Sagot (2012) relied on setting 3 and setting 4. In order to make the comparison as fair as possible, we use setting 3 in our experiment, i.e., only a punctua- tion set for simplified Chinese is used in all our experiments. We will compare our experiment re- sults to previous work on the same setting if they are provided . <ref type="table">Table 2</ref> summarizes the F-Scores obtained by dif- ferent models on PKU and MSRA corpus, as well as several state-of-the-art systems. Detailed infor- mation about the presented models are listed as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment Results</head><p>• nVBE: the model based on Variation of Branching Entropy in <ref type="bibr" target="#b8">Magistry and Sagot (2012)</ref>. We re-implement their model on set- ting 3 1 .</p><p>• HDP: the HDP-based model proposed by <ref type="bibr" target="#b3">Goldwater et al. (2009)</ref>, initialized randomly.</p><p>• HDP+HMM: the model combining HDP- based model and HMM-based model as pro- posed in Section 3, initialized randomly.</p><p>• HDP+nVBE: the HDP-based model, initial- ized with the results of nVBE model.</p><p>• Joint: the "HDP+HMM" model initialized with nVBE model.</p><p>• ESA: the model proposed in <ref type="bibr" target="#b15">Wang et al. (2011)</ref>, as mentioned above, the conducted experiments on four different settings, we re- port their results on setting 3.</p><p>• NPY(2): the 2-gram language model pre- sented by <ref type="bibr" target="#b9">Mochihashi et al. (2009)</ref>.</p><p>• NPY(3): the 3-gram language model pre- sented by <ref type="bibr" target="#b9">Mochihashi et al. (2009)</ref>.</p><p>For all of our Gibbs samplers, we run 5 times to get the averaged F-Scores. We also give the vari- ance of the F-Scores in <ref type="table">Table 2</ref>. For each run, we find that random initialization takes around 1,000 iterations to converge, while initialing with nVBE only takes as few as 10 iterations. This <ref type="table">makes System   PKU  MSRA  R  P  F  R  P  F  nVBE</ref> 78.3 77.5 77.9 79.1 77.3 78.2 HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020) HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013) HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005) Joint 83.1 79.2 81.1(0.002) 84.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">79.3 81.7(0.005) ESA N/A N/A 77.4 N/A N/A 78.4 NPY(2) N/A N/A N/A N/A N/A 80.NPY(3) N/A N/A N/A N/A N/A 80.7 Topline</head><p>N/A N/A 84.8 N/A N/A 84.8 <ref type="table">Table 2</ref>: Experiment results and comparison to state-of-the-art systems. The figures in parentheses denote the variance the of F-Scores.</p><p>our joint model very efficient and possible to work in practical applications as well. At last, a single sample (the last one) is used for evaluation.</p><p>From <ref type="table">Table 2</ref>, we can see that the joint model (Joint) outperforms all the presented sys- tems in F-Score on all testing corpora. Specifi- cally, comparing "HDP+HMM" with "HDP", the former model increases the overall F-Score from 68.7% to 75.3% (+6.6%) in PKU corpora and from 69.9% to 76.3% (+6.4%) in MSRA corpora, which proves that the character information in the HMM-based model can actually enhance the performance of the HDP-based model. Compar- ing "HDP+nVBE" with "HDP", the former model also increases the overall F-Score by 10.6%/9.6% in PKU/MSRA corpora, which demonstrates that initializing the HDP-based model with nVBE will improve the performance by a large margin. Fi- nally, the joint model "Joint" take advantage from both from the character-based HMM model and the nVBE model, it achieves a F-Score of 81.1% on PKU and 81.7% on MSRA. This result outper- forms all its component baselines such as "HDP", "HDP+HMM" and "HDP+nVBE".</p><p>Our joint model also shows competitive advan- tages over several state-of-the-art systems. Com- pared with nVBE,the F-Score increases by 3.2% on PKU corpora and by 3.5% on MSRA cor- pora. Compared with ESA, the F-Score increases by 3.7%/3.3% in PKU/MSRA corpora. Lastly, compared to the nonparametric Bayesian models (NPY(n)), our joint model still increases the F- Score by 1.5% (NPY <ref type="formula">(2)</ref>) and 1.0% (NPY(3)) on MSRA corpora. Moreover, compared with the empirical topline figure 84.8%, our joint model achieves a pretty close F-Score. The differences are 3.7% on PKU corpora and 3.1% on MSRA corpora.</p><p>An phenomenon we should pay attention to is the poor performance of the HMM-based model. With our implementation of the Bayesian HMM, we achieves a 34.3% F-Score on PKU corpora and a 34.9% F-Score on MSRA corpora, just slightly better than random segmentation. The result show that the hidden Markov Model alone is not suit- able for character-based Chinese word segmenta- tion problem. However, it still substantially con- tributes to the joint model. We find that the variance of the results are rather small, this shows the stability of our Gibbs sam- plers. From the segmentation results generated by the joint model, we also found that quite a large amount of errors it made are related to dates, numbers (both Chinese and English) and English words. This problem can be easily addressed dur- ing preprocessing by considering encoding infor- mation as previous work, and we believe this will bring us much better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Disambiguation Ability</head><p>Previous unsupervised work usually evaluated their models using F-score, regardless of goodness measure based model or nonparametric Bayesian model. However, segmentation ambiguity is a very important factor influencing accuracy of Chinese word segmentation systems <ref type="bibr" target="#b5">(Huang and Zhao, 2007)</ref>. We believe that the disambigua- tion ability of the models should also be consid- ered when evaluating different types of unsuper- vised segmentation systems, since different type of models shows different disambiguation ability. We will compare the disambiguation ability of dif-ferent systems in this section.</p><p>In general, there are mainly two kinds of ambi- guity in Chinese word segmentation problem:</p><p>• Combinational Ambiguity: Given charac- ter strings "A" and "B", if "A", "B", "AB" are all in the vocabulary, and "AB" or "A-B" (here "-" denotes a space) occurred in the real text,then "AB" can be called a combinational ambiguous string.</p><p>• Overlapping Ambiguity: Given character strings "A", "J" and "B", if "A", "B", "AJ" and "JB" are all in the vocabulary, and "A- JB" or "AJ-B" occurred in the real text, then "AJB" can be called an overlapping ambigu- ous string.</p><p>We count the total number of mistakes differ- ent systems made at ambiguous strings (the vo- cabulary is obtained from the gold standard an- swer of testing set). As we have mentioned in Section 2, goodness measure based methods such as nVBE do not have any disambiguation ability in theory. Our observation is identical to this ar- gument. We find that nVBE always segments am- biguous strings into the same result. Take a combi- national string "" as an example, " (just)", " (have)" and " (only)" are all in the vo- cabulary. In the PKU test set, this string occurs 14 times as "-(just have)" and 18 times as " (only)", 32 times in total. nVBE segments all the 32 strings into " (only)" (i.e. 18 of them are correct), while the joint model segments it 22 times as " (only)" and 10 times as "- (just have)" according to its context, and 24 of them are correct. <ref type="table" target="#tab_1">Table 3 and 4</ref> show the statistics of combi- national ambiguity and overlapping ambiguity re- spectively. The numbers in parentheses denote the total number of ambiguous strings. From these tables, we can see that HDP+nVBE makes less mistakes than nVBE in most circumstances, ex- cept that it solves less combinational ambigui- ties on MSRA corpora. But our proposed joint model solves the most combinational and over- lapping ambiguities, on both PKU and MSRA corpora. Specifically, compared to nVBE, the joint model correctly solves 171/871 more com- binational ambiguities on PKU/MSRA corpora, which is a 0.6%/13.8% relative error reduction. It also solves 28/45 more overlapping ambiguities on PKU/MSRA corpora, which is a 11.5%/23.4% relative error reduction. This indicates that the joint model has a stronger ability of disambigua- tion over the compared systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>PKU <ref type="formula" target="#formula_4">(35371)</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Statistical Significance Test</head><p>The main results presented in <ref type="table">Table 2</ref> has shown that our proposed joint model outperforms the two baselines as well as state-of-the-art systems. But it is also important to know if the improve- ment is statistically significant over these sys- tems. So we conduct statistical significance tests of F-scores among these various models. Follow- ing <ref type="bibr" target="#b14">Wang et al. (2010)</ref>, we use the bootstrapping method ( <ref type="bibr" target="#b18">Zhang et al., 2004</ref>). Here is how it works: suppose we have a testing set T 0 to test several word segmentation systems, there are N testing examples (sentences or line of characters) in T 0 . We create a new testing set T 1 with N examples by sampling with replacement from T 0 , then repeat these process M − 1 times. And we will have a total M + 1 testing sets. In our test procedures, M is set to 2000.</p><p>Since we just implement our joint model and its component models, we can not generate paired samples for other models (i.e. ESA and NPY(n)). Instead, we follow <ref type="bibr" target="#b14">Wang et al. (2010)</ref>'s method and first calculate the 95% confidence interval for our proposed model. Then other systems can be compared with the joint model in this way: if the F-score of system B doesn't fall into the 95% con- fidence interval of system A, they are considered as statistically significantly different from each other.</p><p>For all significant tests, we measure the 95% confidence interval for the difference between two models. First, the test results show that "HDP+nVBE" and "HDP+HMM" are both sig- nificantly better than "HDP". Second, the "Joint" model significantly outperforms all its component models, including "HDP", "nVBE", "HDP+nVBE" and "HDP+HMM". Finally, the comparison also shows that the joint model signif- icantly outperforms state-of-the-art systems like ESA and NPY(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a joint model for un- supervised Chinese word segmentation. Our joint model is a combination of the HDP-based model, which is a word-based model, and HMM-based model, which is a character-based model. The way we combined these two component base- lines makes it natural and simple to inference with Gibbs sampling. Then the joint model take ad- vantage of a goodness-based method (nVBE) by using it to initialize the sampler. Experiment re- sults conducted on PKU and MSRA datasets pro- vided by the second SIGHAN Bakeoff show that the proposed joint model not only outperforms the baseline systems but also achieves better perfor- mance (F-Score) over several state-of-the-art sys- tems. Significance tests showed that the improve- ment is statistically significant. Analysis also in- dicates that the joint model has a stronger abil- ity to solve ambiguities in Chinese word segmen- tation. In summary, the joint model we pro- posed combines the strengths of character-based model, nonparametric Bayesian language model and goodness-based model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Statistics of combinational ambiguity. 
This table shows the total number of mistakes 
made by different systems at combinational am-
biguous strings. The numbers in parentheses de-
note the total number of combinational ambiguous 
strings. 

System 
PKU(603) MSRA(467) 
nVBE 
244 
192 
HDP+nVBE 
239 
164 
Joint 
216 
157 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Statistics of overlapping ambiguity. This 
table shows the total number of mistakes made 
by different systems at overlapping ambiguous 
strings. The numbers in parentheses denote the to-
tal number of overlapping ambiguous strings. 

</table></figure>

			<note place="foot" n="3"> Joint Model In this section, we will discuss our joint model in detail.</note>

			<note place="foot" n="1"> The results we got with our implementation is slightly lower than what was reported in Magistry and Sagot (2012). According to Pei et al. (2013), they had contacted the authors and confirmed that the higher results was due to a bug in code. So we report the results with our bug free implementation as Pei et al. (2013) did. Our reported results are identical to those of Pei et al. (2013)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The contact author of this paper, according to the meaning given to this role by Key Labora-tory of Computational Linguistics, Ministry of Ed-ucation, <ref type="bibr">School</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explaining the Gibbs sampler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="167" to="174" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second international chinese word segmentation bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
		<idno>133. MLA</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Accessor variety criteria for Chinese word extraction Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="75" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Products of experts. Artificial Neural Networks. Ninth International Conference on</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chinese word segmentation: A decade review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised segmentation of Chinese text by use of branching entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumiko</forename><surname>Tanaka-Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of word boundary with description length gain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguis-tics</title>
		<meeting><address><addrLine>Bergen, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Proceedings of the CoNLL99 ACL Workshop</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervized word segmentation: the case for mandarin chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Magistry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="383" to="387" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Mochihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A Refined HDP-Based Model for Unsupervised Chinese Word Segmentation. Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="44" to="51" />
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Beal</surname></persName>
		</author>
		<title level="m">Sharing Clusters among Related Groups: Hierarchical Dirichlet Processes. NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Chinese segmentation and new word detection using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangfang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="562" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A conditional random field word segmenter for sighan bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fourth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A character-based joint model for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1173" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A new unsupervised approach to word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanshi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="421" to="454" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="29" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Goodness Measures for Unsupervised Chinese Word Segmentation with a Unified Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Kit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNLP</title>
		<imprint>
			<biblScope unit="page" from="6" to="16" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Interpreting BLEU/NIST scores: How much improvement do we need to have a better system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>LREC</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
