<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deriving Machine Attention from Human Rationales</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018. 1903</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<country>MIT</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deriving Machine Attention from Human Rationales</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1903" to="1913"/>
							<date type="published">October 31-November 4, 2018. 2018. 1903</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15% average error reduction on benchmark datasets. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attention-based models have become architec- tures of choice for many NLP tasks. In addi- tion to significant performance gains, these mod- els are attractive, as attention is often used as a proxy for human interpretable rationales. Their success, however, is conditioned on access to large amounts of training data. To make these mod- els applicable in low-resource scenarios, we utilize this connection in the opposite direction. Specif- ically, we propose an approach to map human ra- tionales to high-performing attention, and use this attention to guide models trained in low-resource scenarios.</p><p>The notions of rationale and attention are closely related. Both of them highlight word im- portance for the final prediction. In the case of rationale, the importance is expressed as a hard selection, while attention provides a soft distribu- tion over the words. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this re- latedness. One obvious approach to improve low- <ref type="bibr">1</ref> Our code and data are available at https://github. com/YujiaBao/R2A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task: Hotel location</head><p>label: negative a nice and clean hotel to stay for business and leisure . but the location is not good if you need public transport . it took too long for transport and waiting for bus . but the swimming pool looks good .</p><p>Task: Beer aroma label: positive poured a deep brown color with little head that dissipated pretty quickly . aroma is of sweet maltiness with chocolate and caramel notes . flavor is also of chocolate and caramel maltiness . mouthfeel is good a bit on the thick side . drinkability is ok . this is to be savored not sessioned . resource performance is to directly use human ra- tionales as a supervision for attention generation. The implicit assumption behind this method is that machine-generated attention should mimic human rationales. However, rationales on their own are not adequate substitutes for machine attention. In- stead of providing a soft distribution, human ratio- nales only provide the binary indication about rel- evance. Furthermore, rationales are subjectively defined and often vary across annotators. Finally, human rationales are not customized for a given model architecture. In contrast, machine attention is always derived as a part of a specific model ar- chitecture.</p><p>To further understand this connection, we em- pirically compare models informed by human ra- tionales and those by high-quality attention. To obtain the latter, we derive an "oracle" attention using a large amount of annotations. This "ora- cle" attention is then used to guide a model that only has access to a small subset of this training data. Not only does this model outperform the oracle-free variant, but it also yields substantial gains over its counterpart trained with human ra-tionales -89.98 % vs 85.22 % average accuracy on three aspects of hotel review (see Section 4 for details). In practice, however, this "oracle" atten- tion is not available. To employ this method, we need to find a way to obtain a substitute for the "oracle" attention.</p><p>In this paper, we show how to achieve this goal using rationales. Specifically, we learn a map- ping from human rationales to high-quality atten- tion (R2A). We hypothesize that this mapping is generalizable across tasks and thus can be trans- ferred from resource-rich tasks. 2 <ref type="figure" target="#fig_0">Figure 1</ref> illus- trates that in both tasks, attention weighs ratio- nale words in a similar fashion: highlighting task- specific nouns and adjectives, while downplaying functional words. To learn and apply this mapping we need access to rationales in both source and tar- get tasks. In the target task, we assume rationales are provided by humans. In the source task(s), col- lecting rationales at scale is infeasible. Therefore, we use machine-generated rationales ( <ref type="bibr" target="#b16">Lei et al., 2016</ref>) as a proxy.</p><p>Our R2A model consists of three components. The first one is an attention-based model for the source task(s) that provides supervision for at- tention generation. The second component fo- cuses on learning a domain-invariant representa- tion to support transfer. The third component combines this invariant representation and ratio- nales together to generate the attention. These three components are trained jointly to optimize the overall objective. Once the model is trained, we apply it to the target task to generate attention from human rationales. This attention is conse- quently used to supervise the training of the target classifier.</p><p>We evaluate our approach on two transfer set- tings: aspect transfer within single domain and do- main transfer across multiple domains. Our exper- iments demonstrate that our approach delivers sig- nificant performance improvements over the base- lines. For instance, the average error reduction over the best baseline in domain transfer is over 15%. In addition, both qualitative and quantitative analyses confirm that our R2A model is capable of generating high-quality attention for target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Attention-based models Attention has been shown to be effective when the model is trained on large amounts of training data ( <ref type="bibr" target="#b1">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b20">Luong et al., 2015;</ref><ref type="bibr">Rush et al., 2015;</ref><ref type="bibr" target="#b29">Yang et al., 2016;</ref><ref type="bibr" target="#b17">Lin et al., 2017;</ref><ref type="bibr" target="#b27">Vaswani et al., 2017)</ref>. In this setting, typically no additional supervision is required for learn- ing the attention. Nevertheless, further refining attention by extra supervision has been shown to be beneficial. Examples include using word alignments to learn attention in neural machine translation ( <ref type="bibr" target="#b18">Liu et al., 2016)</ref>, employing argu- ment words to supervise attention in event de- tection ( <ref type="bibr" target="#b19">Liu et al., 2017)</ref>, utilizing linguistically- motivated annotations to guide attention in con- stituency parsing ( <ref type="bibr" target="#b13">Kamigaito et al., 2017</ref>). These supervision mechanisms are tailored to specific applications. In contrast, our approach is based on the connection between rationales and attention, and can be used for multiple applications.</p><p>Rationale-based models <ref type="bibr" target="#b30">Zaidan et al. (2007)</ref> was the first to explore the value of rationales in low-resource scenarios. They hypothesize that the model confidence should decrease when the rationale words are removed from the inputs, and validate this idea for linear models. Recent work ( <ref type="bibr" target="#b31">Zhang et al., 2016</ref>) explores the potential of integrating rationales with more complex neu- ral classifiers. In their model, human rationales are directly used to guide the sentence-level atten- tion for a CNN-based classifier. To reach good performance, their model still requires a sufficient amount of training data. Our work differs from theirs as we discern the intrinsic difference be- tween human rationales and machine attention. Moreover, we learn a model to map human ratio- nales into high-quality attention so as to provide a richer supervision for low-resource models.</p><p>Transfer learning When labeled data on the tar- get task is available, existing approaches typically transfer the knowledge by either fine-tuning an encoder trained on the source tasks(s) ( <ref type="bibr" target="#b7">Conneau et al., 2017;</ref><ref type="bibr" target="#b24">Peters et al., 2018)</ref> or multi-task learn- ing on all tasks with a shared encoder <ref type="bibr" target="#b6">(Collobert et al., 2011)</ref>. In this paper, we explore the trans- ferability of the task-specific attention through hu- man rationales. We believe this will further assist learning in low-resource scenarios.</p><p>Our work is also related to unsupervised domain adaptation, as the R2A model has never seen any target annotations during training. Existing meth- ods commonly adapt the classifier by aligning the representations between the source and target do- mains <ref type="bibr" target="#b9">(Glorot et al., 2011;</ref><ref type="bibr" target="#b5">Chen et al., 2012;</ref><ref type="bibr" target="#b33">Zhou et al., 2016;</ref><ref type="bibr" target="#b8">Ganin et al., 2016;</ref><ref type="bibr" target="#b32">Zhang et al., 2017)</ref>.</p><p>In contrast, our model adapts the mapping from ra- tionales to attention; thus after training, it can be applied to different target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Problem formulation We assume that we have</p><formula xml:id="formula_0">N source tasks {S i } N i=1</formula><p>, where each of them has sufficient amounts of labeled examples. Using ex- isting methods ( <ref type="bibr" target="#b16">Lei et al., 2016)</ref>, we can gener- ate rationales for each source example automati- cally (see Appendix 1 for details). In the target task T , we only have a limited amount of labeled examples with large amounts of unlabeled data. For those labeled examples, we assume access to human-annotated rationales.</p><p>Overview Our goal is to improve classification performance on the target task by learning a map- ping from human rationales to high-quality ma- chine attention (R2A). Given the scarcity of our target data, we learn this mapping on resource rich tasks where high-quality attention can be readily obtained during training. Next, the mapping be- tween rationales and attention derived from the source tasks is exported into the target task. To enable this transfer, models have to operate over an invariant representation which we construct via an adversarial objective. Once the mapping is de- rived, we can translate human rationales in the tar- get task into high-quality attention. This generated attention is then used to provide additional training signal for an attention-based classifier for the tar- get task. The overall pipeline is shown in <ref type="figure">Figure 2</ref>.</p><p>Alternatively, we can view the R2A mapping as a meta model that produces a prior over the atten- tion distribution across different tasks. <ref type="figure">Figure 3</ref> illustrates the ar- chitecture of our R2A model, which consists of three components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model architecture</head><p>• Multi-task learning In order to learn the R2A mapping, we need annotation for the attention. This module generates high-quality attention as an intermediate result by minimizing the predic- tion error on the source tasks (Section 3.1).</p><p>unlabeled target data R2A labeled target data with rationales labeled target data with R2A-generated attention R2A labeled target data with R2A-generated attention target classifier</p><p>Step 1: Training R2A</p><p>Step 2: R2A inference</p><p>Step 3: Training target classifier labeled source data with rationales</p><p>Figure 2: Overall pipeline of our approach (Sec- tion 3.4). The R2A mapping is learned from la- beled source data and unlabeled target data. Then we applied it to the target task to derive attention based on human rationales. Finally, a target clas- sifier is trained under the supervision of both the annotated labels and the R2A-generated attention.</p><p>• Domain-invariant encoder This module aims to transform the contextualized represen- tation obtained from the first module into a domain-invariant version. We achieve this goal through domain adversarial training over the source data and the unlabeled target data (Sec- tion 3.2).</p><p>• Attention generation This module learns to predict the intermediate attention obtained from the first module based on the domain-invariant representation and the rationales (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-task learning</head><p>The goal of the multi-task learning module is to learn good attention for each source task. This learned attention will be used later to supervise the attention generation module. This module takes the input text from the source tasks and predicts the labels. To accomplish the previously stated goal, we minimize the prediction error over all la- beled source data. Let (x t , y t ) be a training instance from any source task t ∈ {S 1 , . . . S N }. We first en- code the input sequence x t into hidden states: h t = enc(x t ), where enc is a bi-directional LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997</ref>) that is shared across all source tasks. For each position i, the dense vector h t i encodes the content and con- text information of the word x t i . We then pass the</p><formula xml:id="formula_1">˜ S ˜ SN˜enc SN˜ SN˜enc a ˆ ↵ S ˆ ↵ SNˆ↵ SNˆ SNˆ↵ a Mu ask ea n ng b Doma n nva an Encode c A en on Gene a on L L a L m L wd enc C F gure 3:</formula><p>Architecture of the R2A model The model is comprised of (a) a multi-task learning compo- nent (b) a domain-invariant encoder and (c) an attention generation component Solid arrows denote computations for training while dotted arrows denote computations for inference sequence h t on to a task-specific attention module to produce attention α t = att t (h t ) as follows:</p><formula xml:id="formula_2">˜ h t = tanh(W t att h t + b t att ), α t = exp( ˜ h t , q t att ) j exp( ˜ h t j , q t att )</formula><p>, where ·, ·· denotes inner product and W t att b t att q t att are learnable parameters We predict the la- bel of x t using the weighted sum of its contex- tualized representation: ˆ y t = pred t ( α t h t ) where pred t is a task-specific multi-layer percep- tron We train this module to minimize the loss L b between the prediction and the annotated la- bel for all source tasks We use cross entropy loss for classification tasks and mean square loss for regression tasks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">2 Domain-invariant encoder</head><p>Supplied with large amounts of source data and unlabeled target data this module has two goals: 1) learning a general encoder for both source and target corpora and 2) learning domain-invariant representation This module enables effective transfer-especially in the presence of significant variance between the source and target domains We achieve the first goal by optimizing a lan- guage modeling objective and the second goal by minimizing the Wasserstein distance between the source and target distribution The representation h is domain-specific as it is trained to encode useful features for language modeling and the source tasks To obtain an invari- ant representation we employ a transformation layer and propose to align the transformed repre- sentation so that it is not distinguishable whether it comes from the source or the target Specifically we transform the representation h at each position i linearly and obtain</p><formula xml:id="formula_3">h nv = W nv h + b nv ,</formula><p>where W nv and b nv are learnable parameters We minimize the Wasserstein distance  between the distribution of h nv from the source and the one from the target denoted as P S and P T respectively Since h nv is a sequence of variable length, L, we summarize it by its first and last element via concatenation,</p><formula xml:id="formula_4">[h inv 1 ; h inv L ].</formula><p>The training objective is defined as:</p><formula xml:id="formula_5">L wd = sup f L ≤K E h inv ∼P S f ([h inv 1 ; h inv L ]) − E h inv ∼P T f ([h inv 1 ; h inv L ]) ,</formula><p>where the supremum is over all K-Lipschitz scalar functions f . Following <ref type="bibr" target="#b10">Gulrajani et al. (2017)</ref>, we approximate f by a multi-layer perceptron, and use gradient penalty to fulfill the Lipschitz con- straint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention generation</head><p>The goal of this module is to generate high-quality attention for each task. This module combines the domain-invariant representation together with task-specific rationales as its input and predicts task-specific attention scores. We minimize the distance between the predicted attention and the intermediate attention obtained from the multi- task learning module. For any source task t ∈ {S i } N i=1 , we denote r t as the task-specific rationales corresponding to the input text x t , and denote h inv,t as the domain- invariant representation of x t . For each position i, we first concatenate r t i with the frequency of x t i occurring as a rationale from all training examples of this task. We denote this augmented sequence as˜ras˜ as˜r t . This frequency term provides the unigram likelihood of each word being a rationale for the task. Then we employ a sequence encoder enc r2a and an attention module att r2a to predict the at- tention scores:</p><formula xml:id="formula_6">u t = enc r2a ([h inv,t ; ˜ r t ]), ˜ u t i = tanh(W r2a att u t i + b r2a att ), ˆ α t i = exp(˜ u t i , q r2a att ) j exp(˜ u t j , q r2a att )</formula><p>, where W r2a att , b r2a att and q r2a att are learnable param- eters, and both enc r2a and att r2a are shareable across all tasks. We minimize the distance be- tweenˆαtweenˆ tweenˆα t and the α t obtained from the first multi- task learning module over all source data:</p><formula xml:id="formula_7">L att = (α t , ˆ α t ),t∈{S i } N i=1 d(α t , ˆ α t ),</formula><p>where d(·, ·) can be any distance metric. In this paper, we consider a soft-margin cosine distance:</p><formula xml:id="formula_8">d(a, b) max(0, 1 − cos(a, b) − 0.1),</formula><p>where cos(·, ·) denotes the cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pipeline</head><p>Training R2A We train the three aforemen- tioned modules jointly using both the source data and the unlabeled target data. The overall objec- tive is given by:</p><formula xml:id="formula_9">L = L lbl + λ att L att + λ lm L lm + λ wd L wd .<label>(1)</label></formula><p>The λs are hyper-parameters that control the im- portance of each training objective and Ls repre- sent the corresponding loss functions.</p><p>R2A inference Once the R2A model is trained, we can generate attention for each labeled target example based on its human-annotated rationales.</p><p>Training target classifier When testing the per- formance on the target task, of course, we are nei- ther provided with labels nor rationales. In order to make predictions for the target task, we train a new classifier under the supervision of both the labels and the R2A-generated attention. Specifi- cally, this target classifier shares the same archi- tecture as the source one in the multi-task learning module. We minimize the prediction loss, L T lbl , on the labeled target data together with the cosine dis- tance, L T att , between the R2A-generated attention and the attention generated by this target classi- fier. The joint objective for this target classifier is defined as</p><formula xml:id="formula_10">L = L T lbl + λ T att L T att ,<label>(2)</label></formula><p>where λ T att controls the importance of L T att . For better transfer, we initialize the encoder in the tar- get classifier as enc from the trained R2A model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We evaluate our approach on two transfer settings: transfer among aspects within the same domain and transfer among different domains.  any two aspects as the source and the other one as the target. We consider a classification setting for each target task. Specifically, reviews with rat- ings ≤ 0.4 are labeled as negative and those with ≥ 0.6 are labeled as positive. We form our dataset by randomly selecting an equal number of positive and negative examples. Then we randomly select 200 examples and ask human annotators to pro- vide rationales (see Appendix 2 for details). These 200 examples are treated as our labeled training data for the target aspect. Unlabeled target data is not required since both source and target tasks are from the same domain. <ref type="table">Table 1</ref> summarizes the statistics of the beer review dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect transfer</head><p>Domain transfer Our second experiment fo- cuses on domain transfer from beer reviews to different aspects of hotel reviews. We use the TripAdvisor 4 hotel review dataset ( <ref type="bibr" target="#b28">Wang et al., 2010)</ref>, with the following three aspects as our transfer target: location, cleanliness, and service. For each aspect, reviews with ratings &gt; 3 are la- beled as positive and those with &lt; 3 are labeled as negative. Similarly, we collect human rationales for 200 examples and treat them as our training data (see Appendix 2 for details). <ref type="table" target="#tab_2">Table 2</ref> summa- rizes the statistics of the hotel review dataset. In this experiment, data from all three aspects of the beer reviews are treated as the source tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our approach (OURS) with four types of baselines:</p><p>4 https://www.tripadvisor.com</p><p>Basic classifier We train a linear SVM using bag-of-ngrams representation on the labeled tar- get data. We combine uni-gram, bi-grams, and tri- grams as features and use tf-idf weighting.</p><p>Rationale augmented classifiers We evaluate two previous methods that incorporate human ra- tionales during training: 1) rationale augmented SVM (RA-SVM) ( <ref type="bibr" target="#b30">Zaidan et al., 2007)</ref>, an SVM- based model that utilizes human rationales to regu- larize the decision boundary of the classifier; 2) ra- tionale augmented CNN (RA-CNN) ( <ref type="bibr" target="#b31">Zhang et al., 2016)</ref>. RA-CNN first trains a CNN-based sen- tence classifier to estimate the probability that a given sentence contains rationale words. Then RA-CNN scales the contribution of each sentence to the overall representation in proportion to these estimates. The final prediction is made from this overall representation.</p><p>Transfer methods We compare against two variants of our method: 1) TRANS, an attention- based classifier that does not use human rationales from the target task; 2) RA-TRANS, an attention- based classifier that directly uses human rationales to supervise the attention. Specifically, TRANS only optimizes the cross-entropy loss L T lbl in the objective <ref type="bibr">(Eq. (2)</ref>). For RA-TRANS, the term L T att in the objective Eq. (2) is replaced by the cosine distance between human rationales and the atten- tion generated by itself. Note that both models still have the ability to transfer, as their encoders are both initialized from enc, which has been trained on source data and unlabeled target data.</p><p>Oracle We also report the performance of an ORACLE which shares the same architecture as ours but is supervised by the oracle attention. The oracle attention is derived from a held-out dataset with large-scale annotations for the target task (see <ref type="bibr">Appendix 3 for details)</ref>. This helps us analyze the contribution of our R2A approach in isolation of the inherent limitations of the target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>We use pre-trained fastText embeddings <ref type="bibr" target="#b3">(Bojanowski et al., 2017</ref>), a 200-dimension bi- directional LSTM (Hochreiter and Schmidhu- ber, 1997) for the language encoder, and a 50- dimension bi-directional LSTM for the R2A en- coder. Dropout ( <ref type="bibr" target="#b26">Srivastava et al., 2014</ref>) is ap- plied with drop rate 0.1 on the word embeddings and the last hidden layers of the classifiers. All   <ref type="table">Table 4</ref>: Accuracy of transferring between domains. Models with † use labeled data from source domains and unlabeled data from the target domain. Models with ‡ use human rationales on the target task.</p><formula xml:id="formula_11">Source Target SVM RA-SVM ‡ RA-CNN ‡ TRANS † RA-TRANS ‡ † OURS ‡ † ORACLE † Beer</formula><p>parameters are optimized using Adam ( <ref type="bibr" target="#b15">Kingma and Ba, 2014</ref>). We set the initial learning rate to 0.001 and divide it by 10 once the perfor- mance on the development set plateaus. For RA- TRANS, OURS and ORACLE, we tuned λ T att from {10 2 , 10 1 , 10 0 , 10 −1 , 10 −2 }. For domain transfer, we set λ lm = 0.1, λ wd = 0.01 and λ att = 0.01. For aspect transfer, we adapt the same hyper- parameters, but set λ wd = 0 as both source tasks and the target task come from the same domain. To encourage the R2A-generated attention to be consistent with the provided rationales in aspect transfer, we augment the overall training objective of R2A in Eq. (3.3) by a consistency regulariza- tion, which is computed from the cosine distance between the R2A-generated attention and the pro- vided rationales.</p><p>In addition, computing L lm is both time and memory inefficient because the complexity is lin- ear to the size of the vocabulary, which can be very large. To expedite the training, we adopt a tech- nique proposed by <ref type="bibr" target="#b23">Mikolov et al. (2011)</ref>, which randomly splits the entire vocabulary into a pre- defined number of bins and minimizes the loss of the bin prediction instead of the exact token pre- diction. We set the bin size as 100 for our experi- ment.  <ref type="table">Table 5</ref>: Ablation study on domain transfer from beer to hotel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect transfer</head><p>Specifically, all rationale-augmented methods (RA-SVM, RA-TRANS and OURS) outperform their rationale-free counterparts on average. This confirms the value of human rationales in the low-resource settings. We observe that the trans- fer baseline that directly uses rationale as aug- mented supervision (RA-TRANS) underperforms ORACLE by a large margin. This validates our hypothesis that human rationales and attention are different. <ref type="table">Table 4</ref> presents the results of domain transfer using 200 training examples. We use the three aspects of the beer review data together as our source tasks while use the three aspects of hotel review data as the target. Our model (OURS) shows marked performance im- provement. The error reduction over the best base- line is 15.08% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain transfer</head><p>We compare the learning curve in <ref type="figure" target="#fig_1">Figure 4</ref>. We observe that the performance of our model steadily improves as more annotations are provided, and the improvement over other baselines is significant and consistent.</p><p>Ablation study <ref type="table">Table 5</ref> presents the results of an ablation study of our model in the setting of domain transfer. As this table indicates, both the language modeling objective and the Wasserstein  distance contribute similarly to the task, with the Wasserstein distance having a bigger impact.</p><p>Visualization of representation <ref type="figure">Figure 5</ref> visu- alizes the hidden representation of 200 beer re- views and 200 hotel reviews using t-SNE <ref type="bibr" target="#b21">(Maaten and Hinton, 2008)</ref>. We observe that our model successfully aligns the source and the target fea- ture distribution. This indicates the effectiveness of optimizing the Wasserstein distance objective.</p><p>Analysis of R2A-generated attention In order to validate that the trained R2A model is able to generate task-specific attention from human ratio- nales, we perform both qualitative and quantitative <ref type="bibr">5</ref> Since the hidden representation is a sequence of variable length, we applied t-SNE on the concatenation of the first and last element:</p><formula xml:id="formula_12">[h inv 1 ; h inv L ].</formula><p>analysis on the R2A-generated attention in the set- ting of domain transfer. It is worth pointing out that our R2A model has never seen any labeled hotel reviews during training. <ref type="table" target="#tab_6">Table 6</ref> presents the average cosine distance be- tween the R2A-generated attention and the oracle attention over the target training set. Compared with human rationales, the R2A-generated atten- tion is much closer to the oracle attention. This ex- plains the large performance boost of our method. <ref type="figure">Figure 6</ref> visualize the R2A-generated atten- tion on the same hotel review with human ra- tionales corresponding to three different aspects. We observe that the trained R2A model is able to produce task-specific attention scores correspond- ing to the provided human rationale. For exam- ple, given the rationale sentence "not the cleanest rooms but bed was clean and so was bathroom", R2A recognizes that not every token is equally im- portant, and the attention should focus more on "clean", "cleanest", "rooms" and "bathroom".</p><p>Annotating rationales versus annotating more labeled data Providing rationales for the training data roughly doubles the annotation cost ( <ref type="bibr" target="#b30">Zaidan et al., 2007)</ref>. Given the same annotation budget, a natural question is: shall we collect a few labeled examples with rationales or annotate more labeled examples? To answer this question, we vary the number of training examples in the target task. <ref type="figure">Figure 7</ref> shows the corresponding learning curve of a classifier that is trained without rationales. The reference line represents the accuracy of our approach trained on 200 examples with rationales. We notice that in order to reach the same level of performance, the rationale-free classifier requires 800, 3100, and 1900 labeled examples on the three target tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task: Hotel location</head><p>Oracle attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b.</p><p>Task: Hotel location R2A-generated attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task: Hotel cleanliness</head><p>Oracle attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b.</p><p>Task: Hotel cleanliness R2A-generated attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task: Hotel service</head><p>Oracle Attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b.</p><p>Task: Hotel service R2A-generated attention you get what you pay for . not the cleanest rooms but bed was clean and so was bathroom . bring your own towels though as very thin . service was excellent , let us book in at 8:30am ! for location and price , this ca n't be beaten , but it is cheap for a reason . if you come expecting the hilton , then book the hilton ! for uk travellers , think of a blackpool b&amp;b. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel approach that uti- lizes the connection between human rationales and machine attention to improve the performance of low-resource tasks. Specifically, we learn a trans- ferrable mapping from rationales to high-quality attention on resource-rich tasks. The learned map- ping is then used to provide an additional super- vision for the target task. Experimental results on both aspect and domain transfer validate that the R2A-generated attention serves as a better form of supervision. Our model produces high-quality at- tention for low-resource tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of rationales versus oracle attention. Words are highlighted according to their relative attention scores. Human rationales are shown in bold with underlines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning curve of transferring from beer review to three aspects of hotel review: location (left), cleanliness (center) and service (right). For neural methods, we ran five different random seeds and plot their mean with their standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Oracle attention versus R2A-generated attention on an example of hotel review for three different tasks. Words are highlighted according to the attention scores. Human rationales are shown in bold with underlines. The oracle attention is derived from large amounts of labeled hotel reviews. The R2A is trained on labeled beer reviews with unlabeled hotel reviews.</figDesc><graphic url="image-1.png" coords="9,94.42,463.91,103.88,59.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>We first consider the transfer problem between multiple aspects of one domain: beer review. We use a subset of the BeerAdvocate 3 review dataset (McAuley et al., 2012) introduced by Lei et al. (2016). This dataset contains reviews with ratings (in the scale of [0, 1]) from three as- pects of the beer: look, aroma and palate. We treat</figDesc><table>Beer 
Aspects 

Source 
Train 

Source 
Dev 

Target 
Train  ‡ 

Target 
Dev 

Target 
Test 

Look 
43,351 10,170 
200 
200 
4,014 
Aroma 
39,825 
8,772 
200 
200 
4,212 
Palate 
30,041 
7,152 
200 
200 
3,804 

Table 1: Statistics of the beer review dataset.  ‡ de-
notes data with human-annotated rationales. 

Hotel 
Aspects 

Target 
Train  ‡ 

Target 
Dev 

Target 
Test 

Target 
Unlabeled 

Location 
200 
200 
1,808 
14,472 
Cleanliness 
200 
200 
12,684 
14,472 
Service 
200 
200 
18,762 
14,472 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the hotel review dataset. ‡ denotes data with human-annotated rationales.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Accuracy of transferring between aspects. Models with † use labeled data from source aspects.</head><label>3</label><figDesc></figDesc><table>Models with  ‡ use human rationales on the target aspect. 

Source 
Target 
SVM 
RA-SVM  ‡ RA-CNN  ‡ TRANS  † RA-TRANS  ‡ † OURS  ‡ † ORACLE  † 

Beer look 
+ 
Beer aroma 
+ 
Beer palate 

Hotel location 
78.65 
79.09 
79.28 
80.42 
82.10 
84.52 
85.43 

Hotel cleanliness 86.44 
86.68 
89.01 
86.95 
87.15 
90.66 
92.09 

Hotel service 
85.34 
86.61 
87.91 
87.37 
86.40 
89.93 
92.42 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 summarizes</head><label>3</label><figDesc>the results of aspect transfer on the beer review dataset. Our model (OURS) obtains substantial gains in accu- racy over the baselines across all three target as- pects. It closely matches the performance of OR- ACLE with only 0.40% absolute difference.</figDesc><table>Model 

Hotel 
location 

Hotel 
cleanliness 

Hotel 
service 

OURS 
84.52 
90.66 
89.93 
w/o L wd 
82.36 
89.79 
89.61 
w/o L lm 
82.47 
90.05 
89.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Avg. cosine distance to the oracle atten-
tion over the target training set. The R2A is trained 
on beer reviews with unlabeled hotel reviews. 

</table></figure>

			<note place="foot" n="2"> In this paper, we consider a more general setting where one domain contains multiple tasks. Also, we assume having one source domain. However, our proposed method is a general framework and can be easily adapted to problems with multiple source domains.</note>

			<note place="foot" n="3"> https://www.beeradvocate.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the MIT NLP group and the reviewers for their helpful discussion and comments. This work is supported by MIT-IBM Watson AI Lab. Any opinions, findings, conclusions, or recom-mendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k Y 5 V T j L N A 9 E b G U 7 C o X B l d t z e h g = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U l E U H c F N 6 6 k o n 1 A E 8 P N d N I O n U z C z E Q o M Q t / x Y 0 L R d z 6 G + 7 8 G y d t F t p 6 Y O B w z r 3 c M y d I G J X K t r + N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 3 L O B W Y t H D M Y t E N Q B J G O W k p q h j p J o J A F D D S C U a X h d 9 5 I E L S m N + p c U K 8 C A a c h h S D 0 p J v 7 r v A k i H c Z 2 4 E a o i B Z b e 5 f 5 3 7 Z s 2 u 2 x N Y 8 8 Q p S Q 2 V a P r m l 9 u P c R o R r j A D K X u O n S g v A 6 E o Z i S v u q k k C e A R D E h P U w 4 R k V 4 2 y Z 9 b R 1 r p W 2 E s 9 O P K m q i / N z K I p B x H g Z 4 s U s p Z r x D / 8 3 q p C s + 9 j P I k V Y T j 6 a E w Z Z a K r a I M q 0 8 F w Y q N N Q E s q M 5 q 4 S E I w E p X V t U l O L N f n i f t k 7 p j 1 5 2 b 0 1 r j o q y j g g 7 Q I T p G D j p D D X S F m q i F M H p E z + g V v R l P x o v x b n x M R x e M c m c P / Y H x + Q O I + Z Z k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k Y 5 V T j L N A 9 E b G U 7 C o X B l d t z e h g = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U l E U H c F N 6 6 k o n 1 A E 8 P N d N I O n U z C z E Q o M Q t / x Y 0 L R d z 6 G + 7 8 G y d t F t p 6 Y O B w z r 3 c M y d I G J X K t r + N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 3 L O B W Y t H D M Y t E N Q B J G O W k p q h j p J o J A F D D S C U a X h d 9 5 I E L S m N + p c U K 8 C A a c h h S D 0 p J v 7 r v A k i H c Z 2 4 E a o i B Z b e 5 f 5 3 7 Z s 2 u 2 x N Y 8 8 Q p S Q 2 V a P r m l 9 u P c R o R r j A D K X u O n S g v A 6 E o Z i S v u q k k C e A R D E h P U w 4 R k V 4 2 y Z 9 b R 1 r p W 2 E s 9 O P K m q i / N z K I p B x H g Z 4 s U s p Z r x D / 8 3 q p C s + 9 j P I k V Y T j 6 a E w Z Z a K r a I M q 0 8 F w Y q N N Q E s q M 5 q 4 S E I w E p X V t U l O L N f n i f t k 7 p j 1 5 2 b 0 1 r j o q y j g g 7 Q I T p G D j p D D X S F m q i F M H p E z + g V v R l P x o v x b n x M R x e M c m c P / Y H x + Q O I + Z Z k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k Y 5 V T j L N A 9 E b G U 7 C o X B l d t z e h g = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U l E U H c F N 6 6 k o n 1 A E 8 P N d N I O n U z C z E Q o M Q t / x Y 0 L R d z 6 G + 7 8 G y d t F t p 6 Y O B w z r 3 c M y d I G J X K t r + N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 3 L O B W Y t H D M Y t E N Q B J G O W k p q h j p J o J A F D D S C U a X h d 9 5 I E L S m N + p c U K 8 C A a c h h S D 0 p J v 7 r v A k i H c Z 2 4 E a o i B Z b e 5 f 5 3 7 Z s 2 u 2 x N Y 8 8 Q p S Q 2 V a P r m l 9 u P c R o R r j A D K X u O n S g v A 6 E o Z i S v u q k k C e A R D E h P U w 4 R k V 4 2 y Z 9 b R 1 r p W 2 E s 9 O P K m q i / N z K I p B x H g Z 4 s U s p Z r x D / 8 3 q p C s + 9 j P I k V Y T j 6 a E w Z Z a K r a I M q 0 8 F w Y q N N Q E s q M 5 q 4 S E I w E p X V t U l O L N f n i f t k 7 p j 1 5 2 b 0 1 r j o q y j g g 7 Q I T p G D j p D D X S F m q i F M H p E z + g V v R l P x o v x b n x M R x e M c m c P / Y H x + Q O I + Z Z k &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 k Y 5 V T j L N A 9 E b G U 7 C o X B l d t z e h g = " &gt; A A A B / 3 i c b V D L S s N A F J 3 4 r P U V F d y 4 C R b B V U l E U H c F N 6 6 k o n 1 A E 8 P N d N I O n U z C z E Q o M Q t / x Y 0 L R d z 6 G + 7 8 G y d t F t p 6 Y O B w z r 3 c M y d I G J X K t r + N h c W l 5 Z X V y l p 1 f W N z a 9 v c 2 W 3 L O B W Y t H D M Y t E N Q B J G O W k p q h j p J o J A F D D S C U a X h d 9 5 I E L S m N + p c U K 8 C A a c h h S D 0 p J v 7 r v A k i H c Z 2 4 E a o i B Z b e 5 f 5 3 7 Z s 2 u 2 x N Y 8 8 Q p S Q 2 V a P r m l 9 u P c R o R r j A D K X u O n S g v A 6 E o Z i S v u q k k C e A R D E h P U w 4 R k V 4 2 y Z 9 b R 1 r p W 2 E s 9 O P K m q i / N z K I p B x H g Z 4 s U s p Z r x D / 8 3 q p C s + 9 j P I k V Y T j 6 a E w Z Z a K r a I M q 0 8 F w Y q N N Q E s q M 5 q 4 S E I w E p X V t U l O L N f n i f t k 7 p j 1 5 2 b 0 1 r j o q y j g g 7 Q I T p G D j p D D X S F m q i F M H p E z + g V v R l P x o v x b n x M R x e M c m c P / Y H x + Q O I + Z Z k &lt; / l a t e x i t &gt; h S1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z J p h W c r I / / / F 6 r + F V d 7 h T E A B k 4 U = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d D B b B V U l E U H c F N y 4 r 2 g e 0 M U y m 0 3 b o Z B J m J k q J + R Q 3 L h R x 6 5 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I O Z M a c f 5 t k o r q 2 v r G + X N y t b 2 z u 6 e X d 1 v q y i R h L Z I x C P Z D b C i n A n a 0 k x z 2 o 0 l x W H A a S e Y X O V + 5 4 F K x S J x p 6 c x 9 U I 8 E m z I C N Z G 8 u 3 q + D 7 t h 1 i P C e b p b e a 7 m W / X n L o z A 1 o m b k F q U K D p 2 1 / 9 Q U S S k A p N O F a q 5 z q x 9 l I s N S O c Z p V + o m i M y Q S P a M 9 Q g U O q v H Q W P U P H R h m g Y S T N E x r N 1 N 8 b K Q 6 V m o a B m c x T q k U v F / / z e o k e X n g p E 3 G i q S D z Q 8 O E I x 2 h v A c 0 Y J I S z a e G Y C K Z y Y r I G E t M t G m r Y k p w F 7 + 8 T N q n d d e p u z d n t c Z l U U c Z D u E I T s C F c 2 j A N T S h B Q Q e 4 R l e 4 c 1 6 s l 6 s d + t j P l q y i p 0 D + A P r 8 w d + h Z Q b &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " z J p h W c r I / / / F 6 r + F V d 7 h T E A B k 4 U = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d D B b B V U l E U H c F N y 4 r 2 g e 0 M U y m 0 3 b o Z B J m J k q J + R Q 3 L h R x 6 5 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s + c I O Z M a c f 5 t k o r q 2 v r G + X N y t b 2 z u 6 e X d 1 v q y i R h L Z I x C P Z D b C i n A n a 0 k x z 2 o 0 l x W H A a S e Y X O V + 5 4 F K x S J x p 6 c x 9 U I 8 E m z I C N Z G 8 u 3 q + D 7 t h 1 i P C e b p b e a 7 m W / X n L o z A 1 o m b k F q U K D p 2 1 / 9 Q U S S k A p N O F a q 5 z q x 9 l I s N S O c Z p V + o m i M y Q S P a M 9 Q g U O q v H Q W P U P H R h m g Y S T N E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b g o l k J i s i Q y w x 0 a a t s i n B n f / y I m m d 1 F y n 5 t 6 c V u s X s z p K c A C H c A w u n E E d r q A B T S D w C M / w C m / W k / V i v V s f 0 9 E l a 7 a z D 3 9 g f f 4 A q p a U O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 U Y b l A 1 U m x q q L A Y E f P d 9 s 6 L B A f Q = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k I 6 q 7 g x p V U t A 9 o Y 5 h M J + 3 Q y S T M T J Q S 8 y l u X C j i 1 i 9 x 5 9 8 4 a b v Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b S 0 t r 6 y u r Z c 2 y p t b 2 z u 7 d m</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b g o l k J i s i Q y w x 0 a a t s i n B n f / y I m m d 1 F y n 5 t 6 c V u s X s z p K c A C H c A w u n E E d r q A B T S D w C M / w C m / W k / V i v V s f 0 9 E l a 7 a z D 3 9 g f f 4 A q p a U O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 U Y b l A 1 U m x q q L A Y E f P d 9 s 6 L B A f Q = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k I 6 q 7 g x p V U t A 9 o Y 5 h M J + 3 Q y S T M T J Q S 8 y l u X C j i 1 i 9 x 5 9 8 4 a b v Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b S 0 t r 6 y u r Z c 2 y p t b 2 z u 7 d m</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b g o l k J i s i Q y w x 0 a a t s i n B n f / y I m m d 1 F y n 5 t 6 c V u s X s z p K c A C H c A w u n E E d r q A B T S D w C M / w C m / W k / V i v V s f 0 9 E l a 7 a z D 3 9 g f f 4 A q p a U O A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 U Y b l A 1 U m x q q L A Y E f P d 9 s 6 L B A f Q = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k I 6 q 7 g x p V U t A 9 o Y 5 h M J + 3 Q y S T M T J Q S 8 y l u X C j i 1 i 9 x 5 9 8 4 a b v Q 1 g M D h 3 P u 5 Z 4 5 Q c K Z 0 o 7 z b S 0 t r 6 y u r Z c 2 y p t b 2 z u 7 d m W v p e J U E t o k M Y 9 l J 8 C K c i Z o U z P N a S e R F E c B p + 1 g d F n 4 7 Q c q F Y v F n R 4 n 1 I v w Q L C Q E a y N 5 N u V 4 X 3 W i 7 A e E s y z 2 9 y / z n 2 7 6 t S c C d A i c W e k C j M 0 f P u r 1 4 9 J</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n G + r d L S 8 s r q W n m 9 s r G 5 t b 1 j 7 + 6 1 V J R I T J o 4 Y p H s B E g R R g V p a q o Z 6 c S S I B 4 w 0 g 5 G 1 7 n e H h O p a C T u 9 S Q m H k c D Q U O K k T a U b x 8 P H 9 I e R 3 o o + f Q O w p S K c X Z a P D B i 6 V 3 m u 1 n m 2 1 W n 5 h Q F F 4 E 7 A 1 U w q 4 Z v f / X 6 E U 4 4 E R o z p F T X d W L t p U h q i h n J K r 1 E k R j h E R q Q r o E C c a K 8 t D C U w S P D 9 G E Y S X O E h g X 7 e y J F X K k J D 0 x n v q a a 1 3 L y P 6 2 b 6 P D S M w b j R B O B p x + F C Y M 6 g n k 6 s E 8 l w Z p N D E B Y U r M r x E M k E d Y m w 4 o J w Z 2 3 v A h a Z z X X q b m 3 5 9 X 6 1 S y O M j g A h + A E u O A C 1 M E N a I A m w O A R P I N X 8 G Y 9 W S / W u / U x b S 1 Z s 5 l 9 8 K e s z x 9 6 Z Z 4 T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s Y w y X K W L x 9 u U R r 8 q 5 x B P h c n g c R 8 = " &gt; A A A C E H i c b V C 7 T s M w F H X K q 5 R X g J H F o k I w o C p B S M B W i Y W x C P q Q 2 h A 5 r t N a t Z 3 I d i p V U T 6 B h V 9 h Y Q A h V k Y 2 / g Y n 7 Q A t V 7 J 8 f M 6 9 8 j 0 n i B l V 2 n G + r d L S 8 s r q W n m 9 s r G 5 t b 1 j 7 + 6 1 V J R I T J o 4 Y p H s B E g R R g V p a q o Z 6 c S S I B 4 w 0 g 5 G 1 7 n e H h O p a C T u 9 S Q m H k c D Q U O K k T a U b x 8 P H 9 I e R 3 o o + f Q O w p S K c X Z a P D B i 6 V 3 m u 1 n m 2 1 W n 5 h Q F F 4 E 7 A 1 U w q 4 Z v f / X 6 E U 4 4 E R o z p F T X d W L t p U h q i h n J K r 1 E k R j h E R q Q r o E C c a K 8 t D C U w S P D 9 G E Y S X O E h g X 7 e y J F X K k J D 0 x n v q a a 1 3 L y P 6 2 b 6 P D S M w b j R B O B p x + F C Y M 6 g n k 6 s E 8 l w Z p N D E B Y U r M r x E M k E d Y m w 4 o J w Z 2 3 v A h a Z z X X q b m 3 5 9 X 6 1 S y O M j g A h + A E u O A C 1 M E N a I A m w O A R P I N X 8 G Y 9 W S / W u / U x b S 1 Z s 5 l 9 8 K e s z x 9 6 Z Z 4 T &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s Y w y X K W L x 9 u U R r 8 q 5 x B P h c n g c R 8 = " &gt; A A A C E H i c b V C 7 T s M w F H X K q 5 R X g J H F o k I w o C p B S M B W i Y W x C P q Q 2 h A 5 r t N a t Z 3 I d i p V U T 6 B h V 9 h Y Q A h V k Y 2 / g Y n 7 Q A t V 7 J 8 f M 6 9 8 j 0 n i B l V 2 n G + r d L S 8 s r q W n m 9 s r G 5 t b 1 j 7 + 6 1 V J R I T J o 4 Y p H s B E g R R g V p a q o Z 6 c S S I B 4 w 0 g 5 G 1 7 n e H h O p a C T u 9 S Q m H k c D Q U O K k T a U b x 8 P H 9 I e R 3 o o + f Q O w p S K c X Z a P D B i 6 V 3 m u 1 n m 2 1 W n 5 h Q F F 4 E 7 A 1 U w q 4 Z v f / X 6 E U 4 4 E R o z p F T X d W L t p U h q i h n J K r 1 E k R j h E R q Q r o E C c a K 8 t D C U w S P D 9 G E Y S X O E h g X 7 e y J F X K k J D 0 x n v q a a 1 3 L y P 6 2 b 6 P D S M w b j R B O B p x + F C Y M 6 g n k 6 s E 8 l w Z p N D E</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O S G T Y 2 Z E t 6 T U w Q v 6 e / N d T u C X A A 0 = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d B I v g q i Q i q L u C G 5 c V 7 Q P a G C b T S T t 0 M g k z E y X E f I o b F 4 q 4 9 U v c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O H z M q l W 1 / G 5 W V 1 b X 1 j e p m b W t 7 Z 3 f P r O 9 3 Z Z Q I T D o 4 Y p H o + 0 g S R j n p K K o Y 6 c e C o N B n p O d P r w q / 9 0 C E p B G / U 2 l M 3 B C N O Q 0 o R k p L n l l P 7 7 N h i N Q E I 5 b d 5 p 6 T e 2 b D b t o z W M v E K U k D S r Q 9 8 2 s 4 i n A S E q 4 w Q 1 I O H D t W b o a E o p i R v D Z M J I k R n q I x G W j K U U i k m 8 2 i 5 9 a x V k Z W E A n 9 u L J m 6 u + N D I V S p q G v J 4 u U c t E r x P + 8 Q a K C C z e j P E 4 U 4 X h + K E i Y p S K r 6 M E a U U G w Y q k m C A u q s 1 p 4 g g T C S r d V 0 y U 4 i 1 9 e J t 3 T p m M 3 n Z u z R u u y r K M K h 3 A E J + D A O b T g G t r Q A Q y P 8 A y v 8 G Y 8 G S / G u / E x H 6 0 Y 5 c 4 B / I H x + Q O Z W Z Q s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O S G T Y 2 Z E t 6 T U w Q v 6 e / N d T u C X A A 0 = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d B I v g q i Q i q L u C G 5 c V 7 Q P a G C b T S T t 0 M g k z E y X E f I o b F 4 q 4 9 U v c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O H z M q l W 1 / G 5 W V 1 b X 1 j e p m b W t 7 Z 3 f P r O 9 3 Z Z Q I T D o 4 Y p H o + 0 g S R j n p K K o Y 6 c e C o N B n p O d P r w q / 9 0 C E p B G / U 2 l M 3 B C N O Q 0 o R k p L n l l P 7 7 N h i N Q E I 5 b d 5 p 6 T e 2 b D b t o z W M v E K U k D S r Q 9 8 2 s 4 i n A S E q 4 w Q 1 I O H D t W b o a E o p i R v D Z M J I k R n q I x G W j K U U i k m 8 2 i 5 9 a x V k Z W E A n 9 u L J m 6 u + N D I V S p q G v J 4 u U c t E r x P + 8 Q a K C C z e j P E 4 U 4 X h + K E i Y p S K r 6 M E a U U G w Y q k m C A u q s 1 p 4 g g T C S r d V 0 y U 4 i 1 9 e J t 3 T p m M 3 n Z u z R u u y r K M K h 3 A E J + D A O b T g G t r Q A Q y P 8 A y v 8 G Y 8 G S / G u / E x H 6 0 Y 5 c 4 B / I H x + Q O Z W Z Q s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O S G T Y 2 Z E t 6 T U w Q v 6 e / N d T u C X A A 0 = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d B I v g q i Q i q L u C G 5 c V 7 Q P a G C b T S T t 0 M g k z E y X E f I o b F 4 q 4 9 U v c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O H z M q l W 1 / G 5 W V 1 b X 1 j e p m b W t 7 Z 3 f P r O 9 3 Z Z Q I T D o 4 Y p H o + 0 g S R j n p K K o Y 6 c e C o N B n p O d P r w q / 9 0 C E p B G / U 2 l M 3 B C N O Q 0 o R k p L n l l P 7 7 N h i N Q E I 5 b d 5 p 6 T e 2 b D b t o z W M v E K U k D S r Q 9 8 2 s 4 i n A S E q 4 w Q 1 I O H D t W b o a E o p i R v D Z M J I k R n q I x G W j K U U i k m 8 2 i 5 9 a x V k Z W E A n 9 u L J m 6 u + N D I V S p q G v J 4 u U c t E r x P + 8 Q a K C C z e j P E 4 U 4 X h + K E i Y p S K r 6 M E a U U G w Y q k m C A u q s 1 p 4 g g T C S r d V 0 y U 4 i 1 9 e J t 3 T p m M 3 n Z u z R u u y r K M K h 3 A E J + D A O b T g G t r Q A Q y P 8 A y v 8 G Y 8 G S / G u / E x H 6 0 Y 5 c 4 B / I H x + Q O Z W Z Q s &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " O S G T Y 2 Z E t 6 T U w Q v 6 e / N d T u C X A A 0 = " &gt; A A A B + n i c b V D L S s N A F L 2 p r 1 p f q S 7 d B I v g q i Q i q L u C G 5 c V 7 Q P a G C b T S T t 0 M g k z E y X E f I o b F 4 q 4 9 U v c + T d O 2 i y 0 9 c D A 4 Z x 7 u W e O H z M q l W 1 / G 5 W V 1 b X 1 j e p m b W t 7 Z 3 f P r O 9 3 Z Z Q I T D o 4 Y p H o + 0 g S R j n p K K o Y 6 c e C o N B n p O d P r w q / 9 0 C E p B G / U 2 l M 3 B C N O Q 0 o R k p L n l l P 7 7 N h i N Q E I 5 b d 5 p 6 T e 2 b D b t o z W M v E K U k D S r Q 9 8 2 s 4 i n A S E q 4 w Q 1 I O H D t W b o a E o p i R v D Z M J I k R n q I x G W j K U U i k m 8 2 i 5 9 a x V k Z W E A n 9 u L J m 6 u + N D I V S p q G v J 4 u U c t E r x P + 8 Q a K C C z e j P E 4 U 4 X h + K E i Y p S K r 6 M E a U U G w Y q k m C A u q s 1 p 4 g g T C S r d V 0 y U 4 i 1 9 e J t 3 T p m M 3 n Z u z R u u y r K M K h 3 A E J + D A O b T g G t r Q A Q y P 8 A y v 8 G Y 8 G S / G u / E x H 6 0 Y 5 c 4 B / I H x + Q O Z W Z Q s &lt; / l a t e x i t &gt; y SN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z 3 5 b r 9 m 3 0 1 K p W J 3 b m 9 K 9 r j d w N t U = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b Y B F c l U Q E d V d w 4 0 o q 2 g e 0 M U y m k 3 b o Z B J m J k q I + R Q 3 L h R x 6 5 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s 8 c P 2 Z U K t v + N p a W V 1 b X 1 i s b 1 c 2 t 7 Z 1 d s 7 b X k V E i M G n j i E W i 5 y N J G O W k r a h i p B c L g k K f k a 4 / u S z 8 7 g M R k k b 8 T q U x c U M 0 4 j S g G C k t e W Y t v c 8 G I V J j j F h 2 m 3 v X u W f W 7 Y Y 9 h b V I n J L U o U T L M 7 8 G w w g n I e E K M y R l 3 7 F j 5 W Z I K I o Z y a u D R J I Y 4 Q k a k b 6 m H I V E u t k 0 e m 4 d a W V o B Z H Q j y t r q v 7 e y F A o Z R r 6 e r J I K e e 9 Q v z P 6 y c q O H c z y u N E E Y 5 n h 4 K E W S q y i h 6 s I R U E K 5 Z q g r C g O q u F x 0 g g r H R b V V 2 C M / / l R d I 5 a T h 2 w 7 k 5 r T c v y j o q c A C H c A w O n E E T r q A F b c D w C M / w C m / G k / F i v B s f s 9 E l o 9 z Z h z 8 w P n 8 A x W q U S Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z 3 5 b r 9 m 3 0 1 K p W J 3 b m 9 K 9 r j d w N t U = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b Y B F c l U Q E d V d w 4 0 o q 2 g e 0 M U y m k 3 b o Z B J m J k q I + R Q 3 L h R x 6 5 e 4 8 2 + c t F l o 6 4 G B w z n 3 c s 8 c P 2 Z U K t v + N p a W V 1 b X 1 i s b 1 c 2 t 7 Z 1 d s 7 b X k V E i M G n j i E W i 5 y N J G O W k r a h i p B c L g k K f k a 4 / u S z 8 7 g M R k k b 8 T q U x c U M 0 4 j S g G C k t e W Y t v c 8 G I V J j j F h 2 m 3 v X u W f W 7 Y Y 9 h b V I n J L U o U T L M 7 8 G w w g n I e E K M y R l 3 7 F j 5 W Z I K I o Z y a u D R J I Y 4 Q k a k b 6 m H I V E u t k 0 e m 4 d a W V o B Z H Q j y t r q v 7 e y F A o Z R r 6 e r J I K e e 9 Q v z P 6</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4683</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02364</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised attention for sequence-to-sequence constituency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsutomu</forename><surname>Hirao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04186</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploiting argument information to improve event detection via supervised attention mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shulin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1020" to="1025" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extensions of recurrent neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5528" to="5531" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05365</idno>
		<title level="m">Deep contextualized word representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexander M Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00685</idno>
		<title level="m">Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence summarization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Latent aspect rating analysis on review text data: a rating regression approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACm</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using annotator rationales to improve machine learning for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
	<note>Proceedings of the Main Conference</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rationale-augmented convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page">795</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aspect-augmented adversarial networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="515" to="528" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bi-transferring deep neural networks for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
