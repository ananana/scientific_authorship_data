<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training for Diversity in Image Paragraph Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
							<email>{lmelaskyriazi@college, hanz@college, srush@seas}.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training for Diversity in Image Paragraph Captioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="757" to="761"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>757</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Image paragraph captioning models aim to produce detailed descriptions of a source image. These models use similar techniques as standard image captioning models, but they have encountered issues in text generation, notably a lack of diversity between sentences, that have limited their effectiveness. In this work, we consider applying sequence-level training for this task. We find that standard self-critical training produces poor results, but when combined with an integrated penalty on trigram repetition produces much more diverse paragraphs. This simple training approach improves on the best result on the Visual Genome paragraph captioning dataset from 16.9 to 30.6 CIDEr, with gains on METEOR and BLEU as well, without requiring any architectural changes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning aims to describe the objects, ac- tions, and details present in an image using natural language. Most image captioning research has fo- cused on single-sentence captions, but the descrip- tive capacity of this form is limited; a single sen- tence can only describe in detail a small aspect of an image. Recent work has argued instead for im- age paragraph captioning with the aim of generat- ing a (usually 5-8 sentence) paragraph describing an image.</p><p>Compared with single-sentence captioning, paragraph captioning is a relatively new task. The main paragraph captioning dataset is the Vi- sual Genome corpus, introduced by <ref type="bibr" target="#b2">Krause et al. (2016)</ref>. When strong single-sentence captioning models are trained on this dataset, they produce repetitive paragraphs that are unable to describe diverse aspects of images. The generated para- graphs repeat a slight variant of the same sentence multiple times, even when beam search is used. Prior work, discussed in the following section, tried to address this repetition with architectural changes, such as hierarchical LSTMs, which sep- arate the generation of sentence topics and words.</p><p>In this work, we consider an approach for train- ing paragraph captioning models that focuses on increasing the diversity of the output paragraph. In particular, we note that self-critical sequence train- ing (SCST) ( <ref type="bibr" target="#b8">Ranzato et al., 2015;</ref><ref type="bibr" target="#b9">Rennie et al., 2016)</ref>, a technique which uses policy gradient methods to directly optimize a target metric, has been successfully employed in standard caption- ing, but not in paragraph captioning. We observe that during SCST training the intermediate results of the system lack diversity, which makes it dif- ficult for the model to improve. We address this issue with a simple repetition penalty which down- weights trigram overlap.</p><p>Experiments show that this technique greatly improves the baseline model. A simple baseline, non-hierarchical model trained with repetition- penalized SCST outperforms complex hierarchi- cal models trained with both cross-entropy and customized adversarial losses. We demonstrate that this strong performance gain comes from the combination of repetition-penalized search and SCST, rather than from either individually, and discuss how this impacts the output paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Nearly all modern image captioning models em- ploy variants of an encoder-decoder architecture. As introduced by <ref type="bibr" target="#b12">Vinyals et al. (2014)</ref>, the en- coder is a CNN pre-trained for classification and the decoder is a LSTM or GRU. Following work in machine translation, <ref type="bibr" target="#b14">Xu et al. (2015)</ref> added an at- tention mechanism over the encoder features. Re- cently, <ref type="bibr" target="#b0">Anderson et al. (2017)</ref> further improved single-sentence captioning performance by incor- porating object detection in the encoder (bottom- up attention) and adding an LSTM layer before attending to spatial features in the decoder (top-down attention).</p><p>Single-sentence and paragraph captioning mod- els are evaluated with a number of metrics, including some designed specifically for cap- tioning (CIDEr) and some adopted from ma- chine translation (BLEU, METEOR). CIDEr and BLEU measure accuracy with n-gram overlaps, with CIDEr weighting n-grams by TF-IDF (term- frequency inverse-document-frequency), and ME- TEOR uses unigram overlap, incorporating syn- onym and paraphrase matches. We discuss these metrics in greater detail when analyzing our ex- periments.</p><p>Related Models <ref type="bibr" target="#b2">Krause et al. (2016)</ref> introduced the first large-scale paragraph captioning dataset, a subset of the Visual Genome dataset, along with a number of models for paragraph captioning. Empirically, they showed that paragraphs con- tain significantly more pronouns, verbs, corefer- ences, and greater overall "diversity" than single- sentence captions. Whereas most single-sentence captions in the MSCOCO dataset describe only the most important object or action in an image, para- graph captions usually touch on multiple objects and actions.</p><p>The paragraph captioning models proposed by <ref type="bibr" target="#b2">Krause et al. (2016)</ref> included template-based (non- neural) approaches and two encoder-decoder mod- els. In both neural models, the encoder is an ob- ject detector pre-trained for dense captioning. In the first model, called the flat model, the decoder is a single LSTM which outputs an entire para- graph word-by-word. In the second model, called the hierarchical model, the decoder is composed of two LSTMs, where the output of one sentence- level LSTM is used as input to the other word-level LSTM.</p><p>Recently, <ref type="bibr" target="#b4">Liang et al. (2017)</ref> extended this model with a third (paragraph-level) LSTM and added adversarial training. In total, their model (RTT-GAN) incorporates three LSTMs, two at- tention mechanisms, a phrase copy mechanism, and two adversarial discriminators. To the best of our knowledge, this model achieves state-of- the-art performance of 16.9 CIDEr on the Visual Genome dataset (without external data).</p><p>For our experiments, we use the top-down single-sentence captioning model in <ref type="bibr" target="#b0">Anderson et al. (2017)</ref>. This model is similar to the "flat" model in <ref type="bibr" target="#b2">Krause et al. (2016)</ref>, except that it incor- porates attention with a top-down mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>The primary issue in current paragraph captioning models, especially non-hierarchical ones, is lack of diversity of topic in the output paragraph. For example, for the image of a skateboarder in <ref type="figure" target="#fig_0">Figure  1</ref>, the flat model outputs "The man is wearing a black shirt and black pants" seven times. This ex- ample is not anomalous: it is a typical failure case of the model. Empirically, in validation, ground truth paragraphs contain 0.62 repeated trigrams on average, whereas paragraphs produced by the flat cross-entropy model contain 25.9 repeated tri- grams on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Critical Sequence Training</head><p>Self-critical sequence training (SCST) is a sequence-level optimization procedure proposed by <ref type="bibr" target="#b9">Rennie et al. (2016)</ref>, which has been widely adopted in single-sentence captioning but has not yet been applied to paragraph captioning. This method provides an alternative approach to word- level cross-entropy which can incorporate a task specific metric.</p><p>Sequence-level training employs a policy gra- dient method to optimize directly for a non- differentiable metric, such as CIDEr or BLEU. This idea was first applied to machine transla- tion by <ref type="bibr" target="#b8">Ranzato et al. (2015)</ref> in a procedure called MIXER, which incrementally transitions from cross-entropy to policy gradient training. To normalize the policy gradient reward and reduce variance during training, MIXER subtracts a base- line estimate of the reward as calculated by a linear regressor.</p><p>SCST replaces this baseline reward estimate with the reward obtained by the test-time infer- ence algorithm, namely the CIDEr score of greedy search. This weights the gradient by the differ- ence in reward given to a sampled paragraph com- pared to the current greedy output (see Eq. 3-9 in ( <ref type="bibr" target="#b9">Rennie et al., 2016)</ref>). Additionally, SCST uses a hard transition from cross-entropy to policy gradi- ent training. The final gradient is:</p><formula xml:id="formula_0">−E w s ∼p θ [(r(w s ) − r(w g )) θ log p θ (w s | x)]</formula><p>Where w s is a sampled paragraph, w g is a greedy decoded paragraph, r is the reward (e.g CIDEr), p θ is the captioning model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Repetition Penalty</head><p>In preliminary experiments, we find that directly applying SCST is not effective for paragraph cap- tioning models. <ref type="table">Table 1</ref> shows that when training with SCST, the model performs only marginally better than cross-entropy. In further analysis, we see that the greedy baseline in SCST training has very non-diverse output, which leads to poor pol- icy gradients. Unlike in standard image caption- ing, the cross-entropy model is too weak for SCST to be effective.</p><p>To address this problem, we take inspiration from recent work in abstractive text summariza- tion, which encounters the same challenge when producing paragraph-length summaries of docu- ments ( <ref type="bibr" target="#b7">Paulus et al., 2017</ref>). These models tar- get the repetition problem by simply preventing the model from producing the same trigram more than once during inference. We therefore intro- duce an inference constraint that penalizes the log- probabilities of words that would result in repeated trigrams. The penalty is proportional to the num- ber of times the trigram has already been gener- ated.</p><p>Formally, denote the (pre-softmax) output of the LSTM by o, where the length of o is the size of the target vocabulary and o w is the log-probability of word w. We modify o w by o w → o w − k w · α, where k w is the number of times the trigram com- pleted by word w has previously been generated in the paragraph, and α is a hyperparameter which controls the degree of blocking. When α = 0, there is no penalty, so we have standard greedy search. When α → ∞, or in practice when α ex- ceeds about 5, we have full trigram blocking.</p><p>We incorporate this penalty into the greedy baseline used to compute policy gradients in SCST. During inference, we employ the same repetition-penalized greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods and Results</head><p>For our paragraph captioning model we use the top-down model from <ref type="bibr" target="#b0">Anderson et al. (2017)</ref>. Our encoder is a convolutional network pretrained for object detection (as opposed to dense captioning, as in <ref type="bibr" target="#b2">Krause et al. (2016)</ref> and <ref type="bibr" target="#b4">Liang et al. (2017)</ref>  <ref type="table">Table 1</ref>: Results of our model compared with prior published results. Note that <ref type="bibr" target="#b4">Liang et al. (2017)</ref> also trains a model on additional data, but here we only compare models trained on Visual Genome. Also note that our models employ greedy search, whereas other models employ beam search.</p><note type="other">). METEOR CIDEr BLEU-1 BLEU-2 BLEU-3 BLEU-4 Krause</note><p>The encoder extracts between 10 and 100 objects per image and applies spatial max-pooling to yield a single feature vector of dimension 2048 per ob- ject. The decoder is a 1-layer LSTM with hidden dimension 512 and top-down attention. Evaluation is done on the Visual Genome dataset with the splits provided by <ref type="bibr" target="#b2">Krause et al. (2016)</ref>. We first train for 25 epochs with cross- entropy (XE) loss, using Adam with learning rate 5 · 10 −4 . We then train an additional 25 epochs with repetition-penalized SCST targeting a CIDEr-based reward, using Adam with learning rate 5 · 10 −5 .</p><p>Our PyTorch-based implementation is available at https://github.com/lukemelas/ image-paragraph-captioning. <ref type="table">Table 1</ref> shows the main experimen- tal results. Our baseline cross-entropy caption- ing model gets similar scores to the original flat model. When the repetition penalty is applied to a model trained with cross-entropy, we see a large improvement on CIDEr and a minor improvement on other metrics. 1 When combining the repetition penalty with SCST, we see a dramatic improve- ment across all metrics, and particularly on CIDEr. Interestingly, SCST only works when its baseline reward model is strong; for this reason the combi- nation of the repetition penalty and SCST is par- ticularly effective. <ref type="table" target="#tab_2">Table 2</ref> compares the effect of training with <ref type="bibr">1</ref> We believe the improvement may be largest on CIDEr because, in the calculation of CIDEr, n-gram counts are clipped to the number of times each n-gram appears in the ref- erence sentence. However, a similar procedure is applied in calculating BLEU, on which we show lesser improvements.   Finally, <ref type="table" target="#tab_3">Table 3</ref> shows quantitative changes in trigram repetition and ground truth matches. The cross-entropy model fails to generate enough unique phrases. Blocking these entirely gives some benefit, but the SCST model is able to raise the total number of matched trigrams while rein- troducing few repeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work targets increased diversity in image paragraph captioning. We show that training with SCST combined with a repetition penalty leads to a substantial improvement in the state-of-the- art for this task, without requiring architectural changes or adversarial training. In future work, we hope to further address the language issues of paragraph generation as well as extend this simple approach to other tasks requiring long-form text or paragraph generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example paragraph outputs of our model. The final example is a failure case of both our model and the non-penalized model. Our model does not suffer from the repetition problem of the non-penalized, but it does not produce a great caption because it does not understand that the image is black-and-white.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,453.55,309.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>different values of the penalty hyperparameter α, demonstrating that intermediate values of α (≈ 2.0) perform slightly better than large values. An intermediate value of α discourages the model from producing repeat trigrams, but still permits the model to output them when there are no likely alternative phrases.</figDesc><table>α 
METEOR CIDEr BLEU-4 
0.0 
13.8 
13.6 
5.9 
1.0 
17.4 
28.9 
10.2 
2.0 
17.7 
31.4 
10.8 
4.0 
17.6 
30.1 
10.4 
10.0 
17.5 
30.6 
9.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Varying the repetition penalty α (on the vali-
dation set). α = 10 is equivalent to trigram blocking. 

XE w/o 
penalty 

XE w/ 
penalty 

SCST w/ 
penalty 
Avg. # of trigram 
repeats in output 

25.9 
0.67 
3.70 

Avg. # unique trigram 
overlaps btw. output 
and ground truth 

2.23 
2.97 
3.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Analysis of different model outputs (α = 2.0 
for models w/ penalty) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Ruotian Luo, Sang Phan, and all con-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<title level="m">Bottom-up and top-down attention for image captioning and vqa</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Re-evaluating automatic metrics for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mert</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07600</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Nazli Ikizler-Cinbis, and Erkut Erdem</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06607</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recurrent topic-transition gan for visual paragraph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discriminability objective for training descriptive captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruotian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>abs/1803.04376</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Steven J Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<idno>abs/1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
