<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Neural CCG Parsing with Optimality Guarantees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Global Neural CCG Parsing with Optimality Guarantees</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2366" to="2376"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not de- compose over substructures in a way that allows ef- fective dynamic programming. Existing work has therefore used greedy inference techniques such as beam search <ref type="bibr" target="#b27">(Vinyals et al., 2015;</ref><ref type="bibr" target="#b9">Dyer et al., 2015)</ref> or reranking ( <ref type="bibr" target="#b21">Socher et al., 2013)</ref>. We introduce the first global recursive neural parsing approach with optimality guarantees for decoding and use it to build a state-of-the-art CCG parser.</p><p>To enable learning of global representations, we modify the parser to search directly in the space of all possible parse trees with no dynamic program- ming. Optimality guarantees come from A * search, which provides a certificate of optimality if run to completion with a heuristic that is a bound on the future cost. Generalizing A * to global models is challenging; these models also break the locality as- sumptions used to efficiently compute existing A * heuristics ( <ref type="bibr" target="#b14">Klein and Manning, 2003;</ref><ref type="bibr" target="#b15">Lewis and Steedman, 2014</ref>). Rather than directly replacing lo- cal models, we show that they can simply be aug- mented by adding a score from a global model that is constrained to be non-positive and has a trivial upper bound of zero. The global model, in effect, only needs to model the remaining non-local phe- nomena. In our experiments, we use a recent fac- tored A * CCG parser ( <ref type="bibr" target="#b17">Lewis et al., 2016</ref>) for the local model, and we train a Tree-LSTM ( <ref type="bibr" target="#b22">Tai et al., 2015</ref>) to model global structure.</p><p>Finding a model that achieves these A * guar- antees in practice is a challenging learning prob- lem. Traditional structured prediction objectives fo- cus on ensuring that the gold parse has the high- est score <ref type="bibr" target="#b5">(Collins, 2002;</ref><ref type="bibr" target="#b12">Huang et al., 2012</ref>). This condition is insufficient in our case, since it does not guarantee that the search will terminate in sub- exponential time. We instead introduce a new ob- jective that optimizes efficiency as well as accuracy. Our loss function is defined over states of the A * search agenda, and it penalizes the model whenever the top agenda item is not a part of the gold parse. </p><formula xml:id="formula_0">N P/N P N P (S\N P )/N P N P &gt; &gt; N P S\N P &lt; S Fruit flies like bananas N P N P \N P (S\N P )/N P N P &lt; &gt; N P S\N P &lt; S Fruit flies like bananas N P S\N P (S\S)/N P N P &lt; &gt; S S\S &lt; S (b)</formula><p>The search space in this work, with one node for each partial parse. Minimizing this loss encourages the model to return the correct parse as quickly as possible.</p><p>The combination of global representations and optimal decoding enables our parser to achieve state-of-the-art accuracy for Combinatory Catego- rial Grammar (CCG) parsing. Despite being in- tractable in the worst case, the parser in practice is highly efficient. It finds optimal parses for 99.9% of held out sentences while exploring just 190 subtrees on average-allowing it to outperform beam search in both speed and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Overview</head><p>Parsing as hypergraph search Many parsing al- gorithms can be viewed as a search problem, where parses are specified by paths through a hypergraph.</p><p>A node y in this hypergraph is a labeled span, rep- resenting structures within a parse tree, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Each hyperedge e in the hypergraph rep- resents a rule production in a parse. The head node of the hyperedge HEAD(e) is the parent of the rule production, and the tail nodes of the hyperedge are the children of the rule production. For example, consider the hyperedge in <ref type="figure" target="#fig_1">Figure 1b</ref> whose head is like bananas. This hyperedge represents a forward application rule applied to its tails, like and bananas.</p><p>To define a path in the hypergraph, we first in- clude a special start node ∅ that represents an empty parse. ∅ has outgoing hyperedges that reach ev- ery leaf node, representing assignments of labels to words (supertag assignments in <ref type="figure" target="#fig_1">Figure 1</ref>). We then define a path to be a set of hyperedges E starting at ∅ and ending at a single destination node. A path therefore specifies the derivation of the parse con- structed from the labeled spans at each node. For example, in <ref type="figure" target="#fig_1">Figure 1</ref>, the set of bolded hyperedges form a path deriving a complete parse.</p><p>Each hyperedge e is weighted by a score s(e) from a parsing model. The score of a path E is the sum of its hyperedge scores:</p><formula xml:id="formula_1">g(E) = e∈E s(e)</formula><p>Viterbi decoding is equivalent to finding the highest scoring path that forms a complete parse.</p><p>Search on parse forests Traditionally, the hyper- graph represents a packed parse chart. In this work, our hypergraph instead represents a forest of parses. <ref type="figure" target="#fig_1">Figure 1</ref> contrasts the two representations.</p><p>In the parse chart, labels on the nodes represent local properties of a parse, such as the category of a span in <ref type="figure" target="#fig_1">Figure 1a</ref>. As a result, multiple parses that contain the same property include the same node in their path, (e.g. the node spanning the phrase Fruit flies with category NP). The number of nodes in this hypergraph is polynomial in the sentence length, permitting exhaustive exploration (e.g. CKY pars- ing). However, the model scores can only depend on local properties of a parse. We refer to these models as locally factored models.</p><p>In contrast, nodes in the parse forest are labeled with entire subtrees, as shown in <ref type="figure" target="#fig_1">Figure 1b</ref>. For ex- ample, there are two nodes spanning the phrase Fruit flies with the same category NP but different inter- nal substructures. While the parse forest requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees.</p><p>A * parsing A * parsing has been successfully ap- plied in locally factored models <ref type="bibr" target="#b14">(Klein and Manning, 2003;</ref><ref type="bibr" target="#b15">Lewis and Steedman, 2014;</ref><ref type="bibr" target="#b16">Lewis et al., 2015;</ref><ref type="bibr" target="#b17">Lewis et al., 2016</ref>). We present a special case of A * parsing that is conceptually simpler, since the parse forest constrains each node to be reachable via a unique path. During exploration, we maintain the unique (and therefore highest scoring) path to a hyperedge e, which we define as PATH(e).</p><p>Similar to the standard A * search algorithm, we maintain an agenda A of hyperedges to explore and a forest F of explored nodes that initially contains only the start node ∅.</p><p>Each hyperedge e in the agenda is sorted by the sum of its inside score g(PATH(e)) and an admissible heuristic h(e). A heuristic h(e) is admissible if it is an upper bound of the sum of hyperedge scores leading to any complete parse reachable from e (the Viterbi outside score). The efficiency of the search improves when this bound is tighter.</p><p>At every step, the parser removes the top of the agenda, e max = argmax e∈A (g(PATH(e)) + h(e)). e max is expanded by combining HEAD(e max ) with previously explored parses from F to form new hy- peredges. These new hyperedges are inserted into A, and HEAD(e max ) is added it to F. We repeat these steps until the first complete parse y * is ex- plored. The bounds provided by h(e) guarantee that the path to y * has the highest possible score. <ref type="figure" target="#fig_1">Fig- ure 1b</ref> shows an example of the agenda and the ex- plored forest at the end of perfectly efficient search, where only the optimal path is explored.</p><p>Approach The enormous search space described above presents a challenge for an A * parser, since computing a tight and admissible heuristic is diffi- cult when the model does not decompose locally.</p><p>Our key insight in addressing this challenge is that existing locally factored models with an informative A * heuristic can be augmented with a global score (Section 3). By constraining the global score to be non-positive, the A * heuristic from the locally fac- tored model is still admissible.</p><p>While the heuristic from the local model offers some estimate of the future cost, the efficiency of the parser requires learning a well-calibrated global score, since the heuristic becomes looser as the global score provides stronger penalties (Section 5).</p><p>As we explore the search graph, we incrementally construct a neural network, which computes repre- sentations of the parses and allows backpropagation of errors from bad search steps (Section 4).</p><p>In the following sections, we present our ap- proach in detail, assuming an existing locally fac- tored model s local (e) for which we can efficiently compute an admissible A * heuristic h(e).</p><p>In the experiments, we apply our model to CCG parsing, using the locally factored model and A * heuristic from <ref type="bibr" target="#b17">Lewis et al. (2016)</ref>.</p><p>In s global (e), we first compute a hidden representa- tion encoding the parse structure of y = HEAD(e). We use a variant of the Tree-LSTM ( <ref type="bibr" target="#b22">Tai et al., 2015)</ref> connected to a bidirectional LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997</ref>) at the leaves. The combination of linear and tree LSTMs allows the hidden repre- sentation of partial parses to condition on both the partial structure and the full sentence. <ref type="figure" target="#fig_2">Figure 2</ref> de- picts the neural network that computes the hidden representation for a parse.</p><p>Formally, given a sentence w 1 , w 2 , . . . , w n , we compute hidden states h t and cell states c t in the for- ward LSTM for 1 &lt; t ≤ n:</p><formula xml:id="formula_2">i t =σ(W i [c t−1 , h t−1 , x t ] + b i ) o t =σ(W o [˜ c t , h t−1 , x t ] + b o ) ˜ c t = tanh(W c [h t−1 , x t ] + b c ) c t =i t • ˜ c t + (1 − i t ) • c t−1 h t =o t • tanh(c t )</formula><p>where σ is the logistic sigmoid, • is the component- wise product, and x t denotes a learned word embed- ding for w t . We also construct a backward LSTM, which produces the analogous hidden and cell states starting at the end of the sentence, which we denote as c t and h t respectively. The start and end latent states, c −1 , h −1 , c n+1 , and h n+1 , are learned embed- dings. This variant of the LSTM includes peephole connections and couples the input and forget gates.</p><p>The bidirectional LSTM over the words serves as a base case when we recursively compute a hid- den representation for the parse y using the tree- structured generalization of the LSTM:</p><formula xml:id="formula_3">i y = σ(W R i [c l , h l , c r , h r , x y ] + b R i ) f y = σ(W R f [c l , h l , c r , h r , x y ] + b R f ) o y = σ(W R o [ c y , h l , h r , x y ] + b R o ) c lr = f y • c l + (1 − f y ) • c r c y = tanh(W R c [h l , h r , x y ] + b R c ) c y = i y • c y + (1 − i y ) • c lr h y = o y • tanh(c y )</formula><p>where the weights and biases are parametrized by the rule R that produces y from its children, and x y denotes a learned embedding for the category at the root of y. For example, in CCG, the rule would cor- respond to the CCG combinator, and the label would correspond to the CCG category. We assume that nodes are binary, unary, or leaves. Their left and right latent states, c l , h l , c r , and h r are defined as follows:</p><p>• In a binary node, c l and h l are the cell and hid- den states of the left child, and c r and h r are the cell and hidden states of the right child.</p><p>• In a unary node, c l and h l are learned embed- dings, and c r and h r are the cell and hidden states of the singleton child.</p><p>• In a leaf node, let w denote the index of the corresponding word. Then c l and h l are c w and h w from the forward LSTM, and c r and h r are c w and h w from the backward LSTM. The cell state of the recursive unit is a linear com- bination of the intermediate cell state c y , the left cell state c l , and the right cell state c r . To preserve the normalizing property of coupled gates, we perform coupling in a hierarchical manner: the input gate i y decides the weights for c y , and the forget gate f y shares the remaining weights between c l and c r .</p><p>Given the hidden representation h y at the root, we score the global component as follows:</p><formula xml:id="formula_4">s global (e) = log(σ(W · h y ))</formula><p>This definition of the global score ensures that it is non-positive-an important property for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>Using the hyperedge scoring model s(e) described in Section 3, we can find the highest scoring path that derives a complete parse tree by using the A * parsing algorithm described in Section 2.  Admissible A * heuristic Since our full model adds non-positive global scores to the existing lo- cal scores, path scores under the full model cannot be greater than path scores under the local model. Upper bounds for path scores under the local model also hold for path scores under the full model, and we simply reuse the A * heuristic from the local model to guide the full model during parsing without sacrificing optimality guarantees.</p><formula xml:id="formula_5">N P/N P N P &gt; N P Fruit flies like bananas N P/N P N P (S\N P )/N P N P &gt; &gt; N P S\N P &lt; S s global (e) + s local (e) → Fruit N P/N P flies N P Fruit flies N P/N P N P &gt; N P Fruit flies N P/N P N P &gt; N P Fruit flies like bananas N P/N P N P (S\N P )/N P N P &gt; &gt; N P S\N P &lt; S s global (e global ) s local (e local )</formula><p>Incremental neural network construction The recursive hidden representations used in s global (e) can be computed in constant time during parsing. When scoring a new hyperedge, its children must have been previously scored. Instead of computing the full recursion, we reuse the existing latent states of the children and compute s global (e) with an in- cremental forward pass over a single recursive unit in the neural network. By maintain the latent states of each parse, we incrementally build a single DAG- structured LSTM mirroring the explored subset of the hypergraph. This not only enables quick for- ward passes during decoding, but also allows back- propagation through the search space after decoding, which is crucial for efficient learning (see Section 5).</p><p>Lazy global scoring The global score is expensive to compute. We introduce an optimization to avoid computing it when provably unnecessary. We split each hyperedge e into two successive hyperedges, e local and e global , as shown in <ref type="figure" target="#fig_4">Figure 3</ref>. The score for e, previously s(e) = s local (e) + s global (e), is also split between the two new hyperedges:</p><formula xml:id="formula_6">s(e local ) = s local (e local ) s(e global ) = s global (e global )</formula><p>Intuitively, this transformation requires A * to verify that the local score is good enough before comput- ing the global score, which requires an incremental forward pass over a recursive unit in the neural net- work. In the example, this involves first summing the supertag scores of Fruit and flies and inserting the result back into the agenda. The score for ap- plying the forward application rule to the recursive representations is only computed if that item appears again at the head of the agenda. In practice, the lazy global scoring reduces the number of recursive units by over 91%, providing a 2.4X speed up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning</head><p>During training (Algorithm 1), we assume access to sentences labeled with gold parse treesˆytreesˆ treesˆy and gold derivationsˆEderivationsˆ derivationsˆE. The gold derivationˆEderivationˆ derivationˆE is a path from ∅ tô y in the parse forest. A * search with our global model is not guar- anteed to terminate in sub-exponential time. This creates challenges for learning-for example, it is not possible in practice to use the standard struc- tured perceptron update <ref type="bibr" target="#b5">(Collins, 2002)</ref>, because the search procedure rarely terminates early in training. Other common loss functions assume inexact search ( <ref type="bibr" target="#b12">Huang et al., 2012</ref>), and do not optimize efficiency.</p><p>Instead, we optimize a new objective that is tightly coupled with the search procedure. During parsing, we would like hyperedges from the gold derivation to appear at the top of the agenda A. When this condition does not hold, A * is searching inefficiently, and we refer to this as a violation of the agenda, which we formally define as:</p><formula xml:id="formula_7">v( ˆ E, A) = max e∈A (g(PATH(e)) + h(e)) − max e∈A∩ˆEe∈A∩ˆ e∈A∩ˆE (g(PATH(e)) + h(e))</formula><p>where g(PATH(e)) is the score of the unique path to e, and h(e) is the A * heuristic. If all violations are zero, we find the gold parse without exploring any incorrect partial parses-maximizing both accuracy and efficiency. <ref type="figure" target="#fig_1">Figure 1b</ref> shows such a case-if any other nodes were explored, they would be violations. <ref type="table">Table 1</ref>: Loss functions optimized by the different update meth- ods. The updates depend on the list of T non-zero violations, V = V1, V2, . . . , VT , as defined in Section 5.</p><formula xml:id="formula_8">Update LOSS(V) Greedy V 1 Max violation max T t=1 V t All violations T t=1 V t</formula><p>In existing work on violation-based updates, com- parisons are only made between derivations with the same number of steps ( <ref type="bibr" target="#b12">Huang et al., 2012;</ref>)-whereas our definition allows subtrees of arbitrary spans to compete with each other, be- cause hyperedges are not explored in a fixed order. Our violations also differ from Huang et al.'s in that we optimize efficiency as well as accuracy.</p><p>We define loss functions over these violations, which are minimized to encourage correct and ef- ficient search. During training, we parse each sen- tence until either the gold parse is found or we reach computation limits. We record V, the list of non- zero violations of the agenda A observed:</p><formula xml:id="formula_9">V = v( ˆ E, A) | v( ˆ E, A) &gt; 0</formula><p>We can optimize several loss functions over V, as defined in <ref type="table">Table 1</ref>. The greedy and max-violation updates are roughly analogous to the violation- fixing updates proposed by <ref type="bibr" target="#b12">Huang et al. (2012)</ref>, but adapted to exact agenda-based parsing. We also introduce a new all-violations update, which min- imizes the sum of all observed violations. The all- violations update encourages correct parses to be ex- plored early (similar to the greedy update) while be- ing robust to parses with multiple deviations from the gold parse (similar to the max-violation update). The violation losses are optimized with subgra- dient descent and backpropagation. For our experi- ments, s local (e) and h(e) are kept constant. Only the parameters θ of s global (e) are updated. Therefore, a subgradient of a violation v( ˆ E, A) can be computed by summing subgradients of the global scores.</p><formula xml:id="formula_10">∂v( ˆ E, A) ∂θ = e∈PATH(emax) ∂s global (e) ∂θ − e∈PATH(ˆ emax) ∂s global (e) ∂θ</formula><p>where e max denotes the hyperedge at the top of the agenda A andêandˆandê max denotes the hyperedge in the gold derivationˆEderivationˆ derivationˆE that is closest to the top of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Violation-based learning algorithm</head><p>Definitions D is the training data containing input sentences x and gold derivationsˆEderivationsˆ derivationsˆE. e variables denote scored hyper- edges. <ref type="bibr">TAG(x)</ref> returns a set of scored pre-terminals for every word. ADD(F , y) adds partial parse y to forest F. RULES(F , y) returns the set of scored hyperedges that can be created by combining y with entries in F . SIZE OK(F , A) returns whether the sizes of the forest and agenda are within predefined limits. 1: function VIOLATIONS( ˆ E, x, θ) 2:</p><p>V ← ∅ Initialize list of violations V 3:</p><formula xml:id="formula_11">F ← ∅ Initialize forest F 4: A ← ∅ Initialize agenda A 5:</formula><p>for e ∈ TAG(x) do 6: PUSH(A, e) 7:</p><p>while |A ∩ ˆ E| &gt; 0 and SIZE OK(F , A) do 8:</p><formula xml:id="formula_12">if v( ˆ E, A) &gt; 0 then 9: APPEND(V , v( ˆ E, A)) Record violation 10:</formula><p>emax ← EXTRACT MAX(A) Pop agenda 11:</p><p>ADD(F , HEAD(emax)) Explore hyperedge 12:</p><p>for e ∈ RULES(F , HEAD(emax), θ) do 13: PUSH(A, e) Expand hyperedge 14:</p><p>return V</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15: 16: function LEARN(D) 17:</head><p>for i = 1 to T do 18:</p><formula xml:id="formula_13">for x, ˆ E ∈ D do 19: V ← VIOLATIONS( ˆ E, x, θ) 20: L ← LOSS(V ) 21: θ ← OPTIMIZE(L, θ) 22:</formula><p>return θ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data</head><p>We trained our parser on Sections 02-21 of CCG- bank <ref type="bibr" target="#b11">(Hockenmaier and Steedman, 2007)</ref>, using Section 00 for development and Section 23 for test. To recover a single gold derivation for each sentence to use during training, we find the right-most branch- ing parse that satisfies the gold dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>For the local model, we use the supertag-factored model of <ref type="bibr" target="#b17">Lewis et al. (2016)</ref>. Here, s local (e) cor- responds to a supertag score if a HEAD(e) is a leaf and zero otherwise. The outside score heuristic is computed by summing the maximum supertag score for every word outside of each span. In the reported results, we back off to the supertag-factored model after the forest size exceeds 500,000, the agenda size exceeds 2 million, or we build more than 200,000 re- cursive units in the neural network.  <ref type="table">Table 2</ref>: Labeled F1 for CCGbank dependencies on the CCG- bank development and test set for our system Global A * and the baselines.</p><p>Our full system is trained with all-violations up- dates. During training, we lower the forest size limit to 2000 to reduce training times. The model is trained for 30 epochs using ADAM <ref type="bibr" target="#b13">(Kingma and Ba, 2014</ref>), and we use early stopping based on develop- ment F1. The LSTM cells and hidden states have 64 dimensions. We initialize word representations with pre-trained 50-dimensional embeddings from <ref type="bibr" target="#b23">Turian et al. (2010)</ref>. Embeddings for categories have 16 di- mensions and are randomly initialized. We also ap- ply dropout with a probability of 0.4 at the word em- bedding layer during training. Since the structure of the neural network is dynamically determined, we do not use mini-batches. The neural networks are implemented using the CNN library, 1 and the CCG parser is implemented using the EasySRL library. <ref type="bibr">2</ref> The code is available online. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines</head><p>We compare our parser to several baseline CCG parsers: the C&amp;C parser <ref type="bibr" target="#b2">(Clark and Curran, 2007)</ref>; C&amp;C + RNN ( , which is the C&amp;C parser with an RNN supertagger; Xu (2016), a LSTM shift-reduce parser;  who combine a bidirectional LSTM supertagger with a beam search parser using global features ; and supertag-factored ( <ref type="bibr" target="#b17">Lewis et al., 2016)</ref>, which uses deterministic A * decoding and an LSTM supertagging model. <ref type="table">Table 2</ref> shows parsing results on the test set. Our global features let us improve over the supertag- factored model by 0.6 F1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Parsing Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>also</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Dev F1 Optimal Explored Supertag-factored 87.5 100.0% 402.5 − dynamic program 87.5 97.1% 17119.6 Span-factored 87.9 99.9% 176.5 − dynamic program 87.8 99.5% 578.5 Global A * 88.4 99.8% 309.6 − lexical inputs 87.8 99.6% 538.5 − lexical context 88.1 99.4% 610.5 <ref type="table">Table 3</ref>: Ablations of our full model (Global A * ) on the de- velopment set. Explored refers to the size of the parse forest.</p><p>Results show the importance of global features and lexical in- formation in context.</p><p>use global features, but our optimal decoding leads to an improvement of 0.4 F1.</p><p>Although we observed an overall improvement in parsing performance, the supertag accuracy was not significantly different after applying the parser.</p><p>On the test data, the parser finds the optimal parse for 99.9% sentences before reaching our computa- tional limits. On average, we parse 27.1 sentences per second, 4 while exploring only 190.2 subtrees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Model Ablations</head><p>We ablate various parts of the model to determine how they contribute to the accuracy and efficiency of the parser, as shown in <ref type="table">Table 3</ref>. For each model, the comparisons include the average number of parses explored and the percentage of sentences for which an optimal parse can be found without backing off.</p><p>Structure ablation We first ablate the global score, s global (y), from our model, thus relying en- tirely on the local supertag-factors that do not explic- itly model the parse structure. This ablation allows dynamic programming and is equivalent to the back- off model (supertag-factored in <ref type="table">Table 3</ref>). Surpris- ingly, even in the exponentially larger search space, the global model explores fewer nodes than the supertag-factored model-showing that the global model efficiently prune large parts of the search space. This effect is even larger when not using dy- namic programming in the supertag-factored model.</p><p>Global structure ablation To examine the impor- tance of global features, we ablate the recursive hid- den representation (span-factored in <ref type="table">Table 3</ref>). The model in this ablation decomposes over labels for U.S. small business is one <ref type="figure">Figure 4</ref>: Example of an incorrect partial parse that appears syntactically plausible in isolation. The full sentence is 'Indeed, for many Japanese trading companies, the favorite U.S. small business is one whose research and development can be milked for future Japanese use.' The global model heavily penalizes this garden path, thereby avoiding regions that lead to dead ends and allowing the global model to explore fewer nodes.</p><formula xml:id="formula_14">N/N (N/N )\(N/N ) N (S dcl \N P )/N P N &lt; N/N N P &gt; &gt; N S dcl \N P N P &lt; S dcl</formula><p>spans, as in <ref type="bibr" target="#b8">Durrett and Klein (2015)</ref>. In this model, the recursive unit uses, instead of latent states from its children, the latent states of the backward LSTM at the start of the span and the latent states of the for- ward LSTM at the end of the span. Therefore, this model encodes the lexical information available in the full model but does not encode the parse struc- ture beyond the local rule production. While the dy- namic program allows this model to find the optimal parse with fewer explorations, the lack of global fea- tures significantly hurts its parsing accuracy.</p><p>Lexical ablation We also show lexical ablations instead of structural ablations. We remove the bidi- rectional LSTM at the leaves, thus delexicalizing the global model. This ablation degrades both accuracy and efficiency, showing that the model uses lexical information to discriminate between parses.</p><p>To understand the importance of contextual infor- mation, we also perform a partial lexical ablation by using word embeddings at the leaves instead of the bidirectional LSTM, thus propagating only lexical information from within the span of each parse. The degradation in F1 is about half of the degradation from the full lexical ablation, suggesting that a sig- nificant portion of the lexical cues comes from the context of a parse. <ref type="figure">Figure 4</ref> illustrates the impor- tance of context with an incorrect partial parse that appears syntactically plausible in isolation. These bottom-up garden paths are typically problematic for parsers, since their incompatibility with the re- maining sentence is difficult to recognize until later stages of decoding. However, our global model learns to heavily penalize these garden paths by us- ing the context provided by the bidirectional LSTM  Our system uses all-violations updates and is the most accurate.  The all-violations update shows the fastest convergence. and avoid paths that lead to dead ends or bad regions of the search space. <ref type="table" target="#tab_2">Table 4</ref> compares the different violation-based learning objectives, as discussed in Section 5. Our novel all-violation updates outperform the alterna- tives. We attribute this improvement to the robust- ness over poor search spaces, which the greedy up- date lacks, and the incentive to explore good parses early, which the max-violation update lacks. Learn- ing curves in <ref type="figure" target="#fig_6">Figure 5</ref> show that the all-violations update also converges more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Update Comparisons</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Decoder Comparisons</head><p>Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we de- code our full model using best-first search, rerank- ing, and beam search. <ref type="table">Table 5</ref> shows the F1 scores with and without the backoff model, the portion of the sentences that each decoder is able to parse, and the time spent decoding relative to the A * parser.</p><p>In the best-first search comparison, we do not in- clude the informative A * heuristic, and the parser completes very few parses before reaching computa- tional limits-showing the importance of heuristics in large search spaces. In the reranking comparison,</p><formula xml:id="formula_15">Decoder Dev F1 Dev F1 Relative − backoff Time Global A * 88.4</formula><p>88.4 (99.8%) 1X Best-first 87.5 2.8 (6.7%) 293.4X 10-best reranking 87.9 87.9 (99.7%) 8.5X 100-best reranking 88. <ref type="bibr">2</ref> 88.0 (99.4%) 72.3X 2-best beam search 88.2 85.7 (94.0%) 2.0X 4-best beam search 88.3 88.1 (99.2%) 6.7X 8-best beam search 88.2 86.8 (98.1%) 26.3X <ref type="table">Table 5</ref>: Comparison of various decoders using the same model from our full system (Global A * ). We report F1 with and with- out the backoff model, the percentage of sentences that the de- coder can parse, and the time spent decoding relative to A * .</p><p>we obtain n-best lists from the backoff model and rerank each result with the full model. In the beam search comparison, we use the approach from  which greedily finds the top-n parses for each span in a bottom-up manner. Results indi- cate that both approximate methods are less accurate and slower than A * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Many structured prediction problems are based around dynamic programs, which are incompatible with recursive neural networks because of their real- valued latent variables. Some recent models have neural factors <ref type="bibr" target="#b8">(Durrett and Klein, 2015)</ref>, but these cannot condition on global parse structure, making them less expressive. Our search explores fewer nodes than dynamic programs, despite an exponen- tially larger search space, by allowing the recursive neural network to guide the search.</p><p>Previous work on structured prediction with re- cursive or recurrent neural models has used beam search-e.g. in shift reduce parsing <ref type="bibr" target="#b9">(Dyer et al., 2015)</ref>, string-to-tree transduction ( <ref type="bibr" target="#b27">Vinyals et al., 2015)</ref>, or reranking ( <ref type="bibr" target="#b21">Socher et al., 2013)</ref>-at the cost of potentially recovering suboptimal solutions. For our model, beam search is both less efficient and less accurate than optimal A * decoding. In the non-neural setting, <ref type="bibr" target="#b31">Zhang et al. (2014)</ref> showed that global features with greedy inference can improve dependency parsing. The CCG beam search parser of , most related to this work, also uses global features. By using neural representations and exact search, we improve over their results.</p><p>A * parsing has been previously proposed for lo- cally factored models ( <ref type="bibr" target="#b14">Klein and Manning, 2003;</ref><ref type="bibr" target="#b18">Pauls and Klein, 2009;</ref><ref type="bibr" target="#b1">Auli and Lopez, 2011;</ref><ref type="bibr" target="#b15">Lewis and Steedman, 2014</ref>). We generalize these methods to enable global features.  apply best-first search to an unlabeled shift-reduce parser. Their use of error states is related to our global model that penalizes local scores. We demon- strated that best-first search is infeasible in our set- ting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for structured predic- tion. SEARN <ref type="bibr" target="#b7">(Daumé III et al., 2009)</ref> and DAG- GER <ref type="bibr" target="#b20">(Ross et al., 2011</ref>) learn greedy policies to pre- dict structure by sampling classification examples over actions from single states. We similarly gen- erate classification examples over hyperedges in the agenda, but actions from multiple states compete against each other. Other learning objectives that up- date parameters based on a beam or agenda of par- tial structures have also been proposed ( <ref type="bibr" target="#b4">Collins and Roark, 2004;</ref><ref type="bibr" target="#b6">Daumé III and Marcu, 2005;</ref><ref type="bibr" target="#b12">Huang et al., 2012;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b28">Wiseman and Rush, 2016)</ref>, but the impact of search errors is unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have shown for the first time that a parsing model with global features can be decoded with optimal- ity guarantees. This enables the use of powerful re- cursive neural networks for parsing without resort- ing to approximate decoding methods. Experiments show that this approach is effective for CCG pars- ing, resulting in a new state-of-the-art parser. In fu- ture work, we will apply our approach to other struc- tured prediction tasks, where neural networks-and greedy beam search-have become ubiquitous.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fruit flies like bananas ? S explored agenda unexplored (a) The search space in chart parsing, with one node for each labeling of a span.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of CCG parsing as hypergraph search, showing partial views of the search space. Weighted hyperedges from child nodes to a parent node represent rule productions scored by a parsing model. A path starting at ∅, for example the set of bolded hyperedges, represents the derivation of a parse. During decoding, we find the highest scoring path to a complete parse. Both figures show an ideal exploration that efficiently finds the optimal path. Figure 1a depicts the traditional search space, and Figure 1b depicts the search space in this work. Hyperedge scores can only depend on neighboring nodes, so our model can condition on the entire parse structure, at the price of an exponentially larger search space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the Tree-LSTM which computes vector embeddings for each parse node. The leaves of the TreeLSTM are connected to a bidirectional LSTM over words, encoding lexical information within and outside of the parse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The hyperedge on the left requires computing both the local and global score when placed on the agenda. Splitting the hyperedge, as shown on the right, saves expensive computation of the global score if the local score alone indicates that the parse is not worth exploring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Learning curves for the first 3 training epochs on the development set when training with different updates strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 : Parsing results trained with different update methods.</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="3"> Model Our model scores a hyperedge e by combining the score from the local model with a global score that conditions on the entire parse at the head node: s(e) = s local (e) + s global (e)</note>

			<note place="foot" n="1"> https://github.com/clab/cnn 2 https://github.com/mikelewis0/EasySRL 3 https://github.com/kentonl/neuralccg</note>

			<note place="foot" n="4"> We use a single 3.5GHz CPU core.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Luheng He, Julian Michael, and Mark Yatskar for valuable discussion, and the anonymous reviewers for feedback and comments. This work was supported by the NSF (IIS-1252835, IIS-1562364), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally Normalized Transition-Based Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient CCG parsing: A* versus Adaptive Supertagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Widecoverage Efficient Statistical Parsing with CCG and Log-Linear Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James R Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The Java Version of the C&amp;C Parser: Version 0.95</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luana</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge Computer Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incremental Parsing with the Perceptron Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">111</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning as search optimization: Approximate Large Margin Methods for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Search-based structured prediction. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural CRF Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transitionbased Dependency Parsing with Stack Long ShortTerm Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CCGbank: a Corpus of CCG derivations and Dependency Structures Extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured Perceptron with Inexact Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suphan</forename><surname>Fayong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="142" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A* Parsing: Fast Exact Viterbi Parse Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A* CCG Parsing with a Supertag-factored Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint A* CCG Parsing and Semantic Role Labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LSTM CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter</title>
		<meeting>the 15th Annual Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">K-best A* Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th</title>
		<meeting>the Joint Conference of the 47th</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="958" to="966" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04-11" />
			<biblScope unit="page" from="627" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with Compositional Vector Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL conference</title>
		<meeting>the ACL conference</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<title level="m">Improved Semantic Representations from Tree-structured Long Short-term Memory Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word representations: A Simple and General Method for Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient Structured Inference for Transition-Based Parsing with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Neural Networks and Error States. Transactions of the Association for Computational Linguistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Supertagging With LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Musa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grammar as a Foreign Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequenceto-Sequence Learning as Beam-Search Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">CCG Supertagging with a Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">250</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">LSTM Shift-Reduce CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Greed is Good if Randomized: New Inference for Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
