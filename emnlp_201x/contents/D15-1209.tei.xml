<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leave-one-out Word Alignment without Garbage Collector Effects</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Translation Research and Development Promotion Center National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Translation Research and Development Promotion Center National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Translation Research and Development Promotion Center National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Translation Research and Development Promotion Center National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Advanced Translation Research and Development Promotion Center National Institute of Information and Communications Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leave-one-out Word Alignment without Garbage Collector Effects</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the field of unsupervised word alignment. However, these algorithms have a problem of over-fitting, leading to &quot;garbage collector effects,&quot; where rare words tend to be erroneously aligned to untranslated words. This paper proposes a leave-one-out expectation-maximization algorithm for unsupervised word alignment to address this problem. The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it. This prevents erroneous alignments within a sentence pair from supporting themselves. Experimental results on Chinese-English and Japanese-English corpora show that the F 1 , precision and recall of alignment were consistently increased by 5.0%-17.2%, and BLEU scores of end-to-end translation were raised by 0.03-1.30. The proposed method also outperformed l 0-normalized GIZA++ and Kneser-Ney smoothed GIZA++.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems. A lot of methods have been pro- posed to raise the accuracy of WA in an effort to improve end-to-end translation quality. This pa- per contributes to this effort through refining the widely used expectation-maximization (EM) algo- rithm for WA <ref type="bibr" target="#b5">(Dempster et al., 1977;</ref><ref type="bibr" target="#b3">Brown et al., 1993b;</ref><ref type="bibr" target="#b19">Och and Ney, 2000</ref>). * The author now is affiliated with Google, Japan.</p><p>The EM algorithm for WA has a great influ- ence in SMT. Many well-known toolkits includ- ing GIZA++ <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref>, the Berkeley Aligner ( <ref type="bibr" target="#b15">Liang et al., 2006;</ref><ref type="bibr" target="#b6">DeNero and Klein, 2007)</ref>, Fast Align ( <ref type="bibr" target="#b8">Dyer et al., 2013)</ref> and SyM- GIZA++ <ref type="bibr" target="#b12">(Junczys-Dowmunt and Sza, 2012</ref>), all employ this algorithm. GIZA++ in particular is frequently used in systems participating in many shared tasks <ref type="bibr" target="#b11">(Goto et al., 2011;</ref><ref type="bibr" target="#b4">Cettolo et al., 2013;</ref><ref type="bibr" target="#b1">Bojar et al., 2013)</ref>.</p><p>However, the EM algorithm for WA is well- known for introducing "garbage collector ef- fects." Rare words have a tendency to collect garbage, that is they have a tendency to be erro- neously aligned to untranslated words ( <ref type="bibr" target="#b2">Brown et al., 1993a;</ref><ref type="bibr" target="#b16">Moore, 2004;</ref><ref type="bibr" target="#b9">Ganchev et al., 2008;</ref><ref type="bibr" target="#b24">V Gra√ßa et al., 2010)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows a real sentence pair, denoted s, from the GALE Chinese- English Word Alignment and Tagging Training corpus (GALE WA corpus) 1 with it's human- annotated word alignment. The Chinese word "HE ZHANG," denoted w r , which means river custodian, only occurs once in the whole corpus. We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sen- tence pairs from the NIST Open Machine Trans- lation (OpenMT) 2006 evaluation 2 . The resulting alignment is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b). It can be seen that w r is erroneously aligned to multiple English words.</p><p>To find the cause of this, we checked the align- ments in each iteration i of s, denoted a i s . We found that in a 1 s , w r together with the other source-side words were aligned with uniform probability to all the target-side words since the alignment models provided no prior information. However, in a 2 s , w r became erroneously aligned, because the alignment distribution 3 of w r was only learned from a 1 s , thus consisted of non-zero values only for generating the target-side words in s. Therefore, the alignment probabilities from the rare word w r to the unaligned words in s were ex- traordinarily high, since almost all of the proba- bility mass was distributed among them. In other words, the story behind these garbage collector ef- fects is that erroneous alignments are able to pro- vide support for themselves; the probability distri- bution learned only from s is re-applied to s. In this way, these "garbage collector effects" are a form of over-fitting.</p><p>Motivated by this observation, we propose a leave-one-out EM algorithm for WA in this pa- per. Recently this technique has been applied to avoid over-fitting in kernel density estima- tion ( <ref type="bibr" target="#b22">Roux and Bach, 2011</ref>); instead of performing maximum likelihood estimation, maximum leave- one-out likelihood estimation is performed. <ref type="bibr">Figure 1(c)</ref> shows the effect of using our technique on the example. The garbage collection has not occurred, and the alignment of the word "HE ZHANG" is identical to the human annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most related work to this paper is train- ing phrase translation models with leave-one-out forced alignment <ref type="bibr" target="#b29">(Wuebker et al., 2010;</ref><ref type="bibr" target="#b30">Wuebker et al., 2012</ref>). The differences are that their work operates at the phrase level, and their aim is to im- prove translation models; while our work operates at the word level, and our aim is to provide better word alignment. As word alignment is a founda- tion of most MT systems, our method have a wider application.</p><p>Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts ( <ref type="bibr" target="#b32">Zhang and Chiang, 2014</ref>) or integrating the smoothed l 0 prior to the estimation of prob- ability ( <ref type="bibr" target="#b25">Vaswani et al., 2012</ref>). Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach.</p><p>Bayesian methods ( <ref type="bibr" target="#b10">Gilks et al., 1996;</ref><ref type="bibr" target="#b0">Andrieu et al., 2003;</ref><ref type="bibr" target="#b7">DeNero et al., 2008;</ref>  2011), also attempt to address the issue of over- fitting, however EM algorithms related to the pro- posed method have been shown to be more effi- cient ( <ref type="bibr" target="#b26">Wang et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation. The main notation used in this section is shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Standard EM for IBM Models 1, 2 and HMM Model</head><p>To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data. In order to maximize the likelihood of the alignment model Œ∏ given the data S, the fol- lowing two steps are conducted iteratively ( <ref type="bibr" target="#b3">Brown et al., 1993b;</ref><ref type="bibr" target="#b19">Och and Ney, 2000;</ref><ref type="bibr" target="#b20">Och and Ney, 2003)</ref>, Expectation</p><p>Step (E step): calculating the con- ditional probability of alignments for each sen- tence pair,</p><formula xml:id="formula_0">P (a|s, Œ∏) = J j=1 Œ∏ ali (a j |a j‚àí1 , I)Œ∏ lex (f j |e a j ),<label>(1)</label></formula><p>where Œ∏ ali (i|i ‚Ä≤ , I) is the alignment probability and Œ∏ lex (f |e) is the translation probability. Note that f a foreign sentence (f 1 , . . . , f J ) e an English sentence (e 1 , . . . , e I ) s a sentence pair (f , e) a an alignment (a 1 , . . . , a J ) where f j is aligned to e a j B i a list of the indexes of the foreign words which are aligned to e i B i,k the index of the k-th foreign word which is aligned to e i B i</p><p>is the average of all elements in B i œÅ i the largest index of an English word s.t. œÅ i &lt; i and</p><formula xml:id="formula_1">|B œÅ i | &gt; 0 œÜ i the fertility of e i E i the word class of e i Œ∏ ¬∑ an probabilistic model Œ∏ ¬Ø s ¬∑ a leave-one-out probabilistic model for s nx(s, a)</formula><p>the number of times that an event x happens in (s, a) N x (s) the marginal number of times that an event x happens in s <ref type="table">Table 1</ref>: Main Notation. Note that N x (s) = a n x (s, a)P (a|s). In practical calculation, for IBM models 1, 2 and HMM model, this summa- tion is performed by dynamic programming; for IBM model 4, it is performed approximately us- ing the best alignment and its neighbors.</p><p>(1) is a general form for IBM model 1, model 2 and the HMM model. Maximization step (M step): re-estimating the probability models,</p><formula xml:id="formula_2">Œ∏ ali (i|i ‚Ä≤ , I) ‚Üê s N i|i ‚Ä≤ ,I (s) s N i ‚Ä≤ ,I (s)<label>(2)</label></formula><formula xml:id="formula_3">Œ∏ lex (f |e) ‚Üê s N f |e (s) s n e (s)<label>(3)</label></formula><p>where N i ‚Ä≤ ,I (s) is the marginal number of times e i ‚Ä≤ is aligned to some foreign word if the length of e is I, or 0 otherwise; N i|i ‚Ä≤ ,I (s) is the marginal number of times the next alignment position after i ‚Ä≤ is i in a if the length of e is I, or 0 otherwise; n e (s) is the count of e in e; N f |e (s, a) is the marginal number of times e is aligned to f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Leave-one-out EM for IBM Models 1, 2 and HMM Model</head><p>Leave-one-out EM for WA differs from standard EM in the way the alignment and translation prob- abilities are calculated. Each sentence pair will have its own alignment and translation probability models calculated by excluding the sentence pair itself. More formally, leave-one-out EM for WA are formulated as follows, Leave-one-out E step: employing leave-one- out models for each s to calculate the conditional probability of alignments</p><formula xml:id="formula_4">P (a|s, Œ∏ ¬Ø s ) = J j=1 Œ∏ ¬Ø s ali (a j |a j‚àí1 , I)Œ∏ ¬Ø s lex (f j |e a j ), (4)</formula><p>where Œ∏ ¬Ø s ali (i|i ‚Ä≤ , I) and Œ∏ ¬Ø s lex (f j |e a j ) are the leave- one-out alignment probability and translation probability, respectively.</p><p>Leave-one-out M step: re-estimating leave- one-out probability models,</p><formula xml:id="formula_5">Œ∏ ¬Ø s ali (i|i ‚Ä≤ , I) ‚Üê s ‚Ä≤ =s N i|i ‚Ä≤ ,I (s ‚Ä≤ ) s ‚Ä≤ =s N i ‚Ä≤ ,I (s ‚Ä≤ ) (5) Œ∏ ¬Ø s lex (f |e) ‚Üê s ‚Ä≤ =s N f |e (s ‚Ä≤ ) s ‚Ä≤ =s n e (s ‚Ä≤ ) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Standard EM for IBM Model 4</head><p>The framework of the standard EM for IBM Model 4 is similar with the one for IBM Models 1, 2 and HMM Model, but the calculation of align- ment probability is more complicated. E step: calculating the conditional probabil- ity through the reverted alignment <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref>,</p><formula xml:id="formula_6">P (a|s, Œ∏) = P (B 0 |B 1 , . . . , B I )¬∑ I i=1 P (B i |B i‚àí1 , e i ) ¬∑ I i=1 j‚ààB i Œ∏ lex (f j |e i ), (7)</formula><p>where B 0 means the set of foreign words aligned with the empty word; P (B 0 |B 1 , . . . , B I ) is as- sumed to be a binomial distribution for the size of B 0 ( <ref type="bibr" target="#b3">Brown et al., 1993b</ref>) or an modified distri- bution to relieve deficiency ( <ref type="bibr" target="#b20">Och and Ney, 2003)</ref>.</p><formula xml:id="formula_7">The distribution P (B i |B i‚àí1 , e i ) is decomposed as P (B i |B i‚àí1 , e i ) = Œ∏ fer (œÜ i |e i )¬∑ Œ∏ hea (B i,1 ‚àí B œÅ i |E œÅ i ) ¬∑ œÜ i k=2 Œ∏ oth (B i,k ‚àí B i,k‚àí1 ),<label>(8)</label></formula><p>where Œ∏ fer is a fertility model; Œ∏ hea is a probabil- ity model for the head (first) aligned foreign word; Œ∏ oth is a probability model for the other aligned foreign words. Œ∏ hea is assumed to be conditioned on the word class E œÅ i , following the paper of <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref> and the implementation of GIZA++ and CICADA.</p><p>M step: re-estimating the probability models,</p><formula xml:id="formula_8">Œ∏ fer (œÜ|e) ‚Üê s N œÜ|e (s) s œÜ ‚Ä≤ N œÜ ‚Ä≤ |e (s) (9) Œ∏ hea (‚àÜi|E) ‚Üê s N hea ‚àÜi|E (s) s ‚àÜi ‚Ä≤ N hea ‚àÜi ‚Ä≤ |E (s)<label>(10)</label></formula><formula xml:id="formula_9">Œ∏ oth (‚àÜi) ‚Üê s N oth ‚àÜi (s) s ‚àÜi ‚Ä≤ N oth ‚àÜi ‚Ä≤ (s) ,<label>(11)</label></formula><p>where ‚àÜi is a difference of the indexes of two for- eign words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Leave-one-out EM for IBM Model 4</head><p>The leave-one-out treatment were applied to the three component probability models Œ∏ fer , Œ∏ hea and Œ∏ oth of IBM model 4.</p><p>Leave-one-out E step: calculating the condi- tional probability through leave-one-out probabil- ity models</p><formula xml:id="formula_10">P (a|s, Œ∏ ¬Ø s ) = P (B 0 |B 1 , . . . , B I )¬∑ I i=1 P ¬Ø s (B i |B i‚àí1 , e i ) ¬∑ I i=1 j‚ààB i Œ∏ ¬Ø s lex (f j |e i ),<label>(12)</label></formula><formula xml:id="formula_11">P ¬Ø s (B i |B i‚àí1 , e i ) = Œ∏ ¬Ø s fer (œÜ i |e i )¬∑ Œ∏ ¬Ø s hea (B i,1 ‚àí B œÅ i |E œÅ i ) ¬∑ œÜ i k=2 Œ∏ ¬Ø s oth (B i,k ‚àí B i,k‚àí1 ).<label>(13)</label></formula><p>Leave-one-out M step: re-estimating the leave- one-out probability models,</p><formula xml:id="formula_12">Œ∏ ¬Ø s fer (œÜ|e) ‚Üê s ‚Ä≤ =s N œÜ|e (s ‚Ä≤ ) s ‚Ä≤ =s œÜ ‚Ä≤ N œÜ ‚Ä≤ |e (s ‚Ä≤ )<label>(14)</label></formula><formula xml:id="formula_13">Œ∏ ¬Ø s hea (‚àÜi|E) ‚Üê s ‚Ä≤ =s N hea ‚àÜi|E (s ‚Ä≤ ) s ‚Ä≤ =s ‚àÜi ‚Ä≤ N hea ‚àÜi ‚Ä≤ |E (s ‚Ä≤ )<label>(15)</label></formula><formula xml:id="formula_14">Œ∏ ¬Ø s oth (‚àÜi) ‚Üê s ‚Ä≤ =s N oth ‚àÜi (s ‚Ä≤ ) s ‚Ä≤ =s ‚àÜi ‚Ä≤ N oth ‚àÜi ‚Ä≤ (s ‚Ä≤ ) .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Handling Singletons</head><p>Singletons are the words that occur only once in corpora. Singletons cause problems when apply- ing leave-one-out to lexicalized models such as the translation model Œ∏ ¬Ø s lex and the fertility model Œ∏ ¬Ø s fer . When calculating <ref type="formula" target="#formula_5">(6)</ref> and <ref type="formula" target="#formula_0">(14)</ref> for singletons, the denominators become zero, thus the probabilities are undefined.</p><p>For singletons, there is no prior information to guide their alignment, so we back off to uniform distributions. In that case, the alignments are pri- marily determined by the rest of the sentence.</p><p>In addition, singletons can be in the target side of the translation model Œ∏ ¬Ø s lex . In that case, the prob- abilities become zero. This is handled by setting a minimum probability value of 1.0 √ó 10 ‚àí12 , which was decided by pilot experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head><p>To alleviate memory requirements and increase speed, our implementation did not build or store the local alignment models explicitly for each sen- tence pair. The following formula was used to effi- ciently calculate (5), (6) and (14-16) to build tem- porary probability models,</p><formula xml:id="formula_15">s ‚Ä≤ =s N x (s ‚Ä≤ ) = ( s ‚Ä≤ N x (s ‚Ä≤ )) ‚àí N x (s),<label>(17)</label></formula><p>where x is a alignment event. Our implemen- tation maintained global counts of all alignment events s ‚Ä≤ N x (s ‚Ä≤ ), and (considerably smaller) lo- cal counts N x (s) from each sentence pair s.</p><p>Take the translation model Œ∏ ¬Ø s lex for example. For a sentence pair s = (f 1 . . . f J , e 1 . . . e I ), it is cau- clulated as,</p><formula xml:id="formula_16">Œ∏ ¬Ø s lex (f j |e i ) = ( s ‚Ä≤ N (f j |e i ) (s ‚Ä≤ )) ‚àí N (f j |e i ) (s) ( s ‚Ä≤ n e i (s ‚Ä≤ )) ‚àí n e i (s) .<label>(18)</label></formula><p>The global counts to be maintained are s ‚Ä≤ N (f j |e i ) (s ‚Ä≤ ) and n e i (s ‚Ä≤ ), and the local counts are s N (f j |e i ) (s) and n e i (s). Therefore the memory cost is,</p><formula xml:id="formula_17">|E| ¬∑ (|F| + 1) + s I s (J s + 1),<label>(19)</label></formula><p>where |E| is the size of English vocabulary, |F| is the size of foreign language vocabulary, I s is the length of the English sentence of s, and J s is the length of the foreign sentence of s. The calculation of the leave-one-out translation model is performed for each English word and for- eign word in s. Therefore, the time cost is,</p><formula xml:id="formula_18">s I s (J s + 1).<label>(20)</label></formula><p>In addition, because the local counts N (f j |e i ) (s) and n e i (s) are read in order, storing them in a ex- ternal memory such as a hard disk will not slow down the running speed much. This will reduce the memory cost to |E| ¬∑ (|F| + 1).</p><p>This cost is independent to the number of sentence pairs <ref type="bibr">4</ref> . The speed of the proposed method can be boosted through parallelism. These calculations on each sentence pair can be performed indepen- dently. We found empirically that when our im- plementation of the proposed method is run on a 16-core computer, it finishes the task earlier than GIZA++ 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>The proposed WA method was tested on two language pairs: Chinese-English and Japanese- English <ref type="table" target="#tab_1">(Table 2)</ref>. Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks. GIZA++ and our own implementation of standard EM were used as baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT cor- pus. They are from the same domain, both con- tain newswire texts and web blogs. The OpenMT evaluation 2005 was used as a development set for MERT tuning <ref type="bibr" target="#b21">(Och, 2003)</ref>, and the OpenMT eval- uation 2006 was used as a test set. The Japanese- English experimental data was the Kyoto Free Translation Task (Neubig, 2011) <ref type="bibr">6</ref> . The corpus contains a set of 1,235 sentence pairs that are man- ually word aligned.</p><p>The corpora were processed using a standard procedure for machine translation. The English texts were tokenized with the tokenization script released with Europarl corpus ( <ref type="bibr" target="#b14">Koehn, 2005</ref>) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter ( <ref type="bibr" target="#b31">Xue et al., 2002)</ref>  <ref type="bibr">7</ref> ; the Japanese texts were segmented into words using the Kyoto Text Analysis Toolkit (KyTea 8 ). Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.</p><p>GIZA++ was run with the default Moses set- tings ( <ref type="bibr" target="#b13">Koehn et al., 2007</ref>). The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations. We imple- mented the proposed leave-one-out EM and stan- dard EM in IBM model 1, HMM model and IBM model 4. In the original work <ref type="bibr" target="#b20">(Och and Ney, 2003)</ref> this combination of models achieved comparable performance to the default Moses settings. They were run with 5, 5 and 6 iterations.</p><p>The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details. Our implementation is based on the toolkit of CICADA ( <ref type="bibr" target="#b28">Watanabe, 2012;</ref><ref type="bibr" target="#b23">Tamura et al., 2013)</ref>  <ref type="bibr">9</ref> . We named the implemented aligner AGRIPPA, to support our in- house decoders OCTAVIAN and AUGUSTUS.</p><p>In all experiments, WA was performed indepen- dently in two directions: from foreign languages to English, and from English to foreign languages. Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation <ref type="bibr" target="#b19">(Och and Ney, 2000;</ref><ref type="bibr" target="#b20">Och and Ney, 2003</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Word Alignment Accuracy</head><p>Word alignment accuracy of the baseline and the proposed method is shown in <ref type="table" target="#tab_2">Table 3</ref> in terms of precision, recall and F 1 (Och and Ney, 2003). The proposed method gave rise to higher quality align- ments in all our experiments. The improvement in F 1 , precision and recall based on IBM Model 4 is in the range 8.3% to 9.1% compared with the GIZA++ baseline, and in the range 5.0% to 17.2% compared with our own baseline.</p><p>The most meaningful result comes from the comparison of the models trained using standard EM log-likelihood training, and the proposed EM leave-one-out log-likelihood training. These mod- els are identical except for way in which the model likelihood is calculated. In all our experiments the proposed method gave rise to higher quality align- ments. The standard EM implementation achieved Corpus # Sent. pairs # Foreign Words # English Words Chinese-English <ref type="table" target="#tab_1">(GALE WA, OpenMT)  WA  18,057  392,447  518,137  Train  442,967  12,265,</ref>  Models standard EM (GIZA++) standard EM (ours) Leave-one-out(prop.)  alignment performance approximately compara- ble to GIZA++, whereas the proposed method ex- ceeded the performance of both implementations.</p><formula xml:id="formula_20">F 1 P R F 1 P R F 1 P R Chinese-English (GALE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">End-to-end Translation Quality</head><p>BLEU scores achieved by the phrase-based and hierachical SMT systems 10 which were trained from different alignment results, are shown in <ref type="table">Table 4</ref>. Each experiment was conducted three times to mitigate the variance in the results due to MERT. The results show that the proposed align- ment method achieved the highest BLEU score in all experiments. The improvement over the base- line is in range 0.03 to 1.03 for phrase-based sys- tems, and ranged from 0.43 to 1.30 for hierarchical systems.</p><p>Hierarchical systems benifit more from the pro- posed method than phrase-based systems. We think this is because that hierarchical systems are more sensitive to word alignment quality than phrase-based systems. Phrase-based systems only <ref type="bibr">10</ref>   SMT Systems standard EM (GIZA++) standard EM (ours) Leave-one-out (prop.) Chinese-English (GALE WA, OpenMT) Phrase-based 31.85 ¬± 0.26 31.01 ¬± 0.18 32.04 ¬± 0.08 Hierarchical 32.27 ¬± 0.23 31.40 ¬± 0.26 32.70 ¬± 0.14 Japanese-English (Kyoto Free Translation) Phrase-based 18.35 ¬± 0.27 18.20 ¬± 0.20 18.38 ¬± 0.11 Hierarchical 19.48 ¬± 0.08 19.39 ¬± 0.02 20.10 ¬± 0.07 <ref type="table">Table 4</ref>: End-to-end translation quality measured by BLEU Corpus size standard EM (GIZA++) standard EM (ours) Leave-one-out(prop.)   <ref type="table">Table 6</ref>: Effect of training corpus size on end-to-end translation quality measured by BLEU (Chinese- English). ‚Ä† the whole manually word aligned corpus take contiguous parallel phrase pairs as translation rules, while hierarchical systems also use patterns made by subtracting (inner) short parallel phrases from (outer) longer parallel phrases. Both the outer and inner phrases typically need to be noise- free in order to produce high quality rules. This puts a high demand on the alignment quality.</p><formula xml:id="formula_21">F 1 P R F 1 P R F 1 P R 1K 0.429</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Training Corpus Size</head><p>Training corpora of different sizes were employed to perform unsupervised WA experiments and MT experiments (see <ref type="table" target="#tab_4">Tables 5 and 6</ref>). The training corpora were randomly sampled from the Chinese-English manual WA corpora and the parallel training corpus. The manual WA cor- pus has a priority for being sampled so that the gold WA annotation is available for MT experi- ments.</p><p>The settings of the unsupervised WA experi- ments and the MT experiments are the same with the previous experiments. In the WA experiments, GIZA++, our implemented standard EM and the proposed leave-one-out EM are applied to training corpora with the same parameter settings as the previous. In the MT experiments, the WA results of different methods and the gold WA (if available) are employed to extract translation rules; the rest settings including language models, development and test corpus, and parameters are the same as the previous.</p><p>On word alignment accuracy, the proposed method achieved improvements of F 1 from 0.041 to 0.090 under the different training corpora <ref type="table" target="#tab_4">(Table  5</ref>. The maximum improvement compared with GIZA++ is 0.069 when the training corpus has 4,000 sentence pairs. The maximum improvement compared with our own implement is 0.090 when the training corpus has 64,000 sentence pairs. <ref type="figure" target="#fig_2">Figure 2</ref> shows that the extent of improvements slightly changes under different training corpora, but they are all quite stable and obvious.</p><p>On translation quality, the proposed method achieved improvements of BLEU under the dif- ferent training corpora. The improvements ranged from 0.19 to 1.72 for phrase-based MT and ranged from 0.25 to 3.02 (see <ref type="table" target="#tab_4">Table 5</ref>). The improve- ments are larger under smaller training corpora (see <ref type="figure" target="#fig_4">Figure 3</ref>).</p><p>In addition, the BLEUs achieved by the pro- posed method is close to the ones achieved by gold WA annotations. The proposed method slightly outperforms the gold WA annotations when us- ing the full manual WA corpus of 18,057 sentence pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison to l 0 -Normalization and Kneser-Ney Smoothing Methods</head><p>The proposed leave-one-word word align- ment method was empirically compared to l 0 -normalized GIZA++ ( <ref type="bibr">Vaswani et al., 2012) 11</ref> and Kneser-Ney smoothed GIZA++ ( <ref type="bibr" target="#b32">Zhang and Chiang, 2014)</ref>  <ref type="bibr">12</ref> . l 0 -normalization and Kneser- Ney smoothing methods are established methods to overcome the sparse problem. This enables the probability distributions on rare words to be estimated more effectively. In this way, these two GIZA++ variants are related to the proposed method. l 0 -normalized GIZA++ and Kneser-Ney smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES. For the settings of l 0 -normalized GIZA++ that are not in common with GIZA++ were the default settings. As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 -4 and HMM model GIZA++ l0-Normalization Kneser-Ney Smooth. Leave-one-out(prop.) Word <ref type="table">Alignment Quality  F1  P  R  F1  P  R  F1  P  R  F1  P  R  All Words</ref> 0.624 0.698 0.565 0.629 0.700 0.571 0.656 0.726 0.599 0.678 0.755 0.615 S.W.F=1 0.458 0.435 0.483 0.448 0.471 0.427 0.515 0.532 0.499 0.398 0.693 0.279 S.W.F‚â§2 0.466 0.451 0.481 0.461 0.485 0.440 0.522 0.545 0.501 0.450 0.707 0.330 S.W.F‚â§5 0.476 0.480 0.473 0.478 0.509 0.451 0.534 0.572 0.501 0.502 0.722 0.385 S.W.F‚â§10 0.485 0.505 0.466 0.491 0.531 0.456 0.541 0.593 0.498 0.529 0.733 0.414 Translation Quality (BLEU) Phrase-based 31.85 ¬± 0.26 31.52 ¬± 0.06 31.94 ¬± 0.19 32.04 ¬± 0.08 Hierarchical 32.27 ¬± 0.23 32.20 ¬± 0.04 32.47 ¬± 0.33 32.70 ¬± 0.14  <ref type="table" target="#tab_5">Ta- ble 7</ref>. The experiments were run on the Chinese- English language pair. The word alignment qual- ity was evaluated separately for all words and for various levels of rare words. The leave-one-out method outperformed related methods in terms of precision, recall and F 1 when evaluated on all words.</p><p>Rare words were categorized based on the num- ber of occurences in the source-language text of the training data. The evaluations were carried out on the subset of alignment links that had a rare word on the source side. <ref type="table" target="#tab_5">Table 7</ref> presents the results for thresholds 1, 2, 5 and 10. The proposed method achieved much higher preci- sion on rare words than the other methods, but performed poorly on recall. The Kneser-Ney Smoothed GIZA++ had higher recall. The ex- planation might be that the leave-one-out method punishes rare words more than the Kneser-Ney smoothing method, by totally removing the de- rived expected counts of current sentence pair from the alignment models. This leads to rare words being passively aligned. In other words, the leave-one-out method would align rare words un- less the confidence is high. Therefore, we plan to seek a method to integrate Kneser-Ney smoothing into the proposed leave-one-out method in the fu- ture work.</p><p>The BLEU scores achieved by phrase-based SMT and hierarchical SMT for different align- ment methods are presented in <ref type="table" target="#tab_5">Table 7</ref>. The proposed method outperforms the other methods. The Kneser-Ney Smoothed GIZA++ performed the second best. We tried to further analyze the relation between word alignment and BLEU, but found the analysis was obscured by the many processing stages. These stages include paral- lel phrase extraction (or translation rule extraction from hierarchical SMT), log-linear model, MERT tuning and practical decoding where a lot of prun- ing happened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a leave-one-out EM algo- rithm for WA to overcome the over-fitting prob- lem that occurs when using standard EM for WA. The experimental results on Chinese-English and Japanese-English corpora show that both the WA accuracy and the end-to-end translation are im- proved.</p><p>In addition, we have a interesting finding about the effect of manual WA annotations on train- ing MT systems. In a Chinese-English parallel training corpus of 18,057 sentence pairs, the man- ual WA annotation outperformed the unsupervised WA results produced by standard EM algorithms. However, the unsupervised WA results produced by proposed leave-one-out EM algorithm outper- formed the manual WA annotation.</p><p>Our future work will focus on increasing the gains in end-to-end translation quality through the proposed leave-one-out aligner. It is a interest- ing question why GIZA++ achieved competitive BLEU scores though its alignment accuracy mea- sured by F 1 was substantially lower. The answer to this question which may reveal essence of good word alignment for MT and eventually help to im- prove MT. In addition, we plan to improve the pro- posed method by integrating Kneser-Ney smooth- ing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of supervised word alignment. (a) gold alignment; (b) standard EM (GIZA++); (c) Leave-one-out alignment (proposed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Curve of word alignment accuracy (F 1 ) under training corpora of different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Curves of translation quality (BLEU) under training corpora of different sizes. (a) Phrase-based MT; (b) Hierarchical MT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Experimental Data.  ‚Ä† Each consists of one foreign sentence and four English reference sen-
tences. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Word alignment accuracy measured by F 1 , precision and recall.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Effect of training corpus size on word alignment accuracy measured by F 1 , precision and recall 
(Chinese-English).  ‚Ä† the whole manually word aligned corpus 

Corpus size stan.(GIZA++) stan.(ours) LOO(prop.) Gold 
Phrase-based 
1k 
7.86 
7.66 
9.38 10.01 
4k 
15.27 
15.49 
17.06 17.57 
18K  ‚Ä† 
22.15 
21.72 
24.41 24.11 
64K 
28.10 
27.91 
29.23 
NA 
256K 
31.05 
30.82 
31.51 
NA 
461K 
31.85 
31.01 
32.04 
NA 
Hierarchical 
1k 
7.53 
7.54 
9.19 10.62 
4k 
14.89 
15.51 
17.91 18.31 
18K  ‚Ä† 
22.85 
22.56 
24.66 24.52 
64K 
28.82 
28.22 
29.78 
NA 
256K 
31.47 
30.21 
31.72 
NA 
461K 
32.27 
31.04 
32.70 
NA 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Empirical Comparision with l 0 -Normalized and Kneser-Ney Smoothed GIZA++'s 

were turned on. 
The experimental results are presented in </table></figure>

			<note place="foot" n="1"> Released by Linguistic Data Consortium, catalog number LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05. 2 http://www.itl.nist.gov/iad/mig/ tests/mt/2006/</note>

			<note place="foot" n="3"> The probability distribution of generating target language words from wr. The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.</note>

			<note place="foot" n="4"> We found the memory of our server is large enough, so we did not implement it 5 We plan to make our code public available. 6 http://www.phontron.com/kftt/ 7 http://nlp.stanford.edu/software/ segmenter.shtml</note>

			<note place="foot" n="8"> http://www.phontron.com/kytea/ 9 http://www2.nict.go.jp/univ-com/multi trans/cicada/</note>

			<note place="foot" n="11"> http://www.isi.edu/ Àú avaswani/ giza-pp-l0.html 12 https://github.com/hznlp/giza-kn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We appreciated the valuable comments from the reviewers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to MCMC for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Nando De Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2013 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">But dictionaries are data too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meredith</forename><forename type="middle">J</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohanty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Language Technology, HLT &apos;93</title>
		<meeting>the Workshop on Human Language Technology, HLT &apos;93<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="202" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Report on the 10th IWSLT evaluation campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>St√ºker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note>Rubin</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tailoring word alignments to syntactic machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sampling alignment structure under a bayesian translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-C√¥t√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">V</forename><surname>Gra√ßa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<title level="m">Better alignments = better translations? Proceedings of the 46th Annual Meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Markov chain Monte Carlo in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spiegelhalter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>CRC press</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the NTCIR-9 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NTCIR</title>
		<meeting>NTCIR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Symgiza++: Symmetrized word alignment models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dowmunt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkadiusz</forename><surname>Sza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Intelligent Information Systems (SIIS)</title>
		<editor>Pascal Bouvry, Mieczyslaw A. Klopotek, Franck Leprvost, Malgorzata Marciniak, Agnieszka Mykowiecka, and Henryk Rybinski</editor>
		<meeting><address><addrLine>Poland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7053</biblScope>
			<biblScope unit="page" from="379" to="390" />
		</imprint>
	</monogr>
	<note>Warsaw</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MT Summit</title>
		<meeting>MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association of Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association of Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving IBM wordalignment model 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 518</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 518</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An unsupervised model for joint phrase alignment and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="632" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Kyoto free translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<ptr target="http://www.phontron.com/kftt" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A comparison of alignment models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th conference on Computational linguistics</title>
		<meeting>the 18th conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Local component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Partof-speech induction in dependency trees for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="841" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning tractable word alignment models with complex constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Jo√£o V Gra√ßa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="504" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smaller alignment models for better translations: unsupervised word alignment with the l 0norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Empirical study of unsupervised chinese word segmentation methods for smt on large-scale corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="752" to="758" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine translation system combination by confusion forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1249" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimized online rank learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training phrase translation models with leaving-one-out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Leave-one-out phrase model training for large-scale deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Yuh</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="460" to="467" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Building a large-scale annotated chinese corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu-Dong</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Computational Linguistics</title>
		<meeting>the 19th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kneser-ney smoothing on expected counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
