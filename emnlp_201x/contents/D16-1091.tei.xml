<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sort Story: Sorting Jumbled Images and Captions into Stories</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Chandrasekaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<email>mbansal@cs.unc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">TTI-Chicago</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">UNC Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sort Story: Sorting Jumbled Images and Captions into Stories</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="925" to="931"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Temporal common sense has applications in AI tasks such as QA, multi-document summa-rization, and human-AI communication. We propose the task of sequencing-given a jumbled set of aligned image-caption pairs that belong to a story, the task is to sort them such that the output sequence forms a coherent story. We present multiple approaches, via unary (position) and pairwise (order) predictions , and their ensemble-based combinations , achieving strong results on this task. We use both text-based and image-based features, which depict complementary improvements. Using qualitative examples, we demonstrate that our models have learnt interesting aspects of temporal common sense.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequencing is a task for children that is aimed at im- proving understanding of the temporal occurrence of a sequence of events. The task is, given a jumbled set of images (and maybe captions) that belong to a single story, sort them into the correct order so that they form a coherent story. Our motivation in this work is to enable AI systems to better under- stand and predict the temporal nature of events in the world. To this end, we train machine learning models to perform the task of "sequencing".</p><p>Temporal reasoning has a number of applications such as multi-document summarization of multiple sources of, say, news information where the relative order of events can be useful to accurately merge information in a temporally consistent manner. In question answering tasks ( <ref type="bibr" target="#b21">Richardson et al., 2013</ref>; * Denotes equal contribution.</p><p>â€  Part of this work was done during an internship at TTIC.</p><p>Figure 1: (a) The input is a jumbled set of aligned image-caption pairs. (b) Actual output of the system -an ordered sequence of image-caption pairs that form a coherent story. <ref type="bibr">Fader et al., 2014;</ref><ref type="bibr" target="#b28">Weston et al., 2015;</ref><ref type="bibr" target="#b20">Ren et al., 2015)</ref>, answering questions related to when an event occurs, or what events occurred prior to a particular event require temporal reasoning. A good temporal model of events in everyday life, i.e., a "temporal common sense", could also improve the quality of communication between AI systems and humans. Stories are a form of narrative sequences that have an inherent temporal common sense structure. We propose the use of visual stories depicting personal events to learn temporal common sense. We use stories from the Sequential Image Narrative Dataset (SIND) <ref type="bibr">(Ting-Hao Huang, 2016</ref>) in which a set of 5 aligned image-caption pairs together form a co- herent story. Given an input story that is jumbled ( <ref type="figure">Fig. 1(a)</ref>), we train machine learning models to sort them into a coherent story ( <ref type="figure">Fig. 1(b)</ref>). <ref type="bibr">1</ref> Our contributions are as follows: -We propose the task of visual story sequencing. -We implement two approaches to solve the task: one based on individual story elements to predict position, and the other based on pairwise story ele- ments to predict relative order of story elements. We also combine these approaches in a voting scheme that outperforms the individual methods. -As features, we represent a story element as both text-based features from the caption and image- based features, and show that they provide comple- mentary improvements. For text-based features, we use both sentence context and relative order based distributed representations. -We show qualitative examples of our models learn- ing temporal common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Temporal ordering has a rich history in NLP re- search. <ref type="bibr">Scripts (Schank and Abelson, 2013)</ref>, and more recently, narrative chains (Chambers and Ju- rafsky, 2008) contain information about the partic- ipants and causal relationships between events that enable the understanding of stories. A number of works ( <ref type="bibr" target="#b13">Mani and Schiffman, 2005;</ref><ref type="bibr" target="#b14">Mani et al., 2006;</ref><ref type="bibr">Boguraev and Ando, 2005</ref>) learn temporal re- lations and properties of news events from the dense, expert-annotated TimeBank corpus ( <ref type="bibr">Pustejovsky et al., 2003</ref>). In our work, however, we use multi- modal story data that has no temporal annotations.</p><p>A number of works also reason about temporal ordering by using manually defined linguistic cues <ref type="bibr" target="#b28">(Webber, 1988;</ref><ref type="bibr" target="#b18">Passonneau, 1988;</ref><ref type="bibr">Lapata and Lascarides, 2006;</ref><ref type="bibr" target="#b6">Hitzeman et al., 1995;</ref><ref type="bibr" target="#b8">Kehler, 2000</ref>). Our approach uses neural networks to avoid feature design for learning temporal ordering.</p><p>Recent works <ref type="bibr" target="#b15">(Modi and Titov, 2014;</ref><ref type="bibr" target="#b16">Modi, 2016)</ref> learn distributed representations for predi- cates in a sentence for the tasks of event ordering and cloze evaluation. Unlike their work, our approach makes use of multi-modal data with free-form nat- ural language text to learn event embeddings. Fur- ther, our models are trained end-to-end while their pipelined approach involves parsing and extracting verb frames from each sentence, where errors may propagate from one module to the next (as discussed in Section 4.3). <ref type="bibr" target="#b3">Chen et al. (2009)</ref> use a generalized Mallows model for modeling sequences for coherence within single documents. Their approach may also be ap- plicable to our task. Recently, <ref type="bibr">Mostafazadeh et al. (2016)</ref> presented the "ROCStories" dataset of 5- sentence stories with stereotypical causal and tem- poral relations between events. In our work though, we make use of a multi-modal story-dataset that con- tains both images and associated story-like captions.</p><p>Some works in vision ( <ref type="bibr" target="#b19">Pickup et al., 2014;</ref><ref type="bibr">Basha et al., 2012</ref>) also temporally order images; typically by finding correspondences between multiple im- ages of the same scene using geometry-based ap- proaches. Similarly, <ref type="bibr">Choi et al. (2016)</ref> compose a story out of multiple short video clips. They define metrics based on scene dynamics and coherence, and use dense optical flow and patch-matching. In contrast, our work deals with stories containing po- tentially visually dissimilar but semantically coher- ent set of images and captions.</p><p>A few other recent works ( <ref type="bibr" target="#b10">Kim et al., 2015;</ref><ref type="bibr" target="#b9">Kim et al., 2014;</ref><ref type="bibr">Kim and Xing, 2014;</ref><ref type="bibr" target="#b23">Sigurdsson et al., 2016;</ref><ref type="bibr">Bosselut et al., 2016;</ref><ref type="bibr" target="#b27">Wang et al., 2016</ref>) sum- marize hundreds of individual streams of informa- tion (images, text, videos) from the web that deal with a single concept or event, to learn a common theme or storyline or for timeline summarization. Our task, however, is to predict the correct sorting of a given story, which is different from summa- rization or retrieval. <ref type="bibr">Ramanathan et al. (2015)</ref> at- tempt to learn temporal embeddings of video frames in complex events. While their motivation is similar to ours, they deal with sampled frames from a video while we attempt to learn temporal common sense from multi-modal stories consisting of a sequence of aligned image-caption pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this section, we first describe the two components in our approach: unary scores that do not use con- text, and pairwise scores that encode relative order- ings of elements. Next, we describe how we com- bine these scores through a voting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unary Models</head><p>Let Ïƒ âˆˆ Î£ n denote a permutation of n elements (image-caption pairs). We use Ïƒ i to denote the posi- tion of element i in the permutation Ïƒ. A unary score S u (Ïƒ) captures the appropriateness of each story el- ement i in position Ïƒ i :</p><formula xml:id="formula_0">S u (Ïƒ) = n i=1 P (Ïƒ i |i)<label>(1)</label></formula><p>where P (Ïƒ i |i) denotes the probability of the ele- ment i being present in position Ïƒ i , which is the output from an n-way softmax layer in a deep neural network. We experiment with 2 networks -  <ref type="bibr">, 2014)</ref>, which have been shown to generalize well. Both embeddings are concatenated and fed as input to an MLP. In both cases, the best ordering of the story elements (optimal permutation) Ïƒ * = arg max ÏƒâˆˆÎ£n S u (Ïƒ) can be found effi- ciently in O(n 3 ) time with the Hungarian algo- rithm <ref type="bibr" target="#b17">(Munkres, 1957)</ref>. Since these unary scores are not influenced by other elements in the story, they capture the semantics and linguistic structures associated with specific positions of stories e.g., the beginning, the middle, and the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pairwise Models</head><p>Similar to learning to rank approaches (Hang, 2011), we develop pairwise scoring models that given a pair of elements (i, j), learn to assign a score:</p><formula xml:id="formula_2">S([[Ïƒ i &lt; Ïƒ j ]] | i, j)</formula><p>indicating whether element i should be placed before element j in the permutation Ïƒ. Here, <ref type="bibr">[[Â·]</ref>] indicates the Iverson bracket (which is 1 if the input argument is true and 0 otherwise). We develop and experiment with the following 3 pair- wise models: (1) A language-alone pairwise model (Skip- Thought+MLP) that takes as input a pair of Skip- Thought embeddings and trains an MLP (with hinge-loss) that outputs S([[Ïƒ i &lt; Ïƒ j ]] | i, j), the score for placing i before j. (2) A language+vision pairwise model (Skip- Thought+CNN+MLP) that concatenates the Skip- Thought and CNN embeddings for i and j and trains a similar MLP as above. (3) A language-alone neural position embedding (NPE) model. Instead of using frozen Skip-Thought embeddings, we learn a task-aware ordered dis- tributed embedding for sentences. Specifically, each sentence in the story is embedded X = (x 1 , . . . , x n ), x i âˆˆ R d + , via an LSTM (Hochreiter and Schmidhuber, 1997) with ReLU non-linearities. Similar to the max-margin loss that is applied to neg- ative examples by <ref type="bibr">Vendrov et al. (2016)</ref>, we use an asymmetric penalty that encourages sentences ap- pearing early in the story to be placed closer to the origin than sentences appearing later in the story.</p><formula xml:id="formula_3">L ij = max(0, Î± âˆ’ (x j âˆ’ x i )) 2 Loss = 1&lt;=i&lt;j=n L ij<label>(2)</label></formula><p>At train time, the parameters of the LSTM are learned end-to-end to minimize this asymmetric or- dered loss (as measured over the gold-standard se- quences). At test time, we use S([[Ïƒ i &lt; Ïƒ j ]] | i, j) = L ij . Thus, as we move away from the origin in the embedding space, we traverse through the sentences in a story. Each of these three pairwise approaches assigns a score S(Ïƒ i , Ïƒ j |i, j) to an ordered pair of elements (i,j), which is used to construct a pairwise scoring model:</p><formula xml:id="formula_4">S p (Ïƒ) = 1&lt;=i&lt;j&lt;=n S([[Ïƒ i &lt; Ïƒ j ]]) âˆ’ S([[Ïƒ j &lt; Ïƒ i ]]) ,<label>(3)</label></formula><p>by summing over the scores for all possible ordered pairs in the permutation. This pairwise score cap- tures local contextual information in stories. Find- ing the best permutation Ïƒ * = arg max ÏƒâˆˆÎ£n S p (Ïƒ) under this pairwise model is NP-hard so approxi- mations will be required. In our experiments, we study short sequences (n = 5), where the space of permutations is easily enumerable (5! = 120). For longer sequences, we can utilize integer program- ming methods or well-studied spectral relaxations for this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Voting-based Ensemble</head><p>To combine the complementary information cap- tured by the unary (S u ) and pairwise models (S p ), we use a voting-based ensemble. For each method in the ensemble, we find the top three permuta- tions. Each of these permutations (Ïƒ k ) then vote for a particular element to be placed at a particu- lar position. Let V be a vote matrix such that V ij stores the number of votes for i th element to oc- cur at j th position, i.e.</p><formula xml:id="formula_5">V ij = k [[Ïƒ k i == j]])</formula><p>. We use the Hungarian algorithm to find the optimal permutation that maximizes the votes assigned, i.e.</p><formula xml:id="formula_6">Ïƒ * vote = arg max ÏƒâˆˆÎ£n n i=1 n j=1 V ij Â· [[Ïƒ i == j]].</formula><p>We experimented with a number of model voting combinations and found the combination of pairwise Skip-Thought+CNN+MLP and neural position em- beddings to work best (based on a validation set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We train and evaluate our model on personal multi- modal stories from the SIND (Sequential Image Narrative Dataset) <ref type="bibr">(Ting-Hao Huang, 2016)</ref>, where each story is a sequence of 5 images and correspond- ing story-like captions. The narrative captions in this dataset, e.g., "friends having a good time" (as op- posed to "people sitting next to each other") capture a sequential, conversational language, which is char- acteristic of stories. We use 40,155 stories for train- ing, 4990 for validation and 5055 stories for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>We evaluate the performance of our model at cor- rectly ordering a jumbled set of story elements using the following 3 metrics: Spearman's rank correlation (Sp.) <ref type="bibr" target="#b24">(Spearman, 1904)</ref> measures if the ranking of story elements in the predicted and ground truth orders are monotoni- cally related (higher is better). Pairwise accuracy (Pairw.) measures the fraction of pairs of elements whose predicted relative order- ing is the same as the ground truth order (higher is better). Average Distance (Dist.) measures the average change in position of all elements in the predicted  <ref type="table">Table 1</ref>: Performance of our different models and features at the sequencing task.</p><p>story from their respective positions in the ground truth story (lower is better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Pairwise Models vs Unary Models As shown in <ref type="table">Table 1</ref>, the pairwise models based on Skip-Thought features outperform the unary models in our task. However, the Pairwise Order Model performs worse than the unary Skip-Thought model, suggesting that the Skip-Thought features, which encode context of a sentence, also provide a crucial signal for temporal ordering of story sentences.  to duplicate the pipelined approach of <ref type="bibr" target="#b15">Modi and Titov (2014)</ref>. For this, we first parse our story sentences to extract SVO (subject, verb, object) tu- ples (using the Stanford Parser <ref type="bibr" target="#b2">(Chen and Manning, 2014)</ref>). However, this step succeeds for only 60% of our test data. Now even if we consider a perfect downstream algorithm that always makes the cor- rect position prediction given SVO tuples, the over- all performance is still a Spearman correlation of just 0.473, i.e., the upper bound performance of this pipelined approach is lower than the performance of our text-only end-to-end model (correlation of 0.546) in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution of Image Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>Visualizations of position predictions from our model demonstrate that it has learnt the three act structure <ref type="bibr" target="#b26">(Trottier, 1998</ref>) in stories -the setup, the middle and the climax. We also present success and failure examples of our sorting model's predictions. See the supplementary for more details and figures. We visualize our model's temporal common sense, in <ref type="figure" target="#fig_0">Fig. 2</ref>. The word clouds show discrim- inative words -the words that the model believes are indicative of sentence positions in a story. The size of a word is proportional to the ratio of its fre- quency of occurring in that position to other po- sitions. Some words like 'party', 'wedding', etc., probably because our model believes that the start the story describes the setup -the occasion or event.</p><p>People often tend to describe meeting friends or family members which probably results in the dis- criminative words such as 'people', 'friend', 'every- one' in the second and the third sentences. More- over, the model believes that people tend to conclude the stories using words like 'finally', 'afterwards', tend to talk about 'great day', group 'pictures' with everyone, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose the task of "sequencing" in a set of image-caption pairs, with the motivation of learn- ing temporal common sense. We implement multi- ple neural network models based on individual and pairwise element-based predictions (and their en- semble), and utilize both image and text features, to achieve strong performance on the task. Our best system, on average, predicts the ordering of sentences to within a distance error of 0.8 (out of 5) positions. We also analyze our predictions and show qualitative examples that demonstrate tempo- ral common sense.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Word cloud corresponding to most discriminative words for each position.</figDesc><graphic url="image-5.png" coords="5,187.64,131.72,117.00,61.25" type="bitmap" /></figure>

			<note place="foot" n="1"> Note that &apos;jumbled&apos; here refers to the loss of temporal ordering; image-caption pairs are still aligned.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ramakrishna Vedantam and the anony-mous reviewers for their helpful suggestions. This work was supported by: NSF CAREER awards to DB and DP, ARO YIP awards to DB and DP, IC-TAS Junior Faculty awards to DB and DP, Google Faculty Research award to DP and DB, ARL grant W911NF-15-2-0080 to DP and DB, ONR grant N00014-14-1-0679 to DB and N00014-16-1-2713 to DP, ONR YIP award to DP, Paul G. Allen Family Foundation Allen Distinguished Investigator award to DP, Alfred P. Sloan Fellowship to DP, AWS in Education Research grant to DB, NVIDIA GPU do-nations to DB and MB, an IBM Faculty Award and Bloomberg Data Science Research Grant to MB.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Photo sequencing. In ECCV. 2 [Boguraev and Ando2005] Branimir Boguraev and Rie Kubota Ando</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<editor>IJCAI. 2 [Bosselut et al.2016] Antoine Bosselut, Jianfu Chen, David Warren, Hannaneh Hajishirzi, and Yejin Choi</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Learning prototypical event structure from photo albums</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Citeseer</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manning2014] Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In EMNLP. 5</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Content modeling using latent permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<editor>CVPR. [Fader et al.2014] Anthony Fader, Luke Zettlemoyer, and Oren Etzioni</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Open question answering over curated and extracted knowledge bases</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A short introduction to learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Hang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Algorithms for analysing the temporal structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hitzeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Sepp Hochreiter and JÃ¼rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coherence and the resolution of ellipsis. Linguistics and Philosophy. 2 [Kim and Xing2014] Gunhee Kim and Eric Xing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Reconstructing storyline graphs for image recommendation from web community photos</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint summarization of large-scale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Gunhee Kim, Leonid Sigal, and Eric Xing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint photo stream and blog post summarization and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
		<idno>CVPR. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Gunhee Kim, Seungwhan Moon, and Leonid Sigal</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kiros</surname></persName>
		</author>
		<idno>NIPS. 3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lapata and Lascarides2006] Mirella Lapata and Alex Lascarides</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Re</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Learning sentence-internal temporal relations</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporally anchoring and ordering events in news. Time and Event Recognition in Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">] Inderjeet</forename><surname>Schiffman2005</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schiffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Benjamins</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learning of temporal relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-ACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titov2014] Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Titov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<editor>CoNLL. 2 [Mostafazadeh et al.2016] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Event embeddings for semantic script modeling</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A computational model of the semantics of tense and aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Seeing the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyndsey</forename><surname>Pickup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Freeman ; Lisa</forename><surname>Ferro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus linguistics</title>
		<editor>Ramanathan, Kevin Tang, Greg Mori, and Li Fei-Fei</editor>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ren</surname></persName>
		</author>
		<idno>NIPS. 1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richardson</surname></persName>
		</author>
		<idno>EMNLP. 1</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Scripts, plans, goals, and understanding: An inquiry into human knowledge structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Abelson2013] Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert P Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning visual storylines with skipping recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sigurdsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556.3</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV. 2 [Simonyan and Zisserman2014] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The proof and measurement of association between two things. The American journal of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
		<editor>Ting-Hao Huang2016] Nasrin Mostafazadeh Ishan Misra Aishwarya Agrawal Jacob Devlin Ross Girshick Xiaodong He Pushmeet Kohli Dhruv Batra C</editor>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<title level="m">Visual storytelling. In NAACL</title>
		<editor>Lawrence Zitnick Devi Parikh Lucy Vanderwende Michel Galley Margaret Mitchell Ting-Hao Huang, Francis Ferraro</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The screenwriter&apos;s bible: A complete guide to writing, formatting, and selling your script</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Trottier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<editor>Vendrov et al.2016] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun</editor>
		<imprint>
			<publisher>Silman-James Press</publisher>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>Order-embeddings of images and language</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A lowrank approximation approach to learning joint embeddings of news stories and images for timeline summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tense as discourse anaphor. Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">Lynn</forename><surname>Webber ; Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698.1</idno>
	</analytic>
	<monogr>
		<title level="m">Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards AIcomplete question answering: A set of prerequisite toy tasks</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhu</surname></persName>
		</author>
		<idno>CVPR. 3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Antonio Torralba, and Sanja Fidler</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
