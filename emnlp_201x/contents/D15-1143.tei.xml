<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Back-off Modeling of Hiero Grammar based on Non-parametric Bayesian Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetaka</forename><surname>Kamigaito</surname></persName>
							<email>kamigaito@lr.pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
							<email>tarow@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Japan Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
							<email>takamura@pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<email>eiichiro.sumita@nict.go.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communication Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Back-off Modeling of Hiero Grammar based on Non-parametric Bayesian Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/Japanese-English. When compared against heuristic models, our model achieved comparable translation quality on a full size German-English language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hierarchical phrase-based statistical machine translation (HPBSMT) <ref type="bibr" target="#b3">(Chiang, 2007</ref>) is a popu- lar alternative to phrase-based SMT (PBSMT), in which synchronous context free grammar <ref type="bibr">(SCFG)</ref> is used as the basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent lan- guage pairs, such as Japanese and English. How- ever, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automati- cally acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower trans- lation quality.</p><p>Pruning a rule table either on the basis of signif- icance test <ref type="bibr" target="#b14">(Johnson et al., 2007</ref>) or entropy ( <ref type="bibr" target="#b18">Ling et al., 2012;</ref><ref type="bibr" target="#b32">Zens et al., 2012</ref>) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods <ref type="bibr" target="#b1">(Blunsom et al., 2009</ref>) solve the spurious rule extraction problem by directly inducing a com- pact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f | 3 |e| 3 ) when we use dynamic programming SCFG bi- parsing ( <ref type="bibr" target="#b30">Wu, 1997)</ref>. Gibbs sampling without bi- parsing ( <ref type="bibr" target="#b17">Levenberg et al., 2012)</ref> can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on ex- haustive heuristic rule extraction from the word- alignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities.</p><p>We propose a model on the basis of the previ- ous work on the non-parametric Inversion Trans- duction Grammar (ITG) model <ref type="bibr" target="#b21">(Neubig et al., 2011</ref>) wherein phrases of various granularities are learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing ap- proach ( <ref type="bibr" target="#b31">Xiao et al., 2012</ref>) which basically runs in a time complexity of O(|f | 3 ). Slice sampling for an SCFG <ref type="bibr" target="#b0">(Blunsom and Cohn, 2010</ref>) is used for effi- ciently sampling a derivation tree from a reduced space of possible derivations.</p><p>Our model achieved higher or at least com- parable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the News- Commentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Eu- roparl v7 corpus with significantly less grammar size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various criteria have been proposed to prune a phrase table without decreasing translation qual- ity, e.g., Fisher's exact test <ref type="bibr" target="#b14">(Johnson et al., 2007)</ref> or relative entropy ( <ref type="bibr" target="#b18">Ling et al., 2012;</ref><ref type="bibr" target="#b32">Zens et al., 2012)</ref>. Although those methods are easily ap- plied for pruning a rule table, they heavily rely on the heuristically determined threshold parame- ter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model ( <ref type="bibr" target="#b19">Marcu and Wong, 2002</ref>) can directly express many-to-many word aligments without heuristic phrase extraction. <ref type="bibr" target="#b9">DeNero et al. (2006)</ref> proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method <ref type="bibr" target="#b7">(Cmejrek and Zhou, 2010</ref>) can generate SCFG rules by combining other rule pairs through an inside- outside algorithm. However, those previous at- tempts were restricted in that the rules and phrases were induced by heuristic combination.</p><p>Bayesian SCFG models can induce a com- pact model by incorporating sophisticated non- parametric Bayesian models for an SCFG, such as a dirichlet process <ref type="bibr" target="#b10">(DeNero et al., 2008;</ref><ref type="bibr" target="#b1">Blunsom et al., 2009;</ref><ref type="bibr" target="#b5">Chung et al., 2014</ref>) or Pitman-Yor process ( <ref type="bibr" target="#b17">Levenberg et al., 2012;</ref><ref type="bibr" target="#b25">Peng and Gildea, 2014)</ref>. A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f | 3 |e| 3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on bi- parsing at the initialization step, and conducted Gibbs sampling by local operators <ref type="bibr" target="#b1">(Blunsom et al., 2009;</ref><ref type="bibr" target="#b17">Levenberg et al., 2012</ref>) or sampling on fixed word alignments ( <ref type="bibr" target="#b5">Chung et al., 2014;</ref><ref type="bibr" target="#b25">Peng and Gildea, 2014)</ref>. As a result, the inference can easily result in local optimum, wherein induced deriva- tion trees may strongly depend on the initial trees. <ref type="bibr" target="#b31">Xiao et al. (2012)</ref> proposed a two-step approach for bi-parsing a bilingual sentence in O(|f | 3 ) in the context of inducing SCFG rules discriminatively; however, their approach violates the detailed bal- ance due to its heuristic k-best pruning. <ref type="bibr" target="#b0">Blunsom and Cohn (2010)</ref> proposed a slice sampling for an SCFG, in the same manner as that for Infi- nite Hiden Markov Model (iHMM) <ref type="bibr" target="#b29">(Van Gael et al., 2008)</ref>, which can efficiently prune a space of possible derivations on the basis of dynamic pro- gramming. Although slice sampling can prune spans without violating the detailed balance, its time complexity of O(|f | 3 |e| 3 ) is still impractical for a large-scale experiment. We efficiently car- ried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn.</p><p>After learning a Bayesian model, it is not di- rectly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuris- tically from the word-aligned data <ref type="bibr" target="#b8">(Cohn and Haffari, 2013)</ref>. The work by <ref type="bibr" target="#b21">Neubig et al. (2011)</ref> was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITG- style binary branching rules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>We use Hiero grammar <ref type="bibr" target="#b3">(Chiang, 2007)</ref>, an in- stance of an SCFG, which is defined as a context- free grammar for two languages. Let Σ denote a set of terminal symbols in the source language, ∆ a set of terminal symbols in the target language, <ref type="figure">Figure 1</ref>: Derivation tree generated from Bayesian SCFG model V a set of non-terminal symbols, S a start symbol and R a set of rewrite rules. An SCFG is denoted as a tuple of ⟨Σ, ∆, V, S, R⟩. Each rewrite rule in R is represented as X → ⟨α/β⟩ in which α is a string of non-terminals and source side termi- nals (V ∪ Σ) * and β is a string of non-terminals and target side terminals (V ∪ ∆) * . An example derivation in an SCFG for the sentence pair "ni- hongo wo eigo ni honyaku suru koto wa muzukasii / Japanese is difficult to translate into English ." is represented as follows:</p><formula xml:id="formula_0">S X 1 eigo X 2 muzukasii / X 1 difficult X 2 English . X 1 X 3 wo / X 3 is X 2 X 4 honyaku suru X 5 wa / X 4 translate X 5 X 3 nihongo / Japanese X 4 ni / into X 5 koto / to .</formula><p>A Hiero grammar has additional constraints over a general SCFG; the number of terminal sym- bols in each rule for both source and target sides is limited to 5. Each rule may contain at most two non-terminal symbols; adjacent non-terminal symbols in the source side are prohibited. For de- tails, refer to (Chiang, 2007).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bayesian SCFG Models</head><p>Previous Bayesian SCFG Models, for instance a model proposed by <ref type="bibr" target="#b17">Levenberg et al. (2012)</ref>, are based on the Pitman-Yor process <ref type="bibr" target="#b26">(Pitman and Yor, 1997</ref>) and learn SCFG rules by sampling a deriva- tion tree for each bilingual sentence. <ref type="figure">Figure 1</ref> shows an example derivation tree for our running example sentence pair under the model. The gen- erative process is represented as follows:</p><formula xml:id="formula_1">G X ∼ P rule (d r , θ r , G r 0 ), X → ⟨α/β⟩ ∼ G X ,<label>(1)</label></formula><p>where G X is a derivation tree and P rule (d r , θ r , G r 0 ) is a Pitman-Yor process <ref type="bibr" target="#b26">(Pitman and Yor, 1997)</ref>, which is a generalization of a Dirichlet process parametrized by a discount parameter d r , a strength parameter θ r and a base measure G r 0 . The output probability of a Pitman- Yor process obeys the power-law distribution with the discount parameter, which is very common in standard NLP tasks. The probability that a rule r k is drawn from a model P rule (d r , θ r , G r 0 ) is determined by a Chi- nese restaurant process which is decomposed into two probability distributions. If r k already exists in a table, we draw r k with probability</p><formula xml:id="formula_2">c k − d r · |φ r k | θ r + n r ,<label>(2)</label></formula><p>where c k is the number of customers of r k , n r is the number of all customers and φ r k is a number of r k 's tables. On the other hand, if r k is a new rule, we draw r k with probability</p><formula xml:id="formula_3">θ r + d r · |φ r | θ r + n r · G r 0 ,<label>(3)</label></formula><p>where |φ r | is the number of tables in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Back-off Model</head><p>In the previous models, the generative process is represented as a rewrite process starting from the symbol S, which can incorporate only minimal rules. Following <ref type="bibr" target="#b21">Neubig et al. (2011)</ref>, our model reverses the process by recursively backing off to smaller phrase pairs as shown in <ref type="figure">Figure 2</ref>. First, our model attempts to generate a phrase pair, i.e., a sentence pair, as a derivation tree. If the model successfully generates the phrase pair, we will fin- ish the generation process. Otherwise, a Hiero rule is generated to fallback to smaller spans rep- resented in each non-terminal symbol X in the rule. Then, each phrase pair corresponding to each smaller span is recursively generated through our model. In <ref type="figure">Figure 2</ref>, a phrase pair with "nil" indi- cates those not in our model; therefore the phrase pair is forced to back-off either by generating a new phrase pair from a base measure (base) or by falling back to smaller phrases using a Hiero rule (back-off). The recursive procedure is done until <ref type="figure">Figure 2</ref>: Derivation tree generated from the hier- archical back-off model we reach phrase pairs which are generated without any back-offs. Let a discount parameter be d p , a strength parameter be θ p , and a base measure be G p 0 . More formally, the generative process is rep- resented as follows:</p><formula xml:id="formula_4">G X ∼ P rule (d r , θ r , G phrase ), G phrase ∼ P phrase (d p , θ p , G X ), X → ⟨s/t⟩ ∼ G phrase , X → ⟨α/β⟩ ∼ G X ,<label>(4)</label></formula><p>where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. P phrase is com- posed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor pro- cess <ref type="bibr" target="#b28">(Teh, 2006</ref>).</p><p>model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation <ref type="formula" target="#formula_2">(2)</ref>:</p><formula xml:id="formula_5">c k − d p · |φ p k | θ p + n p ,<label>(5)</label></formula><p>where c k is the numbers of customers of a phrase pair p k and n p is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation <ref type="formula" target="#formula_2">(2)</ref>.</p><p>back-off: We will back off to smaller phrases using a rule generated by P rule as follows:</p><formula xml:id="formula_6">θ p + d p · |φ p | θ p + n p · c back + γ b · G b c back + c base + γ b ·P rule (d r , θ r , G phrase ) · ∏ X∈⟨α/β⟩ P phrase (d p , θ p , G X ),<label>(6)</label></formula><p>where c back and c base are the number of customers sampled from the back-off and base phrases, re- spectively, with a base measure G b and hyper- parameter γ b . We use a uniform distribution for G b = 0.5 since we consider only two states, back- off and base. Unlike the model state, P phrase may reach this state even when a phrase pair is not in the model. The phrase pair is backed-off to smaller phrase pairs using P phrase through the non-terminals in the generated rule X ∈ ⟨α/β⟩.</p><p>base: As an alternative to the back-off state, we may reach the base state which follows the proba- bility distribution on the basis of the base measure</p><formula xml:id="formula_7">G p 0 , θ p + d p · |φ p | θ p + n p · c base + γ b · G b c back + c base + γ b · G p 0 .<label>(7)</label></formula><p>In summary, P phrase (d p , θ p , G X ) is defined as a joint probability of Equations <ref type="formula" target="#formula_5">(5)</ref> through <ref type="formula" target="#formula_7">(7)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Base Measure</head><p>Similar to <ref type="bibr" target="#b17">Levenberg et al. (2012)</ref>, the base mea- sure for rule probability G r 0 is composed of four generative processes. First, a number of symbols in a source side of a rule |α| is gen- erated from a Poisson distribution, i.e., |α| ∼ P oisson(0.1). Let t(x) denote a function that re- turns terminals from a string x. The number of target side terminal symbols |t(β)| is also gener- ated from a Poisson distribution and represented as |t(β)| ∼ P oisson(α + λ 0 ) 1 . The type of symbol α i in the source side, type i , either ter- minal or non-terminal symbol, is determined by type i ∼ Bernoulli(ϕ |α| ) where ϕ is a hyper- parameter taking 0 &lt; ϕ &lt; 1. ϕ |α| is based on an intuition that shorter rules should be rela- tively more likely to contain terminal symbols than longer rules. Source and target terminal symbol pair ⟨t(α), t(β)⟩ are generated from the geomet- ric means of two directional IBM Model 1 word alignment probabilities and monolingual unigram probabilities for two languages, and represented as:</p><formula xml:id="formula_8">⟨t(α), t(β)⟩ ∼ (P uni (t(α))P− − → M 1 (t(α), t(β)) · P uni (t(β))P← − − M 1 (t(α), t(β))) 1 2 . (8)</formula><p>When the t(α) or t(β) is empty, we use the con- stant 0.01 instead of the Model1 probabilities.</p><p>The base measure for phrases G p 0 is composed of three generative processes, in a similar man- ner as <ref type="bibr" target="#b17">Levenberg et al. (2012)</ref>, the number of terminal symbols in a phrase pair in the source side, |s|, is generated from a Poisson distribution |s| ∼ P oisson(0.1). The length for the target side |t| is generated in the same manner as the source side of the phrase pair. The alignments between s and t are also generated in the same manner as those for the base measure in a rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Inference</head><p>In inference, we use a sentence-wise block sam- pling of <ref type="bibr" target="#b0">Blunsom and Cohn (2010)</ref>, which has a better convergence property when compared with a step-wise Gibbs sampling. We repeat the follow- ing steps given a sentence pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Decrement customers of the rules and phrase</head><p>pairs used in the current derivation for the sentence pair.</p><p>2. Bi-parse the sentence pair in a bottom up manner.</p><p>3. Sample a new derivation tree in a top-down manner.</p><p>4. Increment customers of the rules and phrase pairs in the sampled derivation tree.</p><p>The most time-consuming step during the infer- ence procedure is bi-parsing of a sentence pair which essentially takes O(|f | 3 |e| 3 ) time using a bottom up dynamic programming algorithm <ref type="bibr" target="#b30">(Wu, 1997)</ref>. When a span is very large, it can easily suf- fer combinatorial explosion. To avoid this prob- lem, we use a two-step slice sampling by perform- ing the two-step bi-parsing of <ref type="bibr" target="#b31">Xiao et al. (2012)</ref> and by pruning possible derivation space <ref type="bibr" target="#b0">(Blunsom and Cohn, 2010</ref>) in each step (Algorithm 1). From lines 1 to 7, a set of word alignment is enu- merated and put into cube a . In addition to the ar- bitrary word alignment of source i to target j , null word alignment is also merged into cube a (line 5). Note that word alignment considered in the algorithm is restricted to one-to-many. The set of word alignments in cube a is pruned and added to the chart a by SliceSampling. From lines 8 to 15, all possible phrases and rules for each span con- strained by the pruned word alignment are enu- merated and temporally stored into cube. The phrases and rules in cube are pruned by SliceSam- pling and the remainders are added to chart. The Algorithm 1 Two-step slice sampling</p><formula xml:id="formula_9">1: for i ← 1, · · · , |source| do 2:</formula><p>for j ← 1, · · · , |target| do </p><note type="other">clear cube a 8: end for 9: for h ← 1, · · · , |source| do 10: for all the i, j s.t j − i = h do 11: for inferable rule, phrase from the sub- spans of [i, j] of all charts do 12: cube ← rule, phrase 13: end for 14: chart ← SliceSampling(cube) 15: clear cube 16: end for 17: end for time complexity for</note><p>the word alignment enumera- tion from lines 1 to 7 is O(|f ||e|) and that for the phrase and rule enumeration from lines 8 to 15 is</p><formula xml:id="formula_10">O(|f | 3 ).</formula><p>The key difference to the slice sampling of Blunsom and Cohn (2010) lies in lines 6 and 3 of Algorithm 1. Let d denote a set of derivation trees d and u be a set of slice variables u. In slice sam- pling, we prune the rules r sp in each source span sp based on a slice variable u sp corresponding to that sp. After pruning, we sample trees from the pruned space of r. The above process is formally represented as:</p><formula xml:id="formula_11">u ∼ P (u|d), d ∼ P (d|u),<label>(9)</label></formula><p>where P (d|u) is computed through sampling in a top-down manner after parsing in a bottom- up manner with Algorithm 1, and is equal to ∏ d P (d|u). The probability P (u|d) is equal to ∏ sp P (u sp |d). Let r * sp denote a currently adopted rule in the span sp and P (u sp |d) be defined using a pruning score Score(r * sp ) as follows:</p><formula xml:id="formula_12">Score(r sp i ) = Inside(r sp i ) · F uture(r sp i ),<label>(10)</label></formula><p>where Inside(r sp ) and F uture(r sp ) are inside and outside probabilities for sp, respectively. Let s rsp denote a set of source side words in r sp , t rsp a set of target side words in r sp , s sp a set of words in a source sentence without s rsp and t sp , a set of words in a target sentence without t rsp . By us- ing IBM Model 1 probabilities in two directions, Inside(r sp ) is calculated by</p><formula xml:id="formula_13">(P− − → M 1 (s sp , t sp ) · P← − − M 1 (s sp , t sp )) 1 2 .<label>(11)</label></formula><p>We use IBM Model1 outside probability for future score F uture(r sp ). Similarly, the future score F uture(r sp ) is computed using the two direc- tional models:</p><formula xml:id="formula_14">(P− − → M 1 (s sp , t sp ) · P← − − M 1 (s sp .t sp )) 1 2 .<label>(12)</label></formula><p>When sp is used in the current derivation d, slice variable u sp is sampled from a uniform distribu- tion 2 :</p><formula xml:id="formula_15">P (u sp |d) = I(u sp &lt; Score(r * sp )) Score(r * sp ) ,<label>(13)</label></formula><p>otherwise, u sp is sampled from a beta distribution if sp is not in the current derivation d:</p><formula xml:id="formula_16">P (u sp |d) = Beta(u sp ; a, 1.0),<label>(14)</label></formula><p>where a &lt; 1 is a parameter for the beta distribu- tion. If the Score(r sp i ) is less than u sp , we prune the r sp i from cube. Similar to <ref type="bibr" target="#b0">Blunsom and Cohn (2010)</ref>, if the span sp is not in the current deriva- tion, the rules with low probability are pruned ac- cording to Equation <ref type="bibr">(14)</ref>. Let r d sp denotes a rule in d with span sp, P (d|u) is calculated by:</p><formula xml:id="formula_17">∏ sp∈d P (r d sp ) ∑ r j ∈rsp P (r j )I(u sp &lt; Score(r j )) .<label>(15)</label></formula><p>In our experiments discussed in Section 6, slice sampling parameter a was set to 0.02 when in- corporating the future score of Equation <ref type="formula" target="#formula_1">(12)</ref>. In contrast, we used a = 0.1 when performing slice sampling without the future score. We empirically found that setting a lower value for a led to slower progress in learning due to a combinatorial explo- sion when inferencing a derivation for each sen- tence pair.</p><p>In the beginning of training, we do not have any derivation trees for given training data, al- though the derivation trees are required for esti- mating parameters for Bayesian models. We use the two-step parsing for generating initial deriva- tion trees from only base measures. The k-best 2 I(·) is a function returns 1 if the condition is satisfied and 0 otherwise pruning is conducted against the score denoted by the equation 10 , which is very similar to <ref type="bibr" target="#b31">Xiao et al. (2012)</ref>. <ref type="bibr">3</ref> For faster bi-parsing, we run sampling in paral- lel in the same way as <ref type="bibr" target="#b33">Zhao and Huang (2013)</ref>, in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The up- dates to the model are synchronized by increment- ing and decrementing customers for the bilingual sentences in the mini-batch. Note that the bi- parsing for each mini-batch is conducted on the fixed model parameters after the synchronised pa- rameter updates.</p><p>In addition to the model parameters, hyperpa- rameters are re-sampled after each training itera- tion following the discount and strength hyperpa- rameter resampling in a hierarchical Pitman-Yor process <ref type="bibr" target="#b28">(Teh, 2006</ref>). In particular, we resample ⟨d p , θ p ⟩, the pair of discount and strength parame- ters for phrases from a distribution:</p><formula xml:id="formula_18">[θ p ]</formula><p>|φp| dp</p><formula xml:id="formula_19">[θ p ] np 1 ∏ ⟨s,t⟩ |φp| ∏ k=1 [1 − d p ] (c ⟨s,t⟩ −1) 1 (16)</formula><p>where [ ] denotes a generalized Pochhammer sym- bol, and c ⟨s,t⟩ the number of customers of phrase pair ⟨s, t⟩. We resample the pair ⟨d r , θ r ⟩ in the same way as ⟨d p , θ p ⟩. The hyperparameter γ b is resampled from distribution:</p><formula xml:id="formula_20">(c back + γ b · G b )(c base + γ b · G b ) (c back + c base + γ b ) 2 ,<label>(17)</label></formula><p>where ϕ, used in the generative process for ei- ther terminal or non-terminal symbol type i ∼ Bernoulli(ϕ α ), is resampled from a distribution:</p><formula xml:id="formula_21">∏ ⟨α/β⟩∈Base Bernoulli(ϕ |α| ) c ⟨α/β⟩ ,<label>(18)</label></formula><p>where c ⟨α/β⟩ denotes the number of customers of rule ⟨α/β⟩, and Base denotes a set of rules gener- ated from the base measure. All the hyperparame- ters are inferred by slice sampling <ref type="bibr" target="#b20">(Neal, 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extraction of Translation Model</head><p>In the previous work on Bayesian approaches <ref type="bibr" target="#b0">(Blunsom and Cohn, 2010;</ref><ref type="bibr" target="#b17">Levenberg et al., 2012)</ref>, it is a standard practice to heuristically ex- tract rules and phrase pairs from the word align- ment derived from the derivation trees sampled from the Bayesian models. Instead of the heuris- tic method, we directly extract rules and phrase pairs from the learned models which are repre- sented as Chinese restaurant tables. To limit gram- mar size, we include only phrase pairs that are se- lected at least once in the sample. During this ex- traction process, we limit the source or target ter- minal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we com- pute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple fea- tures is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data <ref type="bibr" target="#b23">(Och, 2003)</ref>. We use the follow- ing six features; the joint model probability P model is calculated by Equation <ref type="formula" target="#formula_2">(2)</ref> for rules and by Equation <ref type="formula" target="#formula_5">(5)</ref> for phrase pairs. The joint posterior probability P posterior (f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count es- timation, motivated by <ref type="bibr" target="#b21">Neubig et al. (2011)</ref>  <ref type="bibr">4</ref> . The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, P model (f |e) and P model (e|f ), are estimated by marginalizing the joint probability P model (f, e):</p><formula xml:id="formula_22">P model (f |e) = P model (f, e) ∑ f ′ P model (f ′ , e) .<label>(19)</label></formula><p>The inverse direction P model (e|f ) is estimated, similarly. The lexical probabilities in two direc- tions, P lex (f |e) and P lex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In ad- dition to the above features, we use Word penalty for each rule and phrase pair used in the cdec de- coder ( <ref type="bibr" target="#b11">Dyer et al., 2010</ref>). As indicated in previous studies ( <ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b9">DeNero et al., 2006</ref>), the translation quality of generative models is lower than that of mod- els with heuristically extracted rules and phrase pairs. <ref type="bibr" target="#b9">DeNero et al. (2006)</ref> reported that con- sidering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when compared with a model estimated using a heuristic method. The Hiero grammar severely suffers from the phrase granularity problem and can overfit to the training data due to the flexibility of the rules.</p><p>To alleviate this problem, Neubig et al. (2011) combined the derivation trees across training it- erations by averaging the features for each rule and phrase pair. During the sampling process, each training iteration draws a different deriva- tion tree for each sentence pair, and the combi- nation of those different derivation trees can pro- vide multiple possible phrase boundaries to the model. Inspired by the averaging over the mod- els from different iterations, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as addi- tional training data, and increment the correspond- ing customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with Previous Bayesian Model</head><p>First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary cor- pus for German/Spanish/French-to-English pairs <ref type="bibr" target="#b2">(Callison-Burch et al., 2010</ref>) and NTCIR10 cor- pus for Japanese-English ( <ref type="bibr" target="#b12">Goto et al., 2013</ref>) for the translation model. All sentences are lower- cased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 it- erations for Gen and Back and combined the last 10 iterations for extracting the translation model. <ref type="bibr">5</ref> The batch size was set to 64. The language mod- els were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language mod- els were estimated using SRILM <ref type="bibr">(Stolcke and others, 2002</ref>) with interpolated Kneser-Ney smooth- ing. The details of the corpus are presented in Ta- ble 2. For detailed analysis, we also evaluate Hiero grammars extracted from GIZA++ ( <ref type="bibr" target="#b22">Och and Ney, 2003)</ref> grow-diag-final bidirectional alignments us- ing Moses ( <ref type="bibr" target="#b16">Koehn et al., 2007</ref>) with Hiero options.</p><p>News <ref type="table">-Commentary   NTCIR10  de-en  es-en  fr-en  ja-en  Model  Sample BLEU  SIZE  BLEU  SIZE  BLEU  SIZE  BLEU  SIZE   *</ref>     <ref type="bibr" target="#b13">Hopkins and May, 2011)</ref>. <ref type="table">Table 1</ref> lists the results measured using BLEU ( <ref type="bibr" target="#b24">Papineni et al., 2002</ref>).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the ex- tracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using <ref type="bibr" target="#b6">Clark et al. (2011)</ref> un- der p-value of 0.05. Back performed better than Gen on Spanish-English and French-English lan- guage pairs. Note that the gains were achieved with the comparable grammar size. When com- paring German-English and Japanese-English lan- guage pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+fu- ture) achieved further gains over the slice sam- pling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-English and Japanese-English language pairs. Sample combi- nation has no or slight gain on BLEU score, in spite of the increase in grammar size. From the results, using last one sample as a grammar is suf- ficient for translation quality. The performance of the Bayesian model did not match with that for the GIZA++ pipeline heuristic approach. In general, complex model, such as Gen and Back, demands larger corpus size for training, and the evaluation on such smaller corpus may not be a fair com- parison, since the sampling approach can rely on only sampled derivations. Thus, we evaluate these methods on large size corpus in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with Heuristic Extraction</head><p>As reported in ( <ref type="bibr" target="#b15">Koehn et al., 2003;</ref><ref type="bibr" target="#b9">DeNero et al., 2006</ref>), the comparison against heuristic ex- traction is a challenging task. We compare the Back+future and a baseline extracted from grow- diag-final alignments of GIZA++ using Moses with Hiero options. We use GIZA++ and Moses default parameters for training. In addition, we present heuristic extraction from the last 1 sample of Back+future in +Exhaustive.</p><p>We used the full europarl-v7 German-English corpus as presented in <ref type="table" target="#tab_3">Table 3</ref>. The experimen- tal set up was similar to that in Section 6.1 with the following exceptions; Slice sampling parame- ter a was set to 0.05. Mini-batch size was set to 1024 and sampling was performed 5 iterations. <ref type="bibr">6</ref> The translation model was extracted by last 1 iter- ations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Analysis</head><p>Intuitively, the use of the hierarchical back-off in- creases the Hiero grammar size, since the phrases of all the granularities in the derivation trees are incorporated in the grammar. In contrast, our hier- archical back-off model achieved gains in transla- tion quality without increasing the size of the ex- tracted grammar when compared to the previous generative model. The major differences were the use of the minimal phrase pairs used in the previ- ous work in which only minimal phrase pairs in the leaves of derivation trees were included in the model. As a result, larger phrase pairs were forced to be constructed from those minimal rules. On the other hand, our back-off model could directly ex- press phrase pairs of multiple granularities. In par- ticular, a complex noun may be composed of sev- eral Hiero rules in the previous model, but it can be directly expressed by a single phrase pair in our model. <ref type="table" target="#tab_6">Table 5</ref> gives an example of a Japanese- English phrase pair which is represented by two Hiero rules in the previous model; it is directly ex- pressed by a single phrase pair in our model.</p><p>The BLEU score of Back+future was higher than the generative baseline with comparable grammar size. We observed that a very different word alignment was sampled in every training it- eration; the tendency was very frequent for func- tion words. Our future score for inferring the slice variables may take into account the context in a sentence better than those without the future score. class informations. Model 3 and our Back-off model dose not use any word class informations.</p><p>As a result, Back+future infers better models by avoiding over pruning spans.</p><p>The BLEU score of our back-off model did not achieve gains over the heuristic baselines. The de- tail analysis of the learned Hiero grammar's CRP tables reveals that the grammar is very sparse and may have little generalization capacity. The ex- pansion of back-off process and the use of word classes will solve the sparsity and increase the translation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a hierarchical back-off model for Hiero grammar. Our back-off model achieved higher or equal translation quality against a previ- ous Bayesian model under BLEU score on various language pairs;German/French/Spanish/Japanese- English. In addition to the hierarchical back-off model, we also proposed a two-step slice sampling approach. We showed that the two-step slice sam- pling approach can avoid over-pruning by incor- porating a future score for estimating slice vari- ables, which led to increase in translation quality through the experiments. The joint use of hierar- chical back-off model and two step slice sampling approach achieved comparable translation quality on a full size Germany-English language pair in Europarl v7 corpus with with significantly smaller grammar size; 10% less than that for he heuristic baseline.</p><p>For future work, we plan to embed a back-off feature to decoder which is computed for all the phrase pairs constructed in a derivation during the decoding process. We will reflect the change of a probability as a statefull feature for decoding step.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : The number of words in training data</head><label>2</label><figDesc></figDesc><table>TM 
LM 
Dev 
Test 
de 31.3M 
-
55.1k 59.4k 
en 32.8M 50.5M 58.8k 55.5k 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : The number of words in training data</head><label>3</label><figDesc></figDesc><table>We use GIZA++ and Moses default parameters for 
training. Decoding was carried out using the cdec 
decoder (?). Feature weights were tuned on the de-
velopment data by running MIRA (Chiang, 2012) 
for 20 iterations with 16 parallel. For other param-
eters, we used cdec's default values. The numbers 
reported here are the average of three tuning runs 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 lists the results 7 . Our Back+future can</head><label>4</label><figDesc></figDesc><table>6 Inference took 5 days. 
7 The row mark up with  *  indicate the model using word 

1224 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 : Results of translation evaluation in de-en full size corpus.</head><label>4</label><figDesc></figDesc><table>Gen 
gin X kamera / silver X camera 
en / salt 
Back + future gin en kamera / silver salt camera 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Example of a grammar 

decrease the grammar size against GIZA++ with 
comparable BLEU score. Surprisingly, exhaustive 
extraction had no gains, probably because of the 
word alignment in each Hiero rules relied on the 
IBM Model 1. 

</table></figure>

			<note place="foot" n="1"> Note that λ0 is a small constant for the input distribution greater than zero.</note>

			<note place="foot" n="3"> Note that we use k = 30 for k-best pruning.</note>

			<note place="foot" n="4"> Note that the correct way to decode from our model is to score every phrase pair created during decoding with back-off states, which is computationally intractable</note>

			<note place="foot" n="5"> Gen and Back took 1 day, Back+future took 1.5 days for inference.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Inducing synchronous grammars with slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="238" to="241" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A gibbs sampler for phrasal synchronous grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="782" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><forename type="middle">F</forename><surname>Zaidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="53" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical phrase-based translation. computational linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="201" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hope and fear for discriminative training of statistical translation models. The Journal of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1159" to="1187" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampling tree fragments from forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielštefankovičdanielˇdanielštefankovič</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="229" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two methods for extending hierarchical rules from the bilingual chart parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cmejrek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010: Posters</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="180" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An infinite hierarchical bayesian model of phrasal translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="780" to="790" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Why generative phrase models underperform surface heuristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on the Workshop on Statistical Machine Translation</title>
		<meeting>on the Workshop on Statistical Machine Translation<address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="31" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sampling alignment structure under a Bayesian translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
	<note>Honolulu, Hawaii, October. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 System Demonstrations</title>
		<meeting>the ACL 2010 System Demonstrations<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the ntcir-10 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</title>
		<meeting>the 10th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-Lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tuning as ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-05" />
			<biblScope unit="page" from="1352" to="1362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving translation quality by discarding most of the phrasetable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06" />
			<biblScope unit="page" from="967" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A bayesian model for learning scfgs with discontiguous rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Entropy-based pruning for phrasebased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A phrasebased,joint probability model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slice sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="705" to="741" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An unsupervised model for joint phrase alignment and extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinsuke</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Kawahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="632" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A systematic comparison of various statistical alignment models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003-07" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Sapporo</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Type-based mcmc for sampling tree fragments from forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1735" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The two-parameter poisson-dirichlet distribution derived from a stable subordinator. The Annals of Probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Yor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="855" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Srilm-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian language model based on pitman-yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beam sampling for the infinite hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Van Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1088" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised discriminative induction of synchronous grammar for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouxun</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee</title>
		<meeting><address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2883" to="2898" />
		</imprint>
	</monogr>
	<note>Proceedings of COLING 2012</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A systematic comparison of phrase table pruning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="972" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Minibatch and parallelization for online large margin structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
