<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rujun</forename><surname>Han</surname></persName>
							<email>rujunhan@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Center for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute University of Southern California</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
								<orgName type="institution" key="instit4">New York University CIFAR Global Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Spirling</surname></persName>
							<email>arthur.spirling@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Center for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute University of Southern California</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
								<orgName type="institution" key="instit4">New York University CIFAR Global Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gill</surname></persName>
							<email>mzgill@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Center for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute University of Southern California</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
								<orgName type="institution" key="instit4">New York University CIFAR Global Scholar</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Center for Data Science</orgName>
								<orgName type="department" key="dep3">Center for Data Science</orgName>
								<orgName type="institution" key="instit1">Information Sciences Institute University of Southern California</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<orgName type="institution" key="instit3">New York University</orgName>
								<orgName type="institution" key="instit4">New York University CIFAR Global Scholar</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4890" to="4895"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4890</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Conventional word embedding models do not leverage information from document meta-data, and they do not model uncertainty. We address these concerns with a model that incorporates document covariates to estimate conditional word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values , and (c) assessments as to whether estimated differences are statistically significant.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Whether a word's meaning varies across contexts has become a major focus of NLP, linguistics, and social science research in recent years. For exam- ple, since the early 20th century, the word "gay" has evolved from describing an emotion to be- ing more aligned with sexual orientation <ref type="bibr" target="#b5">(Hamilton et al., 2016b</ref>). Popular word embedding tech- niques (e.g., <ref type="bibr" target="#b12">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b14">Pennington et al., 2014</ref>) have proven useful for analyzing lan- guage evolution. But to use these models for such research, scholars often divide a corpus into dis- tinct training sets (e.g., train independent language models on different decades of text) and compare model output across specifications in an ad hoc way ( <ref type="bibr" target="#b3">Garg et al., 2018)</ref>. Such splitting inhibits many within-and across-word comparisons, since embeddings are only comparable within a given model. Additionally, most methods ignore the variance of words, mechanically treating words equally regardless of the volatility, or uncertainty, in their meanings. If one inspects semantics with only point estimates of embeddings, it is hard to tell whether embeddings represent meaningful traits or are simply noise in the data.</p><p>We address these concerns in three ways. First, we estimate a vector for each distinct value of the document covariates, using a multilayer percep- tron (MLP) with a non-linear activation function. Second, we parametrize the covariance matrix of each embedding vector explicitly in the model, adopting the Bayes-by-Backprop algorithm <ref type="bibr" target="#b0">(Blundell et al., 2015)</ref>. Third, we utilize Hotelling T 2 statistics <ref type="bibr" target="#b7">(Hotelling, 1931)</ref> to assess whether esti- mated differences in word vectors are statistically differentiable under a null χ 2 distribution <ref type="bibr" target="#b8">(Ito, 1956</ref>). To our knowledge, no prior work evaluates word embeddings with this statistical framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Drift Analysis using Word Embeddings There are several ways to measure drifts in word mean- ings. <ref type="bibr" target="#b6">Hamilton et al. (2016c)</ref> propose the use of cosine similarities of words in different contexts to detect changes. <ref type="bibr" target="#b5">Hamilton et al. (2016b)</ref> pro- vide an alternative measure based on the distance of words from their nearest neighbors. <ref type="bibr" target="#b16">Rudolph and Blei (2018)</ref> analyze absolute drift of words using Euclidean distance in (two discrete) slices of data. All of these methods compute the word distance based only on the point (i.e., mean) esti- mates of the word embeddings.</p><p>Conditional Word Embedding Rudolph and Blei (2018) estimate dynamic Bernoulli embed- dings (DBE), extending the exponential family embedding ( <ref type="bibr" target="#b17">Rudolph et al., 2016</ref>) generalization of <ref type="bibr" target="#b12">Mikolov et al. (2013a)</ref>, to learn conditional word embeddings over time. Their amortized approach builds a separate neural network that transforms a global word vector into a covariate- specific vector, and is closely related to our ap- proach in this paper. However, a noticeable omis- sion in their model is that they do not explicitly model parameter covariance or uncertainty.</p><p>Word Embedding with Uncertainty <ref type="bibr" target="#b19">Vilnis and McCallum (2017)</ref> earlier proposed an energy-based learning framework in which each word is represented as a multivariate Gaussian distribu- tion with a diagonal covariance. The energy func- tion is defined by the divergence (e.g., KL) be- tween two Gaussian embeddings, and the mar- gin ranking loss <ref type="bibr" target="#b20">(Weston et al., 2011</ref>) is mini- mized. A related model is the Bayesian skip-gram in <ref type="bibr" target="#b1">Brazinskas et al. (2017)</ref>, which posits a genera- tive model where words are associated with multi- variate Gaussian latent variables that generate con- text words. The parameters of those prior distri- butions over the multivariate Gaussian latent vari- ables are estimated by maximizing the variational lowerbound, and act as word embeddings.</p><p>These works replace mean estimates of embed- dings with Gaussian distributions, similar to our proposal here. However, they arrive at this dif- ferently; <ref type="bibr" target="#b19">Vilnis and McCallum (2017)</ref> from the energy-based learning ( <ref type="bibr" target="#b11">LeCun et al., 2006</ref>), and <ref type="bibr" target="#b1">Brazinskas et al. (2017)</ref> from generative modeling.</p><p>We provide yet another angle: via (approximate) Bayesian neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conditional Word Embedding</head><p>Adopting Bayes-by-Backprop for Estimation Given a tuple of a word v, a covariate x and a context word v c , we define the conditional log- probability as</p><formula xml:id="formula_0">log p(v c |v, x) = θ v|x θ c vc − log v c ∈V exp θ v|x θ c v c ,</formula><p>where θ v|x and θ c vc are the conditional word em- bedding of v given x and the context embedding of v c , respectively. V is the vocabulary of all unique words. To avoid the expensive computation of the partition function, we use negative sampling ( <ref type="bibr" target="#b13">Mikolov et al., 2013b</ref>), which stochastically ap- proximates the log-probability above by:</p><formula xml:id="formula_1">log p(v c |v, x) ≈ log σ(θ v|x θ c vc )<label>(1)</label></formula><formula xml:id="formula_2">+ 1 M M m=1 log(1 − σ(θ v|x θ c v m c )),</formula><p>where v m c ∈ V is the m-th negative sample drawn from a unigram distribution estimated from D.</p><p>We define a prior distribution over each param- eter θ to be a scaled mixture of two Gaussians, as suggested by <ref type="bibr" target="#b0">Blundell et al. (2015)</ref>:</p><formula xml:id="formula_3">log p(θ i ) = log uN (θ i |0, σ 2 1 )<label>(2)</label></formula><formula xml:id="formula_4">+(1 − u)N (θ i |0, σ 2 2 ) ,</formula><p>where σ 1 , σ 2 and u are the hyperparameters. As exactly marginalizing out the parameters θ · and θ c · is not scalable, we maximize the vari- ational lowerbound of the marginal probability. To do so, we introduce a variational posterior q(θ|φ) parametrized by its own parameter set φ. Then, the variational lowerbound is defined as</p><formula xml:id="formula_5">−F(θ, D) = E q [log p(D|θ)] − KL(q(θ)p(θ)),</formula><p>where log p(D|θ) = (v,x,vc)∈D log p(v c |v, x) in our case. This is stochastically approximated by</p><formula xml:id="formula_6">−F(θ, D) ≈ 1 M M m=1 log p(D|θ (m) ) (3) − log q(θ (m) |φ) + log p(θ (m) ),</formula><p>where θ (m) is the m-th sample from the variational posterior q ( <ref type="bibr" target="#b0">Blundell et al., 2015</ref>) via the Gaussian reparametrization in <ref type="bibr" target="#b9">Kingma and Welling (2013)</ref>.</p><p>We formulate the variational posterior as a multi- variate Gaussian with diagonal covariance. We use stochastic gradient descent (SGD) to minimize F with respect to the variational param- eters φ. At each SGD step, we compute the gra- dient of the following per-example cost given an</p><formula xml:id="formula_7">example (v, v c , x) ∈ D: f (θ, (v,v c , x)) ≈ − log p(v c |v, x) + log q( ˜ θ v|x |φ) + log q( ˜ θ c vc ) + log q( ˜ θ c v c ) − log p( ˜ θ),</formula><p>where˜θwhere˜ where˜θ is a single sample from the approximate posterior, and log p(v c |v, x) and log p( ˜ θ) are from Eqs. (1)-(2). We then estimate the (approximate) posterior distribution of each conditional word embedding θ v|x rather than its point estimate, by minimizing F. See Sec. A of the supplementary material for the detailed steps for computing the per-example cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parametrized Conditional Word Embedding</head><p>An issue with the approach described so far is the number of parameters grows linearly in the size of the vocabulary and in the number of covariate par- titions, i.e., O(|V | × |C|), where C is the set of all partitions. This effectively excludes any potential sharing of structures underlying words across dif- ferent covariate values and decreases the number of examples per parameter. To avoid this issue, we use a single parametrized function to compute the variational parameters φ of each conditional word embedding θ v|x .</p><p>For each covariate-word v|x, there are two vari- ational parameters µ v|x and σ v|x . We use an MLP without any hidden layer and tanh output layer, i.e., the affine transformation followed by point- wise tanh, that takes as input both a global word vector µ <ref type="bibr">(v)</ref> v and a covariate vector µ <ref type="bibr">(x)</ref> x and outputs µ v|x , i.e.,</p><formula xml:id="formula_8">µ v|x = f ψ ( µ (v) v ; µ (x) x )</formula><p>, where ψ is the parameters of this mean-transformation network. The diagonal covariance σ v|x is parametrized as σ v|x = log(1 + exp(ρ v )), where ρ v is a pa- rameter shared across all covariate configurations. We then minimize F w.r.t. these parameters ψ, µ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Divergences for Word Embeddings</head><p>As we estimate the approximate posterior uncer- tainty of conditional word vectors, we can estimate richer relations between vectors (e.g., KL) in addi- tion to more common comparisons (e.g., cosine or Euclidean distance). Moreover, we can explicitly test for whether two vectors are (un)likely to have the same mean in the population. Below, we intro- duce how Hotelling's T 2 may be used for word- drift or across-word hypothesis testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hotelling's T 2 Statistic</head><p>We use the estimated posterior mean vector µ v|x and the diagonal co- variance vector σ v|x of two word-covariate pairs v|x i and v|x j to compute the T 2 statistic, as if they were estimates from two sets of samples:   Model and Learning For each word in the cor- pus, we consider six surrounding words as its con- text. The size of embedding is set to 100. We use six negative samples to compute Eq. (1). We use Adagrad ( <ref type="bibr" target="#b2">Duchi et al., 2011</ref>) with the initial learning rate 0.05 for learning. <ref type="bibr">2</ref> For other hyper- parameters, see the supplementary material. We refer to our approach by BBP. For comparison, we also train analogous DBE embeddings using code from the authors.</p><formula xml:id="formula_9">T 2 = (µ i − µ j ) diag(s) −1 (µ i − µ j ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result and Analysis</head><p>Impact of Covariates To demonstrate how doc- ument covariates influence conditional word em- beddings, we compare the vector for "currency" against "sterling" and "pound" according to the KL divergence in each decade, which is shown in <ref type="figure" target="#fig_3">Fig. 1</ref>. In each time period we report the ranking of each w.r.t "currency". Here, we observe that piv- otal points for both "sterling" and "pound" occur in the 1970s, which coincides with the moment the UK began to abandon the 'sterling area' <ref type="bibr">(Part III in Schenk, 2010)</ref>. As such, this financial policy appears to have encouraged semantic drift of the word "pound" towards "currency". See Sec. D in the supplementary material for more details.</p><p>We also show a few more examples in <ref type="figure" target="#fig_4">Figure 2</ref> and <ref type="figure" target="#fig_5">Figure 3</ref> from the Dictionary Induction section below.</p><p>Dictionary Induction As a quantitative com- parison between the proposed approach and the DBE, we take a dictionary of (British) political terms by <ref type="bibr" target="#b10">Laver and Garry (2000)</ref> and look at the The ranks between "market" and "money" across the decades according to KL di- vergence. average pair-wise, directional rank in each cate- gory ("pro-state", "con-state" and "neutral-state"). We only consider the 2,000 most frequent words in the vocabulary and embeddings with the covari- ate (decade) set to 2000s. We observed that the proposed model using KL divergence has signif- icantly smaller average pair-wise ranks in "pro- state" (4052 vs. 5047) and "con-state" (2578 vs. 3758) while performs slightly worse than DBE in "neutral-state" category (5414 vs. 5031) suggest- ing that the proposal approach can cluster words from similar semantic group into closer neighbors than DBE.</p><p>Furthermore, we pick 5 most frequent words from "pro-state" and "con-state" and show their average pair-wise rankings and percentile in <ref type="table">Table  1</ref>. Out of 25K words, our proposed model is able to rank most chosen words within top 10% per- centile.</p><p>Statistical Word Drift Analysis Our BBP ap- proach permits meaningful downstream hypothe- sis tests of word drift, i.e, Diff(v|x i , v|x j ) = 0, and across-word similarity, i.e., Diff(v i , v j ) = 0. Among the 2,000 most frequent words in our sam-  ple, we perform hypothesis tests of word drifts, comparing vectors from the 1940s against those from the 2000s. We compare results from BBP against the top-100 estimated drifts via DBE. We first observe that most of the top-ranked words by L2 distance in the DBE model are not statistically significant. With the p-value threshold of α = 0.1, only eleven words were deemed to have had sig- nificant drift, including "council", "labour", "eu- ropean" and "defence". Sec. E of the supplement includes entire lists of this drift analysis.</p><p>In <ref type="table">Table 1</ref>, we show results from five illustra- tive tests, drawn from the top-100 word drifts es- timated by the DBE model. We report words' drift ranks in DBE against their corresponding L2 distance, cosine similarity, KL divergence and Hotelling T 2 using the embeddings estimated in our BBP model. Based on the distance metrics that ignore the covariance matrix, these words do not appear to change much over time as their cosine similarities are fairly large and their L2 distances are relatively small with little variation across the five words. This suggests their mean vectors are projected into close space between 1940s and 2000s. However, by taking into account their un- certainty, we observe greater variation in both KL divergence and T 2 statistic. For example, "coun- cil" has the eighth largest drift in DBE by L2, but shows the largest T 2 statistic among the five words and is statistically significant at α = 0.01. So too, the largest DBE drift ("uk") is insignificant once you take into account the covariance structure. )). Edge weights in 2.C and 2.D are computed by arccos(w cos i,j ), following <ref type="bibr" target="#b4">Hamilton et al. (2016a)</ref>. Edges with weights below 90th percentiles are dropped for visual clarity. Note that with the same number of edges being eliminated, the KLD charts appear more clustered around seed words, implying that incorporating covariance matrix creates useful segregation of words within local contexts; graphs constructed via cosine similarity seem to disperse edge weights in a more diffuse manner.</p><p>T 2 -based Significance In the context of uncertainty-aware word embeddings, we can use the T 2 statistic to filter out additional words from a nearest neighbor set. For instance, in <ref type="figure" target="#fig_4">Figure  2</ref>.B and 2.D, we drop edges for word pairs that fall below the 90th percentile of computed T 2 statistics. Filtering with Hotelling T 2 results in more sparse semantic graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed an uncertainty-aware conditional word embedding model that combines two ideas; (1) variational Bayesian learning for estimating parameter uncertainty, and (2) structured embed- dings conditioned on covariates. This provides a principled direction to investigate hypothesis tests of word vectors in various forms. We evaluated various aspects of the proposed approach on U.K. Parliament speech records from 1935-2012. We believe the proposed approach will serve as a more rigorous tool in social science and other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This approach of parametrized conditional word embeddings significantly reduces the number of parameters from O(|V | × |C|) to O(|V | + |C|), while maintaining posterior uncertainty of the es- timated conditional word embedding θ v|x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The pooled (diagonal) covariance s of word pairs is computed by s = (n i −1)·σ 2 i +(n j −1)·σ 2 j n i +n j −2 , where n i and n j are the numbers of occurrences of v|x i and v|x j in D, respectively. 1 Unlike other divergence measures, this T 2 statistic explicitly takes into account the frequencies of the word-covariate pairs. Under general conditions, e.g., D is large, the sampling distribution of T 2 converges to a χ 2 d dis- tribution (Ito, 1956) with d equal to the embedding dimensionality. This allows us to statistically test such a null hypothesis as Diff(v i |x, v j |x) = 0 and Diff(v|x i , v|x j ) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5</head><label></label><figDesc>Application: Political Speech in UK Data We use U.K. Parliament speech records from 1935-2012 as our training data (Rheault 1 T 2 is valid only when ni &gt; 1 and nj &gt; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The ranks of "sterling" (solid line) and "pound" (dotted line) w.r.t. "currency" across the decades according to KL divergence.</figDesc><graphic url="image-1.png" coords="3,317.85,62.81,194.40,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2:</figDesc><graphic url="image-2.png" coords="4,82.57,62.81,194.40,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The ranks between "benefit" and "children" across the decades according to KL divergence.</figDesc><graphic url="image-3.png" coords="4,82.57,242.28,194.40,129.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Semantic Graphs with KLD vs. Cosine Similarity</figDesc><graphic url="image-4.png" coords="5,80.50,62.81,436.53,109.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Top word drifts selected based on DBE 
model and estimated by BBP. * and ** indicate 
p-value ≤ 0.05 and 0.01, respectively. 

</table></figure>

			<note place="foot" n="2"> https://github.com/rhan1207/ConditionalEmbeddings</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>arXiv, 1505.05424</idno>
		<title level="m">Weight uncertainty in neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Embedding words as distributions with a bayesian skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Brazinskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serhii</forename><surname>Havrylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno>arXiv, 1711.11027</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Word embeddings quantify 100 years of gender and ethnic stereotypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Londa</forename><surname>Schiebinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences (Preprint)</title>
		<meeting>the National Academy of Sciences (Preprint)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inducing domain-specific sentiment lexicons from unlabeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="595" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cultural shift or linguistic drift? comparing two computational measures of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page">2116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Diachronic word embeddings reveal statistical laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno>arXiv, 1605.09096v4</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The generalization of student&apos;s ratio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="360" to="378" />
			<date type="published" when="1931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asymptotic formulae for the distribution of hotelling&apos;s generalized t 2 0 statistic. The Annals of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koichi</forename><surname>Ito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1091" to="1105" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating policy positions from political texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="page" from="619" to="634" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Predicting structured data</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distributed representations of words and phrases and their compositionality. arXiv, 1310</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4546</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Measuring emotion in parliamentary debates with automated textual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Rheault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaspar</forename><surname>Beelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cochrane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">168843</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic embeddings for language evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW 2018: The 2018 Web Conference</title>
		<imprint>
			<date type="published" when="2018-04-23" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exponential family embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The decline of sterling: managing the retreat of an international currency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Catherine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="1945" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno>1412.6623</idno>
		<title level="m">Word representations via gaussian embedding. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2764" to="2770" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
