<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Low-Rank Tensors for Multilingual Transfer Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CSAIL</roleName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
							<email>yuanzh@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<email>regina@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Low-Rank Tensors for Multilingual Transfer Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Accurate multilingual transfer parsing typically relies on careful feature engineering. In this paper, we propose a hierarchical tensor-based approach for this task. This approach induces a compact feature representation by combining atomic features. However, unlike traditional tensor models, it enables us to incorporate prior knowledge about desired feature interactions , eliminating invalid feature combinations. To this end, we use a hierarchical structure that uses intermediate em-beddings to capture desired feature combinations. Algebraically, this hierarchical tensor is equivalent to the sum of traditional tensors with shared components, and thus can be effectively trained with standard online algorithms. In both unsu-pervised and semi-supervised transfer scenarios , our hierarchical tensor consistently improves UAS and LAS over state-of-the-art multilingual transfer parsers and the traditional tensor model across 10 different languages. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of multilingual syntactic transfer is to parse a resource lean target language utilizing an- notations available in other languages. Recent ap- proaches have demonstrated that such transfer is possible, even in the absence of parallel data. As a main source of guidance, these methods rely on the commonalities in dependency structures across languages. These commonalities manifest them- selves through a broad and diverse set of indi- cators, ranging from standard arc features used in monolingual parsers to typological properties <ref type="bibr">1</ref> The source code is available at https://github. com/yuanzh/TensorTransfer. Verb-subject: {head POS=VERB} ∧ {modifier POS=NOUN} ∧{label=subj} ∧ {direction=LEFT}∧ {82A=SV} Noun-adjective: {head POS=NOUN} ∧ {modifier POS=ADJ}∧ {direction=LEFT} ∧ {87A=Adj-Noun} <ref type="table">Table 1</ref>: Example verb-subject and noun-adjective typological features. 82A and 87A denote the WALS ( <ref type="bibr" target="#b5">Dryer et al., 2005</ref>) feature codes for verb- subject and noun-adjective ordering preferences.</p><p>needed to guide cross-lingual sharing (e.g., verb- subject ordering preference). In fact, careful fea- ture engineering has been shown to play a cru- cial role in state-of-the-art multilingual transfer parsers <ref type="bibr">(Täckström et al., 2013)</ref>.</p><p>Tensor-based models are an appealing alterna- tive to manual feature design. These models auto- matically induce a compact feature representation by factorizing a tensor constructed from atomic features (e.g., the head POS). No prior knowledge about feature interactions is assumed. As a result, the model considers all possible combinations of atomic features, and addresses the parameter ex- plosion problem via a low-rank assumption.</p><p>In the multilingual transfer setting, however, we have some prior knowledge about legitimate fea- ture combinations. Consider for instance a ty- pological feature that encodes verb-subject pref- erences. As <ref type="table">Table 1</ref> shows, it is expressed as a conjunction of five atomic features. Ideally, we would like to treat this composition as a single non-decomposable feature. However, the tradi- tional tensor model decomposes this feature into multiple dimensions, and considers various com- binations of these features as well as their indi- vidual interactions with other features. Moreover, we want to avoid invalid combinations that con-join the above feature with unrelated atomic fea- tures. For instance, there is no point to construct- ing features of the form {head POS=ADJ}∧{head POS=VERB} ∧ · · · ∧ {82A=SV} as the head POS takes a single value. However, the traditional tensor technique still considers these unobserved feature combinations, and assigns them non-zero weights (see Section 7). This inconsistency be- tween prior knowledge and the low-rank assump- tion results in a sub-optimal parameter estimation.</p><p>To address this issue, we introduce a hierarchi- cal tensor model that constrains parameter repre- sentation. The model encodes prior knowledge by explicitly excluding undesired feature combi- nations over the same atomic features. At the bot- tom level of the hierarchy, the model constructs combinations of atomic features, generating inter- mediate embeddings that represent the legitimate feature groupings. For instance, these groupings will not combine the verb-subject ordering feature and the POS head feature. At higher levels of the hierarchy, the model combines these embed- dings as well as the expert-defined typological fea- tures over the same atomic features. The hierar- chical tensor is thereby able to capture the interac- tion between features at various subsets of atomic features. Algebraically, the hierarchical tensor is equivalent to the sum of traditional tensors with shared components. Thus, we can use standard online algorithms for optimizing the low-rank hi- erarchical tensor. We evaluate our model on labeled dependency transfer parsing using the newly released multi- lingual universal dependency treebank . We compare our model against the state-of-the-art multilingual transfer dependency parser <ref type="bibr">(Täckström et al., 2013)</ref> and the direct transfer model ). All the parsers utilize the same training resources but with different feature representations. When trained on source languages alone, our model outperforms the baselines for 7 out of 10 languages on both unlabeled attachment score (UAS) and labeled at- tachment score (LAS). On average, it achieves 1.1% UAS improvement over <ref type="bibr">Täckström et al. (2013)</ref>'s model and 4.8% UAS over the direct transfer. We also consider a semi-supervised set- ting where multilingual data is augmented with 50 annotated sentences in the target language. In this case, our model achieves improvement of 1.7% UAS over <ref type="bibr">Täckström et al. (2013)</ref>'s model and 4.5% UAS over the direct transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multilingual Parsing The lack of annotated parsing resources for the vast majority of world languages has kindled significant interest in multi- source parsing transfer ( <ref type="bibr" target="#b11">Hwa et al., 2005;</ref><ref type="bibr" target="#b7">Durrett et al., 2012;</ref><ref type="bibr" target="#b30">Zeman and Resnik, 2008;</ref><ref type="bibr" target="#b29">Yu et al., 2013b;</ref><ref type="bibr" target="#b2">Cohen et al., 2011;</ref><ref type="bibr" target="#b22">Rasooli and Collins, 2015)</ref>. Recent research has focused on the non-parallel setting, where transfer is driven by cross-lingual commonalities in syntactic struc- ture ( <ref type="bibr" target="#b17">Naseem et al., 2010;</ref><ref type="bibr" target="#b0">Berg-Kirkpatrick and Klein, 2010;</ref><ref type="bibr" target="#b1">Cohen and Smith, 2009;</ref><ref type="bibr" target="#b6">Duong et al., 2015)</ref>.</p><p>Our work is closely related to the selective- sharing approaches <ref type="bibr" target="#b18">(Naseem et al., 2012;</ref>. The core of these methods is the assumption that head-modifier attachment preferences are universal across different languages. However, the sharing of arc direction is selective and is based on typological features. While this selective sharing idea was first realized in the generative model <ref type="bibr" target="#b18">(Naseem et al., 2012</ref>), higher performance was achieved in a discriminative arc-factored model <ref type="bibr">(Täckström et al., 2013</ref>). These gains were obtained by a careful construction of features templates that combine standard dependency parsing features and typological features. In contrast, we propose an automated, tensor-based approach that can effectively capture the interaction between these features, yielding a richer representation for cross- lingual transfer. Moreover, our model handles labeled dependency parsing while previous work only focused on the unlabeled dependency parsing task.</p><p>Tensor-based Models Our approach also relates to prior work on tensor-based modeling. <ref type="bibr" target="#b12">Lei et al. (2014)</ref> employ three-way tensors to obtain a low-dimensional input representation optimized for parsing performance. <ref type="bibr" target="#b25">Srikumar and Manning (2014)</ref> learn a multi-class label embedding tai- lored for document classification and POS tag- ging in the tensor framework. <ref type="bibr" target="#b27">Yu and Dredze (2015)</ref>, <ref type="bibr" target="#b10">Fried et al. (2015)</ref> apply low-rank ten- sor decompositions to learn task-specific word and phrase embeddings. Other applications of tensor framework include low-rank regularization <ref type="bibr" target="#b20">(Primadhanty et al., 2015;</ref><ref type="bibr" target="#b21">Quattoni et al., 2014;</ref><ref type="bibr" target="#b23">Singh et al., 2015</ref>) and neural tensor networks (Socher et</p><formula xml:id="formula_0">c φ m ,φ m c φ d ,φ l ree-way tensor φ m c φ h c φ l φ h φ m φ d φ t u + φ t l + M c φ m c H c φ h c Lφ l Hφ h Mφ m Dφ d = e 2 = e 3</formula><p>Figure 1: Visual representation for traditional mul- tiway tensor.</p><p>al., 2013; <ref type="bibr" target="#b28">Yu et al., 2013a</ref>). While these methods can automatically combine atomic features into a compact composite representation, they cannot take into account constraints on feature combina- tion. In contrast, our method can capture features at different composition levels, and more gener- ally can incorporate structural constraints based on prior knowledge. As our experiments show, this approach delivers higher transfer accuracy.</p><p>3 Hierarchical Low-rank Scoring for Transfer Parsing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>We start by briefly reviewing the traditional three- way tensor scoring function ( <ref type="bibr" target="#b12">Lei et al., 2014</ref>). The three-way tensor characterizes each arc h → m using the tensor-product over three feature vec- tors: the head vector (φ h ∈ R n ), the modifier vec- tor (φ m ∈ R n ) and the arc vector (φ h→m ∈ R l ). φ h captures atomic features associated with the head, such as its POS tag and its word form. Simi- larly, φ m and φ h→m capture atomic features asso- ciated with the modifier and the arc respectively. The tensor-product of these three vectors is a rank- 1 tensor:</p><formula xml:id="formula_1">φ h ⊗ φ m ⊗ φ h→m ∈ R n×n×l</formula><p>This rank-1 tensor captures all possible combina- tions of the atomic features in each vector, and therefore significantly expands the feature set. The tensor score is the inner product between a three- way parameter tensor A ∈ R n×n×l and this rank-1 feature tensor:</p><formula xml:id="formula_2">vec(A) · vec(φ h ⊗ φ m ⊗ φ h→m )</formula><p>where vec(·) denotes the vector representation of a tensor. This tensor scoring method avoids the pa- rameter explosion and overfitting problem by as- suming a low-rank factorization of the parameters</p><formula xml:id="formula_3">M c φ m c Lφ l Hφ h Mφ m Dφ d T u φ t u + T l φ t l + e 2 e 4 H c φ h c = e 1 = e 3</formula><p>Figure 2: Visual representation for hierarchical tensor, represented as a tree structure. The ten- sor first captures the low-level interaction (Hφ h , M φ m and Dφ d ) by an element-wise product, and then combines the intermediate embedding with other components higher in the hierarchy, e.g. e 2 and Lφ l . The equations show that we composite two representations by an element-wise sum.</p><p>A. Specifically, A is decomposed into the sum of r rank-1 components:</p><formula xml:id="formula_4">A = r i=1 U (i) ⊗ V (i) ⊗ W (i)</formula><p>where r is the rank of the tensor, U, V ∈ R r×n and W ∈ R r×l are parameter matrices. U (i) de- notes the i-th row of matrix U and similarly for V (i) and W (i). <ref type="figure">Figure 1</ref> shows the representation of a more general multiway factorization. With this factorization, the model effectively alleviates the feature explosion problem by projecting sparse feature vectors into dense r-dimensional embed- dings via U , V and W . Subsequently, the score is computed as follows:</p><formula xml:id="formula_5">S tensor (h → m) = r i=1 [U φ h ] i [V φ m ] i [W φ h→m ] i</formula><p>where <ref type="bibr">[·]</ref> i denotes the i-th element of the matrix. In multilingual transfer, however, we want to incorporate typological features that do not fit in any of the components. For example, if we add the verb-subject ordering preference into φ h→m , the tensor will represent the concatenation of this preference with a noun-adjective arc, even though this feature should never trigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Low-rank Tensor</head><p>To address this issue, we propose the hierarchi- cal factorization of tensor parameters. <ref type="bibr">2</ref> The key idea is to generate intermediate embeddings that capture the interaction of the same set of atomic features as other expert-defined features. As <ref type="figure">Fig- ure 2</ref> shows, this design enables the model to han- dle expert-defined features over various subsets of the atomic features. Now, we will illustrate this idea in the context of multilingual parsing. <ref type="table" target="#tab_1">Table 2</ref> summarizes the no- tations of the feature vectors and the correspond- ing parameters. Specifically, for each arc h → m with label l, we first compute the intermediate fea- ture embedding e 1 that captures the interaction be- tween the head φ h , the modifier φ m and the arc direction and length φ d , by an element-wise prod- uct.</p><p>[</p><formula xml:id="formula_6">e 1 ] i = [Hφ h ] i [M φ m ] i [Dφ d ] i (1)</formula><p>where <ref type="bibr">[·]</ref> i denotes the i-th value of the feature em- bedding, and H, M and D are the parameter ma- trices as in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The embedding e 1 cap- tures the unconstrained interaction over the head, the modifier and the arc. Note that φ tu includes expert-defined typological features that rely on the specific values of the head POS, the modifier POS and the arc direction, such as the example noun- adjective feature in <ref type="table">Table 1</ref>. Therefore, the em- bedding T u φ tu captures an expert-defined interac- tion over the head, the modifier and the arc. Thus e 1 and T u φ tu provide two different representations of the same set of atomic features (e.g. the head) and our prior knowledge motivates us to exclude the interaction between them since the low-rank assumption would not apply. Thus, we combine e 1 and T u φ tu as e 2 using an element-wise sum</p><formula xml:id="formula_7">[e 2 ] i = [e 1 ] i + [T u φ tu ] i<label>(2)</label></formula><p>and thereby avoid such combinations. As <ref type="figure">Fig- ure 2</ref> shows, e 2 in turn is used to capture the higher level interaction with arc label features φ l ,</p><formula xml:id="formula_8">[e 3 ] i = [Lφ l ] i [e 2 ] i<label>(3)</label></formula><p>Now e 3 captures the interaction between head, modifier, arc direction, length and label. It is over the same set of atomic features as the typological features that depend on arc labels φ t l , such as the example verb-subject ordering feature in <ref type="table">Table 1</ref>. Therefore, we sum over these embeddings as  </p><formula xml:id="formula_9">[e 4 ] i = [e 3 ] i + [T l φ t l ] i<label>(4)</label></formula><formula xml:id="formula_10">S tensor (h l − → m) = r i=1 [H c φ hc ] i [M c φ mc ][e 4 ] i</formula><p>(5) By combining Equation 1 to 5, we observe that our hierarchical tensor score decomposes into three multiway tensor scoring functions.</p><formula xml:id="formula_11">S tensor (h l − → m) = r i=1 [H c φ hc ] i [M c φ mc ] i [T l φ t l ] i + [Lφ l ] i [T u φ tu ] i + [Hφ h ] i [M φ m ] i [Dφ d ] i = r i=1 [H c φ hc ] i [M c φ mc ] i [T l φ t l ] i +[H c φ hc ] i [M c φ mc ] i [Lφ l ] i [T u φ tu ] i +[H c φ hc ] i [M c φ mc ] i [Lφ l ] i [Hφ h ] i [M φ m ] i [Dφ d ] i<label>(6)</label></formula><p>This decomposition provides another view of our tensor model. That is, our hierarchical tensor is algebraically equivalent to the sum of three mul- tiway tensors, where H c , M c and L are shared. <ref type="bibr">3</ref> From this perspective, we can see that our tensor model effectively captures the following three sets of combinations over atomic features:</p><formula xml:id="formula_12">f 1 : φ hc ⊗ φ mc ⊗ φ t l f 2 : φ hc ⊗ φ mc ⊗ φ l ⊗ φ tu f 3 : φ hc ⊗ φ mc ⊗ φ l ⊗ φ h ⊗ φ m ⊗ φ d</formula><p>The last set of features f 3 captures the interac- tion across standard atomic features. The other two sets of features f 1 and f 2 focus on combin- ing atomic typological features with atomic label and context features. Consequently, we explicitly assign zero weights for invalid assignments, by ex- cluding the combination of φ tu with φ h and φ m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexicalization Components</head><p>In order to encode lexical information in our tensor-based model, we add two additional com- ponents, H w φ hw and M w φ mw , for head and mod- ifier lexicalization respectively. We compute the final score as the interaction between the delexi- calized feature embedding in Equation 5 and the lexical components. Specifically:</p><formula xml:id="formula_13">[e 5 ] i = [H c φ hc ] i [M c φ mc ] i [e 4 ] i S tensor (h l − → m) = r i=1 [H w φ hw ] i [M w φ mw ] i [e 5 ] i (7)</formula><p>where e 5 is the embedding that represents the delexicalized transfer results. We describe the fea- tures in φ hw and φ mw in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Combined Scoring</head><p>Similar to previous work on low-rank tensor scor- ing models ( <ref type="bibr" target="#b12">Lei et al., 2014;</ref><ref type="bibr" target="#b13">Lei et al., 2015)</ref>, we combine the traditional scoring and the low-rank tensor scoring. More formally, for a sentence x and a dependency tree y, our final scoring func- tion has the form</p><formula xml:id="formula_14">S(x, y) = γ h l − →m∈y w · φ(h l − → m) + (1 − γ) h l − →m∈y S tensor (h l − → m) (8)</formula><p>where φ(h l − → m) is the traditional features for arc h → m with label l and w is the correspond- ing parameter vector. γ ∈ [0, 1] is the balanc- ing hyper-parameter and we tune the value on the development set. The parameters in our model are θ = (w, H, M, D, L, T u , T l , H c , M c ), and our goal is to optimize all parameters given the train- ing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning</head><p>In this section, we describe our learning method. <ref type="bibr">4</ref> Following standard practice, we optimize the pa- rameters θ = (w, H, M, D, L, T u , T l , H c , M c ) in a maximum soft-margin framework, using online passive-aggressive (PA) updates <ref type="bibr" target="#b3">(Crammer et al., 2006</ref>).</p><p>For tensor parameter update, we employ the joint update method originally used by <ref type="bibr" target="#b13">Lei et al. (2015)</ref> in the context of four-way tensors. While our tensor has a very high order (8 components for the delexicalized parser and 10 for the lexicalized parser) and is hierarchical, the gradient computa- tion is nevertheless similar to that of traditional tensors. As described in Section 3.2, we can view our hierarchical tensor as the combination of three multiway tensors with parameter sharing. There- fore, we can compute the gradient of each mul- tiway tensor and take the sum accordingly. For example, the gradient of the label component is</p><formula xml:id="formula_15">∂L = h l − →m∈y * (H c φ hc ) (M c φ mc ) [(T u φ tu ) + (Hφ h ) (M φ m ) (Dφ d )] ⊗ φ l − h l − →m∈˜y→m∈˜y (H c φ hc ) (M c φ mc ) [(T u φ tu ) + (Hφ h ) (M φ m ) (Dφ d )] ⊗ φ l (9)</formula><p>where is the element-wise product and + de- notes the element-wise addition. y * and˜yand˜ and˜y are the gold tree and the maximum violated tree respec- tively. For each sentence x, we find˜yfind˜ find˜y via cost- augmented decoding.</p><p>Tensor Initialization Given the high tensor or- der, initialization has a significant impact on the learning quality. We extend the previous power method for high-order tensor initialization ( <ref type="bibr" target="#b13">Lei et al., 2015</ref>) to the hierarchical structure using the al- gebraic view as in computing the gradient. Briefly, the power method incrementally com- putes the most important rank-1 component for H(i), M (i) etc, for i = 1 . . . r. In each iteration, the algorithm updates each component by taking the multiplication between the tensor T and the rest of the components. When we update the label component l, we do the multiplication for different  Order of Subject and Verb 83A</p><p>Order of Object and Verb 85A</p><p>Order of Adposition and Noun Phrase 86A</p><p>Order of Genitive and Noun 87A</p><p>Order of Adjective and Noun , we use 82A and 83A instead of 81A (order of subject, object and verb) because we can distinguish between subject and object relations based on dependency labels.</p><p>multiway tensors and then take the sum.</p><formula xml:id="formula_16">l = T 0 , h c , m c , −, t u + T 1 , h c , m c , −, h, m, d</formula><p>where the operator T 0 , h c , m c , −, t u returns a vector in which the i-th element is computed as</p><formula xml:id="formula_17">uvw T 0 (i, u, v, w)h c (u)m c (v)t u (w).</formula><p>The algo- rithm updates other components in a similar fash- ion until convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Features</head><p>Linear Scoring Features Our traditional lin- ear scoring features in φ(h l − → m) are mainly drawn from previous work <ref type="bibr">(Täckström et al., 2013)</ref>. <ref type="table" target="#tab_2">Table 3</ref> lists the typological features from "The World Atlas of Language Structure (WALS)" <ref type="bibr" target="#b5">(Dryer et al., 2005</ref>) used to build the fea- ture templates in our work. We use 82A and 83A for verb-subject and verb-object order respectively because we can distinguish between these two re- lations based on dependency labels. <ref type="table" target="#tab_3">Table 4</ref> sum- marizes the typological feature templates we use. In addition, we expand features with dependency labels to enable labeled dependency parsing.</p><p>Tensor Scoring Features For our tensor model, feature vectors listed in <ref type="table" target="#tab_1">Table 2</ref>   <ref type="table" target="#tab_3">Table 4</ref>. 82A-87A denote the WALS typological feature value. δ(·) is the indicator function. subj ∈ l denotes that the arc label l indicates a subject rela- tion, and similarly for obj ∈ l.</p><formula xml:id="formula_18">φ t l dir·82A·δ(hp=VERB∧mp=NOUN∧subj∈ l) dir·82A·δ(hp=VERB∧mp=PRON∧subj∈ l) dir·83A·δ(hp=VERB∧mp=NOUN∧obj∈ l) dir·83A·δ(hp=VERB∧mp=PRON∧obj∈ l) φ tu dir·85A·δ(hp=ADP∧mp=NOUN) dir·85A·δ(hp=ADP∧mp=PRON) dir·86A·δ(hp=NOUN∧mp=NOUN) dir·87A·δ(hp=ADJ∧mp=NOUN)</formula><p>We further conjoin atomic features (b) and (d) with the family and the typological class of the lan- guage, because the arc direction and the word or- der distribution depends on the typological prop- erty of languages <ref type="bibr">(Täckström et al., 2013</ref>). We also add a bias term into each feature vector.</p><p>Partial Lexicalization We utilize multilingual word embeddings to incorporate partial lexical information in our model. We use the CCA method <ref type="bibr" target="#b9">(Faruqui and Dyer, 2014</ref>) to generate multilingual word embeddings. Specifically, we project word vectors in each non-English language to the English embedding space. To reduce the noise from the automatic projection process, we only incorporate lexical information for the top- 100 most frequent words in the following closed classes: pronoun, determiner, adposition, conjunc- tion, particle and punctuation mark. Therefore, we call this feature extension partial lexicalization. <ref type="bibr">5</ref> We follow previous work ( <ref type="bibr" target="#b12">Lei et al., 2014</ref>) for adding embedding features. For the linear scoring model, we simply append the head and the modi- fier word embeddings after the feature vector. For the tensor-based model, we add each entry of the word embedding as a feature value into φ hw and φ mw . In addition, we add indicator features for the English translation of words because this improves performance in preliminary experiments. For ex- ample, for the German word und, we add the word and as a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>Dataset We evaluate our model on the newly re- leased multilingual universal dependency treebank v2.0 ( ) that consists of 10 languages: English (EN), French (FR), German (DE), Indonesian (ID), Italian (IT), Japanese (JA), Korean (KO), Brazilian-Portuguese (PT), Spanish (ES) and Swedish (SV). This multilingual tree- bank is annotated with a universal POS tagset and a universal dependency label set. Therefore, this dataset is an excellent benchmark for cross-lingual transfer evaluation. For POS tags, the gold uni- versal annotation used the coarse tagset ) that consists of 12 tags: noun, verb, ad- jective, adverb, pronoun, determiner, adposition, numeral, conjunction, particle, punctuation mark, and a catch-all tag X. For dependency labels, the universal annotation developed the Stanford de- pendencies <ref type="bibr" target="#b4">(De Marneffe and Manning, 2008</ref>) into a rich set of 40 labels. This universal annota- tion enables labeled dependency parsing in cross- lingual transfer.</p><p>Evaluation Scenarios We first consider the un- supervised transfer scenario, in which we assume no target language annotations are available. Fol- lowing the standard setup, for each target language evaluated, we train our model on the concatenation of the training data in all other source languages.</p><p>In addition, we consider the semi-supervised transfer scenario, in which we assume 50 sen- tences in the target language are available with an- notation. However, we observe that random sen- tence selection of the supervised sample results in a big performance variance. Instead, we se- lect sentences that contain patterns that are absent or rare in source language treebanks. To this end, each time we greedily select the sentence that min- imizes the KL divergence between the trigram dis- tribution of the target language and the trigram dis- tribution of the training data after adding this sen- tence. The training data includes both the target and the source languages. The trigrams are based on universal POS tags. Note that our method does not require any dependency annotations. To incor- porate the new supervision, we simply add the new sentences into the original training set, weighing their impact by a factor of 10.</p><p>Baselines We compare against different variants of our model.</p><p>• Direct: a direct transfer baseline ) that uses only delexicalized features in the MSTParser ( <ref type="bibr" target="#b14">McDonald et al., 2005</ref>).</p><p>• NT-Select: our model without the tensor com- ponent. This baseline corresponds to the prior feature-based transfer method <ref type="bibr">(Täckström et al., 2013</ref>) with extensions to labeled parsing, lexi- calization and semi-supervised parsing. 6 • Multiway: tensor-based model where typolog- ical features are added as an additional compo- nent and parameters are factorized in the multi- way structure similarly as in <ref type="figure">Figure 1</ref>.</p><p>• Sup50: our model trained only on the 50 sentences in the target language in the semi- supervised scenario. In all the experiments we incorporate partial lexi- calization for all variants of our model and we fo- cus on labeled dependency parsing.</p><p>Supervised Upper Bound As a performance upper bound, we train the RBGParser ( <ref type="bibr" target="#b12">Lei et al., 2014)</ref>, the state-of-the-art tensor-based parser, on the full target language training set. We train the first-order model <ref type="bibr">7</ref> with default parameter settings, using the current version of the code. 8</p><p>Evaluation Measures Following standard prac- tices, we report unlabeled attachment score (UAS) and labeled attachment score (LAS), excluding punctuation. For all experiments, we report results on the test set and omit the development results be- cause of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Details</head><p>For all experiments, we use the arc-factored model and use Eisner's algo- rithm <ref type="bibr" target="#b8">(Eisner, 1996)</ref> to infer the projective Viterbi parse. We train our model and the baselines for 10 epochs. We set a strong regularization C = 0.001 during learning because cross-lingual transfer con- tains noise and the models can easily overfit. Other hyper-parameters are set as γ = 0.3 and r = 200 (rank of the tensor). For partial lexicalization, we set the embedding dimension to 50.   the baselines in both cases. Moreover, it achieves best UAS and LAS on 7 out of 10 languages. The difference is more pronounced in the semi- supervised case. Below, we summarize our find- ings when comparing the model with the base- lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Hierarchical Tensors</head><p>We first ana- lyze the impact of using a hierarchical tensor by comparing against the Multiway baseline that im- plements traditional tensor model. As <ref type="table" target="#tab_6">Table 6</ref> shows, this model learns non-zero weights even for invalid feature combinations. This disregard to known constraints impacts the resulting performance. In the unsupervised sce- nario, our hierarchical tensor achieves an aver- age improvement of 0.5% on UAS and 1.3% on LAS. Moreover, our model obtains better UAS on all languages and better LAS on 9 out of 10 lan- guages. This observation shows that the multi- lingual transfer consistently benefits more from a hierarchical tensor structure. In addition, we ob- serve a similar gain over this baseline in the semi- supervised scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Tensor Models</head><p>To evaluate the effec- tiveness of tensor modeling in multilingual trans- fer, we compare our model against the NT-Select baseline. In the unsupervised scenario, our ten- sor model yields a 1.1% gain on UAS and a 1.5% on LAS. In the semi-supervised scenario, the im- provement is more pronounced, reaching 1.7% on UAS and 1.9% on LAS. The relative error reduc- tion almost doubles, e.g. 7.1% vs. 3.8% on UAS. While both our model and NT-Select outper- form Direct baseline by a large margin on UAS, we observe that NT-Select achieves a slightly worse LAS than Direct. By adding a tensor com- ponent, our model outperforms both baselines on LAS, demonstrating that tensor scoring function is able to capture better labeled features for transfer comparing to Direct and NT-Select baselines.</p><p>Transfer Performance in the Context of Super- vised Results To assess the contribution of mul- tilingual transfer, we compare against the Sup50 results in which we train our model only on 50 target language sentences. As  <ref type="table" target="#tab_7">Table 7</ref>: Semi-supervised and Supervised: UAS and LAS of different variants of our model when 50 annotated sentences in the target language are available. "Sup50" columns show the results of our model when only supervised data in the target language is available. We also include in the last two columns the supervised training results with partial or full lexicalization as the performance upper bound. Other columns have the same meaning as in <ref type="table" target="#tab_4">Table 5</ref>. Boldface numbers indicate the best UAS or LAS.</p><p>by training RBGParser on the full training set. <ref type="bibr">9</ref> When trained with partial lexical information as in our model, RBGParser gives 82.9% on UAS and 74.5% on LAS with partial lexical informa- tion. By utilizing source language annotations, our model closes the performance gap between train- ing on the 50 sentences and on the full training set by about 30% on both UAS and LAS. We further compare to the performance upper bound with full lexical information (87.3% UAS and 83.5% LAS). In this case, our model still closes the performance gap by 21% on UAS and 15% on LAS.</p><p>Time Efficiency of Hierarchical Tensors We observe that our hierarchical structure retains the time efficiency of tensor models. On the English test set, the decoding speed of our hierarchical ten- sor is close to the multiway counterpart (58.6 vs. 61.2 sentences per second), and is lower than the three-way tensor by a factor of 3.1 (184.4 sen- tences per second). The time complexity of ten- sors is linear to the number of low-rank com- ponents, and is independent of the factorization structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we introduce a hierarchical tensor based-model which enables us to constrain learned representation based on desired feature interac- tions. We demonstrate that our model outperforms state-of-the-art multilingual transfer parsers and</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Feature Description</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>82A</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>capture the five types of atomic features as follows: (a) φ h , φ m : POS tags of the head or the modifier. (b) φ hc , φ mc : POS tags of the left/right neighbor- ing words. (c) φ l : dependency labels. (d) φ d : dependency length conjoined with direc- tion. (e) φ tu , φ t l : selectively shared typological fea- tures, as described in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Notations and descriptions of parame- ter matrices and feature vectors in our hierarchical tensor model. M c φ mc and compute the tensor score as</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Typological features from WALS (Dryer 
et al., 2005) used to build the feature tem-
plates in our work, inspired by Naseem et al. 
(2012). Unlike previous work (Naseem et al., 
2012; Täckström et al., 2013)</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Typological feature templates used in our 
work. hp/mp are POS tags of the head/modifier. 
dir ∈ {LEFT, RIGHT} denotes the arc direction. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 and</head><label>5</label><figDesc>7 summarize the results for the unsu- pervised and the semi-supervised scenarios. Aver- aged across languages, our model outperforms all</figDesc><table>Direct 

NT-Select 
Multiway 
Ours 
UAS LAS UAS LAS UAS LAS UAS LAS 
EN 
65.7 
56.7 
67.6 
55.3 
69.8 
56.3 
70.5 
59.8 
FR 
77.9 
67.4 
79.1 
68.9 
78.4 
68.3 
78.9 
68.8 
DE 
62.1 
53.1 
62.1 
53.3 
62.1 
54.0 
62.5 
54.1 
ID 
46.8 
39.3 
57.4 
37.1 
59.5 
38.9 
61.0 
43.5 
IT 
77.9 
67.9 
79.4 
69.4 
79.0 
69.0 
79.3 
69.4 
JA 
57.8 
16.8 
69.2 
20.8 
69.9 
20.4 
71.7 
21.3 
KO 
59.9 
34.3 
70.4 
29.1 
70.5 
28.1 
70.7 
30.5 
PT 
77.7 
71.0 
78.5 
72.0 
78.3 
71.9 
78.6 
72.5 
ES 
76.8 
65.9 
77.2 
67.7 
77.6 
68.0 
78.0 
68.3 
SV 
75.9 
64.5 
74.5 
62.2 
74.8 
62.9 
75.0 
62.5 
AVG 67.8 
53.7 
71.5 
53.6 
72.0 
53.8 
72.6 
55.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Unsupervised: Unlabeled attachment scores (UAS) and Labeled attachment scores (LAS) of 
different variants of our model with partial lexicalization in unsupervised scenario. "Direct" and "Multi-
way" indicate the direct transfer and the multiway variants of our model. "NT-Select" indicates our model 
without tensor component, corresponding to a re-implementation of previous transfer model (Täckström 
et al., 2013) with extensions to partial lexicalization and labeled parsing. The last column shows the 
results by our hierarchical tensor-based model. Boldface numbers indicate the best UAS or LAS. 

Feature 
Weight 
87A∧hp=NOUN∧mp=ADJ 
2.24 × 10 −3 
87A∧hp=VERB∧mp=NOUN 8.88 × 10 −4 
87A∧hp=VERB∧mp=PRON 1.21 × 10 −4 
87A∧hp=NOUN∧mp=NOUN 9.48 × 10 −4 
87A∧hp=ADP∧mp=NOUN 
3.87 × 10 −4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Examples of weights for feature 
combinations between the typological feature 
87A=Adj-Noun and different types of arcs. The 
first row shows the weight for the valid feature 
(conjoined with noun→adjective arcs) and the rest 
show weights for the invalid features (conjoined 
with other types of arcs). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 shows</head><label>7</label><figDesc></figDesc><table>, 
our model improves UAS by 2.3% and LAS by 
2.7%. We also provide a performance upper bound </table></figure>

			<note place="foot" n="2"> In this section we focus on delexicalized transfer, and describe the lexicalization process in Section 3.3.</note>

			<note place="foot" n="3"> We could also associate each multiway tensor with a different weight. In our work, we keep them weighted equally.</note>

			<note place="foot" n="4"> Our description focuses on delexicalized transfer, and we can easily extend the method to the lexicalized case.</note>

			<note place="foot" n="5"> In our preliminary experiments, we observe that our lexicalized model usually outperforms the unlexicalized counterparts by about 2%.</note>

			<note place="foot" n="6"> We use this as a re-implementation of Täckström et al. (2013)&apos;s model because their code is not publicly available. 7 All multilingual transfer models in our work and in Täckström et al. (2013)&apos;s work are first-order. Therefore, we train first-order RBGParser for consistency. 8 https://github.com/taolei87/RBGParser</note>

			<note place="foot" n="9"> On average, each language has more than 10,000 training sentences. traditional tensors. These observations, taken together with the fact that hierarchical tensors are efficiently learnable, suggest that the approach can be useful in a broader range of parsing applications; exploring the options is an appealing line of future research.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is developed in a collaboration of MIT with the Arabic Language Technologies (ALT) group at Qatar Computing Research In-stitute (QCRI) within the Interactive sYstems for Answer Search (IYAS) project. The authors ac-knowledge the support of the U.S. Army Research Office under grant number W911NF-10-1-0533. We thank the MIT NLP group and the EMNLP reviewers for their comments. Any opinions, find-ings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding orga-nizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Phylogenetic grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised structure prediction with nonparallel multilingual guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The world atlas of language structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Comrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-lingual transfer for unsupervised dependency parsing without parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Duong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">113</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Syntactic transfer using a bilingual lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the European Chapter</title>
		<meeting>the Annual Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-rank tensors for verbs in compositional distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural language engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High-order lowrank tensors for semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Ryan T Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Täckström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harr</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1234" to="1244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Selective sharing for multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="629" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1104.2086</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Low-rank regularization for sparse conjunctive feature spaces: An application to named entity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Audi</forename><surname>Primadhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectral regularization for max-margin sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1710" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Density-driven cross-lingual transfer of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadegh Rasooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards combined matrix and tensor factorization for universal schema relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktaschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Vector Space Modeling for NLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning distributed representations for structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems</title>
		<meeting>the Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3266" to="3274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Target language adaptation of discriminative transfer parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning composition models for phrase embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="227" to="242" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The deep tensor neural network with applications to large vocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="388" to="396" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cross-lingual projections between languages from different families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Crosslanguage parser adaptation between related languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Natural Language Processing</title>
		<meeting>the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
