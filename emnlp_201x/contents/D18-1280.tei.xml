<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<country>Ann Arbor</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">NTU</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2594" to="2604"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2594</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Emotion recognition in conversations is crucial for building empathetic machines. Current work in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts mul-timodal features from conversational videos and hierarchically models the self-and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotions play an important role in our daily life. A long-standing goal of AI has been to create af- fective agents that can detect and comprehend emo- tions. Research in affective computing has mainly focused on understanding affect (emotions and sen- timent) in monologues. However, with increasing interactions of humans with machines, researchers now aim at building agents that can seamlessly an- alyze affective content in conversations. This can help in creating empathetic dialogue systems, thus improving the overall human-computer interaction experience ( <ref type="bibr">Young et al., 2018)</ref>.</p><p>Analyzing emotional dynamics in conversations, however, poses complex challenges. This is due to the presence of intricate dependencies between the affective states of speakers participating in the dialogue. In this paper, we address the problem of emotion recognition in conversational videos. We specifically focus on dyadic conversations where two entities participate in a dialogue.</p><p>We propose Interactive COnversational mem- ory Network (ICON), a multimodal network for identifying emotions in utterance-videos. Here, ut- terances are units of speech bounded by breaths or pauses of the speaker. Emotional dynamics in conversations consist of two important properties: self and inter-personal dependencies <ref type="bibr" target="#b23">(Morris and Keltner, 2000</ref>). Self-dependencies, also known as emotional inertia, deal with the aspect of emotional influence that speakers have on themselves during conversations ( <ref type="bibr" target="#b20">Kuppens et al., 2010</ref>). On the other hand, inter-personal dependencies relate to the emo- tional influences that the counterparts induce into a speaker. Conversely, during the course of a dia- logue, speakers also tend to mirror their counter- parts to build rapport ( <ref type="bibr" target="#b24">Navarretta et al., 2016)</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates a sample conversation from the dataset involving both self and inter- personal dependencies. While most conversa- tional frameworks only focus on self dependencies, ICON leverages both such dependencies to gen- erate affective summaries of conversations. First, it extracts multimodal features from all utterance- videos. Next, given a test utterance to be classified, ICON considers the preceding utterances of both speakers falling within a context-window and mod- els their self-emotional influences using local gated recurrent units <ref type="bibr">(GRUs)</ref>.</p><p>Furthermore, to incorporate inter-speaker influ- ences, a global representation is generated using a GRU that intakes output of the local GRUs. For each instance in the context-window, the output of this global GRU is stored as a memory cell. These memories are then subjected to multiple read/write cycles that include attention mechanism for gener- ating contextual summaries of the conversational history. At each iteration, the representation of the test utterance is improved with this summary representation and finally used for prediction.</p><p>I don't think I can do this anymore.</p><p>[ frustrated ]</p><p>Well I guess you aren't trying hard enough.</p><p>[ neutral ]</p><p>Its been three years. I have tried everything.</p><p>[ frustrated ]</p><p>Maybe you're not smart enough.</p><p>[ neutral ]</p><p>Just go out and keep trying.</p><p>[ neutral ]</p><p>I am smart enough. I am really good at what I do. I just don't know how to make someone else see that.</p><p>[anger] Pa is frustrated over her long term unemployment and seeks encouragement (u1, u3). P b , however, is pre-occupied and replies sarcastically (u4). This enrages Pa to appropriate an angry response (u6). In this dialogue, emotional inertia is evident in P b who does not deviate from his nonchalant behavior. Pa, however, gets emotionally influenced by her counterpart. This influence is content-based, not label-based.</p><p>The contributions of this paper are as follows:</p><p>• We propose ICON, a novel model for emo- tion recognition that incorporates self and inter- speaker influences in a dialogue. Memory net- works are used to model contextual summaries for prediction.</p><p>• We introduce a multimodal approach that pro- vides comprehensive features from modalities such as language, visual, and audio in utterance- videos.</p><p>• ICON can be considered as a generic framework for conversational modeling that can be extended to multi-party conversations.</p><p>• Experiments on two benchmark datasets show that ICON significantly outperforms existing models on multiple discrete and continuous emo- tional categories.</p><p>The remainder of the paper is organized as fol- lows: Section 2 presents related works; Section 3 formalizes the problem statement and Section 4 de- scribes our proposed approach; Section 5 provides details on experimental setup; Section 6 reports the results and related analysis; finally, Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Emotion recognition is an interdisciplinary field of research with contributions from psychology, cog- nitive science, machine learning, natural language processing, and others <ref type="bibr" target="#b26">(Picard, 2010)</ref>.</p><p>Initial research in this area primarily involved visual and audio processing <ref type="bibr" target="#b11">(Ekman, 1993;</ref><ref type="bibr" target="#b8">Datcu and Rothkrantz, 2008)</ref>. The role of text in emo- tional analysis became evident with later research such as <ref type="bibr" target="#b0">Alm et al. (2005)</ref>; <ref type="bibr" target="#b36">Strapparava and Mihalcea (2010)</ref>. Current research in this domain is mainly performed from a multimodal learning perspective ( <ref type="bibr" target="#b27">Poria et al., 2017a;</ref><ref type="bibr" target="#b2">Baltrušaitis et al., 2018)</ref>. Numerous previous approaches have relied on fusion techniques that leverage multiple modali- ties for affect recognition <ref type="bibr" target="#b33">(Soleymani et al., 2012;</ref><ref type="bibr" target="#b6">Zadeh et al., 2017;</ref><ref type="bibr" target="#b6">Chen et al., 2017;</ref><ref type="bibr" target="#b39">Tzirakis et al., 2017;</ref><ref type="bibr">Zadeh et al., 2018b</ref>).</p><p>Understanding conversations is crucial for ma- chines to replicate human language and discourse. Emotions play an important role in shaping such social interactions <ref type="bibr" target="#b30">(Ruusuvuori, 2013)</ref>. <ref type="bibr" target="#b29">Richards et al. (2003)</ref> attribute emotional dynamics to be an interactive phenomena, rather than being within- person. We utilize this trait in the design of our model that accommodates inter-personal dynam- ics. Being a temporal event, context also plays an important role in conversational analysis. <ref type="bibr" target="#b28">Poria et al. (2017b)</ref> use contextual information from neighboring utterances of the same speaker to pre- dict emotions. However, there is no provision to model interactive influences. Work by <ref type="bibr">Yang et al., 2011;</ref><ref type="bibr">Xiaolan et al., 2013</ref> stresses the study of pat- terns for emotion transitions. In contrast, we posit the use of utterance content to model context with multimodal features.</p><p>In the literature, memory networks have been successfully applied in many areas, includ- ing question-answering ( <ref type="bibr">Weston et al., 2014;</ref><ref type="bibr" target="#b37">Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b19">Kumar et al., 2016)</ref>, ma- chine translation ( ), speech recognition ( <ref type="bibr" target="#b14">Graves et al., 2014)</ref>, and others. In emotional analysis, <ref type="bibr">Zadeh et al. (2018a)</ref> propose a memory-based sequential learning for multi-view signals. Although we utilize memory networks, our work is different as we use memories to en- code whole utterances. Also, each memory cell in our network is processed using GRUs to capture temporal dependencies. This technique deviates from the traditional use of embedding matrices to encode information into memory cells.</p><p>ICON builds on our previous research <ref type="bibr" target="#b15">(Hazarika et al., 2018</ref>) that used separate memory networks for both interlocutors participating in a dyadic con- versation. In contrast, ICON adopts an interac- tive scheme that actively models inter-speaker emo- tional dynamics with fewer trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setting</head><p>Let us define a conversation U to be a set of asyn- chronous exchange of utterances between two per- sons P a and P b over time. With T utterances, U = {u 1 , u 2 , ..., u T } is a totally ordered set which can be arranged as a sequence (u 1 , ..., u T ) based on temporal occurrence. Here, each utterance u i is spoken by either P a or P b . Furthermore, for each λ ∈ {a, b}, U λ denotes person P λ 's in- dividual utterances in U , i.e., U λ = {u i u i ∈ U and u i spoken by P λ , ∀i ∈ [1, U ]}. This pro- vides two sets of utterances for both the respective speakers, such that U = U a ∪ U b .</p><p>Our aim is to identify the emotions of utterances in conversational videos. At each time step t ∈ <ref type="bibr">[1, T ]</ref> of video U , our model is provided with the utterance spoken at that time, i.e. u t , and tasked to predict its emotion. Moreover, we also utilize the previous utterances within U spoken by both persons. Considering a context-window of size K, the preceding utterances of P a and P b (starting with the most recent) within this context-window can be represented by H a and H b , respectively. Formally, for each λ ∈ {a, b}, H λ is created as,</p><formula xml:id="formula_0">H λ = {u i i ∈ [t − K, t − 1] and u i ∈ U λ } (1) and H a + H b ≤ K<label>(2)</label></formula><p>Table 1 provides a sample conversation with a context-window of size K = 5. <ref type="table">Table 1</ref>: Sample conversation U with test utterance u7.</p><formula xml:id="formula_1">U { u a 1 , u a 2 , u b 3 , u a 4 , u a 5 , u b 6 } U a , U b { u 1 , u 2 , u 4 , u 5 }, { u 3 , u 6 } test utterance u a 7 H a , H b { u 2 , u 4 , u 5 }, { u 3 , u 6 }</formula><p>Context-window K = 5. Here, u λ i = i th utterance by P λ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>ICON has been designed as a generic framework for affective modeling of conversations. Its compu- tations can be categorized as a sequence of four suc- cessive modules: Multimodal Feature Extraction, Self-Influence Module, Dynamic Global-Influence Module, and Multi-hop Memory. <ref type="figure" target="#fig_3">Figure 2</ref> illus- trates the overall model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Feature Extraction</head><p>ICON adopts a multimodal framework and per- forms feature extraction from three modalities, i.e., language (transcripts), audio and visual.</p><p>These features are extracted for each utterance in the conversation and their concatenated vec- tors serve as the utterance representations. The motivation of this setup derives from previous works that demonstrate the effectiveness of mul- timodal features in creating rich feature represen- tations <ref type="bibr" target="#b10">(D'mello and Kory, 2015)</ref>. These features provide complementary information from hetero- geneous sources which helps to accumulate com- prehensive features. Its need is particularly pro- nounced in videos as they are often plagued with noisy signals and missing-information within indi- vidual modalities (e.g., facial occlusion, loud back- ground music, imperfect transcriptions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Textual Features</head><p>We employ a convolutional neural network (CNN) to extract textual features from the transcript of each utterance. CNNs are capable of learning abstract semantic representations of a sentence based on its words and n-grams ( <ref type="bibr" target="#b16">Kalchbrenner et al., 2014</ref>). For our purpose, we utilize a simple CNN with a single convolutional layer followed by max-pooling <ref type="bibr" target="#b17">(Kim, 2014)</ref>. The input to this net- work consists of pre-trained word embeddings ex- tracted from the 300-dimensional FastText embed- dings ( <ref type="bibr" target="#b3">Bojanowski et al., 2016)</ref>. The convolution layer consists of three filters with sizes f 1 t , f 2 t , f 3 t with f out feature maps each. We perform 1D convo- lutions using these filters followed by max-pooling on its output. The pooled features are finally pro- jected onto a dense layer with dimension d t and its activations are used as the textual representation t u ∈ R dt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Audio Features</head><p>Audio plays a significant role in determining the emotional states of a speaker <ref type="bibr" target="#b9">(De Silva and Ng, 2000;</ref><ref type="bibr" target="#b34">Song et al., 2004</ref>). To extract audio features, we first format the audio of each utterance-video as a 16-bit PCM WAV file and use the open-sourced software openSMILE ( <ref type="bibr" target="#b12">Eyben et al., 2010)</ref>. This tool provides high dimensional vectors for audio files that summarizes important statistical descrip- tors such as loudness, pitch, Mel-spectra, MFCC, etc. Specifically, we use the IS13 ComParE 1 ex- tractor which provides 6373 features for each ut- terance. The features are then normalized using Min-Max scaling followed by L2-based feature se- lection. This selection provides low-dimensional audio features a u ∈ R da of dimensions d a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Visual Features</head><p>Visual indicators such as facial expressions are key to understand emotions. In our work, we use a deep 3D-CNN to model spatiotemporal features of each utterance video ( <ref type="bibr" target="#b38">Tran et al., 2015</ref>). 3D-CNN helps to understand emotional concepts such as smiling or frowning that are often spread across multiple frames of a video with no predefined spatial lo- cation. The input to this network is a video with dimensions (c, h, w, f ), where c is the number of channels, h, w are the height and width of each frame, with a total of f frames per video. The network contains three blocks of convolu- tion where each block contains two convolutional layers followed by max-pooling. For the convolu- tion, 3D filters are employed having dimensions (f out , f in , f h , f w , f d ), where, f <ref type="bibr">[outinhwd]</ref> repre- sents the number of feature maps, input channels, height, width, and depth of the filter, respectively. After a non-linear reLU activation ( <ref type="bibr" target="#b21">LeCun et al., 2015)</ref>, max-pooling is performed using a sliding window of dimensions (m p , m p , m p ). For an in- put utterance video, the final features of the third convolutional block is mapped onto a dense layer of dimension d v whose activations are used as the visual features v u ∈ R dv .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Fusion</head><p>We generate the final representation of an utterance u by concatenating all three multimodal features:</p><formula xml:id="formula_2">u = tanh((W f [t u ; a u ; v u ]) + b f )<label>(3)</label></formula><p>Concatenation is one of the most common fusion methods <ref type="bibr" target="#b32">(Shwartz et al., 2016)</ref>. Its simplicity also allows us to emphasize the contribution of the re- maining components of ICON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SIM: Self-Influence Module</head><p>Given a test utterance u t to be classified, this mod- ule independently processes the histories of both speakers. SIM consists of two GRUs, GRU s a and GRU s b , for H a and H b , respectively. For each λ ∈ {a, b}, GRU s λ attempts to model the emotional inertia of speaker P λ which represents the emo- tional dependency of a speaker with their own pre- vious states. In particular, for each historical ut- terance u i&lt;t ∈ H λ , an internal memory state h . This can be abbreviated as h</p><formula xml:id="formula_3">(j) λ = GRU s λ (u i , h (j−1) λ ).</formula><p>Gated Recurrent Unit: GRUs are gated recur- rent cells introduced by . At time step j, GRU computes hidden state s j ∈ R dem by calculating two gates, r j (reset gate) and z j (up- date gate) with j th input x j and previous state s j−1 . The computations are:</p><formula xml:id="formula_4">z j = σ(V z x j + W z s j−1 + b z ) r j = σ(V r x j + W r s j−1 + b r ) v j = tanh(V h x j + W h (s j−1 ⊗ r j ) + b h ) s j = (1 − z j ) ⊗ v j + z j ⊗ s j−1</formula><p>In this work, input x j = u i and s j = h (j)</p><p>λ . SIM computes both sequences H * a ∈ R dem×Ha and H * b ∈ R dem×H b using the respective GRUs,</p><formula xml:id="formula_5">H * λ = [h (j) λ ] H λ j=1 = GRU s λ (H λ ) , λ ∈ {a, b} (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DGIM: Dynamic Global Influence Module</head><p>Emotions are not only regarded as internal- psychological phenomena but also interpreted and processed communicatively through social inter- actions <ref type="bibr" target="#b13">(Fiehler, 2002)</ref>. Conversations exemplify such a scenario where inter-personal emotional in- fluence persists. Theories in cognitive science also suggest the existence of emotional contagion that causes humans to mirror their counterpart's ges- ture, posture and emotional state <ref type="bibr" target="#b5">(Chartrand and Bargh, 1999;</ref><ref type="bibr" target="#b24">Navarretta et al., 2016)</ref>. Additionally, these interactions occur dynamically through the discourse of a dialogue. While modeling the contextual history, we incor- porate such properties using a dynamic influence module. This module maintains a global represen- tation of the conversation and updates it recurrently at each time step of the K-length conversation his- tory. For any k ∈ <ref type="bibr">[1, K]</ref>, the global state is updated using a GRU operation on the previous state s k−1 and current speaker P λ 's SIM memory h (j) λ for the corresponding spoken utterance u (t−K+k−1) , i.e.,</p><formula xml:id="formula_6">h (j) λ = GRU s λ (u (t−K+k−1)</formula><p>). Formally, DGIM consists of a GRU network, GRU g , where the k th global state s k is computed as: </p><formula xml:id="formula_7">s k = GRU g (h (j) a , s k−1 ), if u (t−K+k−1) ∈ H a GRU g (h (j) b , s k−1 ), if u (t−K+k−1) ∈ H b<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-hop Memory</head><p>The overall operation of the GRU g produces a se- quence of memories M = [s 1 , ..., s K ] ∈ R dem×K . These memories incorporate dynamic influences from each of the K utterances spoken in the his- tory. They serve as a contextual memory bank from which selective person-specific information can be incorporated into test utterance u t to get discriminative features. To achieve this, a series of R memory read/write cycles are performed that are coupled with soft attention for refinement of u t into a context-aware representation. The need for multiple hops is inspired by recent works on memory networks ( <ref type="bibr" target="#b19">Kumar et al., 2016;</ref><ref type="bibr">Weston et al., 2014</ref>), which suggests the impor- tance of multiple read/write iterations for perform- ing transitive inference. Multiple hops also help in improving the focus of attention heads which might miss essential memories in a single hop. At the r th hop, the computations are as follows:</p><p>• Memory Read: An attention mechanism is used to read the memories from r th memory bank</p><note type="other">M (r) (Weston et al., 2014). First, each memory m (r)</note><p>k ∈ M (r) is matched with test utterance u (r) t (initially, u</p><p>(1) t = u t and M (1) = M ). This matching generates an attention vector p (r) attn ∈ R K whose k th normalized score rep- resents the relevance of k th memory cell with respect to the test utterance. Inner product is used for the matching as follows:</p><formula xml:id="formula_8">p (r) attn = sof tmax( (M (r) ) T u (r) t ) (6)</formula><p>Where, sof tmax(x i ) = e x i ∑ j e x j . These scores are then used to find a weighted repre- sentation of the memories as</p><formula xml:id="formula_9">m (r) = K k=1 (p (r) attn ) k .(m k ) = M (r) p (r)</formula><p>attn <ref type="formula">(7)</ref> This vector denotes the summary of the context that is person-specific and based on the test ut- terance. Finally, the representation of the test utterance is updated by consolidating itself with the weighted memory m as:</p><formula xml:id="formula_10">u (r+1) t = tanh(m (r) + u (r) t )<label>(8)</label></formula><p>• Memory Write: After the read operation at each hop, memories are updated for the next hop. For this purpose, a GRU network, GRU m , takes the r th memory cells M (r) as input and reprocesses this sequence to generate memories M (r+1) , i.e., M (r+1) = GRU m (M (r) ). Across all hops, this write operation can be viewed as that of a stacked recurrent neural network (RNN) where each level (or hop) improves the representational output of the RNN. The parameters of GRU m are shared across all hops.</p><p>Final Prediction: We use the (R + 1) th test ut- terance vector u (R+1) t and get the final prediction vector through its affine transformation,</p><formula xml:id="formula_11">o = sof tmax(W o u (R+1) t + b o )<label>(9)</label></formula><p>For classification, dimensions of vector o is the number of classes C, i.e., o ∈ R C and categorical cross-entropy loss is used as the cost measure for training. For regression, o is a scalar (without softmax normalization) whose scores are used to calculate the mean squared error cost metric.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We perform experiments on two benchmark datasets in dialogue-based emotion detection: IEMOCAP is a database consisting of videos of dyadic conversations between pairs of 10 speakers. Grouped into five sessions, each pair is assigned with diverse scenarios for dialogues. Videos are segmented into utterances with annotations of fine- grained emotion categories. We consider six such categories for the classification task: anger, happi- ness, sadness, neutral, excitement, and frustration. The training set is curated using the first 8 speakers from session 1-4 while session 5 is used for testing.</p><p>SEMAINE is a video database of human-agent interactions. Here, users interact with characters whose responses are based on users' emotional state. Specifically, we utilize the AVEC 2012's fully continuous sub-challenge ( <ref type="bibr" target="#b31">Schuller et al., 2012</ref>) that requires predictions of four continuous affec- tive dimensions: arousal, expectancy, power, and valence. The gold annotations are available for ev- ery 0.2 seconds in each video ( <ref type="bibr" target="#b25">Nicolle et al., 2012)</ref>. However, to align with our problem statement, we approximate the utterance-level annotation as the mean of the continuous values within the spoken utterance. The sub-challenge provides standard training and testing splits which has been summa- rized in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Details</head><p>20% of the training set is used as validation set for hyper-parameter tuning. We use the Adam op- timizer ( <ref type="bibr" target="#b18">Kingma and Ba, 2014</ref>) for training the parameters starting with an initial learning rate of 0.001. Termination of the training-phase is decided by early-stopping with a patience of 10 epochs. The network is subjected to regularization in the form of Dropout ( <ref type="bibr" target="#b35">Srivastava et al., 2014</ref>) and Gradient-clipping for a norm of 40. Finally, the best hyper-parameters are decided using a grid- search. Their values are summarized in <ref type="table" target="#tab_3">Table 3</ref>.</p><formula xml:id="formula_12">(f 1 t , f 2 t , f 3 t ) = (3, 4, 5) f [h,w,d] = 3 fout = 64 d [t,a] = 100 dv = 512 dem = 100 K = 40 R = 3</formula><p>For multimodal feature extraction, we explore different designs for the employed CNNs. For text, we find the single layer CNN to perform at par with deeper variants. For visual features, however, a deeper CNN provides better representations. We also find that contextually conditioned features per- form better than context-less features. Thus, in our experiments, we extract video-level contextual features for utterances from each modality using the network proposed by <ref type="bibr" target="#b28">Poria et al. 2017b</ref>. These modified features are then used to form the multi- modal utterance representations using equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>We compare our proposed model with multiple state-of-the-art networks in multimodal utterance- level emotion detection.</p><p>• memnet ( <ref type="bibr" target="#b37">Sukhbaatar et al., 2015</ref>) is an end-to- end memory network. For comparison, we mod- ify our network to adopt their embedding-based memory-encoding in the multi-hop stage.</p><p>• cLSTM 4 ( <ref type="bibr" target="#b28">Poria et al., 2017b</ref>) classifies utterances using neighboring utterances (of same speaker) as context. LSTM is used for this purpose.</p><p>• TFN 5 ( <ref type="bibr" target="#b6">Zadeh et al., 2017</ref>) models intra-and inter- modality dynamics by explicitly aggregating uni- , bi-and trimodal interactions. Unlike cLSTM, contextual utterances are not considered.</p><p>• MFN (Zadeh et al., 2018a) performs multi-view learning by using Delta-memory Attention Net- work, a fusion mechanism to learn cross-view interactions. Similar to TFN, the modeling is performed within utterances.</p><p>• CMN (Hazarika et al., 2018) models separate contexts for both speaker and listener to an ut- terance. These contexts are stored as memories and combined with test utterance using attention mechanism.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Tables 4 and 5 present the results on the IEMO- CAP and SEMAINE testing sets, respectively. In <ref type="table" target="#tab_5">Table 4</ref>, we evaluate the mean classification per- formance using Weighted Accuracy (acc.) and F1-Score (F1) on the discrete emotion categories. ICON performs better than the compared models with significant performance increase in emotions (∼2.1% acc.). For each emotion, ICON outper- forms all the compared models except for happi- ness emotion. However, its performance is still at par with cLSTM without a significant gap. Also, ICON manages to correctly identify the relatively similar excitement emotion by a large margin. In <ref type="table">Table 5</ref>, evaluations of the four continuous labels from SEMAINE are performed using Mean   Absolute Error (MAE) and Pearson's Correlation Coefficient (r). In all the labels, ICON attains im- proved performance over its counterparts, suggest- ing the efficacy of its context-modeling scheme.</p><p>Hyperparameters: We plot the performance trends of ICON on the IEMOCAP dataset concern- ing the two main hyperparameters, R (number of hops) and K (context-window size). For R, the performance initially improves showing the im- portance of multiple hops in the memories. How- ever, with a further increase, the hopping recur- rence deepens and causes the vanishing gradient problem. This leads to decrease in performance. The best performance is obtained at R = 3. For K, similar trends are observed where performance improvement is seen by increasing the number of historical utterances. The best results are obtained for K = 40 which also aligns with the average num- ber of historical utterances in the dataset <ref type="table" target="#tab_2">(Table 2)</ref>. Further increase in context does not provide rele- vant information and rather leads to performance degradation due to model confusion.</p><p>Multimodality: We investigate the importance of multimodal features for our task.  unimodals, language modality performs the best, reaffirming its significance in multimodal systems. Interestingly, the audio and visual modality, on their own, do not provide good performance, but when used with text, complementary data is shared to improve overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>To check the importance of the modules present in ICON, we perform an ablation study where we remove constituent components and evaluate the model's performance. <ref type="table" target="#tab_10">Table 7</ref> provides the results on this study. In the first variant, none of the his- tories and the associated context-modeling is used. This provides the worst relative performance.</p><p>Self vs Dual History: We evaluate the scenarios where only self-history of the speaker is considered (variants 2, 4, and 6). Compared to the dual-history variants (variants 3, 5, and 7), these models provide lesser performance. Reasons involve the provision of partial information from the conversational his- tories. Similar trends can be seen for the cLSTM model in <ref type="table" target="#tab_5">Table 4</ref> which works in the same regime. DGIM prevents the storage of dynamic influences between speakers at each historical time step and leads to performance deterioration.</p><p>Multi-hop vs No-hop: Variants 2 and 3 repre- sent cases where multi-hop is omitted, i.e., R = 1. Performance for them are poorer than variants hav- ing multi-hop mechanism (variants 4-7). Also, re- moval of multi-hop leads to worse performance than the removal of DGIM. This suggests that multi-hop is more crucial than the latter. However, best performance is achieved by variant 6 which contains all the proposed modules in its pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dependency on distant history</head><p>For all the test utterances of IEMOCAP correctly classified by ICON, we analyze the global memo- ries receiving the highest attention. First, we divide the conversational history (context-length K = 40) into three regions: long, short, and medium. <ref type="figure" target="#fig_6">Fig- ure 4</ref> provides a summary of how much the model attends each of these regions. The short region (la- beled green) covering 10 utterances, corresponds to conversational history just preceding the test utterance. Utterances which occur more than 30 time steps behind the current test utterance are con- sidered part of the long region (labeled red). Re- maining utterances in between fall on the medium region (labeled blue).</p><p>The distribution of top-valued attention scores across the histories reveal interesting insights. Most of the correctly classified instances focus on the immediate or short history. In other words, 63% of the time, at least one of the top-5 attention value belongs to a memory in the short-history range. A significant share is also present for distant his- tory (22%). This result indicates the presence of long-term emotional dependencies and the need to consider histories far away from the current test utterance.  Tell them this is ridiculous!</p><p>[angry]</p><p>We will have your problem solved.</p><p>[neutral]</p><p>They have never worked for me. <ref type="bibr">[angry]</ref> u1 u14 Test utterance u11 u8</p><p>It won't listen to me. <ref type="bibr">[angry]</ref> u6 u17 You wouldn't have heard about it. <ref type="bibr">[neutral]</ref> Nice point of view I must say. <ref type="bibr">[angry]</ref> And what of it? <ref type="bibr">[neutral]</ref> u1 u17 u11 u12 a) Self-Emotional Influence b) Inter-speaker Influence <ref type="figure">Figure 6</ref>: Case studies for emotional influence. 20 memories in the history which are nearest to test utterance, i.e. k ∈ <ref type="bibr">[21,</ref><ref type="bibr">40]</ref> are visualized from the trained ICON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dynamic Modeling of Global Memories:</head><p>ICON holds the capability to model dynamic in- teractions between speakers. The memories by its DGIM ( §4.3) are used to create summaries condi- tioned on the test utterance. Consequently, these summaries contain characteristics that are specific to the affective state of the current speaker (of the test utterance). <ref type="figure" target="#fig_4">Figure 5</ref> presents a sample slice of conversa- tion from the dataset. As seen, summary selec- tion for Person A varies from Person B. Such dif- ferences arise due to person-specific characteris- tics and unique affective interpretations of the con- versation. Apart from the inter-speaker variance, the emotional state of a speaker also varies across turns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Case Studies</head><p>To understand ICON's behavior while processing the global memories through multi-hop, we man- ually explore the utterances in the testing set of IEMOCAP. <ref type="figure">Figure 6</ref> presents two cases which pro- vide traces of self and inter-personal emotional influences and were correctly classified by ICON. Both the figures show the trend where multiple hops gradually improve the focus of attention mech- anism on relevant memories.</p><p>In <ref type="figure">Figure 6a</ref>, person P a registers a complaint to an operator P b . Throughout the dialogue, P a main- tains an angry demeanor while P b remains calm and neutral (u 14 ). While classifying utterance u 15 , ICON focuses more on the histories uttered by P a (u 6 , u 8 , and u 11 ). This demonstrates ICON's abil- ity to model self-emotional influences. It should be noted that emotion of P a here also depends on the utterances of P b but compared to self-utterances, this dependency is much less. <ref type="figure">Figure 6b</ref> presents another scenario where a couple argue over an al- leged affair.</p><p>A man (P b ) is angry over this fact and questions his partner (P a ) asking for details. The woman tries to behave unperturbed by providing neutral responses (u 12 , u 16 ) but is eventually affected by P b 's continuous anger and expresses a frustrated response (u 18 ). These characteristics are captured by the attention mechanism applied on the global memories (generated by DGIM), which finds con- textual information from histories that are relevant to the test utterance u 18 . This example displays the role of inter-speaker influences and how ICON processes such dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented ICON, a multimodal framework for emotion detection in conversations. ICON capitalizes on modeling contextual infor- mation that incorporates self and inter-speaker in- fluences. We accomplish this by using an RNN- based memory network with multi-hop attention modeling. Experiments show that ICON outper- forms state-of-the-art models on multiple bench- mark datasets. Extensive evaluations and case stud- ies demonstrate the effectiveness of our proposed model. Additionally, the ability to visualize the attentions brings a sense of interpretability to the model, as it allows us to investigate which utter- ances in the conversational history provide impor- tant emotional cues for the current emotional state of the speaker.</p><p>In the future, we plan to test ICON on other relevant dialogue-based applications and also use it for empathetic dialogue generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An abridged dialogue from the IEMOCAP dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>j) λ is computed by GRU s λ conditioned on utterance u i and previous memory state h (j−1) λ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>IEMOCAP 2 (</head><label>2</label><figDesc>Busso et al., 2008) and SE- MAINE 3 (McKeown et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>5 :</head><label>5</label><figDesc>Performance on the SEMAINE dataset. Note: MAE = Mean Absolute Error, r = Pearson's correlation coefficient, DV = Valence, DA = Activation/Arousal, DP = Power, DE = Anticipation/Expectation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Trends in the performance of ICON on IEMOCAP dataset with varying R (hops) and K (Context-window size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distribution of top-attention by ICON on correctly classified instances in the testing set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Do you want my jacket? [hap] PA: Conversation Its after eleven. Lets just go home. [ang] PB: Are you kidding? We just got here! [fru] PA: There is no point in coming here [ang] PB:Figure 5 :</head><label>5</label><figDesc>Figure 5: As a conversation develops, different speakers induce different affective bias which reflects in the memory selection for generation of the summaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>0</head><label>0</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Summary of datasets. Note: Avg. history length rep-

resents the expected number of historical utterances available 
for any utterance in the dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Hyper-parameter values for the best model.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Performance of ICON on the IEMOCAP dataset.  † represents statistical significance over state-of-the-art scores under 

the paired-t test (p &lt; 0.05). 

Models 

SEMAINE 
DV 
DA 
DP 
DE 

MAE r 
MAE r 
MAE r 
MAE 
r 

memnet .20 .16 
.21 .24 
.21 .23 8.97 .05 
cLSTM .18 .14 
.21 .23 
.20 .25 8.90 -.04 
TFN 
.21 .01 
.22 .10 
.21 .12 9.19 .12 
MFN 
.19 .14 
.20 .25 
.18 .26 8.60 .15 
CMN 
.18 .23 
.20 .30 
.18 .26 8.89 -.02 
ICON 
.18 .24 
.19 .31 
.18 .27 8.45 0.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Modality 
IEMOCAP 
SEMAINE 

Emotions 
DV 
DA 
DP 
DE 
acc. 
F1 
r 
r 
r 
r 

T 
58.3 57.9 .237 .297 .260 .225 
A 
50.7 50.9 .021 .082 .250 .035 
V 
41.2 39.8 .001 .068 .251 .001 
A+V 
52.0 51.2 .031 .122 .283 .050 
T+A 
63.8 63.2 .237 .310 .272 .242 
T+V 
61.4 61.2 .238 .293 .268 .239 
T+A+V 
64.0 63.5 .243 .312 .279 .244 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison of the performance of ICON on both 

IEMOCAP and SEMAINE considering different modality 
combinations. Note: T=Text, A=Audio, V=Video 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 presents</head><label>6</label><figDesc></figDesc><table>the results for different combinations of 
modes used by ICON on IEMOCAP. As seen, the 
trimodal network provides the best performance 
which is preceded by the bimodal variants. Among </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Ablation study for components of ICON. 

</table></figure>

			<note place="foot" n="1"> http://audeering.com/technology/opensmile</note>

			<note place="foot" n="2"> http://sail.usc.edu/iemocap/ 3 http://sspnet.eu/avec2012/</note>

			<note place="foot" n="4"> http://github.com/senticnet/ contextual-sentiment-analysis 5 http://github.com/A2Zadeh/TensorFusionNetwork</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported in part by Singa-pore's Ministry of Education (MOE) Academic Re-search Fund Tier 1, grant number T1 251RES1713.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotions from text: machine learning for text-based emotion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Ovesdotter Alm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on human language technology and empirical methods in natural language processing</title>
		<meeting>the conference on human language technology and empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The chameleon effect: the perception-behavior link and social interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John A</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bargh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">893</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic audiovisual data fusion for automatic emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Datcu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Liyanage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><forename type="middle">Chi</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fourth IEEE International Conference on</title>
		<meeting>Fourth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="332" to="335" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review and meta-analysis of multimodal affect detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K D&amp;apos;</forename><surname>Sidney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">384</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">How to do emotions with words: Emotionality in conversations. The verbal communication of emotions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Fiehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="79" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2122" to="2132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotional inertia and psychological maladjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kuppens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">B</forename><surname>Nicholas B Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheeber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="984" to="991" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Deep learning. nature</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How emotions work: The social functions of emotional expression in negotiations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keltner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research in organizational behavior</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mirroring facial expressions and emotions in dyadic conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costanza</forename><surname>Navarretta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust continuous prediction of human emotions using multiscale dynamic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Nicolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kévin</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Chetouani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Affective computing: from laughter to ieee</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rosalind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="17" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">formation Fusion</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2017</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Emotion regulation in romantic relationships: The cognitive consequences of concealing feelings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">A</forename><surname>Jane M Richards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Social and Personal Relationships</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="599" to="620" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Emotion, affect and conversation. The handbook of conversation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johanna</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="330" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Avec 2012: the continuous audio/visual emotion challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roddy</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction</title>
		<meeting>the 14th ACM international conference on Multimodal interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Improving hypernymy detection with an integrated path-based and distributional method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06076</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in response to videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Pun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Audio-visual based emotion recognition-a new approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2004</title>
		<meeting><address><addrLine>II-II</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Annotating and identifying emotions in text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Information Access</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end multimodal emotion recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Tzirakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Björn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1301" to="1309" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
