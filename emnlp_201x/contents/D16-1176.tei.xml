<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modelling Interaction of Sentence Pair with Coupled-LSTMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<addrLine>825 Zhangheng Road</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modelling Interaction of Sentence Pair with Coupled-LSTMs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1703" to="1712"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recently, there is rising interest in modelling the interactions of two sentences with deep neural networks. However, most of the existing methods encode two sequences with separate encoders, in which a sentence is encoded with little or no information from the other sentence. In this paper, we propose a deep architecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification <ref type="bibr" target="#b15">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b17">Liu et al., 2015)</ref>, ques- tion answering and machine translation <ref type="bibr" target="#b25">(Sutskever et al., 2014</ref>) and so on. Among these tasks, a com- mon problem is modelling the relevance/similarity of the sentence pair, which is also called text seman- tic matching.</p><p>Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses ( <ref type="bibr" target="#b14">Hu et al., 2014;</ref><ref type="bibr" target="#b26">Wan et al., 2016)</ref>. * Corresponding author.</p><p>According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Models Some early works fo- cus on sentence level interactions, such as ARC- I( <ref type="bibr" target="#b14">Hu et al., 2014</ref>), CNTN(  and so on. These models first encode two sequences with some basic (Neural Bag-of-words, BOW) or advanced (RNN, CNN) components of neural net- works separately, and then compute the matching score based on the distributed vectors of two sen- tences. In this paradigm, two sentences have no in- teraction until arriving final phase. Semi-interaction Models Some improved meth- ods focus on utilizing multi-granularity represen- tation (word, phrase and sentence level), such as MultiGranCNN (  and Multi- Perspective CNN ( <ref type="bibr" target="#b10">He et al., 2015)</ref>. Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN ( , Attention LSTM( <ref type="bibr" target="#b21">Rocktäschel et al., 2015;</ref><ref type="bibr" target="#b12">Hermann et al., 2015</ref>). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models di- rectly build an interaction space between two sen- tences and model the interaction at different posi- tions, such as ARC-II ( <ref type="bibr" target="#b14">Hu et al., 2014</ref>), MV-LSTM ( <ref type="bibr" target="#b26">Wan et al., 2016)</ref> and DF-LSTMs( <ref type="bibr" target="#b18">Liu et al., 2016</ref>). These models can easily capture the difference be- tween semantic capacity of two sentences.</p><p>In this paper, we propose a new deep neural net- work architecture to model the strong interactions of two sentences. Different with modelling two sen- tences with separated LSTMs, we utilize two inter- dependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps. The out- put of coupled-LSTMs at each step depends on both sentences. Specifically, we propose two interdepen- dent ways for the coupled-LSTMs: loosely coupled model (LC-LSTMs) and tightly coupled model (TC- LSTMs). Similar to bidirectional LSTM for single sentence <ref type="bibr" target="#b22">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b6">Graves and Schmidhuber, 2005</ref>), there are four directions can be used in coupled-LSTMs. To utilize all the informa- tion of four directions of coupled-LSTMs, we aggre- gate them and adopt a dynamic pooling strategy to automatically select the most informative interaction signals. Finally, we feed them into a fully connected layer, followed by an output layer to compute the matching score.</p><p>The contributions of this paper can be summa- rized as follows.</p><p>1. Different with the architectures of using sim- ilarity matrix, our proposed architecture di- rectly model the strong interactions of two sen- tences with coupled-LSTMs, which can cap- ture the useful local semantic relevances of two sentences. Our architecture can also capture the multiple granular interactions by several stacked coupled-LSTMs layers.</p><p>2. Compared to previous works on text matching, we perform extensive empirical studies on two very large datasets. The massive scale of the datasets allows us to train a very deep neu- ral network and present an elaborate qualitative analysis of our models, which gives an intuitive understanding how our model worked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Sentence Modelling with LSTM</head><p>Long short-term memory network (LSTM) (Hochre- iter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) <ref type="bibr" target="#b5">(Elman, 1990)</ref>, and specifi- cally addresses the issue of learning long-term de- pendencies.</p><p>We define the LSTM units at each time step t to be a collection of vectors in R d : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a hidden state h t . d is the number of the LSTM units. The elements of the gating vectors i t , f t and o t are in <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>The LSTM is precisely specified as follows.</p><formula xml:id="formula_0">    ˜ c t o t i t f t     =     tanh σ σ σ     T A,b x t h t−1 ,<label>(1)</label></formula><formula xml:id="formula_1">c t = ˜ c t i t + c t−1 f t ,<label>(2)</label></formula><formula xml:id="formula_2">h t = o t tanh (c t ) ,<label>(3)</label></formula><p>where x t is the input at the current time step; T A,b is an affine transformation which depends on param- eters of the network A and b. σ denotes the logistic sigmoid function and denotes elementwise multi- plication.</p><p>The update of each LSTM unit can be written pre- cisely as follows</p><formula xml:id="formula_3">(h t , c t ) = LSTM(h t−1 , c t−1 , x t ).<label>(4)</label></formula><p>Here, the function LSTM(·, ·, ·) is a shorthand for Eq.</p><p>(1-3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Coupled-LSTMs for Strong Sentence Interaction</head><p>To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local in- teractions of two sentences. An improved way is to introduce attention mechanism, which has been used in many tasks, such as machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2014</ref>) and question answering <ref type="bibr" target="#b12">(Hermann et al., 2015)</ref>. Inspired by the multi-dimensional recurrent neu- ral network ( <ref type="bibr" target="#b8">Graves et al., 2007;</ref><ref type="bibr" target="#b7">Graves and Schmidhuber, 2009;</ref><ref type="bibr" target="#b3">Byeon et al., 2015</ref>) and grid LSTM ( <ref type="bibr" target="#b16">Kalchbrenner et al., 2015</ref>) in computer vi- sion community, we propose two models to capture the interdependences between two parallel LSTMs, called coupled-LSTMs (C-LSTMs).</p><p>To facilitate our models, we firstly give some def- initions. Given two sequences X = x 1 , x 2 , · · · , x n and Y = y 1 , y 2 , · · · , y m , we let x i ∈ R d denote the embedded representation of the word x i . The stan- dard LSTM have one temporal dimension. When dealing with a sentence, LSTM regards the posi- tion as time step. At position i of sentence x 1:n ,  the output h i reflects the meaning of subsequence</p><formula xml:id="formula_4">h (1) 1 h (1) 2 h (1) 3 h (2) 1 h (2) 2 h (2) 3 (a) Parallel LSTMs h (1) 1 h (1) 2 h (1) 3 h (2) 1 h (2) 2 h (2) 3 (b) Attention LSTMs h (1) 41 h (2) 41 h (1) 42 h (2) 42 h (1) 43 h (2) 43 h (1) 44 h (2) 44 h (1) 31 h (2) 31 h<label>(</label></formula><formula xml:id="formula_5">x 0:i = x 0 , · · · , x i .</formula><p>To model the interaction of two sentences as early as possible, we define h i,j to represent the interac- tion of the subsequences x 0:i and y 0:j . <ref type="figure" target="#fig_0">Figure 1</ref>(c) and 1(d) illustrate our two propose models. For intuitive comparison of weak interac- tion parallel LSTMs, we also give parallel LSTMs and attention LSTMs in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and 1(b) <ref type="bibr">1</ref> .</p><p>We describe our two proposed models as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Loosely Coupled-LSTMs (LC-LSTMs)</head><p>To model the local contextual interactions of two sentences, we enable two LSTMs to be interde- pendent at different positions. Inspired by Grid LSTM ( <ref type="bibr" target="#b16">Kalchbrenner et al., 2015)</ref> and word-by- word attention LSTMs (Rocktäschel et al., 2015), we propose a loosely coupling model for two inter- dependent LSTMs.</p><p>More concretely, we refer to h 1 is produced conditioned on h</p><formula xml:id="formula_6">(2) 3 h (1)</formula><p>i,j and h <ref type="formula" target="#formula_1">(2)</ref> i,j are computed as</p><formula xml:id="formula_7">h (1) i,j = LSTM 1 (H (1) i−1 , c (1) i−1,j , x i ),<label>(5)</label></formula><formula xml:id="formula_8">h (2) i,j = LSTM 2 (H (2) j−1 , c (2) i,j−1 , y j ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">H (1) i−1 = [h (1) i−1,j , h (2) i−1,j ],<label>(7)</label></formula><formula xml:id="formula_10">H (2) j−1 = [h (1) i,j−1 , h (2) i,j−1 ].<label>(8)</label></formula><p>3.2 Tightly Coupled-LSTMs (TC-LSTMs)</p><p>The hidden states of LC-LSTMs are the combi- nation of the hidden states of two interdependent LSTMs, whose memory cells are separated. In- spired by the configuration of the multi-dimensional LSTM ( <ref type="bibr" target="#b3">Byeon et al., 2015)</ref>, we further conflate both the hidden states and the memory cells of two LSTMs. We assume that h i,j directly model the interaction of the subsequences x 0:i and y 0:j , which depends on two previous interaction h i−1,j and h i,j−1 , where i, j are the positions in sentence X and Y . We define a tightly coupled-LSTMs units as fol- lows.</p><formula xml:id="formula_11">      ˜ c i,j o i,j i i,j f 1 i,j f 2 i,j       =       tanh σ σ σ σ       T A,b     x i y j h i,j−1 h i−1,j     ,<label>(9)</label></formula><formula xml:id="formula_12">c i,j = ˜ c i,j i i,j + [c i,j−1 , c i−1,j ] T f 1 i,j f 2 i,j<label>(10)</label></formula><formula xml:id="formula_13">h i,j = o t tanh (c i,j )<label>(11)</label></formula><p>where the gating units i i,j and o i,j determine which memory units are affected by the inputs through˜cthrough˜through˜c i,j , and which memory cells are written to the hidden units h i,j . T A,b is an affine transformation which depends on parameters of the network A and b. In contrast to the standard LSTM defined over time, each memory unit c i,j of a tightly coupled-LSTMs has two preceding states c i,j−1 and c i−1,j and two corresponding forget gates f 1 i,j and f 2 i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis of Two Proposed Models</head><p>Our two proposed coupled-LSTMs can be formu- lated as where C-LSTMs can be either TC-LSTMs or LC-LSTMs.</p><formula xml:id="formula_14">(hi,j, ci,j) = C-LSTMs(hi−1,j, hi,j−1, ci−1,j, ci,j−1, xi, yj),<label>(12)</label></formula><formula xml:id="formula_15">x1, · · · , xn y1, · · · , ym · · · Pooling</formula><p>The input consists of two type of information at step (i, j) in coupled-LSTMs: temporal dimen- sion h i−1,j , h i,j−1 , c i−1,j , c i,j−1 and depth dimen- sion x i , y j . The difference between TC-LSTMs and LC-LSTMs is the dependence of information from temporal and depth dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Between Temporal Dimensions</head><p>The TC-LSTMs model the interactions at position (i, j) by merging the internal memory c i−1,j c i,j−1 and hidden state h i−1,j h i,j−1 along row and column di- mensions. In contrast with TC-LSTMs, LC-LSTMs firstly use two standard LSTMs in parallel, produc- ing hidden states h 1 i,j and h 2 i,j along row and column dimensions respectively, which are then merged to- gether flowing next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interaction Between Depth Dimension</head><p>In TC- LSTMs, each hidden state h i,j at higher layer re- ceives a fusion of information x i and y j , flowed from lower layer. However, in LC-LSTMs, the in- formation x i and y j are accepted by two corre- sponding LSTMs at the higher layer separately.</p><p>The two architectures have their own charac- teristics, TC-LSTMs give more strong interactions among different dimensions while LC-LSTMs en- sures the two sequences interact closely without be- ing conflated using two separated LSTMs.</p><p>Comparison of LC-LSTMs and word-by-word Attention LSTMs The characteristic of attention LSTMs is that they obtain the attention weighted representation of one sentence considering he align- ment between the two sentences, which is asymmet- ric unidirectional encoding. Nevertheless, in LC- LSTM, each hidden state of each step is obtained with the consideration of interaction between two sequences with symmetrical encoding fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">End-to-End Architecture for Sentence Matching</head><p>In this section, we present an end-to-end deep ar- chitecture for matching two sentences, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Embedding Layer</head><p>To model the sentences with neural model, we firstly need transform the one-hot representation of word into the distributed representation. All words of two sequences X = x 1 , x 2 , · · · , x n and Y = y 1 , y 2 , · · · , y m will be mapped into low dimensional vector representations, which are taken as input of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stacked Coupled-LSTMs Layers</head><p>A basic block consists of five layers. We firstly use four directional coupled-LSTMs to model the local interactions with different information flows. And then we sum the outputs of these LSTMs by aggre- gation layer. To increase the learning capabilities of the coupled-LSTMs, we stack the basic block on top of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Four Directional Coupled-LSTMs Layers</head><p>The C-LSTMs is defined along a certain pre- defined direction, we can extend them to access to the surrounding context in all directions. Similar to bi-directional LSTM, there are four directions in coupled-LSTMs.</p><formula xml:id="formula_16">(h 1 i,j , c 1 i,j ) = C-LSTMs(hi−1,j, hi,j−1, ci−1,j, ci,j−1, xi, yj), (h 2 i,j , c 2 i,j ) = C-LSTMs(hi−1,j, hi,j+1, ci−1,j, ci,j+1, xi, yj), (h 3 i,j , c 3 i,j ) = C-LSTMs(hi+1,j, hi,j+1, ci+1,j, ci,j+1, xi, yj), (</formula><note type="other">h 4 i,j , c 4 i,j ) = C-LSTMs(hi+1,j, hi,j−1, ci+1,j, ci,j−1, xi, yj).</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Aggregation Layer</head><p>The aggregation layer sums the outputs of four di- rectional coupled-LSTMs into a vector.</p><formula xml:id="formula_17">ˆ h i,j = 4 d=1 h d i,j ,<label>(13)</label></formula><p>where the superscript t of h i,j denotes the different directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Stacking C-LSTMs Blocks</head><p>To increase the capabilities of network of learning multiple granularities of interactions, we stack sev- eral blocks (four C-LSTMs layers and one aggrega- tion layer) to form deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pooling Layer</head><p>The output of stacked coupled-LSTMs layers is a tensor H ∈ R n×m×d , where n and m are the lengths of sentences, and d is the number of hidden neurons. We apply dynamic pooling to automatically extract R p×q subsampling matrix in each slice H i ∈ R n×m , similar to <ref type="bibr" target="#b23">(Socher et al., 2011</ref>).</p><p>More formally, for each slice matrix H i , we par- tition the rows and columns of H i into p × q roughly equal grids. These grid are non-overlapping. Then we select the maximum value within each grid thereby obtaining a p × q × d tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Fully-Connected Layer</head><p>The vector obtained by pooling layer is fed into a full connection layer to obtain a final more abstractive representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Output Layer</head><p>The output layer depends on the types of the tasks, we choose the corresponding form of output layer. There are two popular types of text matching tasks in NLP. One is ranking task, such as community ques- tion answering. Another is classification task, such as textual entailment.</p><p>1. For ranking task, the output is a scalar matching score, which is obtained by a linear transforma- tion after the last fully-connected layer.</p><p>MQA RTE Embedding size 100 100 Hidden layer size 50 50 Initial learning rate 0.05 0.005 Regularization 5E−5 1E−5 Pooling (p, q) (2,1) (1,1) <ref type="table">Table 1</ref>: Hyper-parameters for our model on two tasks.</p><p>2. For classification task, the outputs are the prob- abilities of the different classes, which is com- puted by a softmax function after the last fully- connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Training</head><p>Our proposed architecture can deal with different sentence matching tasks. The loss functions varies with different tasks. More concretely, we use max- margin loss ( <ref type="bibr" target="#b1">Bordes et al., 2013;</ref><ref type="bibr" target="#b24">Socher et al., 2013)</ref> for ranking task and cross-entropy loss for classifi- cation task.</p><p>To minimize the objective, we use stochastic gra- dient descent with the diagonal variant of AdaGrad ( <ref type="bibr" target="#b4">Duchi et al., 2011</ref>). To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold (Graves, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiment</head><p>In this section, we investigate the empirical perfor- mances of our proposed model on two different text matching tasks: classification task (recognizing tex- tual entailment) and ranking task (matching of ques- tion and answer).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Hyperparameters and Training</head><p>The word embeddings for all of the models are ini- tialized with the 100d GloVe vectors (840B token version, ( <ref type="bibr" target="#b19">Pennington et al., 2014)</ref>) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sam- pling from uniform distribution in [−0.1, 0.1].</p><p>For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the ini- tial learning rate [0.05, 0.0005, 0.0001], l 2 regular- ization [0.0, 5E−5, 1E−5, 1E−6] and the threshold value of gradient norm <ref type="bibr">[5,</ref><ref type="bibr">10,</ref><ref type="bibr">100]</ref>. The final hyper- parameters are set as <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Competitor Methods</head><p>• Neural bag-of-words (NBOW): Each sequence as the sum of the embeddings of the words it contains, then they are concatenated and fed to a MLP.</p><p>• Single LSTM: A single LSTM to encode the two sequences, which is used in <ref type="bibr" target="#b21">(Rocktäschel et al., 2015</ref>).</p><p>• Parallel LSTMs: Two sequences are encoded by two LSTMs separately, then they are con- catenated and fed to a MLP.</p><p>• Attention LSTMs: An attentive LSTM to en- code two sentences into a semantic space, which used in ( <ref type="bibr" target="#b12">Hermann et al., 2015;</ref><ref type="bibr" target="#b21">Rocktäschel et al., 2015</ref>).</p><p>• Word-by-word Attention LSTMs: An improve- ment of attention LSTM by introducing word- by-word attention mechanism, which used in ( <ref type="bibr" target="#b12">Hermann et al., 2015;</ref><ref type="bibr" target="#b21">Rocktäschel et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experiment-I: Recognizing Textual Entailment</head><p>Recognizing textual entailment (RTE) is a task to de- termine the semantic relationship between two sen- tences. We use the Stanford Natural Language In- ference Corpus (SNLI) <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref>. This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. Our proposed two C-LSTMs models with four stacked blocks outperform all the competitor mod- els, which indicates that our thinner and deeper net- work does work effectively.  Besides, we can see both LC-LSTMs and TC- LSTMs benefit from multi-directional layer, while the latter obtains more gains than the former. We at- tribute this discrepancy between two models to their different mechanisms of controlling the information flow from depth dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Results</head><p>Compared with attention LSTMs, our two mod- els achieve comparable results to them using much fewer parameters (nearly 1/5). By stacking C- LSTMs 2 , the performance of them are improved significantly, and the four stacked TC-LSTMs achieve 85.1% accuracy on this dataset.</p><p>Moreover, we can see TC-LSTMs achieve better performance than LC-LSTMs on this task, which need fine-grained reasoning over pairs of words as well as phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Understanding Behaviors of Neurons in C-LSTMs</head><p>To get an intuitive understanding of how the C- LSTMs work on this problem, we examined the neu- ron activations in the last aggregation layer while evaluating the test set using TC-LSTMs. We find that some cells are bound to certain roles.</p><p>Let h i,j,k denotes the activation of the k-th neu- ron at the position of (i, j), where i ∈ {1, . . . , n} and j ∈ {1, . . . , m}. By visualizing the hidden state h i,j,k and analyzing the maximum activation, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index of Cell Word or Phrase Pairs</head><p>3-th (in a pool, swimming), (near a fountain, next to the ocean), (street, outside) 9-th (doing a skateboard, skateboarding), (sidewalk with, inside), (standing, seated) 17-th (blue jacket, blue jacket), (wearing black, wearing white), (green uniform, red uniform) 25-th (a man, two other men), (a man, two girls), (an old woman, two people)  can find that there exist multiple interpretable neu- rons. For example, when some contextualized local perspectives are semantically related at point (i, j) of the sentence pair, the activation value of hidden neuron h i,j,k tend to be maximum, meaning that the model could capture some reasoning patterns. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates this phenomenon. In <ref type="figure" target="#fig_3">Fig- ure 3(a)</ref>, a neuron shows its ability to monitor the local contextual interactions about color. The activation in the patch, including the word pair "(red, green)", is much higher than others. This is informative pattern for the relation predic- tion of these two sentences, whose ground truth is contradiction. An interesting thing is there are two words describing color in the sentence " A person in a red shirt and black pants hunched over.". Our model ignores the useless word "black", which indicates that this neuron selectively captures pattern by contextual un- derstanding, not just word level interaction.</p><p>In <ref type="figure" target="#fig_3">Figure 3</ref>(b), another neuron shows that it can capture the local contextual interactions, such as "(walking down the street, outside)". These patterns can be easily captured by pooling layer and provide a strong support for the final prediction. <ref type="table" target="#tab_4">Table 3</ref> illustrates multiple interpretable neurons and some representative word or phrase pairs which can activate these neurons. These cases show that our models can capture contextual interactions be- yond word level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Error Analysis</head><p>Although our models C-LSTMs are more sen- sitive to the discrepancy of the semantic capacity between two sentences, some semantic mistakes at the phrasal level still exist. For example, our models failed to capture the key informative pattern when predicting the entailment sentence pair "A girl takes off her shoes and eats blue cotton candy/The girl is eating while barefoot."</p><p>Besides, despite the large size of the training corpus, it's still very different to solve some cases, which depend on the combination of the world knowledge and context-sensitive infer- ences. For example, given an entailment pair "a man grabs his crotch during a political demonstration/The man is making a crude gesture", all models predict "neutral". This analysis suggests that some architectural improvements or external world knowledge are necessary to eliminate all errors instead of simply scaling up the basic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experiment-II: Matching Question and Answer</head><p>Matching question answering (MQA) is a typical task for semantic matching. Given a question, we need select a correct answer from some candidate answers.</p><p>In this paper, we use the dataset collected from Yahoo! Answers with the getByCategory function  <ref type="formula" target="#formula_4">(5)</ref>   provided in Yahoo! Answers API, which produces 963, 072 questions and corresponding best answers. We then select the pairs in which the length of ques- tions and answers are both in the interval <ref type="bibr">[4,</ref><ref type="bibr">30]</ref>, thus obtaining 220, 000 question answer pairs to form the positive pairs. For negative pairs, we first use each question's best answer as a query to retrieval top 1, 000 results from the whole answer set with Lucene, where 4 or 9 answers will be selected randomly to construct the negative pairs.</p><p>The whole dataset is divided into training, vali- dation and testing data with proportion 20 : 1 : 1. Moreover, we give two test settings: selecting the best answer from 5 and 10 candidates respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Results</head><p>Results of MQA are shown in the <ref type="table" target="#tab_6">Table 4</ref>. For our models, due to stacking block more than three layers can not make significant improvements on this task, we just use three stacked C-LSTMs.</p><p>By analyzing the evaluation results of question- answer matching in table 4, we can see strong in- teraction models (attention LSTMs, our C-LSTMs) consistently outperform the weak interaction mod- els (NBOW, parallel LSTMs) with a large margin, which suggests the importance of modelling strong interaction of two sentences.</p><p>Our proposed two C-LSTMs surpass the competi- tor methods and C-LSTMs augmented with multi- directions layers and multiple stacked blocks fully utilize multiple levels of abstraction to directly boost the performance.</p><p>Additionally, LC-LSTMs is superior to TC- LSTMs. The reason may be that MQA is a relative simple task, which requires less reasoning abilities, compared with RTE task. Moreover, the parameters of LC-LSTMs are less than TC-LSTMs, which en- sures the former can avoid suffering from overfitting on a relatively smaller corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Our architecture for sentence pair encoding can be regarded as strong interaction models, which have been explored in previous models.</p><p>An intuitive paradigm is to compute similari- ties between all the words or phrases of the two sentences. <ref type="bibr" target="#b23">Socher et al. (2011)</ref> firstly used this paradigm for paraphrase detection. The represen- tations of words or phrases are learned based on re- cursive autoencoders.</p><p>A major limitation of this paradigm is the inter- action of two sentence is captured by a pre-defined similarity measure. Thus, it is not easy to in- crease the depth of the network. Compared with this paradigm, we can stack our C-LSTMs to model multiple-granularity interactions of two sentences. <ref type="bibr" target="#b21">Rocktäschel et al. (2015)</ref> used two LSTMs equipped with attention mechanism to capture the it- eration between two sentences. This architecture is asymmetrical for two sentences, where the obtained final representation is sensitive to the two sentences' order.</p><p>Compared with the attentive LSTM, our proposed C-LSTMs are symmetrical and model the local con- textual interaction of two sequences directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this paper, we propose an end-to-end deep archi- tecture to capture the strong interaction information of sentence pair. Experiments on two large scale text matching tasks demonstrate the efficacy of our pro- posed model and its superiority to competitor mod- els. Besides, we present an elaborate qualitative analysis of our models, which gives an intuitive un- derstanding how our model worked.</p><p>In future work, we would like to incorporate some gating strategies into the depth dimension of our pro- posed models, like highway or residual network, to enhance the interactions between depth and other di-mensions thus training more deep and powerful neu- ral networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Four different coupled-LSTMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as the encoding of subsequence x 0:i in the first LSTM influenced by the output of the second LSTM on subsequence y 0:j . Meanwhile, h (2) i,j is the encoding of subsequence y 0:j in the second LSTM influenced by the output of the first LSTM on subsequence x 0:i 1 In Rocktäschel et al. (2015) model, conditioned LSTM was used, meaning that h (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Architecture of coupled-LSTMs for sentence-pair encoding. Inputs are fed to four C-LSTMs followed by an aggregation layer. Blue cuboids represent different contextual information from four directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of two interpretable neurons and some word-pairs capture by these neurons. The darker patches denote the corresponding activations are higher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>41 h 42 h 43 h 44 h 31 h 32 h 33 h 34 h 21 h 22 h 23 h 24 h 11 h 12 h 13 h 14</head><label></label><figDesc></figDesc><table>1) 

32 h 

(2) 
32 

h 

(1) 

33 h 

(2) 
33 

h 

(1) 

34 h 

(2) 
34 

h 

(1) 

21 h 

(2) 
21 

h 

(1) 

22 h 

(2) 
22 

h 

(1) 

23 h 

(2) 
23 

h 

(1) 

24 h 

(2) 
24 

h 

(1) 

11 h 

(2) 
11 

h 

(1) 

12 h 

(2) 
12 

h 

(1) 

13 h 

(2) 
13 

h 

(1) 

14 h 

(2) 
14 

(c) Loosely coupled-LSTMs 

h (d) 
Tightly 
coupled-
LSTMs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows</head><label>2</label><figDesc>the evaluation results on SNLI. The 3rd column of the table gives the number of param- eters of different models without the word embed- dings.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Results on SNLI corpus.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Multiple interpretable neurons and the word-pairs/phrase-pairs captured by these neurons.</head><label>3</label><figDesc></figDesc><table>A 
person 
is 
wearing 
a 
green 
shirt 
. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Results on Yahoo question-answer pairs 
dataset. 

</table></figure>

			<note place="foot" n="2"> To make a fair comparison, we also train a stacked attention-based LSTM with the same setting as our models, while it does not make significant improvement with 83.7% accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable comments. This work was partially funded by National Natural Science Foundation of China (No. 61532011 and 61672162), the National High Technology Research and Development Pro-gram of China (No. 2015AA015408).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonmin</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3547" to="3555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN 2007</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiperspective sentence similarity modeling with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convolutional neural network architectures for matching natural language sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baotian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<title level="m">Grid long short-term memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-timescale long shortterm memory neural network for modelling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on EMNLP</title>
		<meeting>the Conference on EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep fusion LSTMs for text semantic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional neural tensor network architecture for communitybased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Karl Moritz Hermann, Tomáš Kočisk`Kočisk`y, and Phil Blunsom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06664</idno>
	</analytic>
	<monogr>
		<title level="m">Reasoning about entailment with neural attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A deep architecture for semantic matching with multiple positional sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Shengxian Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional neural network for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="901" to="911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Abcnn: Attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.05193</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
