<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conversation Trees: A Grammar Model for Topic Structure in Forums</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh Edinburgh</orgName>
								<address>
									<postCode>EH8 9AB</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Conversation Trees: A Grammar Model for Topic Structure in Forums</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Online forum discussions proceed differently from face-to-face conversations and any single thread on an online forum contains posts on different subtopics. This work aims to characterize the content of a forum thread as a conversation tree of topics. We present models that jointly perform two tasks: segment a thread into sub-parts, and assign a topic to each part. Our core idea is a definition of topic structure using probabilistic grammars. By leveraging the flexibility of two grammar formalisms, Context-Free Grammars and Linear Context-Free Rewriting Systems, our models create desirable structures for forum threads: our topic segmentation is hierarchical, links non-adjacent segments on the same topic, and jointly labels the topic during segmentation. We show that our models outperform a number of tree generation baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Online forums are commonplace today and used for various purposes: product support and trou- bleshooting, opining about events and people, and student interaction on online course platforms. Threads in these forums become long, involve posts from multiple users, and the chronological order of the posts in a thread does not represent a continuous flow of dialog. Adding structure to these threads is important for tasks such as infor- mation extraction, search, and summarization.</p><p>One such aspect of structure is topic. <ref type="figure">Figure 1</ref> shows a computer-troubleshooting related thread with six posts. The first post is the troubleshoot- ing question and the remaining posts can be seen as focusing on either of two topics, the driver soft- ware (posts p 1 , p 2 , p 5 ) or the speaker hardware p0 Bob: When I play a recorded video on my camera, it looks and sounds fine. On my computer, it plays at a really fast rate and sounds like Alvin and the Chipmunks! p1 Kate: I'd find and install the latest audio driver. p2 Mary: The motherboard supplies the clocks for audio feedback. So update the audio and motherboard drivers. p3 Chris: Another fine mess in audio is volume and speaker settings. You checked these? p4 Jane: Yes, under speaker settings, look for hardware ac- celeration. Turning it off worked for me. p5 Matt: Audio drivers are at this link. Rather than just audio drivers, I would also just do all drivers. <ref type="table">Table 1</ref>: Example forum thread conversation (p 3 , p 4 ). By categorizing posts into such topics, we can provide a useful division of content in a thread and even across multiple threads. Note that the driver topic is not a contiguous sequence but present in non-adjacent parts, (p 1 , p 2 ) and (p 5 ).</p><p>We tackle the problem of joint topic segmenta- tion and topic labeling of forum threads. Given a thread's posts in chronological order (the order in which they were posted), we create a phrase structure tree indicating how the posts are grouped hierarchically into subtopics and super-topics. In these conversation trees, leaves span entire posts. Each non-terminal identifies the topic character- izing the posts in its span. Topics are concepts or themes which summarize the content of a group of posts. Specifically, a topic is a set of words which frequently co-occur in posts which are similar in content and other conversation regularities.</p><p>Our key insight in this work is to formalize topic structure using probabilistic grammars. We define a base grammar for topic structure of fo- rum threads and refine it to represent finer topics and subtrees. We learn to predict trees under our grammar based on two formalisms: Probabilis- tic Context-Free Grammars (PCFG) and Proba- bilistic Linear Context-Free Rewriting Systems (PLCFRS). In the PCFG model, a non-terminal spans a contiguous sequence of posts. In the PLCFRS model, non-terminals are allowed to span discontinuous segments of posts. We lever- age algorithms from probabilistic parsing of nat- ural language sentences and modify them for our domain. We show that our model performs well and sidesteps a number of limitations of prior topic segmentation approaches. In particular:</p><p>• Our models perform joint topic segmentation and topic labeling while most existing models identify unlabeled segments. Labeling topics on segments creates richer annotation, and links non- adjacent segments on the same topic.</p><p>• Our grammar-based probabilistic models have two key benefits. They naturally create tree struc- tures which are considered linguistically suitable for topic segmentation but were difficult to create under previous approaches. Second, the flexibility of grammars such as PLCFRS allow our models to seamlessly learn to produce trees where non- adjacent segments on the same topic are explicitly linked, an issue that was not addressed before.</p><p>We present large-scale experiments on a col- lection of forum threads from the computer- troubleshooting domain. <ref type="bibr">1</ref> We show that our gram- mar models achieve a good balance between iden- tifying when posts should be in the same topic ver- sus a different topic. These grammar models out- perform other tree generation baselines by a sig- nificant margin especially on short threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The ideas in this paper are related to three areas of prior research. Forum thread analysis. Finding structure in fo- rum threads has been previously addressed in two ways. The first is reply structure prediction where a parent post is linked to its children (replies) which were posted later in time ( <ref type="bibr" target="#b4">Cong et al., 2008)</ref>. Reply links are some- times augmented with a dialog act label indicat- ing whether the child post is a question, answer, or confirmation to the parent post ( <ref type="bibr" target="#b16">Kim et al., 2010;</ref><ref type="bibr" target="#b31">Wang et al., 2011</ref>). The second set of methods partition sentences in emails or blog comments into topical clusters and then show salient words per cluster as topic tags ( <ref type="bibr" target="#b13">Joty et al., 2013)</ref>.</p><p>We focus on producing rich hierarchical seg- mentation going beyond clusters which do not contain any cluster-internal structure. We also be-  that topic structure is complementary to di- alog act and reply link annotations. Tasks on fo- rum data such as user expertise <ref type="bibr" target="#b19">(Lui and Baldwin, 2009</ref>) and post quality prediction ( <ref type="bibr" target="#b1">Agichtein et al., 2008)</ref>, and automatic summarization (Nenkova and <ref type="bibr" target="#b25">Bagga, 2003)</ref> can be carried out on a fine- grained level using topic information. Conversation disentanglement. A related prob- lem of clustering utterances is defined specifically for Internet Relay Chat (IRC) and speech conver- sations. In this case, multiple conversations, on different topics, are mixed in and systems extract threads which separate out individual conversa- tions ( <ref type="bibr" target="#b26">Shen et al., 2006;</ref><ref type="bibr" target="#b0">Adams and Martell, 2008;</ref><ref type="bibr" target="#b8">Elsner and Charniak, 2010)</ref>.</p><p>Disentanglement is typically applied for the coarse-level problem of identifying a coherent conversation. In addition, these methods do not create any structure upon the clustered utterances in contrast to the focus of this work. Topic Segmentation. is a task which directly fo- cuses on the topic aspect of text and speech. This task is of greater importance for speech which lacks explicit structure such as paragraphs and sections. Many approaches perform linear seg- mentation where boundaries are inserted in the text or speech to divide it into a flat set of topic segments <ref type="bibr" target="#b11">(Hearst, 1994;</ref><ref type="bibr" target="#b28">Utiyama and Isahara, 2001;</ref><ref type="bibr" target="#b9">Galley et al., 2003;</ref><ref type="bibr" target="#b21">Malioutov and Barzilay, 2006;</ref><ref type="bibr" target="#b6">Eisenstein and Barzilay, 2008)</ref>. Very few methods recursively combine smaller segments into larger ones. Such hierarchical models <ref type="bibr" target="#b7">(Eisenstein, 2009</ref>) have been applied at a coarse level for segmenting very long texts such as books into sections. Other work has focused on linear seg- mentation for documents within the same domain and having a regular structure <ref type="bibr" target="#b2">(Chen et al., 2009;</ref><ref type="bibr" target="#b12">Jeong and Titov, 2010;</ref><ref type="bibr" target="#b5">Du et al., 2015</ref>). These latter approaches rely on three assumptions: that the documents contain a regular set of topics, that these topics are discussed in a fairly regular con- sensus order, and that the same topic does not re- cur in the same document.</p><p>Our models address two deficiencies in these approaches. First, text is commonly understood to have a hierarchical structure ( <ref type="bibr" target="#b10">Grosz and Sidner, 1986)</ref> and our grammar model is an ideal frame- work for this goal. Tree structures also have other advantages, for example, we do not predefine the number of expected topic segments in a conversa- tion tree, a requirement posed by many prior seg-mentation algorithms. The second limitation of prior studies is assuming that topics do not recur in the same document. But linguistic theories al- low for non-adjacent utterances to belong to the same topic segment ( <ref type="bibr" target="#b10">Grosz and Sidner, 1986)</ref> and this fact is empirically true in chat and forum con- versations <ref type="bibr" target="#b8">(Elsner and Charniak, 2010;</ref><ref type="bibr" target="#b31">Wang et al., 2011</ref>). Our models can flexibly handle and link recurring topics within and across threads.</p><p>As a final note, because of the annotations re- quired, most prior work on forums or IRC chats have typically used few hundred threads. We present a heuristically derived large corpus of topic structure on which we evaluate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>Our topic discovery methods are based on two constituency grammar formalisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic Context-Free Grammars</head><p>A PCFG is defined by a 5-tuple G C =(N , T , P , S, D) where N is a set of non-terminal sym- bols, T a set of terminal symbols, and S is the start symbol. P is a set of production rules of the form A → β where A is a non-terminal and β ∈ {N ∪ T } * . D is a function that associates each production rule with a conditional probabil- ity of the form p(A → β|A). This probability in- dicates how often the non-terminal A expands into β. The probabilities of all the rules conditioned on a particular non-terminal should sum to 1.</p><p>The joint probability of a tree T with yield Y , P (T, Y ), is the product of the probabilities of all the productions used to construct T . The parsing problem is to find the treê T which is most likely given the yield Y . ˆ T = arg max T P (T |Y ) = arg max T P (T, Y ).</p><p>Given training trees, we can enumerate the pro- ductions and compute their probabilities using maximum likelihood estimates (MLE):</p><formula xml:id="formula_0">p(A → β|A) = count(A → β) count(A)</formula><p>which is the fraction of the times the non-terminal A expands into β.</p><p>The most likely parse tree can be found using a number of algorithms. In this work, we use the CYK algorithm for PCFGs in Chomsky Nor- mal Form. This algorithm has complexity O(n 3 ) where n is the length of the yield.</p><p>PCFGs do not capture a frequently occurring property of forum threads, discontinuous seg- ments on the same topic. Indirectly however, a PCFG may assign the same non-terminal for each of these segments. To model these discontinuities more directly, we present a second model based on PLCFRS where non-terminals are allowed to span discontinuous yield strings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilistic Linear Context-Free Rewriting Systems</head><p>LCFRS grammars <ref type="bibr" target="#b29">(Vijay-Shanker et al., 1987)</ref> generalize CFGs, where non-terminals can span discontinuous constituents. Formally, the span of an LCFRS non-terminal is a tuple, with size k ≥ 1, of strings, where k is the non-terminal "fan-out". As such, the fan-out of a CFG non- terminal is 1.</p><formula xml:id="formula_1">An LCFRS G L =(N , T , P , S, V )</formula><p>where N is the set of non-terminals, T the terminals and S is the start symbol. A function f : N → N gives the fan-out of each non-terminal. P is the set of productions or otherwise called rewriting rules of the LCFRS. V is a set of variables used to indicate the spans of each non-terminal in these rules. A rewriting rule has the form:</p><formula xml:id="formula_2">A(α 1 , α 2 , . . . , α f (A) ) → A 1 (x 1 1 . . . , x 1 f (A 1 ) ), . . . , A m (x m 1 , . . . , x m f (Am) )</formula><p>Here A,A 1 ,. . .,A m ∈ N . Since there are m non-terminals on the RHS, this rule has rank m.</p><formula xml:id="formula_3">x i j ∈ V for 1 ≤ i ≤ m and 1 ≤ j ≤ f (A i ) indi- cate the f (A i ) discontinuous spans dominated by A i . α i ∈ (T ∪ V ) * , 1 ≤ i ≤ f (A) are the spans of the LHS non-terminal A.</formula><p>A rewriting rule explains how the left-hand side (LHS) non-terminal's span can be com- posed from the yields of the right-hand side (RHS) non-terminals. For example, in the rule A(x 1 x 2 , x 3 ) → B(x 1 )C(x 2 , x 3 ), A and C have fan-out 2, B has fan-out 1. The two spans of A, x 1 x 2 and x 3 , are composed from the spans of B and C. For comparison, the productions of a CFG take the single spans of each non-terminal on the RHS and concatenate them in the same order to yield a single span of the LHS non-terminal.</p><p>A Probabilistic LCFRS (PLCFRS) ( <ref type="bibr" target="#b18">Levy, 2005</ref>) also contains D, a function which assigns conditional probabilities p(A( x) → φ|A( x)) to the rules. The probabilities conditioned on a par- ticular non-terminal and span configuration, A( x) should sum to 1. Given a training corpus, LCFRS rules can be read out and probabilities computed similar to CFG rules.</p><p>To find the most likely parse tree, we use the parsing algorithm proposed by <ref type="bibr" target="#b14">Kallmeyer and Maier (2013)</ref> for binary PLCFRS. The approach uses weighted deduction rules ( <ref type="bibr" target="#b27">Shieber et al., 1995;</ref><ref type="bibr" target="#b24">Nederhof, 2003)</ref>, which specify how to compute a new item from other existing items. Each item is of the form <ref type="bibr">[A, ρ]</ref> where A is a non-terminal and ρ is a vector indicating the spans dominated by A. A weight w is attached to each item which gives the |log| of the Viterbi inside probability of the subtree under that item. A set of goal items specify the form of complete parse trees. By using the Knuth's generalization <ref type="bibr" target="#b17">(Knuth, 1977)</ref> of the shortest paths algorithm, the most likely tree can be found without exhaustive pars- ing as in Viterbi parsing of CFGs. The complexity of parsing is O(n 3k ) where k is the fan-out of the grammar (the maximum fan-out of its rules).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Formulation</head><p>Given a thread consisting of a sequence of posts (p 1 , p 2 , . . . , p n ) in chronological order, the task is to produce a constituency tree with yield (p 1 , p 2 . . . p n ). A leaf in this tree spans an entire post. Non-terminals identify the topic of the posts within their span. Non-terminals at higher levels of the tree represent coarser topics in the conver- sation (the span covered by these nodes contain more posts) than those lower in the tree. The root topic node indicates the overall topic of the thread. Below we define a Context-Free Grammar (CFG) for such trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Grammar for Conversation Trees</head><p>G B is our base grammar which is context-free and has four non-terminals {S, X, T , C}. Each post p in the corpus is a terminal symbol (i.e. a ter- minal symbol is a bag of words). The productions in G B are: S → T X * , X → T X * and T → p.</p><p>G B generates trees with the following structure.</p><p>A root-level topic S characterizes the content of the entire thread. Thread-starting rules are of the form, S → T X * , where X * indicates a sequence of zero or more X non-terminals. T nodes are pre-terminals analogous to part-of-speech tags in the case of syntactic parsing. In our grammar, the T → p rule generates a post in the thread.</p><p>In the thread-starting rules, T generates the first post of the thread which poses the query or com- ment that elicits the rest of the conversation. The X * sequence denotes topic branches, the subtree under each X is assumed to correspond to a dif-</p><formula xml:id="formula_4">S[11] T[3] p0 X[5] T[6] p1 X[8] T[7] p2 X[12] T[2] p3 X[7] T[4] p4 X[5] T[6] p5</formula><p>Figure 1: Example conversation tree for the thread in <ref type="table">Table 1</ref> ferent topic. These X's characterize all but the first post of the thread. The continuation rules, X → T X * , recursively subdivide the X subtrees into topics spanning fewer posts. In each case, the T node on the right-hand side of these rules gener- ates the first post (in terms of posting time) in that subtree. Therefore posts made earlier in time al- ways dominate (in the tree structure) those which come later in the thread. We define the head of a non-terminal as the first post as per the chronolog- ical order of posts in the span of the non-terminal. This grammar does not generate binary trees. We binarize the tree with C nodes to obtain an equivalent grammar in Chomsky Normal Form (CNF) (CNF yields parsing algorithms with lower complexity) 2 : S → T C | T , X → T C | T , T → p and C → X | X X| X C. The C nodes can be collapsed and its daughters attached to the parent of C to revert back to the non-binary tree.</p><p>While this CFG defines the structure of conver- sation trees, by itself this grammar is insufficient for our task. In particular, it contains a single non- terminal of each type (S, X, T , C) and so does not distinguish between topics. We extend this grammar to create G E which has a set of non- terminals corresponding to each non-terminal in G B , these fine-grained non-terminals correspond to different topics. G E is created using latent an- notations ( <ref type="bibr" target="#b22">Matsuzaki et al., 2005</ref>) on the X, T , S and C non-terminals from G B . The resulting</p><formula xml:id="formula_5">non-terminals for G E are S[i], X[j], T [k] and C[l], such that 1 ≤ i ≤ N S , 1 ≤ j ≤ N X , 1 ≤ k ≤ N T , 1 ≤ l ≤ N C .</formula><p>i, j, k and l identify specific topics attached to a particular node type.</p><p>Our output trees are created with G E to depict the topic segmentation of the thread and are non- binary. The binary trees produced by our algo- rithms are converted by collapsing the C. As a result, conversation trees have S[i], X[j] and T <ref type="bibr">[k]</ref> nodes but no C <ref type="bibr">[l]</ref> nodes.</p><p>An example conversation tree for the thread in <ref type="table">Table 1</ref> is shown in <ref type="figure">Figure 1</ref>. At level 1, T [3] de- scribes the topic of the first post while the remain- ing posts are under X <ref type="bibr">[5]</ref> which may indicate a driver topic, and X <ref type="bibr">[12]</ref>, a speaker hardware topic. Note how X <ref type="bibr">[5]</ref> may re-occur in the conversation to accommodate post p 5 on the driver topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised learning framework</head><p>We use a supervised framework for learning the models. We assume that we have training trees according to the base grammar, G B . The follow- ing section describes our data and how we obtain these G B -based trees. In Section 6, we present a method for creating G E -type trees with non- terminal refinements. Estimates of rule probabili- ties from this augmented training corpus are used to develop the parsers for topic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data</head><p>We collected 13,352 computer-troubleshooting related threads from http://forums.cnet. com/. The number of posts per thread varies greatly between 1 and 394, and the average is around 5 posts. We divide these threads into train- ing, development and test sets. The most frequent 100 words from the training set are used as stop- words. After filtering stopwords, a post contains 39 tokens on average and the vocabulary size of our corpus is 81,707. For development and test- ing, we only keep threads with a minimum of 3 posts (so that the problem is non-trivial) and a maximum of 50 posts (due to complexity of pars- ing). We have 9,243 training threads, 2,014 for development, and 2,071 for testing.</p><p>A particular feature of the forums on cnet. com is the explicit reply structure present in the threads. The forum interface elicits these reply re- lationships as users develop a thread. When a user replies in a particular thread, she has to choose (only) one of the earlier posts in the thread (in- cluding the question post) to attach her reply to. In this way, each post is linked to a unique post ear- lier in time in the same thread. This reply structure forms a dependency tree. <ref type="figure">Figure 2</ref> (a) is a possible reply tree for the thread in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Deriving conversation trees</head><p>Next we convert these reply-link trees into phrase- structure conversation trees. We developed a de- terministic conversion method that uses the gen-</p><formula xml:id="formula_6">(a) hp 0 h hp 1 h hp 2 h hp 3 h hp 4 h hp 5 h (b) S T X X p 0 T X T X p 1 T T T X p 2 p 3 p 4 T p 5</formula><p>Figure 2: (a) A reply structure tree for the thread in <ref type="table">Table 1</ref> and (b) the derived conversation tree erative process defined by the base grammar G B .</p><p>The key idea is to track when the conversation branches into sub-topics and when the replies are proceeding within the same topic.</p><p>The algorithm traverses the nodes of the depen- dency tree D in breadth-first order, starting at the root (first) post. We create a root S node in the phrase structure tree H. Then the thread-starting rule from G B , S → T X * , is used to create one T and k X nodes as children of S. The first post p 0 is attached as a child of the T node. k is equal to the number of replies to p 0 (children of p 0 in D). For each of these k X nodes, we instanti- ate a X → T X * rule in H. The k replies of p 0 are attached one each as a child of the T nodes in these rules. Any set of children are always in- stantiated in chronological order. So the span of a non-terminal in H always contains posts in non- decreasing time order. We continue the procedure with the next post from D in the traversal order.</p><p>This procedure converts the reply tree of <ref type="figure">Figure  2</ref> (a) into the conversation tree (b). Note that (a) is a possible reply structure for our example thread in <ref type="table">Table 1</ref>. The conversation tree (b) derived ac- cording to this reply structure has a non-projective structure where p 1 , p 2 and p 5 are linked under one X node (at level 1). Such a tree can be produced by our LCFRS model. The ideal PCFG tree will repeat the topic branch as in <ref type="figure">Figure 1</ref>.</p><p>The derived trees at this stage follow G B and contain only the S, X, T non-terminals (without any latent annotations). This tree is converted into Chomsky Normal Form using C nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discontinuous topic segments</head><p>As in our example above, non-projective edges in the reply structure are rather frequent. Of the total threads in our corpus 14.5% contain a non- projective edge. A thread should have a mini- mum of four posts to have the possibility of non- projective edges. Among the 7,691 threads with at least four posts, the percentage of non-projective trees is even higher, 25%. This finding suggests that in any thread of reasonable size which we wish to summarize or categorize, non-projective edges will be common. Hence a direct approach for addressing discontinuous segments such as our PLCFRS model is important for this domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Parsers for Conversation Trees</head><p>The training data are conversation trees with rules from G B . We refine the non-terminals to cre- ate G E , extract PCFG or PLCFRS rules from the training trees, and build a CYK parser that pre- dicts the most likely tree according to G E .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Refining the non-terminals</head><p>We use a clustering approach, akin to the spectral algorithm of <ref type="bibr" target="#b3">Cohen et al. (2013)</ref> and <ref type="bibr" target="#b23">Narayan and Cohen (2015)</ref>, <ref type="bibr">3</ref> to create finer grained categories corresponding to G B 's non-terminals: S, X, C and T . Each node in each tree in the training data is associated with a feature vector, which is a func- tion of the tree and the anchor node. These vectors are clustered (for each of the non-terminals sepa- rately) and then, each node is annotated with the corresponding cluster. This process gives us the non-terminals</p><formula xml:id="formula_7">S[i], X[j], T [k] and C[l] of G E .</formula><p>The features for a node n l are: depth of n l in the tree, root is at depth 0; maximum depth of the sub- tree under n l ; number of siblings of n l ; number of children of n l ; number of posts in the span of n l ; average length (in terms of tokens) of the posts in the span of n l ; average similarity of the span of n l with the span of n l 's siblings <ref type="bibr">4</ref> ; similarity of n l 's span with the span of its left-most sibling; elapsed time between the first and last posts in n l 's span.</p><p>We use CLUTO toolkit <ref type="bibr" target="#b15">(Karypis, 2002</ref>) to per- form clustering. The algorithm maximizes the pairwise cosine similarity between the feature vectors of nodes within the same cluster. The <ref type="bibr">3</ref> The main difference between our algorithm and the al- gorithm by <ref type="bibr" target="#b23">Narayan and Cohen (2015)</ref> is that we do not de- compose the trees into "inside" trees and "outside" trees, or use a singular value decomposition step before clustering the features. <ref type="bibr">4</ref> The span of n l and that of a sibling are each represented by binary vectors indicating the presence and absence of a term in the span. The similarity value is computing using cosine overlap between the vectors and the average across all siblings is recorded. best number of clusters for the four non-terminal node types are tuned jointly to give the best per- formance on our final topic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learning rule probabilities</head><p>As mentioned previously, each terminal in our grammar is an entire post's text. For the pre- terminal to terminal productions in our grammar</p><formula xml:id="formula_8">T [j] → p i , we compute p(T [j] → p i |T [j])</formula><p>as the probability under a unigram language model L j which is trained on the collection of the posts from the training corpus which are dominated by</p><formula xml:id="formula_9">T [j] nodes. p(T [j] → p i |T [j]) = Np i k=1 L j (w i k ) where w i 1 , w i 2 ...w i Np i</formula><p>are the tokens in post p i . The rest of the production probabilities are learned using MLE on the training trees. In the case of LCFRS rules, the gap information is also obtained during the extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">CYK parsing</head><p>For both PCFG and LCFRS we use CYK style algorithms, as outlined in §3, to obtain the most likely tree. For the more computationally com- plex LCFRS model, we make a number of addi- tions to improve speed. First, we restrict the fan- out of the grammar to 2, i.e. any non-terminal can only span a maximum of two discontinuous segments. 97% of the productions in fact have only non-terminals with fan-out ≤ 2. Second, we use A * search <ref type="bibr" target="#b20">(Maier et al., 2012</ref>) to prioritize our agenda. Last, we reduce the number of items added to the agenda. An item has the form <ref type="bibr">[A, ρ]</ref>, A is a non-terminal and ρ is the spans cov- ered by A. For every span, we only keep the top 5 non-terminal items according to the score. In ad- dition, we only allow spans with a gap of at most 2 since 77% of all gaps (dominated by fan-out ≤ 2) non-terminals are ≤ 2 posts. Moreover, after a certain number of items (10,000) are added to the chart, we only allow the creation of new items which have a contiguous span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Systems for comparison</head><p>We compare our models to two types of systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">STRUCTURE ONLY</head><p>The first type generate tree structures without con- sidering the content of the threads. Right-branching tree (RBT). produces a strictly right branching tree where each post is dominated by the immediately previous (accord- ing to time) post in the thread. It uses the grammar with the rules {S → T X, X → T X, X → T , T → p}. This method does not perform use- ful topic segmentation as it produces only a single topic branch containing all the posts.</p><p>Attach-to-root tree (ART). attaches each post to the root of the tree. The grammar rules are {S → T X 1 ...X n , X → T , T → p}, where n is the number of posts in the thread. This approach assumes each post belongs to a different topic in the thread. In contrast to RBT, ART contains too many topic branches, one per post in the thread.</p><p>Random tree (RAND). mixes decisions to cre- ate a new topic branch or continue in the same branch. The generation process is top down, at each step, the algorithm chooses a certain num- ber of topic branches (Xs) to create (≤ number of posts left to add to the tree). Then, the number of posts under each branch is sampled (such that each branch has at least one post). This process is then recursively done at the new topic branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">STRUCTURE AND CONTENT</head><p>These approaches produce tree structures in- formed by content. We build these parsers by modifying prior models for chat disentanglement and linear topic segmentation of documents.</p><p>Similarity tree (SIM). produces trees by at- taching each post as a child of the most similar of the previous (by time) posts ( . We use cosine similarity between vector represen- tations of two posts in order to compute similar- ity. When the similarity exceeds a threshold value, the post is added under the topic branch of the prior post. Otherwise the post is under a new topic branch attached to the root of the tree. A thresh- old of 0.15 was chosen after tuning on the devel- opment data.</p><p>Cluster tree (CLUS). uses an approach re- lated to chat disentanglement <ref type="bibr" target="#b8">(Elsner and Charniak, 2010</ref>). The posts within each thread are clus- tered separately into k l clusters where k l = l/h depends on the number of posts in the thread, l. h = 6 was chosen by tuning. The posts in each cluster are ordered by time and a right branching tree is created over them. These k l cluster-level trees are then attached as children of a new node to create a thread-level tree. The cluster-trees are ordered left to right in the thread-tree according to the time of the earliest post in each cluster.</p><p>Linear segmentation tree (LSEG). is based on postprocessing the output of a Bayesian linear topic segmentation model <ref type="bibr" target="#b6">(Eisenstein and Barzilay, 2008)</ref>. Each post's content is treated as a sen- tence and a document is created for each thread by appending its posts in their time order. The model is then used to group consecutive sentences into k l segments. For each thread of length l, k l = l/h, h = 6 was chosen by tuning. For each segment, a right branching tree is created and these segment- level trees are made siblings in a thread-level tree. The segments are added left to right in the thread- tree as per their order in the text.</p><p>All STRUCTURE trees contain thread structure but no topic labels. In other words, they have coarse non-terminals (X, T and S) only. The STRUCTURE AND CONTENT trees, LSEG and CLUS contain topics or groups but only at one top level, and further the number and labels of these topics are different per thread. Hence there is no linking across threads. Within a thread, the SIM and CLUS tree can link non-adjacent posts under the same topic. These links are also not available from a LSEG tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation metrics</head><p>To evaluate the topic segmentation, we develop a node-governance based measure. Our score com- pares two conversation trees g and h, where g is the gold-standard tree and h is the hypothesized one. We assume that g and h are in dependency format, reversing the transformation from §5.1.</p><p>We break g (and h) into a set of pairs, for each pair of nodes in the tree (each node is a post in the thread). For each such pair, p and q, we find their least common ancestor, ℓ(p, q|g) (or ℓ(p, q|h). If these nodes are in a governing relation (p domi- nates q or vice versa), then ℓ(p, q) is the dominat- ing node. We then define the following sets and quantities for · ∈ {g, h}:</p><formula xml:id="formula_10">• S 1 (·) = {(p, q, ℓ(p, q)) | ℓ(p, q|·) ∈ {p, q}}. • S 2 (·) = {(p, q, ℓ(p, q)) | ℓ(p, q|·) / ∈ {p, q}}. • n 1 (g, h) = |S 1 (g) ∩ S 1 (h)|. • n 2 (g, h) = |S 2 (g) ∩ S 2 (h)|.</formula><p>s 1 (·) and s 2 (·) are defined as the size of S 1 (·) and S 2 (·), repsectively. Let g 1 , . . . , g n and h 1 , . . . , h n be a corpus of gold-standard conver- sation trees and their corresponding hypothesized conversation trees. Then the evaluation metric we compute is the harmonic mean (Fscore) of the micro-average of the precision for governing (G- p) and non-governing (NG-p) pairs, and recall for governing (G-r) and non-governing (NG-r) pairs. For example, G-p is calculated as</p><formula xml:id="formula_11">G-p = n i=1 n 1 (g i , h i ) n i=1 s 1 (h i )</formula><p>.</p><p>Traditional parsing evaluation measures such as constituency bracketting and dependency attach- ment scores were too local for our purpose. For example, if a long chain of posts is placed in a dif- ferent topic but their local dependencies are main- tained, we only penalize one constituent and one node's parent in the constituency and dependency scores respectively. But the topic segmentation created by this change has several posts placed in the wrong topic branch. Our scores overcome this problem by considering the relationship between all pairs of posts and also dividing the relationship in the pair as governing or non-governing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results and discussion</head><p>We tune the number of latent topic annotations for the non-terminals using grid search on the devel- opment set. The best settings are 40 S, 100 X, 20 C, 80 T clusters for PCFG and 10 S, 5 X, 15 C,</p><formula xml:id="formula_12">40 T for LCFRS.</formula><p>Below we show an example non-projective tree created by our LCFRS parser. The topics are indi- cated with the most frequent 5 words in the match- ing cluster. Here post p 4 though later in posting time is pre- dicted to be on the same topic as p 1 .</p><p>The non-terminals in our trees enable useful topic segmentation and we found that perfor- mance is extremely sensitive to the number of non-terminals of each type S, X, C and T . Cur- rently, we do not have a direct method to evaluate the non-terminals in our tree but we plan to use the information in other applications as an evaluation. <ref type="table" target="#tab_2">Table 2 and 3</ref> shows the segmentation perfor- mance of the models (as percentages). The per- formance varied greatly depending on the length of the threads and hence we show the results sepa- rately for threads with up to 15 posts (SHORT) and those with 16 to 50 posts (LONG). The results are divided into sections based on the subset of test data on which the evaluation is performed. The first section (R1.) is performance on all threads, (R2.) only on the projective threads in the test data, and (R3.) only on the non-projective threads.</p><p>Among the baselines, the Right-branching trees (RBT) or Attaching to the root (ART) have some advantages: the RBT receives 100% recall of the governing pairs and the ART tree has high recall of the non-governing pairs. However, their Fs- cores are 0. Recall that the RBT contains a single topic branch and hence no useful segmentation is done; ART is the other extreme where every post is put in a separate topic branch. RAND is the av- erage performance of 3 randomly generated trees for each thread. This method has a better balance between branching and depth leading to 33.4 Fs- core for SHORT and 21.5 for LONG threads.</p><p>The PCFG and the LCFRS models clearly out- perform these baselines. The Fscore improves up to 15% over RAND on SHORT and LONG threads. The grammar models also consistently outperform SIM systems.</p><p>With regard to CLUS and LSEG, there is a difference in performance between SHORT and LONG threads and based on whether the desired structure was projective or non-projective. On SHORT threads, the grammar models outperform LSEG and CLUS particularly on the projective threads (the LCFRS model has a 22% higher Fs- core). On the longer threads however, the CLUS and LSEG models perform best overall and for non-projective threads. CLUS and LSEG directly model the content similarity of posts while the grammar models make many decisions at level of topic nodes. Remember that the clustering is done per thread in CLUS and LSEG compared to using a common set of topics across all threads. Making such fine-grained similarity comparison appears to be helpful especially for longer threads and even though LSEG does not make non-projective decisions, its accuracy is high on the attachments it makes leading to good performance on non- projective threads too. In future work, we plan to explore how we can combine the advantages of direct similarity with the grammar models.</p><p>Between the two grammar models, the LCFRS model is better than PCFG, even on projective threads, and can produce non-projective trees. Part of this improvement on projective trees could be due to more data being available in the LCFRS model since all the data can be used for training it. <ref type="figure">(1,971 threads, 24,620 post pairs</ref>  For the PCFG model, only the projective data can be used for training. Overall, the LCFRS model is powerful on pro- jective threads and SHORT non-projective threads. Compared to PCFG, the LCFRS model has a num- ber of advantages: we can use more data, can pre- dict non-projective trees. Some of the constraints we imposed on the LCFRS parser, such as restrict- ing the gap degree are likely to have limited the ability of the model to generate more flexible non- projective edges. We believe that as we figure out how to make these parsers faster, we will see even more improvements from the LCFRS models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ex</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G-p G-r NG-p NG-r F R1. On all gold threads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions</head><p>This work represents a first approach to learn dis- course structure of forum threads within an ex- plicit grammar framework. We show that a coarse</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Ex G-p G-r NG-p NG-r F R1. On all gold threads (100 threads, <ref type="bibr">27</ref>    <ref type="table">Table 3</ref>: Results on threads with &gt; 15 posts grammar for structure can be refined using latent annotations to indicate the finer topic differences. Our trees have good segmentation performance and provide useful summaries of the thread con- tent at the non-terminal nodes. A main goal for future work is to incorporate further domain spe- cific constraints on the models to improve parsing speed and at the same time allow more flexible trees. We also plan to evaluate the usefulness of conversation trees in tasks such as predicting if a thread is resolved, and user expertise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R2. On projective gold threads only</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Results on threads with up to 15 posts: 
for the grammar models (PCFG and LCFRS) and 
comparison systems (See Section 7). 'Ex' is per-
centage of fully correct trees and other scores are 
from Section 8. Top two Fscores are in bold. 

</table></figure>

			<note place="foot" n="2"> Any context-free grammar can be converted to an equivalent CNF grammar. Our algorithms support unary rules.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers for their sug-gestions. We also thank Bonnie Webber, Adam Lopez and other members of the Probabilistic Models of Language reading group at the Univer-sity of Edinburgh for helpful discussions. The first author was supported by a Newton International Fellowship (NF120479) from the Royal Society and the British Academy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic detection and extraction in chat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Martell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Semantic Computing</title>
		<meeting>the IEEE International Conference on Semantic Computing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding high-quality content in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM</title>
		<meeting>WSDM</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="183" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Content modeling using latent permutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R K</forename><surname>Branavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="163" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experiments with spectral learning of latent-variable PCFGs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding question-answer pairs from online forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Topic segmentation with an ordering-based topic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2232" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian unsupervised topic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical text segmentation from multi-scale lexical cohesion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT:NAACL</title>
		<meeting>HLT:NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="353" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Disentangling chat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="389" to="409" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discourse segmentation of multiparty conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="562" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention, intentions, and the structure of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="204" />
			<date type="published" when="1986-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-paragraph segmentation of expository text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hearst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised discourse segmentation of documents with inherently parallel structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Short papers</title>
		<meeting>ACL: Short papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="151" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Topic segmentation and labeling in asynchronous conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="573" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven parsing using probabilistic linear context-free rewriting systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kallmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="119" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cluto-a clustering toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<idno>TR-02-017</idno>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Science, University of Minnesota</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Classifying dialogue acts in one-on-one live chats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavedon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="862" to="871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A generalization of dijkstra&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Probabilistic models of word order and syntactic discontinuity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classifying user forum participants: Separating the gurus from the hacks, and other tales of the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Australasian Language Technology Workshop</title>
		<meeting>the 2010 Australasian Language Technology Workshop</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Plcfrs parsing revisited: Restricting the fan-out to two</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaeshammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kallmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms</title>
		<meeting>the 11th International Workshop on Tree Adjoining Grammar and Related Formalisms</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimum cut model for spoken lecture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Malioutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING-ACL</title>
		<meeting>COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic cfg with latent annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuzaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diversity in spectral learning for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weighted deductive parsing and knuth&apos;s algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nederhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="143" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facilitating email thread access by extractive summary generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RANLP</title>
		<meeting>RANLP</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Thread detection in dynamic text message streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR</title>
		<meeting>SIGIR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Principles and implementation of deductive parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schabes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1&amp;2</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A statistical model for domain-independent text segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Characterizing structural descriptions produced by various grammatical formalisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recovering implicit thread structure in newsgroup style conversations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predicting thread discourse structure over technical web forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
