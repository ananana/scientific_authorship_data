<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Neural Relation Extraction with Global Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
							<email>ghfu@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Heilongjiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Neural Relation Extraction with Global Optimization</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1730" to="1740"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural networks have shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective , achieving the best performances on two standard benchmarks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting entities <ref type="bibr" target="#b15">(Florian et al., 2006</ref><ref type="bibr" target="#b16">(Florian et al., , 2010</ref> and relations ( <ref type="bibr" target="#b57">Zhao and Grishman, 2005;</ref><ref type="bibr" target="#b22">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b42">Sun et al., 2011;</ref><ref type="bibr" target="#b35">Plank and Moschitti, 2013</ref>) from unstructured texts have been two central tasks in information extraction <ref type="bibr" target="#b18">(Grishman, 1997;</ref><ref type="bibr" target="#b11">Doddington et al., 2004</ref>). Traditional approaches to relation extraction take entity recog- nition as a predecessor step in a pipeline ( <ref type="bibr" target="#b54">Zelenko et al., 2003;</ref><ref type="bibr" target="#b6">Chan and Roth, 2011)</ref>, predicting re- lations between given entities.</p><p>In recent years, there has been a surge of inter- est in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs ( <ref type="bibr" target="#b29">Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b32">Miwa and Bansal, 2016;</ref>. End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task de- pendencies to be modeled explicitly for entity recognition. As a result, it gives better relation ex- traction accuracies compared to pipelines. <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref> were among the first to use neural networks for end-to-end relation extrac- tion, showing highly promising results. In partic- ular, they used bidirectional LSTM ( <ref type="bibr" target="#b17">Graves et al., 2013</ref>) to learn hidden word representations under a sentential context, and further leveraged tree- structured LSTM <ref type="bibr" target="#b43">(Tai et al., 2015)</ref> to encode syn- tactic information, given the output of a parser. The resulting representations are then used for making local decisions for entity and relation ex- traction incrementally, leading to much improved results compared with the best statistical model ( <ref type="bibr" target="#b29">Li and Ji, 2014)</ref>. This demonstrates the strength of neural representation learning for end-to-end rela- tion extraction.</p><p>On the other hand, <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>'s model is trained locally, without considering struc- tural correspondences between incremental deci- sions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem ( <ref type="bibr" target="#b29">Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Miwa and Sasaki, 2014</ref>). As has been commonly understood, learning local decisions for structured prediction can lead to label bias ( <ref type="bibr" target="#b26">Lafferty et al., 2001</ref>), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a struc- tural neural model for end-to-end relation extrac- tion, following a recent line of efforts on globally optimized models for neural structured prediction ( <ref type="bibr" target="#b60">Zhou et al., 2015;</ref><ref type="bibr" target="#b50">Watanabe and Sumita, 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b51">Wiseman and Rush, 2016)</ref>.</p><p>In particular, we follow <ref type="bibr" target="#b34">Miwa and Sasaki (2014)</ref>, casting the task as an end-to-end table- filling problem. This is different from the action- based method of <ref type="bibr" target="#b29">Li and Ji (2014)</ref>, yet has shown to be more flexible and accurate <ref type="bibr" target="#b34">(Miwa and Sasaki, 2014)</ref>. We take a different approach to representa- tion learning, addressing two potential limitations of <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>.</p><p>First, <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref> rely on exter- nal syntactic parsers for obtaining syntactic in- formation, which is crucial for relation extraction ( <ref type="bibr" target="#b10">Culotta and Sorensen, 2004;</ref><ref type="bibr" target="#b58">Zhou et al., 2005;</ref><ref type="bibr" target="#b3">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b37">Qian et al., 2008)</ref>. However, parsing errors can lead to encoding in- accuracies of tree-LSTMs, thereby hurting rela- tion extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine at- tention parser <ref type="bibr" target="#b12">(Dozat and Manning, 2016)</ref> to aug- ment input representations. Pretrained for parsing, such hidden layers contain rich syntactic informa- tion on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential is- sues caused by incorrect parses.</p><p>Our method is also free from a particular syn- tactic formalism, such as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In con- trast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different <ref type="bibr" target="#b43">(Tai et al., 2015</ref>).</p><p>Second, Miwa and Bansal (2016) did not ex- plicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in sev- eral studies ( <ref type="bibr" target="#b49">Wang and Chang, 2016;</ref>. We take the LSTM-Minus method of <ref type="bibr" target="#b49">Wang and Chang (2016)</ref>, modelling a segment as the dif- ference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words <ref type="bibr" target="#b9">(Cross and Huang, 2016)</ref>.</p><p>Evaluation on two benchmark datasets shows that our method outperforms previous methods of <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, <ref type="bibr" target="#b29">Li and Ji (2014)</ref> and <ref type="bibr" target="#b34">Miwa and Sasaki (2014)</ref>, giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on dis- crete parser outputs. We make our code publicly  <ref type="table">writer Patrick McDowell in Kuwait City  ORG  PER  PER  GPE</ref> ORG-AFF PHYS <ref type="figure">Figure 1</ref>: Relation extraction. The example is chosen from the ACE05 dataset, where ORG, PER and GPE denote organization, person and geo-political entities, respectively; ORG-AFF and PHYS denote organization affiliation and physical relations, respectively.</p><p>available under Apache License 2.0. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Task Definition</head><p>As shown in <ref type="figure">Figure 1</ref>, the goal of relation extrac- tion is to mine relations from raw texts. It consists of two sub-tasks, namely entity detection, which recognizes valid entities, and relation classifica- tion, which determines the relation categories over entity pairs. We follow recent studies and recog- nize entities and relations as one single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Method</head><p>We follow <ref type="bibr" target="#b34">Miwa and Sasaki (2014)</ref> and , treating relation extraction as a table- filling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref> by performing the task end-to-end. Formally, given a sentence w 1 w 2 · · · w n , we maintain a table T n×n , where T (i, j) denotes the relation between w i and w j . When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, In- side, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence ( <ref type="bibr" target="#b29">Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b32">Miwa and Bansal, 2016)</ref>. Only the upper triangular table is necessary for indicating the relations.</p><p>We adopt the close-first left-to-right order <ref type="bibr" target="#b34">(Miwa and Sasaki, 2014</ref>) to map the two- dimensional table into a sequence, in order to fill the table incrementally. As shown in <ref type="figure">Fig- ure 2</ref>, first {T (i, i)} are filled by growing i, and then the sequence {T (i, i + 1)} is filled, and then {T (i, i + 2)}, · · · , {T (i, i + n)} are filled incre- mentally, until the table is fully annotated.</p><p>During the table-filling process, we take two la- bel sets for entity detection (i = j) and relation   <ref type="table">Table-</ref>filling example, where numbers indicate the filling order. classification (i &lt; j), respectively. The labels for entity detection include {B-*, I-*, L-*, O, U-* }, where * denotes the entity type, and the labels for relation classification are { − → * , ← − * , ⊥}, where * de- notes the relation category and ⊥ denotes a NULL relation. <ref type="bibr">2</ref> At each step, given a partially-filled table T , we determine the most suitable label l for the next step using a scoring function:</p><formula xml:id="formula_0">1 B-ORG 9 ⊥ 16 ⊥ 22 ⊥ 27 ⊥ 31 ⊥ 34 ⊥ 36 ⊥ 2 L-ORG 10 ← −−−− − ORG-AFF 17 ⊥ 23 ⊥ 28 ⊥ 32 ⊥ 35 ⊥ 3 U-PER 11 ⊥ 18 ⊥ 24 ⊥ 29 ⊥ 33 ⊥ 4 B-PER 12 ⊥ 19 ⊥ 25 ⊥ 30 ⊥ 5 L-PER 13 ⊥ 20 ⊥ 26 − −− → PHYS 6 O 14 ⊥ 21 ⊥ 7 B-GPE 15 ⊥ 8 L-GPE Figure 2:</formula><formula xml:id="formula_1">score(T, l) = W l h T ,<label>(1)</label></formula><p>where W l is a model parameter and h T is the vec- tor representation of T . Based on the function, we aim to find the best label sequence l 1 · · · l m , where m = n(n+1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>, and the resulting sequence of partially-filled tables is T 0 T 1 · · · T m , where T i = FILL(T i−1 , l i ), and T 0 is an empty table. Differ- ent from previous work, we investigate a structural model that is optimized for the label sequence l 1 · · · l m globally, rather than for each l i locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representation Learning</head><p>At the ith step, we determine the label l i of the next table slot based on the current hypothesis T i−1 . Following <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, we use a neural network to learn the vector representation of T i−1 , and then use Equation 1 to rank candidate next labels. There are two types of input features, including the word sequence w 1 w 2 · · · w n , and the readily filled label sequence l 1 l 2 · · · l i−1 . We build a neural network to represent T i−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Word Representation</head><p>Shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we represent each word w i by a vector h w i using its word form, POS tag and characters. Two different forms of embeddings are used based on the word form, one being obtained by using a randomly initialized look-up table E w , <ref type="bibr">2</ref> We remove the illegal table-filling labels during decod- ing for training and testing. For example, tuned during training and represented by e w , and the other being a pre-trained external word embed- ding from E w , which is fixed and represented by e w . 3 For a POS tag t, its embedding e t is obtained from a look-up table E t similar to E w .</p><formula xml:id="formula_2">T (i, j) must be ⊥ if T (i, i) or T (j, j) equals O.</formula><p>The above two components have also been used by <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>. We further enhance the word representation by using its character se- quence ( <ref type="bibr" target="#b27">Lample et al., 2016)</ref>, taking a convolution neural network (CNN) to derive a character-based word representation h char , which has been demonstrated effective for several NLP tasks <ref type="bibr" target="#b41">(dos Santos and Gatti, 2014)</ref>. We obtain the final h w i based on a non-linear feed- forward layer on e w ⊕ e w ⊕ e t ⊕ h char , where ⊕ denotes concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Label Representation</head><p>In addition to the word sequence, the history la- bel sequence l 1 l 2 · · · l i−1 , and especially the la- bels representing detected entities, are also use- ful disambiguation. For example, the previous en- tity boundary label can be helpful to deciding the boundary label of the current word. During re- lation classification, the types of the entities in- volved can indicate the relation category between them. We exploit the diagonal label sequence of partial table T , which denotes entity boundaries, to enhance the representation learning. A word's entity boundary label embedding e l is obtained by using a randomly initialized looking-up table E l .</p><formula xml:id="formula_3">h i−1 ...... h i ...... h j ...... h seg = h j − h i−1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">LSTM Features</head><p>We follow Miwa and Bansal <ref type="formula" target="#formula_1">(2016)</ref> </p><formula xml:id="formula_4">2 · · · w n , − −−− → LSTM w gives h w,→ 1 h w,→ 2 · · · h w,→ n .</formula><p>Different from <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, who use the output hidden vectors {h i } of LSTMs to represent words, we exploit segment representa- tions as well. In particular, for a segment of text <ref type="bibr">[i, j]</ref>, the representation is computed by using LSTM-Minus ( <ref type="bibr" target="#b49">Wang and Chang, 2016)</ref>, shown by <ref type="figure" target="#fig_3">Figure 4</ref>, where h j − h i−1 in a left-to-right LSTM and h i − h j+1 in a right-to-left LSTM are used to represent the segment <ref type="bibr">[i, j]</ref>. The segment rep- resentations can reflect entities in a sentence, and thus can be potentially useful for both entity de- tection and relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">Feature Representation</head><p>We use separate feature representations for entity detection and relation classification, both of which are extracted from the above three LSTM struc- tures. In particular, we first extract a set of base neural features, and then concatenate them and feed them into a non-linear neural layer for entity detection and relation classification, respectively. <ref type="figure" target="#fig_5">Figure 5</ref> shows the overall representation.</p><p>[Entity Detection] , where j denotes the start position of the previ- ous entity. <ref type="bibr">4</ref> The segment features are computed dynamically from the partial outputs of entity de- tection, according to the boundaries of the lastly-  (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the en-</p><formula xml:id="formula_5">h T − −−− → LSTM w ← −−− − LSTM w − −−− → LSTM e ...... j − 1 j ...... i − 1 i i + 1 ......</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Encoder LAS S-LSTM <ref type="formula" target="#formula_1">(2015)</ref> 1-Layer LSTM 90.9 K&amp;G <ref type="formula" target="#formula_1">(2016)</ref> 2-Layer Bi-LSTM 91.9 D&amp;M <ref type="bibr">(2016)</ref> 4-Layer Bi-LSTM 93.8 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.5">Syntactic Features</head><p>Previous work has shown that syntactic features are useful for relation extraction ( <ref type="bibr" target="#b58">Zhou et al., 2005</ref>). For example, the shortest dependency path has been used by several relation extraction models ( <ref type="bibr" target="#b3">Bunescu and Mooney, 2005;</ref><ref type="bibr" target="#b32">Miwa and Bansal, 2016)</ref>. Here we propose a novel method to integrate syntax, without need for prior knowl- edge on concrete syntactic structures.</p><p>In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models ( <ref type="bibr" target="#b4">Buys and Blunsom, 2015;</ref><ref type="bibr" target="#b24">Kiperwasser and Goldberg, 2016)</ref>, where the encoder represents the syn- tactic features of the input sentences. For exam- ple, LSTM hidden states over the input word/tag sequences has been used frequently as syntac- tic features <ref type="bibr" target="#b24">(Kiperwasser and Goldberg, 2016)</ref>. Such features represent input words with syntac- tic information. The parser decoder also leverages partially-parsed results, such as features from par- tial syntactic trees, although we do not use explicit output features. <ref type="table" target="#tab_2">Table 1</ref> shows the encoder struc- tures of three state-of-the-art dependency parsers.</p><p>Our method is to leverage trained syntactic parsers, dumping the encoder feature represen- tations given our inputs, using them directly as part of input embeddings in our proposed model. Denoting the dumped syntactic features on each word as h In this paper, we exploit the parser of <ref type="bibr" target="#b12">Dozat and Manning (2016)</ref>, since it achieves the current best performance for dependency parsing. Our method can be easily generalized to other parsers, which are potentially useful for our task as well. For ex- ample, we can use a constituent parser in the same way by dumping the implicit encoder features.</p><p>Our exploration of syntactic features has two main advantages over the method of <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, where dependency path LSTMs are used for relation classification. On the one hand, incorrect dependency paths between entity pairs can propagate to relation classification in <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, because these paths rely on ex- plicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by us- ing our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic en- tity detection outputs. When beam search is ex- ploited during decoding, increasing number of de- pendency paths can be used by a surge of entity pairs from beam outputs.</p><p>Our method can be extended into neural stack- ing <ref type="bibr" target="#b48">Wang et al. (2017)</ref>, by doing back-propagation training of the parser parameters during model training, which are leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Local Optimization</head><p>Previous work <ref type="bibr" target="#b32">(Miwa and Bansal, 2016;</ref> trains model parameters by model- ing each step for labeling one input sentence sepa- rately. Given a partial table T , its neural represen- tation h T is first obtained, and then compute the next label scores {l 1 , l 2 , · · · , l s } using Equation 1. The output scores are regularized into a probabil- ity distribution {p l 1 , p l 2 , · · · , p ls } by using a soft- max layer. The training objective is to minimize the cross-entropy loss between this output distri- bution with the gold-standard distribution:</p><formula xml:id="formula_6">loss(T, l g i , Θ) = − log p l g i ,<label>(2)</label></formula><p>where l g i is the gold-standard next label for T , and Θ is the set of all model parameters. We refer this training method as local optimization, because it maximizes the score of the gold-standard label at each step locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Beam-search.</head><p>agenda ← { (empty table, score=0.0) } for i in 1 · · · max-step next scored tables ← { } for scored table in agenda labels ← NEXTLABELS <ref type="table">(scored table)</ref> for next label in labels new ← FILL(scored table, next label) ADDITEM(next scored tables, new) agenda ← TOP-B(next scored tables, B)</p><p>During the decoding phase, the greedy search strategy is applied in consistence with the train- ing. At each step, we find the highest-scored label based on the current partial table, before going on to the next step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Global Optimization</head><p>We exploit the global optimization strategy of <ref type="bibr" target="#b60">Zhou et al. (2015)</ref> and <ref type="bibr" target="#b0">Andor et al. (2016)</ref>, max- imizing the cumulative score of the gold-standard label sequence for one sentence as a unit. Global optimization has achieved success for several NLP tasks under the neural setting ( <ref type="bibr" target="#b60">Zhou et al., 2015;</ref><ref type="bibr" target="#b50">Watanabe and Sumita, 2015)</ref>. For relation extrac- tion, global learning gives the best performances under the discrete setting ( <ref type="bibr" target="#b29">Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Miwa and Sasaki, 2014</ref>). We study such models here for neural network models.</p><p>Given a label sequence of l 1 l 2 · · · l i , the score of T i is defined as follows:</p><formula xml:id="formula_7">score(T i ) = i j=0 score(T j−1 , l j ) = score(T i−1 ) + score(T i−1 , l i ),<label>(3)</label></formula><p>where score(T 0 ) = 0 and score(T i−1 , l i ) is com- puted by Equation 1. By this definition, we maxi- mize the scores of all gold-standard partial tables.</p><p>Again cross-entropy loss is used to perform model updates. At each step i, the objective func- tion is defined by:</p><formula xml:id="formula_8">loss(x, T g i , Θ) = − log p T g i = − log score(T g i ) T i score(T i ) ,<label>(4)</label></formula><p>where x denotes the input sentence, T g i denotes the gold-standard state at step i, and T i are all par- tial tables that can be reached at step i.</p><p>The major challenge is to compute p T g i , be- cause we cannot traverse all partial tables that are valid at step i, since their count increases expo- nentially by the step number. We follow <ref type="bibr" target="#b0">Andor et al. (2016)</ref>, approximating the probability by us- ing beam search and early-update.</p><p>Shown in Algorithm 1, we use standard beam search, maintaining the B highest-scored partially-filled tables in an agenda at each step. When each action of table filling is taken, all hy- potheses in the agenda are expanded by enumer- ating the next labels, and the B highest-scored re- sulting tables are used to replace the agenda for the next step. Search begins with the agenda contain- ing an empty table, and finishes when all cells of the tables in the agenda have been filled. When the beam size is 1, the algorithm is the same as greedy decoding. When the beam size is larger than 1, however, error propagation is alleviated. For train- ing, the same beam search algorithm is applied to training examples, and early-update ( <ref type="bibr" target="#b8">Collins and Roark, 2004</ref>) is used to fix search errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data and Evaluation</head><p>We evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04), respectively. The ACE05 dataset defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 dataset defines four entity types and five relation categories.</p><p>For the ACE05 dataset, we follow <ref type="bibr" target="#b29">Li and Ji (2014)</ref> and <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, splitting and preprocessing the dataset into training, develop- ment and test sets. <ref type="bibr">5</ref> For the CONLL04 dataset, we follow <ref type="bibr" target="#b34">Miwa and Sasaki (2014)</ref> to split the data into training and test corpora, and then divide 10% of the training corpus for development.</p><p>We use the micro F1-measure as the major met- ric to evaluate model performances, treating an en- tity as correct when its head region and type are both correct, <ref type="bibr">6</ref> and regard a relation as correct when the argument entities and the relation category are all correct. We exploit pairwise t-test for measur- ing significance values.  <ref type="table">Table 3</ref>: Feature ablation tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Structure</head><note type="other">Size Word Embedding 200 Tag Embedding 50 Char Embedding 50 Entity Label Embedding 50 Input/Output of Word LSTMs 250 Input/Output of Entity Label LSTMs 100 Table Representation 300</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Tuning</head><p>We update all model parameters by back propa- gation using Adam ( <ref type="bibr" target="#b23">Kingma and Ba, 2014</ref>) with a learning rate 10 −3 , using gradient clipping by a max norm 10 and l 2 -regularization by a pa- rameter 10 −5 . The dimension sizes of various vectors in neural network structure are shown in <ref type="table" target="#tab_3">Table 2</ref>. All the hyper-parameters are tuned by development experiments. All experiments are conducted using gcc version 4.9.4 (Ubuntu 4.9.4- 2ubuntu1 14.04.1), on an Intel(R) Xeon(R) CPU E5-2670 @ 2.60GHz.</p><p>Online training is used to learn parameters, traversing over the entire training examples by 300 iterations. We select the best iteration number ac- cording to the development results. In particu- lar, we exploit pre-training techniques <ref type="bibr" target="#b51">(Wiseman and Rush, 2016</ref>) to learn better model parameters. For the local model, we follow <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>, training parameters only for entity detec- tion during the first 20 iterations. For the global model, we pretrain our model using local opti- mization for 40 iterations, before conducting beam global optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Development Experiments</head><p>We conduct several development experiments on the ACE05 development dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Feature Ablation Tests</head><p>We consider the baseline system with no syntac- tic features using local training. Compared with Miwa and Bansal (2016), we introduce character- level features, and in addition exploit segmental</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Beam Relation F1 Speed Local 1 50.9 95. <ref type="table" target="#tab_2">6  Local(+SS)  1  51.2  95.1   Global  1  51.4  95.3  3</ref> 51.8 52.0 5 52.6 36.9 <ref type="table">Table 4</ref>: Comparisons between local and global models, where SS denotes scheduled sampling, and speed is measured by the number of sentences per second.</p><p>features for entity detection. Feature ablation ex- periments are conducted for the two types of fea- tures. <ref type="table">Table 3</ref> shows the experimental results, which demonstrate that the character-level fea- tures and the segment features we use are both use- ful for relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Local v.s. Global Training</head><p>We study the influence of training strategies for re- lation extraction without using syntactic features. For the local model, we apply scheduled sampling ( <ref type="bibr" target="#b2">Bengio et al., 2015)</ref>, which has been shown to improve the performance of relation extraction by <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref>. <ref type="table">Table 4</ref> shows the results. Scheduled sampling achieves improved F-measure scores for the local model. With the same greedy search strategy, the globally normalized model gives slightly better re- sults than the local model with scheduled sam- pling. The performance of the global model in- creases with a larger beam size. When beam size 5 is exploited, we obtain a further gain of 1.2% on the relation F-measure, which is significantly better than our baseline local model with sched- uled sampling (p ≈ 10 −4 ). However, the decoding speed becomes intolerably slow when the beam size increases beyond 5. Thus we exploit a beam size of 5 for global training considering both per- formance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Syntactic Features</head><p>We examine the effectiveness of the proposed im- plicit syntactic features. <ref type="table" target="#tab_4">Table 5</ref> shows the devel- opment results using both local and global opti- mization. The proposed features improve the rela- tion performances significantly under both settings (p &lt; 10 −4 ), demonstrating that our use of syntac- tic features is highly effective.</p><p>We also compare our feature integration method with the traditional methods based on syntactic</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Features Entity F1 Relation F1</head><p>Local all 81.6 53.0 -syn 81.5 50.9</p><p>Global all 81.9 54.2 -syn 81.6 52.6   <ref type="formula" target="#formula_1">(2016)</ref> to extract neural features, and then exploit a non- linear feed-forward neural network to combine the two features. Similarly, we extract segment fea- tures but by using max pooling instead over the sequential outputs of the feed-forward layer, since the vector minus is nonsense here. The final rela- tion results are 53.1% and 53.9% for the local and global models, respectively, which have no signif- icantly differences compared with our models. On the other hand, our method is relatively more effi- cient, and flexible to the grammar formalism.  <ref type="figure">Figure 7</ref>: F-scores with respect to the distance be- tween entity pairs. than statistical models, and global optimization can give improved performances as well. Our fi- nal model achieves the best performances on both datasets. Compared with the best reported re- sults, our model gives improvements of 1.9% on ACE05, and 6.8% on CONLL04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Final Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis</head><p>We conduct analysis on the ACE05 test dataset in order to better understand our models, on its two major contributions, first examining the influences of global optimization, and then studying the gains by using the proposed syntactic features.</p><p>Intuitively global optimization should give bet- ter accuracies at the sentence level. We verify this by examining the sentence-level accuracies, where one sentence is regarded as correct when all the labels in the resulted table are correct. <ref type="figure">Fig- ure 6</ref> shows the result, which is consistent with our intuition. The sentence-level accuracies of the globally normalized model are consistently better than the local model. In addition, the accuracy de- creases sharply as the sentence length increases, with the local model suffering more severely from larger sentences.</p><p>To understand the effectiveness of the proposed syntactic features, we examine the relation F- scores with respect to entity distances. <ref type="bibr" target="#b32">Miwa and Bansal (2016)</ref> exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential dis-tance, thus facilitating relation extraction. We ver- ify whether the proposed syntactic features can benefit our model similarly. As shown in <ref type="figure">Fig- ure 7</ref>, the F-scores of entity-pairs with large dis- tances see apparent improvements, demonstrating that our use of syntactic features has a similar ef- fect compared to the shortest dependency path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Entity recognition <ref type="bibr" target="#b14">(Florian et al., 2004</ref><ref type="bibr" target="#b15">(Florian et al., , 2006</ref><ref type="bibr" target="#b38">Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b16">Florian et al., 2010;</ref><ref type="bibr" target="#b25">Kuru et al., 2016</ref>) and relation extraction <ref type="bibr" target="#b57">(Zhao and Grishman, 2005;</ref><ref type="bibr" target="#b22">Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b59">Zhou et al., 2007;</ref><ref type="bibr" target="#b36">Qian and Zhou, 2010;</ref><ref type="bibr" target="#b5">Chan and Roth, 2010;</ref><ref type="bibr" target="#b42">Sun et al., 2011;</ref><ref type="bibr" target="#b35">Plank and Moschitti, 2013;</ref><ref type="bibr" target="#b46">Verga et al., 2016</ref>) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given ( <ref type="bibr" target="#b54">Zelenko et al., 2003;</ref><ref type="bibr" target="#b33">Miwa et al., 2009;</ref><ref type="bibr" target="#b6">Chan and Roth, 2011;</ref><ref type="bibr" target="#b30">Lin et al., 2016)</ref>.</p><p>Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models <ref type="bibr" target="#b21">(Ji and Grishman, 2005;</ref><ref type="bibr">Yih, 2004, 2007)</ref>. Recent work shows that joint learning and decod- ing with a single model brings more benefits for the two tasks ( <ref type="bibr" target="#b29">Li and Ji, 2014;</ref><ref type="bibr" target="#b34">Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b32">Miwa and Bansal, 2016;</ref>, and we follow this line of work in the study.</p><p>LSTM features have been extensively exploited for NLP tasks, including tagging ( <ref type="bibr" target="#b27">Lample et al., 2016)</ref>, parsing <ref type="bibr" target="#b24">(Kiperwasser and Goldberg, 2016;</ref><ref type="bibr" target="#b12">Dozat and Manning, 2016)</ref>, relation classification ( <ref type="bibr" target="#b47">Vu et al., 2016;</ref><ref type="bibr" target="#b32">Miwa and Bansal, 2016)</ref> and sentiment analysis ( ). Based on the output of LSTM structures, <ref type="bibr" target="#b49">Wang and Chang (2016)</ref> introduce segment features, and ap- ply it to dependency parsing. The same method is applied to constituent parsing by <ref type="bibr" target="#b9">Cross and Huang (2016)</ref>. We exploit this segmental representation for relation extraction.</p><p>Global optimization and normalization has been successfully applied on many NLP tasks that in- volve structural prediction ( <ref type="bibr" target="#b26">Lafferty et al., 2001;</ref><ref type="bibr" target="#b7">Collins, 2002;</ref><ref type="bibr" target="#b31">McDonald et al., 2010;</ref><ref type="bibr" target="#b56">Zhang and Clark, 2011</ref>), using traditional discrete features. For neural models, it has recently received increas- ing interests ( <ref type="bibr" target="#b60">Zhou et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016;</ref><ref type="bibr" target="#b52">Xu, 2016;</ref><ref type="bibr" target="#b51">Wiseman and Rush, 2016)</ref>, and im- proved performances can be achieved with global optimization accompanied by beam search. Our work is in line with these efforts. To our knowl- edge, we are the first to apply globally optimized neural models for end-to-end relation extraction, achieving the best results on standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by <ref type="bibr" target="#b34">Miwa and Sasaki (2014)</ref>. Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to in- tegrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Associated Press</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Word representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Segment representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, learn- ing global context representations using LSTMs. Three basic LSTM structures are used: a left- to-right word LSTM ( − −−− → LSTM w ), a right-to-left word LSTM ( ← −−− − LSTM w ) and a left-to-right entity boundary label LSTM ( − −−− → LSTM e ). Each LSTM derives a sequence of hidden vectors for inputs. For example, for w 1 w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 (</head><label>5</label><figDesc>a) shows the fea- ture representation for the entity detection. First, we extract six feature vectors from the three basic LSTMs, three of which are word features, namely h w,→ i , h w,← i and h e,→ i−1 , and the remaining are seg- ment features, namely h w,→ [j,i−1] , h w,← [j,i−1] and h e,→ [j,i−1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Feature representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>n</head><label></label><figDesc>, we feed them into a non- linear neural layer, and then generate two LSTMs (bi-directional) based on the outputs, namely − −−− → LSTM syn and ← −−− − LSTM syn , respectively, augment- ing the original three LSTMs into five LSTMs. Features are extracted from the two new LSTMs in the same way as from the basic bi-directional word LSTMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Encoder structures and performances of 
three state-of-the-art dependency parsers, where 
S-LSTM (2015) refers to Dyer et al. (2015), K&amp;G 
(2016) refers to the best parser of Kiperwasser and 
Goldberg (2016), D&amp;M (2016) refers to Dozat 
and Manning (2016), and LAS (labeled attach-
ment score) is the major evaluation metric. 

tity label LSTM, we only use the segment features 
of entity i and entity j . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Dimension sizes.</head><label>2</label><figDesc></figDesc><table>Model 
Entity F1 Relation F1 
baseline 
81.5 
50.9 
-character 
80.9 
50.2 
-segment 
80.2 
49.8 
(entity detection) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : The influence of syntactic features.</head><label>5</label><figDesc></figDesc><table>model 
ACE05 
CONLL04 
Entity Relation Entity Relation 
Our Model 83.6 
57.5 
85.6 
67.8 
M&amp;B (2016) 83.4 
55.6 
-
-
L&amp;J (2014) 80.8 
49.5 
-
-
M&amp;S (2014) -
-
80.7 
61.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Final results on the test datasets. 

outputs which Miwa and Bansal (2016) and all 
previous methods use. We use the same parser 
of Dozat and Manning (2016), building features 
on its dependency outputs. We exploit the bi-
directional tree LSTM of Teng and Zhang </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 shows</head><label>6</label><figDesc></figDesc><table>the final results on the test datasets 
of ACE05 and CONLL04. We show several top-
performing systems in the table as well, where 
M&amp;B (2016) refers to Miwa and Bansal (2016), 
who exploit end-to-end LSTM neural networks 
with local optimization, and L&amp;J (2014) and M&amp;S 
(2014) refer to Li and Ji (2014) and Miwa and 
Sasaki (2014), respectively, which are both glob-
ally optimized models using discrete features, giv-
ing the top F-scores among statistical models. 7 
Overall, neural models give better performances </table></figure>

			<note place="foot" n="1"> https://github.com/zhangmeishan/NNRelationExtraction</note>

			<note place="foot" n="3"> We use the set of pre-trained glove word embeddings available at http://nlp.stanford.edu/data/glove.6B.zip as external word embeddings.</note>

			<note place="foot" n="4"> The non-entity word is treated as a special unit entity to extract segmental features.</note>

			<note place="foot" n="5"> https://github.com/tticoin/LSTM-ER/. 6 For the ACE05 dataset, the head region is defined by the corpus, and for the CONLL04 dataset, the head region covers the entire scope of an entity.</note>

			<note place="foot" n="7"> Gupta et al. (2016) proposed a locally optimized model but used a different test dataset from CONLL04 and a different evaluation method, reporting entity and relation F-scores of 93.6% and 72.1%, respectively. Their results are not directly comparable to the results in Table 6. In particular, they regard an entity as correct if at least one token is tagged correctly, which influences the results significantly since multiword entities accounts for over 50% of all entities.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their con-structive comments, which help to improve the paper, and Zhiyang Teng for dumping intermedi-ate outputs from the bi-affine parser. This work is supported by National Natural Science Foun-dation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Hei-longjiang Province (China) grant F2016036, Spe-cial business expenses in Heilongjiang Province (China) grant 2016-KYYWF-0183, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design. Yue Zhang is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="page" from="2442" to="2452" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative incremental dependency parsing with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="863" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting background knowledge for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="152" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting syntactico-semantic structures for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Seng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="551" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incremental parsing with the perceptron algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependency tree kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="423" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The automatic content extraction (ace) program-tasks, data, and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>George R Doddington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Przybocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lance A Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<title level="m">Deep biaffine attention for neural dependency parsing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A statistical model for multilingual entity detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorizing complex models: A case study in mention detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING/ACL</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
	<note>Nanda Kambhatla, and Imed Zitouni</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving mention detection robustness to noisy input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pitrelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="335" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Information extraction a multidisciplinary approach to an emerging information technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="10" to="27" />
		</imprint>
	</monogr>
	<note>Information extraction: Techniques and challenges</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Table filling multi-task recurrent neural network for joint entity and relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Andrassy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving name tagging by reference resolution and relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simple and accurate dependency parsing using bidirectional lstm feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Charner: Character-level named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Kuru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Ozan Arkan Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="911" to="921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When are tree structures necessary for deep learning of representations?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed training strategies for the structured perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="456" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A rich feature vector for protein-protein interaction extraction from multiple corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rune</forename><surname>Saetre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Sasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1858" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Clusteringbased stratified seed sampling for semi-supervised relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="346" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting constituent dependencies for tree kernel-based semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
	<note>Qiaoming Zhu, and Peide Qian</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Global inference for entity and relation identification via a linear programming formulation. Introduction to statistical relational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="553" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Cicero Dos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014</title>
		<meeting>COLING 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semi-supervised relation extraction with large-scale word clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="521" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context-sensitive lexicon features for neural sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP</title>
		<meeting>the EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1629" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Bidirectional tree-structured lstm with head lexicalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06788</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 NAACL</title>
		<meeting>the 2016 NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Combining recurrent and convolutional neural networks for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><forename type="middle">Thang</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heike</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL</title>
		<meeting>the NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="534" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Universal dependencies parsing for colloquial singaporean english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Leonard Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Chieu</surname></persName>
		</author>
		<idno>abs/1705.06463</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2306" to="2315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1169" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence learning as beam-search optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1296" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Lstm shift-reduce ccg parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1754" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 EMNLP</title>
		<meeting>the 2015 EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Transition-based neural word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="421" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Syntactic processing using the generalized perceptron and beam search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="151" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Extracting relations with integrated information using kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tree kernel-based relation extraction with context-sensitive structured parse tree information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoming</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="728" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1213" to="1222" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
