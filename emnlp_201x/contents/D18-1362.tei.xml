<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Socher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Salesforce Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3243" to="3253"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3243</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment , the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search <ref type="bibr" target="#b1">(Berant et al., 2013</ref>) and dialogue generation ( <ref type="bibr" target="#b11">He et al., 2017)</ref>. Whether curated au- tomatically or manually, practical KGs often fail to include many relevant facts. A popular ap- proach for modeling incomplete KGs is knowl- edge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vec- tors ( <ref type="bibr" target="#b31">Yang et al., 2014;</ref><ref type="bibr" target="#b5">Dettmers et al., 2018)</ref>. Embedding based approaches ignore the sym- bolic compositionality of KG relations, which limit their application in more complex rea- soning tasks. An alternative solution for KG reasoning is to infer missing facts by synthe- sizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable. Early work treats it as a link prediction prob- lem and perform maximum-likelihood classifica- tion over either discrete path features <ref type="bibr" target="#b14">(Lao et al., 2011</ref><ref type="bibr" target="#b15">(Lao et al., , 2012</ref><ref type="bibr" target="#b6">Gardner et al., 2013)</ref> or their hidden representations in a vector space ( <ref type="bibr" target="#b8">Guu et al., 2015;</ref><ref type="bibr" target="#b27">Toutanova et al., 2016;</ref><ref type="bibr" target="#b16">McCallum et al., 2017)</ref>.</p><p>More recent work formulates multi-hop reason- ing as a sequential decision problem, and lever- ages reinforcement learning (RL) to perform ef- fective path search ( <ref type="bibr" target="#b4">Das et al., 2018;</ref><ref type="bibr" target="#b23">Shen et al., 2018;</ref>. In par- ticular, MINERVA ( <ref type="bibr" target="#b4">Das et al., 2018</ref>) uses the RE- INFORCE algorithm <ref type="bibr" target="#b29">(Williams, 1992)</ref> to train an end-to-end model for multi-hop KG query answer- ing: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths.  <ref type="bibr" target="#b13">(Kok and Domingos, 2007)</ref>.</p><p>We refer to the RL formulation adopted by MINERVA as "learning to walk towards the an- swer" or "walk-based query-answering (QA)". Walk-based QA eliminates the need to pre- compute path features, yet this setup poses sev- eral challenges for training. First, because prac- tical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative tar- gets, <ref type="figure" target="#fig_1">Figure 2</ref>). Second, since no ground truth path is available for training, the agent may tra- verse spurious paths that lead to a correct answer only incidentally (false positive paths). Because REINFORCE <ref type="bibr" target="#b29">(Williams, 1992</ref>) is an on-policy RL algorithm <ref type="bibr" target="#b25">(Sutton and Barto, 1998</ref>) which encour- ages past actions with high reward, it can bias the policy toward spurious paths found early in train- ing ( <ref type="bibr" target="#b9">Guu et al., 2017)</ref>.</p><p>We propose two modeling advances for RL ap- proaches in the walk-based QA framework to ad- dress the aforementioned problems. First, in- stead of using a binary reward based on whether the agent has reached a correct answer or not, we adopt pre-trained state-of-the-art embedding- based models <ref type="bibr" target="#b5">(Dettmers et al., 2018;</ref><ref type="bibr" target="#b28">Trouillon et al., 2016</ref>) to estimate a soft reward for target entities whose correctness cannot be determined. As embedding-based models capture link seman- tics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained model. Sec- ond, we perform action dropout which randomly blocks some outgoing edges of the agent at each training step so as to enforce effective exploration of a diverse set of paths and dilute the negative im- pact of the spurious ones. Empirically, our over- all model significantly improves over state-of-the- art multi-hop reasoning approaches on four out of five benchmark KG datasets <ref type="bibr">(UMLS, Kinship, FB15k-237, WN18RR)</ref>. It is also the first path- based model that achieves consistently compara- ble or better performance than embedding-based models. We perform a thorough ablation study and result analysis, demonstrating the effect of each modeling innovation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>In this section, we first review the walk-based QA framework ( §2.2) and the on-policy reinforcement learning approach proposed by <ref type="bibr" target="#b4">Das et al. (2018)</ref> ( §2.3, §2.4). Then we describe our proposed so- lutions to the false negative reward and spurious path problems: knowledge-based reward shaping ( §2.5) and action dropout ( §2.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Formal Problem Definition</head><p>We formally represent a knowledge graph as G = (E, R), where E is the set of entities and R is the set of relations. Each directed link in the knowl- edge graph l = (e s , r, e o ) ∈ G represents a fact (also called a triple).</p><p>Given a query (e s , r q , ?), where e s is the source entity and r q is the relation of interest, the goal is to perform an efficient search over G and col- lect the set of possible answers E o = {e o } where (e s , r q , e o ) / ∈ G due to incompleteness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning Formulation</head><p>The search can be formulated as a Markov De- cision Process (MDP) <ref type="bibr" target="#b25">(Sutton and Barto, 1998)</ref>: starting from e s , the agent sequentially selects an outgoing edge l and traverses to a new entity until it arrives at a target. Specifically, the MDP consists of the following components ( <ref type="bibr" target="#b4">Das et al., 2018)</ref>.</p><formula xml:id="formula_0">States Each state s t = (e t , (e s , r q )) ∈ S is a</formula><p>tuple where e t is the entity visited at step t and (e s , r q ) are the source entity and query relation. e t can be viewed as the state-dependent information while (e s , r q ) are the global context shared by all states.</p><p>Actions The set of possible actions A t ∈ A at step t consists of the outgoing edges of e t in G, i.e., A t = {(r ′ , e ′ )|(e t , r ′ , e ′ ) ∈ G}. To give the agent the option to terminat a search, a self-loop edge is added to every A t . When search is unrolled for a fixed number of steps T , the self-loop acts similarly to a "stop" action.</p><p>Transition A transition function δ : S × A → S is defined by δ(s t , A t ) = δ(e t , (e s , r q ), A t ). In walk-based QA, the transition is determined by G.</p><p>Rewards In the default formulation, the agent receives a terminal reward of 1 if it arrives at a correct target entity when search ends and 0 other- wise.</p><formula xml:id="formula_1">R b (s T ) = {(e s , r q , e T ) ∈ G}.<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Policy Network</head><p>The search policy is parameterized using state in- formation and global context, plus the search his- tory ( <ref type="bibr" target="#b4">Das et al., 2018)</ref>. Specifically, every entity and relation in G is assigned a dense vector embedding e ∈ d and r ∈ d . A particular action a t = (r t+1 , e t+1 ) ∈ A t is represented as the concatenation of the re- lation embedding and the end node embedding</p><formula xml:id="formula_2">a t = [r; e ′ t ]. The search history h t = (e s , r 1 , e 1 , .</formula><p>. . , r t , e t ) ∈ H consists of the sequence of actions taken up to step t, and can be encoded using an LSTM:</p><formula xml:id="formula_3">h 0 = LSTM(0, [r 0 ; e s ])<label>(2)</label></formula><formula xml:id="formula_4">h t = LSTM(h t−1 , a t−1 ), t &gt; 0,<label>(3)</label></formula><p>where r 0 is a special start relation introduced to form a start action with e s . The action space A t is encoded by stacking the embeddings of all actions in it: A t ∈ |At|×2d . And the policy network π is defined as:</p><formula xml:id="formula_5">π θ (a t |s t ) = σ(A t × W 2 ReLU(W 1 [e t ; h t ; r q ])),<label>(4)</label></formula><p>where σ is the softmax operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimization</head><p>The policy network is trained by maximizing the expected reward over all queries in G:</p><formula xml:id="formula_6">J(θ) = (es,r,eo)∈G [ a 1 ,...,a T ∼π θ [R(s T |e s , r)]].<label>(5)</label></formula><p>The optimization is done using the REIN- FORCE (Williams, 1992) algorithm, which iter- ates through all (e s , r, e o ) triples in G 1 and updates θ with the following stochastic gradient:</p><formula xml:id="formula_7">∇ θ J(θ) ≈ ∇ θ T t=1</formula><p>R(s T |e s , r) log π θ (a t |s t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(6)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Knowledge-Based Reward Shaping</head><p>According to Equation 1, the agent receives a bi- nary reward based solely on the observed answers in G. However, G is intrinsically incomplete and this approach penalizes the false negative search attempts identically to true negatives. To allevi- ate this problem, we adopt existing KG embedding models designed for the purpose of KG comple- tion ( <ref type="bibr" target="#b28">Trouillon et al., 2016;</ref><ref type="bibr" target="#b5">Dettmers et al., 2018)</ref> to estimate a soft reward for target entities whose correctness is unknown.</p><p>Formally, the embedding models map E and R to a vector space, and estimate the likelihood of each fact l = (e s , r, e t ) ∈ G using f (e s , r, e t ), a composition function of the entity and relation embeddings. f is trained by maximizing the like- lihood of all facts in G. We propose the following reward shaping strategy ( <ref type="bibr" target="#b18">Ng et al., 1999</ref>):</p><formula xml:id="formula_8">R(s T ) = R b (s T ) + (1 − R b (s T ))f (e s , r q , e T ).</formula><p>(7) Namely, if the destination e T is a correct answer according to G, the agent receives reward 1. Oth- erwise the agent receives a fact score estimated by f (e s , r q , e T ), which is pre-trained. Here we keep f in its general form and it can be replaced by any state-of-the-art model ( <ref type="bibr" target="#b28">Trouillon et al., 2016;</ref><ref type="bibr" target="#b5">Dettmers et al., 2018</ref>) or ensemble thereof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Action Dropout</head><p>The REINFORCE training algorithm performs on- policy sampling according to π θ (a t |s t ), and up- dates θ stochastically using Equation 6. Because the agent does not have access to any oracle path, it is possible for it to arrive at a correct answer e o via a path irrelevant to the query relation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the path Obama −endorsedBy→ Mc- Cain −liveIn→ U.S. ←locatedIn− Hawaii does not infer the fact bornIn(Obama, Hawaii).</p><p>Discriminating paths of different qualities is non-trivial, and existing RL approaches for walk- based KGQA largely rely on the terminal reward to bias the search. Since there are usually more spurious paths than correct ones, spurious paths are often found first, and following exploration can be increasingly biased towards them (Equation 6).   <ref type="figure">Figure 3</ref>: Overall training approach. At each time step t, the agent samples an outgoing link according tõ π θ (a t |s t ), which is the stochastic REINFORCE policy π θ (a t |s t ) perturbed by a random binary mask m. The agent receives reward 1 if stopped at an observed answer of the query (e s , r q , ?); otherwise, it receives reward f (e s , r q , e T ) estimated by the reward shaping (RS) network. The RS network is pre-trained and doesn't receive gradient updates.</p><p>Entities with larger fan-in (in-degree) and fan-out (out-degree) often exacerbate this problem. <ref type="bibr" target="#b9">Guu et al. (2017)</ref> identified a similar issue in RL-based semantic parsing with weak supervi- sion, where programs that do not semantically match the user utterance frequently pass the tests. To solve this problem, <ref type="bibr" target="#b9">Guu et al. (2017)</ref> proposed randomized beam search combined with a meri- tocratic update rule to ensure all trajectories that obtain rewards are up-weighted roughly equally.</p><p>Here we propose the action dropout tech- nique which achieves similar effect as randomized search and is simpler to implement over graphs. Action dropout randomly masks some outgoing edges for the agent in the sampling step of REIN- FORCE. The agent then performs sampling 2 ac- cording to the adjusted action distributioñ</p><formula xml:id="formula_9">distributioñ π θ (a t |s t ) ∝ (π θ (a t |s t ) · m + ϵ)<label>(8)</label></formula><formula xml:id="formula_10">m i ∼ Bernoulli(1 − α), i = 1, . . . |A t |,<label>(9)</label></formula><p>where each entry of m ∈ {0, 1} |At| is a binary variable sampled from the Bernoulli distribution with parameter 1 − α. A small value ϵ is used to smooth the distribution in case m = 0, where˜π where˜ where˜π θ (a t |s t ) becomes uniform. Our overall approach is illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>In this section, we summarize the related work and discuss their connections to our approach. <ref type="bibr">2</ref> We only modify the sampling distribution and still use π θ (at|st) to compute the gradient update in equation 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Knowledge Graph Embeddings</head><p>KG embeddings ( <ref type="bibr" target="#b2">Bordes et al., 2013;</ref><ref type="bibr" target="#b24">Socher et al., 2013;</ref><ref type="bibr" target="#b31">Yang et al., 2014;</ref><ref type="bibr" target="#b28">Trouillon et al., 2016;</ref><ref type="bibr" target="#b5">Dettmers et al., 2018</ref>) are one-hop KG modeling approaches which learn a scoring func- tion f (e s , r, e o ) to define a fuzzy truth value of a triple in the embedding space. These mod- els can be adapted for query answering by sim- ply return the e o 's with the highest f (e s , r, e o ) scores. Despite their simplicity, embedding-based models achieved state-of-the-art performance on KGQA ( <ref type="bibr" target="#b4">Das et al., 2018)</ref>. However, such models ignore the symbolic compositionality of KG rela- tions, which limits their usage in more complex reasoning tasks. The reward shaping (RS) strategy we proposed is a step to combine their capabil- ity in modeling triple semantics with the symbolic reasoning capability of the path-based approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Hop Reasoning</head><p>Multi-hop reasoning focus on learning symbolic inference rules from relational paths in the KG and has been formulated as sequential decision prob- lems in recent works ( <ref type="bibr" target="#b4">Das et al., 2018;</ref><ref type="bibr" target="#b23">Shen et al., 2018;</ref>. In par- ticular, DeepPath ( ) first adopted REINFORCE to search for generic representative paths between pairs of entities. DIVA ( ) also performs generic path search between entities using RL and its variational objective can be interpreted as model-based reward assignment. MINERVA ( <ref type="bibr" target="#b4">Das et al., 2018</ref>) first introduced RL to search for answer entities of a particular KG query end-to-end. MINERVA uses entropy reg- ularization to softly encourage the policy to sam- ple diverse paths, and we show that hard action dropout is more effective in this setup. Reinforce- Walk ( <ref type="bibr" target="#b23">Shen et al., 2018</ref>) further proposed to solve the reward sparsity problem in walk-based QA using off-policy learning. ReinforceWalk scores the search targets with a value function which is updated based on the search history cached through epochs. In comparison, we leveraged ex- isting embedding-based models for reward shap- ing, which is much more efficient during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reinforcement Learning</head><p>Recently, RL has seen a variety of applications in NLP including machine translation ( <ref type="bibr" target="#b21">Ranzato et al., 2015</ref>), summarization ( <ref type="bibr" target="#b20">Paulus et al., 2017)</ref>, and se- mantic parsing ( <ref type="bibr" target="#b9">Guu et al., 2017</ref>). Compared to the domain of gaming ( <ref type="bibr" target="#b17">Mnih et al., 2013)</ref> where RL is mostly applied for, RL formulations in NLP often have a large discrete action space. For ex- ample, in machine translation, the space of possi- ble actions is the entire vocabulary of a language. Walk-based QA also suffers from this problem, as some entities may have thousands of neigh- bors (e.g. U.S.). Since often there is no golden path available for a KG reasoning problem, we cannot leverage supervised pre-training to initial- ize the path search following the common practice in RL-based natural language generation ( <ref type="bibr" target="#b21">Ranzato et al., 2015</ref>). On the other hand, the inference paths being studied in a KG are often much shorter (usually containing 2-5 steps) compared to the tar- get sentences in the NL generation problems (of- ten containing 20-30 words), which simplifies the training to some extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Setup</head><p>We evaluate our modeling contributions on five KGs from different domains and exhibiting differ- ent graph properties ( § 4.1). We compare with two classes of state-of-the-art KG models: multi-hop neural symbolic approaches and KG embeddings ( §4.2). In this section, we describe the datasets and our experiment setup in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We adopt five benchmark KG datasets for query answering: <ref type="formula" target="#formula_1">(1)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Beam Search Decoding We perform beam search decoding to obtain a list of unique en- tity predictions. Because multiple paths may lead to the same target entity, we compute the list of unique entities reached in the final search step and assign each of them the maximum score of all paths that led to it. We then output the top-ranked unique entities. We find this approach to improve over directly taking the entities ranked at the beam top, as many of them are repetitions.  test set (and its reversed link) from the train set. Following <ref type="bibr" target="#b4">Das et al. (2018)</ref>, we cut the maximum number of outgoing edges of an entity by thresh- old η to prevent GPU memory overflow: for each entity we keep its top-η neighbors with the highest PageRank scores ( <ref type="bibr" target="#b19">Page et al., 1999</ref>) in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KG Setup</head><p>Hyperparameters We set the entity and relation embedding size to 200 for all models. We use Xavier initialization <ref type="bibr" target="#b7">(Glorot and Bengio, 2010)</ref> for the embeddings and the NN layers. For ConvE, we use the same convolution layer and label smooth- ing hyperparameters as <ref type="bibr" target="#b5">Dettmers et al. (2018)</ref>. For path-based models, we use a three-layer LSTM as the path encoder and set its hidden dimension to 200. We perform grid search on the reasoning path length <ref type="bibr">(2,</ref><ref type="bibr">3)</ref>, the node fan-out threshold η (256- 512) and the action dropout rate α (0.1-0.9). Fol- lowing <ref type="bibr" target="#b4">Das et al. (2018)</ref>, we add an entropy regu- larization term in the objective and tune the weight parameter β within 0-0.1. We use Adam optimiza- tion ( <ref type="bibr" target="#b12">Kingma and Ba, 2014</ref>) and search the learn- ing rate (0.001-0.003) and mini-batch size . <ref type="bibr">4</ref> For all models we apply dropout to the en- tity and relation embeddings and all feed-forward layers, and search the dropout rates within 0-0.5.</p><p>We use a decoding beam size of 512 for NELL- 995 and 128 for the other datasets.</p><p>Evaluation Protocol We convert each triple (e s , r, e o ) in the test set into a query and com- pute ranking-based evaluation metrics. The mod- els take e s , r as the input and output a list of can- didate answers E o = [e 1 , . . . , e L ] ranked in de- creasing order of confidence score. We compute r eo , the rank of e o among E o , after removing the other correct answers from E o and use it to com- pute two types of metrics: <ref type="formula" target="#formula_1">(1)</ref> Hits@k which is the percentage of examples where r eo ≤ k and <ref type="formula" target="#formula_3">(2)</ref> mean reciprocal rank (MRR) which is the mean of 1/r eo for all examples in the test set. We use the entire test set for evaluation, with the exception of NELL-995, where test triples with unseen entities are removed following <ref type="bibr" target="#b4">Das et al. (2018)</ref>. Our Pytorch implementation of all experi- ments is released at https://github.com/ salesforce/MultiHopKG. <ref type="table" target="#tab_3">Table 2</ref> shows the evaluation results of our pro- posed approach and the baselines. The top part presents embedding based approaches and the bottom part presents multi-hop reasoning ap- proaches. <ref type="bibr">5</ref> We find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on UMLS, Kinship, FB15K- 237 and NELL-995 despite their simplicity. While previous path based approaches achieve com- parable performance on some of the datasets (WN18RR, NELL-995, and UMLS), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on Kinship and FB15k-237 respec- tively). A possible reason for this is that embed- ding based methods map every link in the KG into the same embedding space, which implicitly en- codes the connectivity of the whole graph. In con- trast, path based models use the discrete represen-   tation of a KG as input, and therefore have to leave out a significant proportion of the combinatorial path space by selection. For some path based ap- proaches, computation cost is a bottleneck. In par- ticular, NeuralLP and NTP-λ failed to scale to the larger datasets and their results are omitted from the table, as <ref type="bibr" target="#b4">Das et al. (2018)</ref> reported. Ours is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets. The best single model, Ours(ConvE), improves the SOTA performance of path-based models on three datasets <ref type="bibr">(UMLS, Kinship, and FB15k-237)</ref> by 4%, 9%, and 39% respectively. On NELL-995, our approach did not significantly improve over existing SOTA. The NELL-995 dataset consists of only 12 relations in the test set and, as we further detail in the analysis ( § 5.3.3), our approach is less effective for those relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model Comparison</head><p>The model variations using different reward shaping modules perform similarly. While a better reward shaping module typically results in a better overall model, an exception is WN18RR, where ComplEx performs slightly worse on its own but is more helpful for reward shaping. We left the study of the relationship between the reward shap- ing module accuracy and the overall model perfor- mance as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>We perform an ablation study where we remove reward shaping (−RS) and action dropout (−AD) from Ours(ConvE) and compare their MRRs to the whole model on the dev sets. <ref type="bibr">6</ref> As shown in <ref type="table" target="#tab_5">Table 3</ref>, on most datasets, removing each com- ponent results in a significant performance drop. The exception is WN18RR, where removing the ConvE reward shaping module improves the per- formance. <ref type="bibr">7</ref> Removing reward shaping on NELL-995 does not change the results significantly. In general, removing action dropout has a greater im- pact, suggesting that thorough exploration of the path space is important across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Convergence Rate</head><p>We are interested in studying the impact of each proposed enhancement on the training conver- gence rate. In particular, we expect reward shap- ing to accelerate the convergence of RL (to a better performance level) as it propagates prior knowl- edge about the underlying KG to the agent. On the other hand, a fair concern for action dropout is that it can be slower to train, as the agent is forced to explore a more diverse set of paths. <ref type="figure" target="#fig_4">Figure 4</ref> eliminates this concern.</p><p>The first row of <ref type="figure" target="#fig_4">Figure 4</ref> shows the changes in dev set MRR of Ours(ConvE) (green * ) and the two ablated models w.r.t. # epochs. In general, the proposed approach is able to converge to a higher accuracy level much faster than either of the ab- lated models and the performance gap often per- sists until the end of training (on UMLS, <ref type="bibr">Kinship, and FB15k-237)</ref>. Particularly, on FB15k-237, our approach still shows improvement even after the two ablated models start to overfit, with −AD be- ginning to overfit sooner. On WN18RR, introduc- ing reward shaping hurt dev set performance from the beginning, as discussed in § 5.2. On NELL- 995, Ours(ConvE) performs significantly better in the beginning, but −RS gradually reaches a com- parable performance level.</p><p>It is especially interesting that introducing ac- tion dropout immediately improves the model per- formance on all datasets. A possible explanation for this is that by exploring a more diverse set of paths the agent learns search policies that general- ize better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Path Diversity</head><p>We also compute the total number of unique paths the agent explores during training and visualize its change w.r.t. # training epochs in the second row of <ref type="figure" target="#fig_4">Figure 4</ref>. When counting a unique path, we in- clude both the edge label and intermediate entity.</p><p>information about the KG. Yet counter-intuitively, we found that adding the ComplEx reward shaping module helps, de- spite the fact that ComplEx performs slightly worse than ConvE on this dataset. This indicates that dev set accuracy is not the only factor which determines the effectiveness of reward shaping.   <ref type="table">Table 4</ref>: MRR evaluation of different relation types (to-many vs. to-one) on five datasets. The % columns show the percentage of examples of each relation type found in the development split of the corresponding dataset. In general, our proposed techniques improve the prediction results for to-many relations more significantly.</p><p>First we observe that, on all datasets, the agent ex- plores a large number of paths before reaching a good performance level. The speed of path discov- ery slowly decreases as training progresses. On smaller KGs (UMLS and Kinship), the rate of en- countering new paths is significantly lower after a certain number of epochs, and the dev set accuracy plateaus correspondingly. On much larger KGs (FB15k-237, WN18RR, and NELL-995), we did not observe a significant slowdown before severe overfitting occurs and the dev set performance starts to drop. A possible reason for this is that the larger KGs are more sparsely connected compared to the smaller KGs <ref type="table">(Table 1)</ref>, therefore it is less efficient to gain generalizable knowledge from the KG by exploring a limited proportion of the path space through sampling. Second, while removing action dropout signifi- cantly lowers the effectiveness of path exploration (orange vs. green * ), we observe that removing reward shaping (blue △) slightly increases the # paths visited if the action dropout rate is kept the same. This indicates that the correlation between # paths explored and dev set performance is not strictly positive. The best performing model is not always the model that explored the largest # paths. It also demonstrates the role of reward shaping as a regularizer which guides the agent to avoid noisy paths with its prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Performance w.r.t. Relation Types</head><p>We investigate the behaviors of our proposed ap- proach w.r.t different relation types. For each KG, we classify its set of relations into two categories based on the answer set cardinality. Specifically, we define the metric ξ r as the average answer set cardinality of all queries with topic relation r. We count r as a "to-many" relation if ξ r &gt; 1.5, which indicates that most queries in relation r has more than 1 correct answer; we count r as a "to-one" relation otherwise, meaning most queries of this relation have only 1 correct answer. <ref type="table">Table 4</ref> shows the percentage of examples of to- many and to-one relations on each dev dataset and the MRR evaluation metrics of previously studied models computed on the examples of each relation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Seen Queries Unseen Queries % Ours(ConvE) −RS −AD % Ours(ConvE) −RS −AD UMLS 97.2 73.1 67.9 (-7%) 61.4 (-16%) 2.8 68.5 61.5 (-10%) 58.7 (-14%) Kinship 96.8 75.1 66.5 (-11%) 65.8 (-12%) 3.2 73.6 64.3 (-13%) 53.3 (-27%) FB15k-237 76.1</p><p>28.3 24.3 (-14%) 20.6 (-27%) 23.9 70.9 69.1 (-2%) 63.9 <ref type="table">(-10%)  WN18RR</ref> 41.8 60.8 62.0 (+2%) 53.4 (-12%) 58.2 31.5 33.9 (+7%) 28.8 (-9%) NELL-995 15.3 40.4 45.9 (+14%) 42.5 (+5%) 84.7 85.5 84.7 (-1%) 84.3 (-1%) <ref type="table">Table 5</ref>: MRR evaluation of seen queries vs. unseen queries on five datasets. The % columns show the percentage of examples of seen/unseen queries found in the development split of the corresponding dataset.</p><p>type. Since UMLS and Kinship are densely con- nected, they almost exclusively contain to-many relations. FB15k-237 mostly contains to-many re- lations. In <ref type="figure" target="#fig_4">Figure 4</ref>, we observe the biggest rela- tive gains from the ablated models on these three datasets. WN18RR is more balanced and con- sists of slightly more to-many relations than to- one relations. The NELL-995 dev set is a unique one which almost exclusively consists of to-one relations. There is no common performance pat- tern over the two relation types across datasets: on some datasets all models perform better on to- many relations (UMLS, WN18RR) while others show the opposite trend (FB15k-237, NELL-995).</p><p>We leave the study of these discrepancies to future work. We show the relative performance change of the ablated models −RS and −AD w.r.t. Ours(ConvE) in parentheses. We observe that in general our proposed enhancements are effective in improving query-answering over both relation types (more effective for to-many relations). However, adding the ConvE reward shaping module on WN18RR hurts the performance over both to-many and to- one relations (more for to-one relations). On NELL-995, both techniques hurt the performance over to-many relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Performance w.r.t. Seen Queries vs. Unseen Queries</head><p>Since most benchmark datasets randomly split the KG triples into train, dev and test sets, the queries that have multiple answers may fall into multi- ple splits. As a result, some of the test queries (e s , r q , ?) are seen in the training set (with a dif- ferent set of answers) while the others are not. We investigate the behaviors of our proposed approach w.r.t. seen and unseen queries. <ref type="table">Table 5</ref> shows the percentage of examples as- sociated with seen and unseen queries on each dev dataset and the corresponding MRR evalua- tion metrics of previously studied models. On most datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations <ref type="table">(Table 4</ref>) as a result of random data split, with the exception of WN18RR. On some datasets, all models perform better on seen queries (UMLS, Kinship, WN18RR) while others reveal the op- posite trend. We leave the study of these model behaviors to future work. On NELL-995 both of our proposed enhancements are not effective over the seen queries. In most cases, our proposed en- hancements improve the performance over unseen queries, with AD being more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose two modeling advances for end-to- end RL-based knowledge graph query answer- ing: (1) reward shaping via graph completion and (2) action dropout. Our approach improves over state-of-the-art multi-hop reasoning models con- sistently on several benchmark KGs. A detailed analysis indicates that the access to a more ac- curate environment representation (reward shap- ing) and a more thorough exploration of the search space (action dropout) are important to the perfor- mance boost.</p><p>On the other hand, the performance gap be- tween RL-based approaches and the embedding- based approaches for KGQA remains. In future work, we would like to investigate learnable re- ward shaping and action dropout schemes and ap- ply model-based RL to this domain.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of an incomplete knowledge graph which contains missing links (dashed lines) that can possibly be inferred from existing facts (solid lines).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of false negatives hit (where the model predicted an answer that exists in the full KG but cannot be identified by the training subset) in the first 20 epochs of walk-based QA training on the UMLS knowledge graph (Kok and Domingos, 2007).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of convergence rate and path exploration efficiency. The three curves in each subplot represents Ours(ConvE) (green * ) and the two ablated models: −RS (blue △) and −AD (orange ). The top row shows the change of dev set MRR and the bottom row shows the growth of # unique paths explored w.r.t. # epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Path sampled w/ πθ ~ ~</head><label></label><figDesc></figDesc><table>Reward Shaping 

es rq eT 

rq? 

f(es, rq, eT) 

es 
eT 
et 

… 
… 
rt 
rT 

… 
… 

h0 
ht 
hT 

Policy Network with Action Dropout 

et 
ht 
rq 
At 

X 

πθ(at|st) m 

X 

πθ(at|st) 

action selection 

if (es, rq, eT) 
observed 

otherwise 

~ 

Reward +1 
LSTM path 
encoder 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>995 @1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR @1 @10 MRR</head><label></label><figDesc></figDesc><table>Following previous work, we treat 
every KG link as bidirectional and augment the 
graph with the reversed (e o , r −1 , e s ) links. We use 
the same train, dev, and test set splits as Das et al. 
(2018). We exclude any link from the dev and Model 
UMLS 
Kinship 
FB15k-237 
WN18RR 
NELL-DistMult (Yang et al., 2014) 
82.1 96.7 86.8 48.7 90.4 61.4 32.4 60.0 41.7 43.1 52.4 46.2 55.2 78.3 64.1 

ComplEx (Trouillon et al., 2016) 89.0 99.2 93.4 81.8 98.1 88.4 32.8 61.6 42.5 41.8 48.0 43.7 64.3 86.0 72.6 

ConvE (Dettmers et al., 2018) 
93.2 99.4 95.7 79.7 98.1 87.1 34.1 62.2 43.5 40.3 54.0 44.9 67.8 88.6 76.1 

NeuralLP (Yang et al., 2017) 
64.3 96.2 77.8 47.5 91.2 61.9 16.6 34.8 22.7 37.6 65.7 46.3 -
-
-

NTP-λ (Rocktäschel et. al. 2017) 84.3 100 91.2 75.9 87.8 79.3 -
-
-
-
-
-
-
-
-

MINERVA (Das et al., 2018) 
72.8 96.8 82.5 60.5 92.4 72.0 21.7 45.6 29.3 41.3 51.3 44.8 66.3 83.1 72.5 

Ours(ComplEx) 
88.7 98.5 92.9 81.1 98.2 87.8 32.9 54.4 39.3 43.7 54.2 47.2 65.5 83.6 72.2 

Ours(ConvE) 
90.2 99.2 94.0 78.9 98.2 86.5 32.7 56.4 40.7 41.8 51.7 45.0 65.6 84.4 72.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Query answering performance compared to state-of-the-art embedding based approaches (top part) and 
multi-hop reasoning approaches (bottom part). The @1, @10 and MRR metrics were multiplied by 100. We 
highlight the best approach in each category. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : Comparison of dev set MRR of Ours(ConvE</head><label>3</label><figDesc></figDesc><table>) 
and models without reward shaping and action dropout. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>). The top row shows the change of dev set MRR and the bottom row shows the growth of # unique paths explored w.r.t. # epochs.</figDesc><table>Dataset 
To-many 
To-one 
% Ours(ConvE) 
−RS 
−AD 
% Ours(ConvE) 
−RS 
−AD 
UMLS 
99.1 
73.1 
67.9 (-7%) 61.3 (-16%) 
0.9 
62.5 55.5 (-11%) 54.4 (-13%) 
Kinship 
100 
75 66.5 (-11%) 65.4 (-13%) 
0 
-
-
-
FB15k-237 76.6 
28.3 24.5 (-13%) 20.9 (-26%) 23.4 
72 
69.8 (-3%) 63.9 (-11%) 
WN18RR 
52.8 
65 
65.7 (+1%) 57.9 (-11%) 47.2 
20.1 23.2 (+16%) 18.1 (-10%) 
NELL-995 12.9 
55.7 62.1 (+12%) 56.9 (+2%) 87.1 
81.4 
80.7 (-1%) 
80.5 (-1%) 

</table></figure>

			<note place="foot" n="1"> This training strategy treats a query with n &gt; 1 answers as n single-answer queries. In particular, given a query (es, rq, ?) with multiple answers {et 1 ,. .. et n }, when training w.r.t. the example (es, rq, et i ), MINERVA removes all {et j |j ̸ = i} observed in the training data from the possible set of target entities in the last search step so as to force the agent to walk towards et i. We adopt the same technique in our training.</note>

			<note place="foot" n="3"> Das et al. (2018) reported MINERVA results with the entity embedding usage as an extra hyperparameter-the quoted performance of MINERVA in Table 2 on UMLS and Kinship were obtained with entity embeddings setting to zero. In contrast, our system always uses trained entity embeddings.</note>

			<note place="foot" n="4"> On some datasets, we found larger batch size to continue improving the performance but had to stop at 512 due to memory constraints.</note>

			<note place="foot" n="5"> We report the model robustness measurements in § A.1.</note>

			<note place="foot" n="6"> According to Table 3 and Table 2, the dev and test set evaluation metrics differ significantly on several datasets. We discuss the cause of this in § A.2. 7 A possible explanation for this is that as path-based models tend to outperform the embedding based approaches on WN18RR, ConvE may be supplying more noise than useful</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Mark O. Riedl, Yingbo Zhou, James Bradbury and Vena Jia Li for their feedback on early draft of the paper, and Mark O. Riedl for helpful conversations on reward shaping. We thank the anonymous reviewers and the Salesforce research team members for their thoughtful com-ments and discussions. We thank Fréderic Godin for pointing out an error in Equation 8 in an early version of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<editor>Regina Barzilay and Min-Yen Kan</editor>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07-30" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Vancouver</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang ; Yarowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Garciadurán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Variational knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1803.06581</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell ; Yarowsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="833" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2010-05-13" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09-17" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">Zheran</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Barzilay and Kan</publisher>
			<biblScope unit="page" from="1051" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Ulrike Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<title level="m">2017. Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Barzilay and Kan</publisher>
			<biblScope unit="page" from="1766" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical predicate invention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference Proceeding Series</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;11<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012-07-12" />
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-04-03" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>abs/1312.5602</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daishi</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML 1999</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML 1999<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999-06-27" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<idno>number = SIDL-WP-1999-0120</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
<note type="report_type">Stanford InfoLab. Previous</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">End-toend differentiable proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel ; Guyon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3791" to="3803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Reinforcewalk: Learning to walk in graph with monte carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno>abs/1802.04394</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Reinforcement learning-an introduction. Adaptive computation and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge base and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Long Papers. The Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2016-06-19" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09" />
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>abs/1412.6575</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen ; Guyon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Karen Livescu, and Steven Bethard</editor>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Grand Hyatt Seattle, Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="volume">2013</biblScope>
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL. ACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
