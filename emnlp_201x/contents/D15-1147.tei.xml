<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamla</forename><surname>Al-Mannai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Qatar Computing Research Institute -Hamad Bin Khalifa University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present novel models for domain adaptation based on the neural network joint model (NNJM). Our models maximize the cross entropy by regularizing the loss function with respect to in-domain model. Domain adaptation is carried out by assigning higher weight to out-domain sequences that are similar to the in-domain data. In our alternative model we take a more restrictive approach by additionally penalizing sequences similar to the out-domain data. Our models achieve better perplexities than the baseline NNJM models and give improvements of up to 0.5 and 0.6 BLEU points in Arabic-to-English and English-to-German language pairs, on a standard task of translating TED talks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rapid influx of digital data has galvanized the use of empirical methods in many fields including Ma- chine Translation (MT). The increasing availabil- ity of bilingual corpora has made it possible to automatically learn translation rules that required years of linguistic analysis previously. While ad- ditional data is often beneficial for a general pur- pose Statistical Machine Translation (SMT) sys- tem, a problem arises when translating new do- mains such as lectures ( <ref type="bibr" target="#b8">Cettolo et al., 2014</ref>), patents ( <ref type="bibr" target="#b25">Fujii et al., 2010</ref>) or medical text ( <ref type="bibr" target="#b6">Bojar et al., 2014)</ref>, where either the bilingual text does not exist or is available in small quantity. All do- mains have their own vocabulary and stylistic pref- erences which cannot be fully encompassed by a system trained on the general domain.</p><p>Machine translation systems trained from a sim- ple concatenation of small in-domain and large out-domain data often perform below par be- cause the out-domain data is distant or over- whelmingly larger than the in-domain data. Ad- ditional data increases lexical ambiguity by in- troducing new senses to the existing in-domain vocabulary. For example, an Arabic-to-English SMT system trained by simply concatenating in- and out-domain data translates the Arabic phrase "</p><p>" to "about the problem of unwanted pregnancy". This translation is incorrect in the context of the in-domain data, where it should be translated to "about the prob- lem of choice overload". The sense of the Ara- bic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adap- tation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been car- ried out recently in domain adaptation. The com- plexity of the SMT pipeline, starting from cor- pus preparation to word-alignment, and then train- ing a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection <ref type="bibr" target="#b41">(Matsoukas et al., 2009</ref>) or model adaptation <ref type="bibr" target="#b22">(Foster and Kuhn, 2007)</ref>. In this paper, we further re- search in model adaptation using the neural net- work framework.</p><p>In recent years, there has been a growing in- terest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by <ref type="bibr" target="#b13">Devlin et al. (2014)</ref>. They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature.</p><p>Our aim in this paper is to advance the state-of- the-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out-domain data coming from heterogeneous sources. We hypothesize that the distributed vector rep- resentation of NNJM helps to bridge the lexical differences between the in-domain and the out- domain data, and adaptation is necessary to avoid deviation of the model from the in-domain data, which otherwise happens because of the large out- domain data.</p><p>To this end, we propose two novel extensions of NNJM for domain adaptation. Our first model minimizes the cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative ap- proach by additionally penalizing data instances similar to the out-domain data.</p><p>We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve bet- ter perplexities <ref type="bibr" target="#b9">(Chen and Goodman, 1999</ref>) than the models trained on in-and in+out-domain data. Improvements are also reflected in BLEU scores ( <ref type="bibr" target="#b48">Papineni et al., 2002</ref>) as we compare these mod- els within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and English- German pairs over a competitive baseline system.</p><p>The remainder of this paper is organized as fol- lows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 dis- cusses our models. Section 5 presents the experi- mental setup and the results. Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Selection</head><p>Data selection has shown to be an effective way to discard poor quality or irrelevant training in- stances, which when included in an MT system, hurts its performance. The idea is to score the out- domain data using a model trained from the in- domain data and apply a cut-off based on the re- sulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when train- ing is expensive and also when memory is con- strained. Data selection was done earlier for lan- guage modeling using information retrieval tech- niques ( <ref type="bibr" target="#b33">Hildebrand et al., 2005</ref>) and perplexity measures <ref type="bibr" target="#b46">(Moore and Lewis, 2010)</ref>. <ref type="bibr" target="#b2">Axelrod et al. (2011)</ref> further extended the work of <ref type="bibr" target="#b46">Moore and Lewis (2010)</ref> to translation model adaptation by using both source-and target-side language mod- els. <ref type="bibr" target="#b14">Duh et al. (2013)</ref> used a recurrent neural lan- guage model instead of an ngram-based language model to do the same. Translation model features were used recently by ( <ref type="bibr" target="#b38">Liu et al., 2014;</ref><ref type="bibr" target="#b35">Hoang and Sima'an, 2014</ref>) for data selection. <ref type="bibr" target="#b16">Durrani et al. (2015a)</ref> performed data selection using operation sequence model (OSM) and NNJM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Adaptation</head><p>The downside of data selection is that finding an optimal cut-off threshold is a time consuming pro- cess. An alternative to completely filtering out less useful data is to minimize its effect by down- weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. <ref type="bibr" target="#b41">Matsoukas et al. (2009)</ref> proposed a classification-based sentence weighting method for adaptation. <ref type="bibr" target="#b24">Foster et al. (2010)</ref> extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation <ref type="bibr" target="#b21">(Finch and Sumita, 2008;</ref><ref type="bibr" target="#b47">Nakov and Ng, 2009)</ref> or log-linear combination <ref type="bibr" target="#b23">(Foster and Kuhn, 2009;</ref><ref type="bibr" target="#b5">Bisazza et al., 2011;</ref><ref type="bibr" target="#b52">Sennrich, 2012)</ref> and through phrase training based adaptation <ref type="bibr" target="#b39">(Mansour and Ney, 2013)</ref>. <ref type="bibr" target="#b16">Durrani et al. (2015a)</ref> applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. <ref type="bibr" target="#b10">Chen et al. (2013b)</ref> used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain.  also applied mixture model adaptation for reordering model.</p><p>Other work on domain adaptation includes but not limited to studies focusing on topic models <ref type="bibr" target="#b19">(Eidelman et al., 2012;</ref><ref type="bibr" target="#b31">Hasler et al., 2014</ref>), dy- namic adaptation without in-domain data <ref type="bibr" target="#b51">(Sennrich et al., 2013;</ref><ref type="bibr" target="#b40">Mathur et al., 2014</ref>) and sense disambiguation <ref type="bibr" target="#b7">(Carpuat et al., 2013)</ref>.</p><p>In this paper, we do model adaptation using a neural network framework. In contrast to pre- vious work, we perform it at the (bilingual) n- gram level, where n is sufficiently large to cap- ture long-range cross-lingual dependencies. The generalized vector representation of the neural net- work model reduces the data sparsity issue of tra- ditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Network Joint Model</head><p>In recent years, there has been a great deal of ef- fort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP ( <ref type="bibr" target="#b3">Bengio et al., 2003;</ref><ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b36">Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b27">Gao et al., 2014;</ref><ref type="bibr" target="#b50">Schwenk, 2012;</ref><ref type="bibr" target="#b12">Collobert et al., 2011;</ref><ref type="bibr" target="#b42">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b53">Socher et al., 2013;</ref><ref type="bibr" target="#b34">Hinton et al., 2012</ref>). Recently, <ref type="bibr" target="#b13">Devlin et al. (2014)</ref> proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly.</p><p>Given a source sentence S and its correspond- ing target sentence T , the NNJM model computes the conditional probability P (T |S) as follows:</p><formula xml:id="formula_0">P (T |S) ≈ |T | i P (ti|ti−1 . . . ti−p+1, s i )<label>(1)</label></formula><p>where, s i is a q-word source window for the tar- get word t i based on the one-to-one (non-NULL) alignment of T to S. As exemplified in <ref type="figure" target="#fig_0">Figure 1</ref>, this is essentially a (p + q)-gram neural network LM (NNLM) originally proposed by <ref type="bibr" target="#b3">Bengio et al. (2003)</ref>. Each input word i.e. source or target word in the context is represented by a D dimensional vector in the shared look-up layer L ∈ R |V i |×D , where V i is the input vocabulary. 1 The look-up layer then creates a context vector x n representing the context words of the (p + q)-gram sequence by concatenating their respective vectors in L. The concatenated vector is then passed through non- linear hidden layers to learn a high-level represen- tation, which is in turn fed to the output layer. The output layer has a softmax activation over the output vocabulary V o of target words. Formally, the probability of getting k-th word in the output given the context x n can be written as:</p><formula xml:id="formula_1">P (yn = k|xn, θ) = exp (w T k φ(xn)) |Vo| m=1 exp (w T m φ(xn))<label>(2)</label></formula><p>1 Note that L is a model parameter to be learned.</p><p>where φ(x n ) defines the transformations of x n through the hidden layers, and w k are the weights from the last hidden layer to the output layer.</p><p>For notational simplicity, henceforth we will use (x n , y n ) to represent a training sequence. By setting p and q to be sufficiently large, NNJM can capture long-range cross-lingual de- pendencies between words, while still overcom- ing the data sparseness issue by virtue of its dis- tributed representations (i.e., word vectors). A ma- jor bottleneck, however, is to surmount the com- putational cost involved in training the model and applying it for MT decoding. <ref type="bibr" target="#b13">Devlin et al. (2014)</ref> proposed two tricks to speed up computation in decoding. The first one is to pre-compute the hid- den layer computations and fetch them directly as needed during decoding. The second technique is to train a self-normalized NNJM to avoid compu- tation of the softmax normalization factor (i.e., the denominator in Equation 2) in decoding. How- ever, self-normalization does not solve the compu- tational cost of training the model. In the follow- ing, we describe a method to address this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training by Noise Contrastive Estimation</head><p>The standard way to train NNLMs is to maximize the log likelihood of the training data:</p><formula xml:id="formula_2">J(θ) = N n=1 |Vo| k=1 y nk log P (yn = k|xn, θ)<label>(3)</label></formula><p>where, y nk = I(y n = k) is an indicator vari- able (i.e., y nk =1 when y n =k, otherwise 0). Op- timization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfor- tunately, training NNLMs are impractically slow because for each training instance (x n , y n ), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary. <ref type="bibr">2</ref> Noise contrastive estimation or NCE ( <ref type="bibr" target="#b29">Gutmann and Hyvärinen, 2010</ref>) provides an effi- cient and stable way to avoid this repetitive com- putation as recently applied to <ref type="bibr">NNLMs (Vaswani et al., 2013;</ref><ref type="bibr" target="#b45">Mnih and Teh, 2012</ref>). We can re-write Equation 2 as follows:</p><formula xml:id="formula_3">P (yn = k|xn, θ) = σ(yn = k|xn, θ) Z(φ(xn), W)<label>(4)</label></formula><p>where σ(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider Source token 1</p><p>Source token 2</p><p>Source token 3</p><p>Target token 1</p><p>Target token 2</p><p>Hidden layer</p><formula xml:id="formula_4">φ(x n )</formula><p>Look-up layer Z(.) as an additional model parameter along with the regular parameters, i.e., weights, look-up vec- tors. However, it has been shown that fixing Z(.) to 1 instead of learning it in training does not affect the model performance <ref type="bibr" target="#b45">(Mnih and Teh, 2012</ref>). For each training instance (x n , y n ), we add M noise samples (x n , y m n ) by sampling y m n from a known noise distribution ψ (e.g., unigram, uniform) M many times (i.e., m = 1 . . . M ); see <ref type="figure" target="#fig_0">Figure 1</ref>. NCE loss is then defined to discriminate a true instance from a noisy one. Let C ∈ {0, 1} denote the class of an instance with C = 1 indicat- ing true and C = 0 indicating noise. NCE maxi- mizes the following conditional log likelihood:</p><formula xml:id="formula_5">J(θ) = N n=1</formula><p>log[P (C = 1|yn, xn, θ)]</p><formula xml:id="formula_6">+ M m=1 log[P (C = 0|y m n , xn, ψ)]<label>(5)</label></formula><formula xml:id="formula_7">= N n=1 log [P (yn|C = 1, xn, θ)P (C = 1|π)] + M m=1</formula><p>log [(P (y m n |C = 0, xn, ψ)) P (C = 0|π)]</p><formula xml:id="formula_8">− (M + 1) log Q<label>(6)</label></formula><p>where Q = P (y n , C = 1|x n , θ, π) + P (y m n , C = 0|x n , ψ, π) is a normalization constant. After re- moving the constant terms, Equation 6 can be fur- ther simplified as:</p><formula xml:id="formula_9">J(θ) = N n=1 |Vo| k=1 y nk log σ nk + M m=1</formula><p>y m nk log ψ nk</p><p>where ψ nk =P (y m n = k|x n , ψ) is the noise dis- tribution, σ nk =σ(y n = k|x n , θ) is the unnormal- ized score at the output layer (Equation 4), and y nk and y m nk are indicator variables as defined before. NCE reduces the number of computations needed at the output layer from |V o | to M + 1, where M is a small number in comparison with |V o |. In all our experiments we use NCE loss with M = 100 samples as suggested by <ref type="bibr" target="#b45">Mnih and Teh (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Domain Adaptation Models</head><p>The ability to generalize and learn complex se- mantic relationships ( <ref type="bibr" target="#b43">Mikolov et al., 2013b</ref>) and its compelling empirical results gives a strong mo- tivation to use the NNJM model for the problem of domain adaptation in machine translation. How- ever, the vanilla NNJM described above is limited in its ability to effectively learn from a large and diverse out-domain data in the best favor of an in- domain data. To address this, we propose two neu- ral domain adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either with respect to in-domain or both in-and out-domains. In both cases, we first present the regularized loss function for the nor- malized output layer with the standard softmax, followed by the corresponding un-normalized one using the noise contrastive estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NDAM v1</head><p>To improve the generalization of word embed- dings, NNLMs are generally trained on very large datasets ( <ref type="bibr" target="#b42">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b55">Vaswani et al., 2013</ref>). Therefore, we aim to train our neural domain adaptation models (NDAM) on in-plus out-domain data, while restricting it to drift away from in-domain. In our first model NDAM v1 , we achieve this by biasing the model towards the in- domain using a regularizer (or prior) based on the in-domain model. Let θ i be an NNJM model al- ready trained on the in-domain data. We train an adapted model θ a on the whole data, but regular- izing it with respect to θ i . We redefine the normal- ized loss function of Equation 3 as follows:</p><formula xml:id="formula_11">J(θa) = N n=1 |Vo| k=1</formula><p>λ y nk logP (yn = k|xn, θa)</p><formula xml:id="formula_12">+ (1 − λ) y nk P (yn = k|xn, θi) logP (yn = k|xn, θa)<label>(8)</label></formula><formula xml:id="formula_13">= N n=1 |Vo| k=1 λ y nk logˆylogˆ logˆy nk (θa) + (1 − λ) y nk p nk (θi) logˆylogˆ logˆy nk (θa)<label>(9)</label></formula><p>wherê y nk (θ a ) is the softmax output and p nk (θ i ) is the probability of the training instance accord- ing to the in-domain model θ i . Notice that the loss function minimizes the cross entropy of the cur- rent model θ a with respect to the gold labels y n and the in-domain model θ i . The mixing param- eter λ ∈ [0, 1] determines the relative strength of the two components. <ref type="bibr">3</ref> Similarly, we can re-define the NCE loss of Equation 7 as:</p><formula xml:id="formula_14">J(θa) = N n=1 |Vo| k=1 λ y nk log σ nk + (1 − λ) y nk p nk (θi) log σ nk + M m=1 y m nk log ψ nk<label>(10)</label></formula><p>We use SGA with backpropagation to train this model. The derivatives of J(θ a ) with respect to the final layer weight vectors w j turn out to be: <ref type="bibr">3</ref> We used a balanced value λ = 0.5 for our experiments.</p><formula xml:id="formula_15">w j J(θa) = N n=1 λ (ynj − σnj) + (1 − λ) [pnj(θi) − k y nk p nk (θi) σnj]<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NDAM v2</head><p>The regularizer in NDAM v1 is based on an in- domain model θ i , which puts higher weights to the training instances (i.e., n-gram sequences) that are similar to the in-domain ones. This might work better when the out-domain data is similar to the in-domain data. In cases where the out- domain data is different, we might want to build a more conservative model that penalizes training instances for being similar to the out-domain ones. Let θ i and θ o be the two NNJMs already trained from the in-and out-domains, respectively, and θ o is trained using the same vocabulary as θ i . We de- fine the new normalized loss function as follows:</p><formula xml:id="formula_16">J(θa) = N n=1 |Vo| k=1</formula><p>λ y nk logˆylogˆ logˆy nk (θa)</p><formula xml:id="formula_17">+ (1 − λ) y nk [p nk (θi) − p nk (θo)] logˆylogˆ logˆy nk (θa)<label>(12)</label></formula><p>where y nk , ˆ y nk (θ a ), p nk (θ i ) and p nk (θ o ) are sim- ilarly defined as before. This loss function min- imizes the cross entropy of the current model θ a with respect to the gold labels y n and the differ- ence between the in-domain model θ i and the out- domain model θ o . Intuitively, the regularizer as- signs higher weights to training instances that are not only similar to the in-domain but also dissim- ilar to the out-domain. The parameter λ ∈ [0, 1] determines the strength of the regularization. The corresponding NCE loss can be defined as follows:</p><formula xml:id="formula_18">J(θa) = N n=1 |Vo| k=1</formula><p>λ y nk log σ nk + (1 − λ) y nk log σ nk</p><formula xml:id="formula_19">(p nk (θi) − p nk (θo)) + M m=1 y m nk log ψ nk<label>(13)</label></formula><p>The derivatives of the above cost function with re- spect to the final layer weight vectors w j are:</p><formula xml:id="formula_20">w j J(θa) = N n=1 λ (ynj − σnj) + (1 − λ)[pnj(θi) − pnj(θo) − k y nk σnj (p nk (θi) − p nk (θo))]<label>(14)</label></formula><p>In a way, the regularizers in our loss functions are inspired from the data selection methods of <ref type="bibr" target="#b2">Axelrod et al. (2011)</ref>, where they use cross entropy between the in-and the out-domain LMs to score out-domain sentences. However, our approach is quite different from them in several aspects. First and most importantly, we take the scoring inside model training and use it to bias the training to- wards the in-domain model. Both the scoring and the training are performed at the bilingual n-gram level rather than at the sentence level. Integrating scoring inside the model allows us to learn a robust model by training/tuning the relevant parameters, while still using the complete data. Secondly, our models are based on NNs, while theirs utilize the traditional Markov-based generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Technical Details</head><p>In this section, we describe some implementation details of NDAM that we found to be crucial, such as: using gradient clipping to handle vanish- ing/exploding gradient problem in SGA training with backpropagation, selecting appropriate noise distribution in NCE, and special handling of out- domain words that are unknown to the in-domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Gradient Clipping</head><p>Two common issues with training deep NNs on large data-sets are the vanishing and the exploding gradients problems ( <ref type="bibr" target="#b49">Pascanu et al., 2013</ref>). The er- ror gradients propagated by the backpropagation may sometimes become very small or very large which can lead to undesired (nan) values in weight matrices, causing the training to fail. We also ex- perienced the same problem in our NDAM quite often. One simple solution to this problem is to truncate the gradients, known as gradient clipping <ref type="bibr" target="#b44">(Mikolov, 2012)</ref>. In our experiments, we limit the gradients to be in the range [−5; +5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Noise Distribution in NCE</head><p>Training with NCE relies on sampling from a noise distribution (i.e., ψ in Equation 5), and the performance of the NDAM models varies consid- erably with the choice of the distribution. We ex- plored uniform and unigram noise distributions in this work. With uniform distribution, every word in the output vocabulary has the same probability to be sampled as noise. The unigram noise dis- tribution is a multinomial distribution over words constructed by counting their occurrences in the output (i.e., n-th word in the n-gram sequence).</p><p>In our experiments, unigram distribution delivered much lower perplexity and better MT results com- pared to the uniform one. <ref type="bibr" target="#b45">Mnih and Teh (2012)</ref> also reported similar findings on perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Handling of Unknown Words</head><p>In order to reduce the training time and to learn better word representations, NNLMs are often trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word in the model. <ref type="bibr">4</ref> Our NDAM models rely on scoring out-domain sequences (of word Ids) using models that are trained based on the in-domain vocabulary. To score out-domain sequences using a model, we need to generate the sequences using the same vo- cabulary based on which the model was trained. In doing so, the out-domain words that are un- known to the in-domain data map to the same unk class. As a result, out-domain sequences contain- ing unks get higher probability although they are distant from the in-domain data.</p><p>A solution to this problem is to have an in- domain model that can differentiate between its own unk class, resulted from the reduced in- domain vocabulary, and actual unknown words that come from the out-domain data. We intro- duce a new class unk o to represent the latter. We train the in-domain model by adding a few dummy sequences containing unk o occurring on both source and target sides. This enables the model to learn unk and unk o separately, where unk o is a less probable class according to the model. Later, the n-gram sequences of the out- domain data contain both unk and unk o classes depending on whether a word is unknown to only pruned in-domain vocabulary (i.e., unk) or is un- known to full in-domain vocabulary (i.e., unk o ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we describe the experimental setup (i.e., data, settings for NN models and MT pipeline) and the results. First we evaluate our models intrinsically by comparing the perplexities on a held-out in-domain testset against the base- line NNJM model. Then we carry out an extrinsic evaluation by using the NNJM and NDAM models as features in machine translation and compare the BLEU scores. Initial developmental experiments were done on the Arabic-to-English language pair.</p><p>We carried out further experiments on the English- to-German pair to validate our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>We experimented with the data made publicly available for the translation task of the Interna- tional Workshop on Spoken Language Translation (IWSLT) ( <ref type="bibr" target="#b8">Cettolo et al., 2014)</ref>. We used TED talks as our in-domain corpus. For Arabic-to- English, we used the QCRI Educational Domain (QED) -A bilingual collection of educational lec- tures <ref type="bibr">5 (Abdelali et al., 2014</ref>), the News, and the multiUN (UN) <ref type="bibr" target="#b20">(Eisele and Chen, 2010)</ref> as our out-domain corpora. For English-to-German, we used the News, the Europarl (EP), and the Com- mon Crawl (CC) corpora made available for the 9 th Workshop of Statistical Machine Translation. 6 <ref type="table">Table 1</ref> shows the size of the data used.</p><p>Training NN models is expensive. We, there- fore, randomly selected subsets of about 300K sentences from the bigger domains (UN, CC and EP) to train the NN models. <ref type="bibr">7</ref> The systems were tuned on concatenation of the dev. and test2010 and evaluated on test2011-2013 datasets. The tun- ing set was also used to measure the perplexities of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System Settings</head><p>NNJM &amp; NDAM: The NNJM models were trained using NPLM 8 toolkit ( <ref type="bibr" target="#b55">Vaswani et al., 2013</ref>) with the following settings. We used a tar- get context of 5 words and an aligned source win- dow of 9 words, forming a joint stream of 14- grams for training. We restricted source and tar- get side vocabularies to the 20K and 40K most frequent words. The word vector size D and the hidden layer size were set to 150 and 750, respec- tively. Only one hidden layer is used to allow faster decoding. Training was done by the stan- dard stochastic gradient ascent with NCE using <ref type="bibr">5 Guzmán et al. (2013)</ref> showed that the QED corpus is similar to IWSLT and adding it improves translation quality.</p><p>6 http://www.statmt.org/wmt14/translation-task.html <ref type="bibr">7</ref> Concatenating all the data results in a corpus of ap- proximately 4.5 million sentences which requires roughly 18 days of wall-clock time (18 hours/epoch on a Linux Ubuntu 12.04.5 LTS running on a 16 Core Intel Xeon E5- 2650 2.00Ghz and 64Gb RAM) to train NNJM models on our machines. We ran one baseline experiment with all the data and did not find it better than the system trained on ran- domly selected subset of the data. In the interest of time, we therefore reduced the NN training to a subset (800K and 1M sentences for AR-EN and EN-DE respectively).</p><p>8 http://nlg.isi.edu/software/nplm/  <ref type="table">Table 1</ref>: Statistics of the Arabic-English and English-German training corpora in terms of Sen- tences and Tokens (Source/Target). Tokens are represented in millions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AR-EN EN-DE</head><p>100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, ex- cept for the special handling of unk tokens.</p><p>Machine Translation System: We trained a Moses system ( <ref type="bibr" target="#b37">Koehn et al., 2007)</ref>, with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments ( <ref type="bibr" target="#b18">Dyer et al., 2013)</ref>, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM <ref type="bibr" target="#b32">(Heafield, 2011</ref>), lexicalized reordering model ( <ref type="bibr" target="#b26">Galley and Manning, 2008</ref>), a 5-gram operation sequence model ( <ref type="bibr" target="#b17">Durrani et al., 2015b</ref>) and other default pa- rameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM mod- els. We used ATB segmentation using the Stanford ATB segmenter <ref type="bibr" target="#b28">(Green and DeNero, 2012)</ref> for Arabic-to-English and the default tokenizer pro- vided with the Moses toolkit ( <ref type="bibr" target="#b37">Koehn et al., 2007)</ref> for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliter- ation module in Moses ( ). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Intrinsic Evaluation</head><p>In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English lan- guage pair for the development experiments and train domain-wise models to measure the related- ness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of  shows results of the models trained from concate- nating each domain to the in-domain data. The perplexity numbers improved significantly in each case showing that there is useful information avail- able in each domain which can be utilized to im- prove the baseline. It also shows the robustness of neural network models. Unlike the n-gram model, the NN-based model improves generalization with the increase in data without completely skewing towards the dominating part of the data. Concatenating in-domain with the NEWS data gave better perplexities than other domains. Best results were obtained by concatenating all the data together (See row ALL). The third and fourth columns show results of our models (NDAM v * ). Both give better perplexities than NNJM cat in all cases. However, it is unclear which of the two is better. Similar observations were made for the English-to-German pair, where we only did exper- iments on the concatenation of all domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Extrinsic Evaluation</head><p>Arabic-to-English: For most language pairs, the conventional wisdom is to train the system with all available data. However, previously re- ported MT results on Arabic-to-English ( <ref type="bibr" target="#b39">Mansour and Ney, 2013)</ref> show that this is not optimal and the results are often worse than only using in- domain data. The reason for this is that the UN domain is found to be distant and overwhelmingly large as compared to the in-domain IWSLT data. We carried out domain-wise experiments and also found this to be true.</p><p>We considered three baseline systems: (i) B in ,   <ref type="table" target="#tab_6">Table 4</ref> shows results of the MT systems S v1 and S v2 using our adapted models NDAM v1 and NDAM v2 . We compare them to the baseline sys- tem B cat , which uses the non-adapted NNJM cat as a feature. S v1 achieved an improvement of up to +0.4 and S v2 achieved an improvement of up to +0.5 BLEU points. However, S v2 performs slightly worse than S v1 on individual domains. We speculate this is because of the nature of the NDAM v2 , which gives high weight to out-domain sequences that are liked by the in-domain model and disliked by the out-domain model. In the case of individual domains, NDAM v2 might be over pe- nalizing out-domain since the out-domain model is only built on that particular domain and always prefers it more than the in-domain model. In case of ALL, the out-domain model is more diverse and has different level of likeness for each domain.</p><p>We analyzed the output of the baseline system (S cat ) and spotted several cases of lexical ambigu- ity caused by out-domain data. For example, the Arabic phrase can be trans- lated to choice overload or unwanted pregnancy. The latter translation is incorrect in the context of in-domain. The bias created due to the out-domain data caused S cat to choose the contextually incor- rect translation unwanted pregnancy. However, the adapted systems S v * were able to translate it QED NEWS UN ALL tst11 tst12 tst13 tst11 tst12 tst13 tst11 tst12 tst13 tst11 tst12 tst13   to proprietary by S cat , a translation frequently ob- served in the out-domain data. S v * translated it correctly to fitness, as preferred by the in-domain.</p><p>English-to-German: Concatenating all training data to train the MT pipeline has been shown to give the best results for English-to-German ( <ref type="bibr" target="#b4">Birch et al., 2014</ref>). Therefore, we did not do domain- wise experiments, except for training a system on the in-domain IWSLT data for the sake of com- pleteness. We also tried B cat,in variation, i.e. training an MT system on the entire data and using in-domain data to train the baseline NNJM. The baseline system B cat gave better results and was used as our reference for comparison. <ref type="table" target="#tab_7">Table 5</ref> shows the results of our systems, S v1 and S v2 , compared to the baselines, B in and B cat . Unlike Arabic-to-English, the baseline system B in is much worse than B cat . Our adapted MT systems S v1 and S v2 both outperformed the best baseline system (B cat ) with an improvement of up to 0.6 points. S v2 performed slightly better than S v1 on one occasion and slightly worse in others.</p><p>Comparison with Data Selection: We also compared our results with the MML-based data   <ref type="table" target="#tab_9">Table 6</ref>. The MML-based baseline systems (B mml ) used 20% selected data for training the MT system and the NNJM. On Arabic-English, both MML-based se- lection and our model (S v1 ) gave similar gains on top of the baseline system (B cat ). Further results showed that both approaches are complementary. We were able to obtain an average gain of +0.3 BLEU points by training an NDAM v1 model over the selected data (see S v1+mml ). However, on English-German, the MML-based selection caused a drop in the performance (see <ref type="table" target="#tab_9">Table 6</ref>). Training an adapted NDAM v1 model over selected data gave improvements over MML in two test sets but could not restore the baseline performance, probably because the useful data has already been filtered by the selection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented two novel models for domain adap- tation based on NNJM. Adaptation is performed by regularizing the loss function towards the in- domain model and away from the unrelated out- of-domain data. Our models show better perplex- ities than the non-adapted baseline NNJM mod- els. When integrated into a machine translation system, gains of up to 0.5 and 0.6 BLEU points were obtained in Arabic-to-English and English- to-German systems over strong baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) and a source context window of size 3. For illustration, the output y n is shown as a single categorical variable (scalar) as opposed to the traditional one-hot vector representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 summarizes</head><label>2</label><figDesc></figDesc><table>the results 
for Arabic-English. The perplexity numbers in the 
second column (NNJM b ) show that NEWS is the </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparing the perplexity of NNJM 
and NDAM models. NNJM b represents the model 
trained on each individual domain separately. 

most related domain from the perspective of in-
domain data, whereas UN is the farthest having 
the worst perplexity. The third column (NNJM cat ) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results of the baseline Arabic-to-English 
MT systems. The numbers are averaged over 
tst2011-2013. 

which is trained on the in-domain data, (ii) B cat , 
which is trained on the concatenation of in-and 
out-domain data, and (ii) B cat,in , where the MT 
pipeline was trained on the concatenation but the 
NNJM model is trained only on the in-domain 
data. Table 3 reports average BLEU scores across 
three test sets on all domains. Adding QED and 
NEWS domains gave improvements on top of the 
in-domain IWSLT baseline. Concatenation of UN 
with in-domain made the results worse. Concate-
nating all out-domain and in-domain data achieves 
+0.4 BLEU gain on top of the baseline in-domain 
system. We will use B cat systems as our baseline 
to compare our adapted systems with. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Arabic-to-English MT Results 

SYS 
tst11 tst12 tst13 
Avg 

Baselines 

Bin 
25.0 
22.5 
23.2 
23.6 
Bcat 
25.7 
22.9 
24.1 
24.2 
Bcat,in 
26.0 
22.4 
23.6 
24.0 

Comparison against NDAM 

Bcat 
25.7 
22.9 
24.1 
24.2 

Sv1 
26.3 
23.1 
24.5 
24.6 
∆ 
+0.6 +0.2 +0.4 
+0.4 

Sv2 
26.2 
23.0 
24.6 
24.6 
∆ 
+0.5 +0.1 +0.5 
+0.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>English-to-German MT Results 

correctly. In another example 






(How about fitness?), the word 



is translated 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison with Modified Moore-Lewis 

selection approach as shown in </table></figure>

			<note place="foot" n="2"> This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data.</note>

			<note place="foot" n="4"> For our Arabic-English in-domain data, 30% of n-gram sequences contain at least one unk word.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The AMARA corpus: Building parallel language resources for the educational domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Edinburgh SLT and MT system description for the IWSLT 2014 evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Spoken Language Translation, IWSLT &apos;14</title>
		<meeting>the 11th International Workshop on Spoken Language Translation, IWSLT &apos;14<address><addrLine>Lake Tahoe, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fill-up versus interpolation methods for phrase-based SMT adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arianna</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT)</title>
		<editor>Marcello Federico, Mei-Yuh Hwang, Margit Rödder, and Sebastian Stüker</editor>
		<meeting>the seventh International Workshop on Spoken Language Translation (IWSLT)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sensespotting: Never let your parallel data tie you to an old domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharine</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Report on the 11th IWSLT Evaluation Campaign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Spoken Language Translation</title>
		<meeting>the International Workshop on Spoken Language Translation<address><addrLine>Lake Tahoe, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="393" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector space model for adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<editor>Boxing Chen, Roland Kuhn, and George Foster</editor>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia; Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch tuning strategies for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;12</title>
		<meeting>the 2012 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;12<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
		</imprint>
	</monogr>
<note type="report_type">JMLR. org</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptation data selection using neural language models: Experiments in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Tsukada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating an Unsupervised Transliteration Model into Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014)</title>
		<meeting>the 15th Conference of the European Chapter of the ACL (EACL 2014)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Joint Models for Domain Adaptation in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Machine Translation Summit (MT Summit XV)</title>
		<meeting>the Fifteenth Machine Translation Summit (MT Summit XV)<address><addrLine>Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Ahmed Abdelali, and Stephan Vogel. To Appear. AMTA</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Operation Sequence Model-Combining N-Grambased and Phrase-based Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="186" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL&apos;13</title>
		<meeting>NAACL&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Topic models for dynamic translation model adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MultiUN: A Multilingual Corpus from United Nation Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eisele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh conference on International Language Resources and Evaluation</title>
		<meeting>the Seventh conference on International Language Resources and Evaluation<address><addrLine>Valleta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic model interpolation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation<address><addrLine>Columbus, Ohio</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixturemodel adaptation for smt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation, StatMT &apos;07</title>
		<meeting>the Second Workshop on Statistical Machine Translation, StatMT &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stabilizing minimum error rate training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09</title>
		<meeting>the Fourth Workshop on Statistical Machine Translation, StatMT &apos;09<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative instance weighting for domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of the patent translation task at the ntcir-8 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikio</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takehito</forename><surname>Utsuro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access</title>
		<meeting>the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Simple and Effective Hierarchical Phrase Reordering Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations for translation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A classbased agreement model for generating accurately inflected translations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spence</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL&apos;12)</title>
		<meeting>the Association for Computational Linguistics (ACL&apos;12)<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noisecontrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</title>
		<editor>Y.W. Teh and M. Titterington</editor>
		<meeting>Int. Conf. on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The AMARA corpus: Building resources for translating the web&apos;s educational content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Abdelali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT-13)</title>
		<meeting>the 10th International Workshop on Spoken Language Technology (IWSLT-13)</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic topic adaptation for phrase-based mt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">KenLM: Faster and Smaller Language Model Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Workshop on Statistical Machine Translation</title>
		<meeting>the Sixth Workshop on Statistical Machine Translation<address><addrLine>Edinburgh, Scotland, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptation of the translation model for statistical machine translation based on information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Almut</forename><surname>Silja Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 10th Conference of the European Association for Machine Translation (EAMT)<address><addrLine>Budapest</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent domain translation models in mix-of-domains haystack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL&apos;07)</title>
		<meeting>the Association for Computational Linguistics (ACL&apos;07)<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Effective selection of translation model 1269 training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Phrase training based adaptation for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saab</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fast domain adaptation of smt models without in-domain parallel data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Venkatapathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discriminative corpus weight estimation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Matsoukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antti-Veikko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rosti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<title level="m">Linguistic regularities in continuous space word representations. HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Intelligent selection of language model training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL&apos;10)</title>
		<meeting>the Association for Computational Linguistics (ACL&apos;10)<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improved statistical machine translation for resource-poor languages using related resource-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;09)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;09)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL&apos;02)</title>
		<meeting>the Association for Computational Linguistics (ACL&apos;02)<address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="16" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012: Posters</title>
		<meeting>COLING 2012: Posters<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A multi-domain translation model framework for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers; Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Perplexity minimization for translation model domain adaptation in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st</title>
		<meeting>the 51st</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Decoding with large-scale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
