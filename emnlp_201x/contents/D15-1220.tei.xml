<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Concept-based Summarization using Integer Linear Programming: From Concept Pruning to Multiple Optimal Solutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Boudin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mougard</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
							<email>benoit.favre@lif.univ-mrs.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">LIF -UMR CNRS 7279</orgName>
								<orgName type="institution">Université Aix-Marseille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LINA -UMR CNRS 6241</orgName>
								<orgName type="institution">Université de Nantes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Concept-based Summarization using Integer Linear Programming: From Concept Pruning to Multiple Optimal Solutions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem. As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently. This work shows that reducing the number of concepts in the model leads to lower ROUGE scores, and more importantly to the presence of multiple optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have witnessed increased interest in global inference methods for extractive summa- rization. These methods formulate summarization as a combinatorial optimization problem, i.e. se- lecting a subset of sentences that maximizes an objective function under a length constraint, and use Integer Linear Programming (ILP) to solve it exactly <ref type="bibr" target="#b8">(McDonald, 2007)</ref>.</p><p>In this work, we focus on the concept-based ILP model for summarization introduced by ). In their model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maxi- mum coverage problem <ref type="bibr">1</ref> .</p><p>As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find opti- mal solutions efficiently <ref type="bibr" target="#b5">Li et al., 2013</ref>). However, re- ducing the number of concepts in the model has two undesirable consequences. First, it forces the model to only use a limited number of concepts to rank summaries, resulting in lower ROUGE scores. Second, by reducing the number of items from which sentence scores are derived, it allows dif- ferent sentences to have the same score, and ulti- mately leads to multiple optimal summaries.</p><p>To our knowledge, no previous work has men- tioned these problems, and only results corre- sponding to the first optimal solution found by the solver are reported. However, as we will show through experiments, these multiple optimal so- lutions cause a substantial amount of variation in ROUGE scores, which, if not accounted for, could lead to incorrect conclusions. More specifically, the contributions of this work are as follows:</p><p>• We evaluate )'s sum- marization model at various concept pruning levels. In doing so, we quantify the impact of pruning on running time, ROUGE scores and the number of optimal solutions.</p><p>• We extend the model to address the prob- lem of multiple optimal solutions, and we sidestep the need for concept pruning by de- veloping a fast approximation algorithm that achieves near-optimal performance.</p><p>2 Concept-based ILP Summarization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model definition</head><p>Gillick and Favre (2009) introduce a concept- based ILP model for summarization that casts sen- tence selection as a maximum coverage problem.</p><p>The key assumption of their model is that the value of a summary is defined as the sum of the weights of the unique concepts it contains. That way, re- dundancy within the summary is addressed im- plicitly at a sub-sentence level: a summary only benefits from including each concept once.</p><p>Formally, let w i be the weight of concept i, c i and s j two binary variables indicating the pres- ence of concept i and sentence j in the summary, Occ ij an indicator of the occurrence of concept i in sentence j, l j the length of sentence j and L the length limit for the summary, the concept-based ILP model is described as:</p><formula xml:id="formula_0">max i w i c i (1) s.t. j l j s j ≤ L (2) s j Occ ij ≤ c i , ∀i, j (3) j s j Occ ij ≥ c i , ∀i<label>(4)</label></formula><formula xml:id="formula_1">c i ∈ {0, 1} ∀i s j ∈ {0, 1} ∀j</formula><p>The constraints formalized in equations 3 and 4 ensure the consistency of the solution: selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept is only possible if it is present in at least one selected sentence.</p><p>Choosing a suitable definition for concepts and a method to estimate their weights are the two key factors that affect the performance of this model. Bigrams of words are usually used as a proxy for concepts <ref type="bibr">BergKirkpatrick et al., 2011</ref>). Concept weights are either estimated by heuristic counting, e.g. docu- ment frequency in , or obtained by supervised learning ( <ref type="bibr" target="#b5">Li et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pruning to reduce complexity</head><p>The concept-level formulation of ) is an instance of the budgeted maxi- mum coverage problem, and solving such a prob- lem is NP-hard ( <ref type="bibr">Khuller et al., 1999</ref>). Keeping the number of variables and constraints small is then critical to reduce the model complexity.</p><p>In previous work, efficient summarization was achieved by pruning concepts. One way to re- duce the number of concepts in the model is to remove those concepts that have a weight below a given threshold ). An- other way is to consider only the top-n highest weighted concepts ( <ref type="bibr" target="#b5">Li et al., 2013)</ref>. Once low- weight concepts are pruned, sentences that do not contain any remaining concepts are removed, fur- ther reducing the number of variables and con- straints in the model. As such, this can be regarded as a way to approximate the problem.</p><p>Pruning concepts to reduce complexity also cuts down the number of items from which summary scores are derived. As we will see in Section 3.2, this results in a lower ROUGE scores and leads to the production of multiple optimal summaries.</p><p>The concept weighting function also plays an important role in the presence of multiple opti- mal solutions. Limited-range functions, such as frequency-based ones, yield many ties and in- crease the likelihood that different sentences have the same score. Redundancy within the set of input sentences exacerbate this problem, since highly similar sentences are likely to contain the same concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Summarization parameters</head><p>For comparison purposes, we use the same system pipeline as in ( ), which is de- scribed below.</p><p>Step 1: clean input documents; a set of rules is used to remove bylines and format markup.</p><p>Step 2: split the text into sentences; we use splitta 2 <ref type="bibr" target="#b3">(Gillick, 2009)</ref> and re-attach multi- sentence quotations.</p><p>Step 3: compute parameters needed by the model; we extract and weight the concepts.</p><p>Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark.</p><p>Step 5: map to ILP format and solve; we use an off-the-shelf ILP solver 3 .</p><p>Step 6: order selected sentences for inclusion in the summary, first by source and then by position.</p><p>Similar to previous work, we use bigrams of words as concepts. Although bigrams are rough approximations of concepts, they are simple to ex- tract and match, and have been shown to perform well at this task. Bigrams of words consisting of two stop words 4 or containing a punctuation mark are discarded. Stemming 5 is then applied to allow more robust matching.</p><p>Concepts are weighted using document fre- quency, i.e. the number of source documents</p><note type="other">DUC'04 TAC'08 DF 1 2 3 4 1 2 3 4</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># solutions 1.3 1.3 1.5 1.5 1.2 1.3 1.8 4.8 # concepts 2 955 676 247 107 2 909 393 127 56 # sentences 184 175 159 139 174 167 149 129</head><p>Avg. time <ref type="bibr">(sec)</ref> 22.3 1.7 0.5 0.3 21.5 0.8 0.3 0.2 where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting ( <ref type="bibr" target="#b13">Woodsend and Lapata, 2012;</ref>. Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iter- atively add new constraints to the problem that eliminate already found optimal solutions and re- run the solver. We stop the iterations when the value of the objective function returned by the solver changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and evaluation measures</head><p>Experiments are conducted on the DUC'04 and TAC'08 datasets. For DUC'04, we use the 50 top- ics from the generic multi-document summariza- tion task (Task 2). For TAC'08, we focus only on the 48 topics from the non-update summarization task. Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens).</p><p>Summaries are evaluated against reference sum- maries using the ROUGE automatic evaluation measures <ref type="bibr" target="#b7">(Lin, 2004</ref>). We set the ROUGE param- eters to those 6 that lead to highest agreement with manual evaluation ( <ref type="bibr" target="#b10">Owczarzak et al., 2012)</ref>, that is, with stemming and stopwords not removed. <ref type="table" target="#tab_0">Table 1</ref> presents the average number of optimal solutions at different levels of concept pruning. Overall, the average number of optimal solutions increases along with the minimum document fre- quency, reaching 4.8 for TAC'08 at DF = 4. Prun-ing concepts also greatly reduces the number of variables in the ILP formulation, and consequently improves the run-time for solving the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Interestingly, we note that, even without any pruning, the model produces multiple optimal solutions. The choice of document frequency for weighting concepts is responsible for this as it generates many ties. Finer-grained concept weighting functions such as frequency estima- tion ( <ref type="bibr" target="#b5">Li et al., 2013</ref>) should therefore be preferred to limit the number of multiple optimal solutions.</p><p>The mean ROUGE recall scores of the multiple optimal solutions for different minimal document frequencies are presented in <ref type="table" target="#tab_2">Table 2</ref>. Here, the higher the concept pruning threshold, the higher the variability of the generated summaries as in- dicated by the standard deviation. Best ROUGE scores are achieved without concept pruning while the best compromise between effectiveness and run-time is given when DF ≥ 3, confirming the findings of .</p><p>To show in a realistic scenario how multiple optimal solutions could lead to different conclu- sions, we compare in <ref type="table">Table 3</ref> the ROUGE-1 scores of the summaries generated from the first op- timal solution found by three off-the-shelf ILP solvers against that of the systems 7 that partici- pated at TAC'08. We set the minimum document frequency to 3, which is often used in previous work <ref type="bibr" target="#b5">Li et al., 2013)</ref>, and use a two-sided Wilcoxon signed-rank to com- pute the number of systems that obtain signifi- cantly lower and higher ROUGE-1 recall scores 8 .</p><p>Despite being comparable (p-value &gt; 0.4), the solutions found by the three solvers support differ- ent conclusions. The solution found using GLPK   <ref type="table">Table 3</ref>: ROUGE-1 recall scores for the first opti- mal solution found by different solvers along with the number of systems that obtain significantly lower (↓) or higher (↑) scores (p-value &lt; 0.05).</p><p>indicates that the concept-based model achieves state-of-the-art performance whereas the solutions provided by Gurobi and CPLEX do not do so. The reason for these differences is the use of differ- ent solving strategies, involving heuristics for find- ing feasible solutions more quickly. This exam- ple demonstrates that multiple optimal solutions should be considered during evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Solving the multiple solution problem</head><p>Multiple optimal solutions occur when concepts alone are not sufficient to distinguish between two competing summary candidates. Extending the model so that it provides a single solution can therefore not be done without introducing a sec- ond term in the objective function. Following the observation that the frequency of a non-stop word in a document set is a good predictor of a word ap- pearing in a human summary (Nenkova and Van- derwende, 2005), we extend equation 1 as follows:</p><formula xml:id="formula_2">max i w i c i + µ k f k t k (5)</formula><p>where f k is the frequency of non-stop word k in the document set, and t k is a binary variable indi- cating the presence of k in the summary. Here, we want to induce a single solution among the multi- ple optimal solutions given by concept weighting, and thus set µ to a small value (10 −6 ). We add further constraints, similar to equations 3 and 4, to ensure the consistency of the solution.</p><p>This extended model succeeds in giving a sin- gle solution that is at least comparable to the mean score of the multiple optimal solutions. How- ever, it requires about twice as much time to solve which makes it impractical for large documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Fast approximation</head><p>Instead of pruning concepts to reduce complex- ity, one may consider using an approximation if results are found satisfactory. Here, simi- larly to <ref type="bibr" target="#b12">(Takamura and Okumura, 2009;</ref><ref type="bibr" target="#b6">Lin and Bilmes, 2010)</ref> we implement the greedy heuristic proposed in ( <ref type="bibr">Khuller et al., 1999</ref>) that solve the budgeted maximum coverage problem with a per- formance guarantee 1 /2 · (1 − 1 /e). <ref type="table">Table 4</ref> com- pares the performance of the model that achieves the best trade off between effectiveness and run- time, that is when DF ≥ 3, with that of the greedy approximation without pruning.</p><p>Overall, the approximate solution is over 96% as good as the average optimal solution. Although the ILP solution marks an upper bound on perfor- mance, its solving time is exponential in the num- ber of input sentences. The approximate method is then relevant as it marks an upper bound on speed (less than 0.01 seconds to compute) while having performance comparable to the ILP model with concept pruning (p-value &gt; 0.3).  <ref type="table">Table 4</ref>: ROUGE recall scores of the approxima- tion. The relative difference from the mean score of the multiple optimal solutions is also reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Multiple optimal solutions are not an issue as long as alternate solutions are equivalent. Unfortu- nately, summaries generated from different sets of sentences are likely to differ. We showed through experiments that concept pruning leads to the pres- ence of multiple optimal solutions, and that the latter cause a substantial amount of variation in ROUGE scores. We proposed an extension of the ILP that obtains unique solutions. If speed is a concern, we showed that a near-optimal approx- imation can be computed without pruning. The implementation of the concept-based summariza- tion model that we use in this study is available at https://github.com/boudinfl/sume.</p><p>In future work, we intend to extend our study to compressive summarization. We expect that the number of optimal solutions will increase as mul- tiple compression candidates, which are likely to be similar in content, are added to the set of input sentences.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Average number of optimal solutions, concepts and sentences for different minimum document 
frequencies. The average time in seconds for finding the first optimal solution is also reported. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean ROUGE recall and standard deviation for different minimum document frequencies. 

Solver 
ROUGE-1 
↓ / ↑ 

GLPK 
37.33 54 / 0 
Gurobi 
37.20 52 / 1 
CPLEX 
37.17 51 / 1 

</table></figure>

			<note place="foot" n="1"> Given a collection S of sets with associated costs and a budget L, find a subset S ⊆ S such that the total cost of sets in S does not exceed L, and the total weight of elements covered by S is maximized (Khuller et al., 1999).</note>

			<note place="foot" n="2"> We use splitta v1.03, https://code.google. com/p/splitta/ 3 We use glpk v4.52, https://www.gnu.org/ software/glpk/ 4 We use the stoplist in nltk, http://www.nltk.org/ 5 We use the Porter stemmer in nltk.</note>

			<note place="foot" n="6"> We use ROUGE-1.5.5 with the parameters: n 4-m-a-l 100-x-c 95-r 1000-f A-p 0.5-t 0</note>

			<note place="foot" n="7"> 71 systems participated at TAC&apos;08 but we removed ICSI1 and ICSI2 systems which are based on the conceptbased ILP model. 8 ROUGE-1 recall is most accurate metric to identify the better summary in a pair (Owczarzak et al., 2012).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially supported by the GOLEM project (grant of CNRS PEPS FaSciDo 2015, http://boudinfl.github.io/GOLEM/). We thank the anonymous reviewers and Rémi Bois for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="481" to="490" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A scalable global model for summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>the Workshop on Integer Linear Programming for Natural Language Processing<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The icsi/utd summarization system at tac</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berndt</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Text Analysis Conference</title>
		<meeting>the Second Text Analysis Conference<address><addrLine>Gaithersburg, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentence boundary detection and the problem with the u.s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers<address><addrLine>Boulder, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="241" to="244" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Seffi) Naor. 1999. The budgeted maximum coverage problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using supervised bigram-based ilp for extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1004" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<editor>Stan Szpakowicz Marie-Francine Moens</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th European Conference on IR Research, ECIR&apos;07</title>
		<meeting>the 29th European Conference on IR Research, ECIR&apos;07<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The impact of frequency on summarization. Microsoft Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno>MSR-TR- 2005-101</idno>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Redmond, Washington</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An assessment of the accuracy of automatic evaluation in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karolina</forename><surname>Owczarzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Evaluation Metrics and System Comparison for Automatic Summarization</title>
		<meeting>Workshop on Evaluation Metrics and System Comparison for Automatic Summarization<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast joint compression and summarization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1492" to="1502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text summarization model based on maximum coverage problem and its variant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009)</title>
		<meeting>the 12th Conference of the European Chapter of the ACL (EACL 2009)<address><addrLine>Athens, Greece, March</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="233" to="243" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
