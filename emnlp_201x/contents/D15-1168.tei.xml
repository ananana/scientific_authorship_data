<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Computing Research Institute -HBKU</orgName>
								<address>
									<settlement>Doha</settlement>
									<country>Qatar, Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution" key="instit1">The Chinese University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task. We propose a general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that can be successfully applied to such tasks without any task-specific feature engineering effort. Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models. Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained opinion mining involves identifying the opinion holder who expresses the opinion, de- tecting opinion expressions, measuring their inten- sity and sentiment, and identifying the target or aspect of the opinion ( ). For ex- ample, in the sentence "John says, the hard disk is very noisy", John, the opinion holder, expresses a very negative (i.e., sentiment with intensity) opin- ion towards the target "hard disk" using the opin- ionated expression "very noisy". A number of NLP applications can benefit from fine-grained opinion mining including opinion summarization and opinion-oriented question answering.</p><p>The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task at the sequence (e.g., phrase) level. For example, iden- tifying opinion holders, opinion expressions and opinion targets can be formulated as a token-level sequence tagging problem, where the task is to  <ref type="table">Table 1</ref>: An example sentence annotated with BIO labels for opinion target (TARG tags) and for opin- ion expression (EXPR tags) extraction.</p><p>label each word in a sentence using the conven- tional BIO tagging scheme. For example, <ref type="table">Table  1</ref> shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion ex- pression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification <ref type="bibr" target="#b31">(Socher et al., 2013)</ref>. Conditional random fields (CRFs) ( <ref type="bibr" target="#b15">Lafferty et al., 2001</ref>) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction <ref type="bibr" target="#b38">(Yang and Cardie, 2012)</ref>. The state-of-the-art model for opinion target ex- traction is also based on a CRF ( <ref type="bibr" target="#b27">Pontiki et al., 2014</ref>). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand.</p><p>An alternative approach of deep learning auto- matically learns latent features as distributed vec- tors and have recently been shown to outperform CRFs on similar tasks. For example, <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref> apply deep recurrent neural net- works (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. <ref type="bibr" target="#b31">Socher et al. (2013)</ref> propose recursive neural net- works for a semantic compositional task to iden- tify the sentiments of phrases and sentences hier- archically using the syntactic parse trees.</p><p>Meanwhile, recent advances in word embed-ding induction methods <ref type="bibr" target="#b5">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013b</ref>) have benefited re- searchers in two ways: (i) they have contributed to significant gains when used as extra word fea- tures in existing NLP systems <ref type="bibr" target="#b35">(Turian et al., 2010;</ref><ref type="bibr" target="#b17">Lebret and Lebret, 2013)</ref>, and (ii) they have en- abled more effective training of RNNs by provid- ing compact input representations of the words <ref type="bibr" target="#b20">(Mesnil et al., 2013;</ref><ref type="bibr" target="#b12">Irsoy and Cardie, 2014)</ref>. Motivated by the recent success of deep learn- ing, in this paper we propose a general class of models based on RNN architecture and word em- beddings, that can be successfully applied to fine- grained opinion mining tasks without any task- specific feature engineering effort. We experiment with several important RNN architectures includ- ing Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations. We acquire pre-trained word embeddings from several exter- nal sources to give better initialization to our RNN models. The RNN models then fine-tune the word vectors during training to learn task-specific em- beddings. We also present an architecture to in- corporate other linguistic features into RNNs.</p><p>Our results on the task of opinion target extrac- tion show that word embeddings improve the per- formance of state-of-the-art CRF models, when included as additional features. They also improve RNNs when used as pre-trained word vectors and fine-tuning them on the task gives the best results. A comparison between models demonstrates that RNNs outperform CRFs, even when they use word embeddings as the only features. Incorporating simple linguistic features into RNNs improves the performance even further. Our best results with LSTM RNN outperform the top performing sys- tem on the Laptop dataset and achieve the second best on the Restaurant dataset in SemEval-2014. We make our source code available. <ref type="bibr">1</ref> In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3. In Section 4, we briefly de- scribe the pre-trained word embeddings. The ex- periments and analysis of results are presented in Section 5. Finally, we summarize our contribu- tions with future directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., <ref type="bibr" target="#b1">Breck et al., 2007)</ref>. The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders ( <ref type="bibr" target="#b4">Choi et al., 2005)</ref>, and intensity and polarity <ref type="bibr" target="#b3">(Choi and Cardie, 2010</ref>).</p><p>Extracting aspect terms or opinion targets have been actively investigated in the past. Typical ap- proaches include association mining to find fre- quent item sets (i.e., co-occurring words) as can- didate aspects ( <ref type="bibr" target="#b11">Hu and Liu, 2004</ref>), classification- based methods such as hidden Markov model ( <ref type="bibr" target="#b13">Jin et al., 2009</ref>) and CRF ( <ref type="bibr" target="#b30">Shariaty and Moghaddam, 2011;</ref><ref type="bibr" target="#b38">Yang and Cardie, 2012;</ref><ref type="bibr" target="#b39">Yang and Cardie, 2013)</ref>, as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants <ref type="bibr" target="#b33">(Titov and McDonald, 2008;</ref><ref type="bibr" target="#b18">Lin and He, 2009;</ref><ref type="bibr" target="#b25">Moghaddam and Ester, 2012</ref>).</p><p>Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to vari- ous sequence prediction tasks, such as language modeling <ref type="bibr" target="#b21">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b32">Sundermeyer et al., 2012)</ref>, speech recognition ( <ref type="bibr" target="#b9">Graves and Jaitly, 2014;</ref><ref type="bibr" target="#b28">Sak et al., 2014</ref>) and spoken language un- derstanding ( <ref type="bibr" target="#b20">Mesnil et al., 2013</ref>). For sentiment analysis, <ref type="bibr" target="#b31">Socher et al. (2013)</ref> propose to use re- cursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. <ref type="bibr" target="#b16">Le and Zuidema (2015)</ref> extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children.</p><p>Most relevant to our work is the recent work of <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref>, where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semi- CRF and shallow RNN. They used word embed- dings from Google without fine-tuning them.</p><p>Although inspired, our work differs from the work of <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref> in many ways. (i) We experiment with not only Elman-type, but also with a Jordan-type and with a more advanced LSTM RNN, and demonstrate the performance of various RNN models. (ii) We use not only Google embeddings as pre-trained word vectors, but also other embeddings including SENNA and Amazon, and show their performances. (iii) We also fine- tune the embeddings for our task, which is shown to be very crucial. (iv) We present an RNN ar-chitecture to include other linguistic features and show its effectiveness. (v) Finally, we present a comprehensive experiment exploring different embedding dimensions and hidden layer sizes for all the variations of the RNNs (i.e., including fea- tures and bi-directionality).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Neural Models</head><p>The recurrent neural models in this section com- pute compositional vector representations for word sequences of arbitrary length. These high- level (i.e., hidden-layer) distributed representa- tions are then used as features to classify each to- ken in the sentence. We first describe the com- mon properties shared among the RNNs below, followed by the descriptions of the specific RNNs.</p><p>Each word in the vocabulary V is represented by a D dimensional vector in the shared look-up table L ∈ R |V |×D . Note that L is considered as a model parameter to be learned. We can initial- ize L randomly or by pre-trained word embedding vectors (see Section 4). Given an input sentence s = (s 1 , · · · , s T ), we first transform it into a fea- ture sequence by mapping each word token s t ∈ s to an index in L. The look-up layer then cre- ates a context vector x t ∈ R mD covering m − 1 neighboring tokens for each s t by concatenating their respective vectors in L. For example, given the context size m = 3, the context vector x t for the word disk in <ref type="figure">Figure 1</ref> is formed by con- catenating the embeddings of hard, disk and is. This window-based approach is intended to cap- ture short-term dependencies between neighbor- ing words in a sentence <ref type="bibr" target="#b6">(Collobert et al., 2011</ref>).</p><p>The concatenated vector is then passed through non-linear recurrent hidden layers to learn high- level compositional representations, which are in turn fed to the output layer for classification using softmax. Formally, the probability of k-th label in the output for classification into K classes:</p><formula xml:id="formula_0">P (y t = k|s, θ) = exp (w T k h t ) K k=1 exp (w T k h t )<label>(1)</label></formula><p>where, h t = φ(x t ) defines the transformations of x t through the hidden layers, and w k are the weights from the last hidden layer to the output layer. We fit the models by minimizing the nega- tive log likelihood (NLL) of the training data. The NLL for the sentence s can be written as:</p><formula xml:id="formula_1">J(θ) = T t=1 K k=1 y tk log P (y t = k|s, θ)<label>(2)</label></formula><p>where, y tk = I(y t = k) is an indicator variable to encode the gold labels, i.e., y tk = 1 if the gold label y t = k, otherwise 0. <ref type="bibr">2</ref> The loss function mini- mizes the cross-entropy between the predicted dis- tribution and the target distribution (i.e., gold la- bels). The main difference between the models described below is how they compute h t = φ(x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Elman-type RNN (Elman, 1990)</head><p>In an Elman-type RNN <ref type="figure">(Fig. 1a)</ref>, the output of the hidden layer h t at time t is computed from a non- linear transformation of the current input x t and the previous hidden layer output h t−1 . Formally,</p><formula xml:id="formula_2">h t = f (U h t−1 + V x t + b) (3)</formula><p>where f is a nonlinear function (e.g., sigmoid) applied to the hidden units. U and V are weight matrices between two consecutive hidden layers, and between the input and the hidden layers, re- spectively, and b is the bias vector. This RNN thus creates internal states by re- membering previous hidden layer, which allows it to exhibit dynamic temporal behavior. We can in- terpret h t as an intermediate representation sum- marizing the past, which is in turn used to make a final decision on the current input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Jordan-type RNN (Jordan, 1997)</head><p>Jordan-type RNNs <ref type="figure">(Fig. 1b)</ref> are similar to Elman- type RNNs except that the hidden layer h t at time t is fed from the previous output layer y t−1 instead of the previous hidden layer h t−1 . Formally,</p><formula xml:id="formula_3">h t = f (U y t−1 + V x t + b)<label>(4)</label></formula><p>where U , V , b, and f are similarly defined as be- fore. Both Elman-type and Jordan-type RNNs are known as simple RNNs. These types of RNNs are generally trained using stochastic gradient de- scent (SGD) with backpropagation through time (BPTT), where errors (i.e., gradients) are propa- gated back through the edges over time.</p><p>One common issue with BPTT is that as the er- rors get propagated, they may soon become very small or very large that can lead to undesired val- ues in weight matrices, causing the training to fail.  <ref type="figure">Figure 1</ref>: Elman-type, Jordan-type and LSTM RNNs with a lookup-table layer, a hidden layer and an output layer. The concatenated context vector for the word "disk" at time t is</p><formula xml:id="formula_4">1 t - h t h 1 t + h t x 1 t - x 1 t + x t y 1 t - y 1 t + y U W V The hard disk is very (a) Elman-type RNN 1 t + h t x 1 t - x 1 t + x t y 1 t - y 1 t + y W U V</formula><formula xml:id="formula_5">x t = [x hard , x disk , x is ]</formula><p>with a context window of size 3. One memory block in the LSTM hidden layer has been enlarged.</p><p>This is known as the vanishing and the exploding gradients problem ( <ref type="bibr" target="#b0">Bengio et al., 1994)</ref>. One sim- ple way to overcome this issue is to use a truncated BPTT (Mikolov, 2012) for restricting the back- propagation to only few steps like 4 or 5. However, this solution limits the RNN to capture long-range dependencies. In the following, we describe an el- egant RNN architecture to address this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long Short-Term Memory RNN</head><p>Long Short-Term Memory or LSTM <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber, 1997)</ref> is specifically designed to model long term dependencies in RNNs. The recurrent layer in a standard LSTM is constituted with special (hidden) units called memory blocks <ref type="figure">(Fig. 1c)</ref>. A memory block is composed of four elements: (i) a memory cell c (i.e., a neuron) with a self-connection, (ii) an input gate i to control the flow of input signal into the neuron, (iii) an out- put gate o to control the effect of the neuron ac- tivation on other neurons, and (iv) a forget gate f to allow the neuron to adaptively reset its cur- rent state through the self-connection. The follow- ing sequence of equations describe how a layer of memory blocks is updated at every time step t:</p><formula xml:id="formula_6">it = σ(Uiht−1 + Vixt + Cict−1 + bi) (5) ft = σ(U f ht−1 + V f xt + C f ct−1 + b f ) (6) ct = it g(Ucht−1 + Vcxt + bc) + ft ct−1 (7) ot = σ(Uoht−1 + Voxt + Coct + bo) (8) ht = ot h(ct)<label>(9)</label></formula><p>where U k , V k and C k are the weight matrices be- tween two consecutive hidden layers, between the input and the hidden layers, and between two con- secutive cell activations, respectively, which are associated with gate k (i.e., input, output, forget and cell), and b k is the associated bias vector. The symbol denotes a element-wise product of the two vectors. The gate function σ is the sigmoid activation, and g and h are the cell input and cell output activations, typically a tanh. LSTMs are generally trained using truncated or full BPTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bidirectionality</head><p>Notice that the RNNs defined above only get infor- mation from the past. However, information from the future could also be crucial. In our example sentence <ref type="table">(Table 1)</ref>, to correctly tag the word hard as a B-TARG, it is beneficial for the RNN to know that the next word is disk. Our window-based ap- proach, by considering the neighboring words, al- ready captures short-term dependencies like this from the future. However, it requires tuning to find the right window size, and it disregards long-range dependencies that go beyond the context window, which is typically of size 1 (i.e., no context) to 5 (see Section 5.2). For instance, consider the two sentences: (i) Do you know about the crunchy tuna here, it is to die for. and (ii) Do you know about the crunchy tuna here, it is imported from Norway. The phrase crunchy tuna is an aspect term in the first (subjective) sentence, but not in the second (objective) one. The RNN models described above will assign the same labels to crunchy and tuna in both sentences, since the preceding sequences and the context window (of size 1 to 5) are the same.</p><p>To capture long-range dependencies from the future as well as from the past, we propose to use bidirectional RNNs ( <ref type="bibr" target="#b29">Schuster and Paliwal, 1997)</ref>, which allow bidirectional links in the network. In an Elman-type bidirectional RNN <ref type="figure">(Fig. 2a)</ref>, the forward hidden layer − → h t and the backward hidden layer ← − h t at time t are computed as follows:</p><formula xml:id="formula_7">− → h t = f ( − → U h t−1 + − → V x t + − → b ) (10) ← − h t = f ( ← − U h t−1 + ← − V x t + ← − b )<label>(11)</label></formula><formula xml:id="formula_8">1 t h f t h f 1 t h f t x 1 t x 1 t x t y 1 t y 1 t y</formula><p>The hard disk is very</p><formula xml:id="formula_9">1 t h t x 1 t x 1 t x t y 1 t y 1 t y U W V</formula><p>The hard disk is very </p><formula xml:id="formula_10">t = [ − → h t , ← − h t ]</formula><p>is passed to the output layer. We can thus interpret h t as an intermediate representation sum- marizing the past and the future, which is then used to make a final decision on the current input.</p><p>Similarly, the unidirectional LSTM RNN can be extended to bidirectional LSTM by allowing bidi- rectional connections in the hidden layer. This amounts to having a backward counterpart for each of the equations from 5 to 9.</p><p>Notice that the forward and the backward com- putations of bidirectional RNNs are independently done until they are combined in the output layer. This means, during training, after backpropagat- ing the errors from the output layer to the forward and to the backward hidden layers, two indepen- dent BPTT can be applied -one to each direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-tuning of Embeddings</head><p>In our RNN framework, we intend to avoid manual feature engineering efforts by using word embed- dings as the only features. As mentioned before, we can initialize the embeddings randomly and learn them as part of model parameters by back- propagating the errors to the look-up layer. One issue with random initialization is that it may lead the SGD to get stuck in local minima <ref type="bibr" target="#b26">(Murphy, 2012)</ref>. On the other hand, one can plug the readily available embeddings from external sources (Sec- tion 4) in the RNN model and use them as features without tuning them further for the task, as is done in any other machine learning model. However, this approach does not exploit the automatic fea- ture learning capability of NN models, which is one of the main motivations of using them.</p><p>In our work, we use the pre-trained word em- beddings to better initialize our models, and we fine-tune them for our task in training, which turns out to be quite beneficial (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Incorporating other Linguistic Features</head><p>Although NNs learn word features (i.e., embed- dings) automatically, we may still be interested in incorporating other linguistic features like part-of- speech (POS) tags and chunk information to guide the training and to learn a better model. However, unlike word embeddings, we want these features to be fixed during training. As shown in <ref type="figure">Figure  2b</ref>, this can be done in our RNNs by feeding these additional features directly to the output layer, and learn their associated weights in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Word Embeddings</head><p>Word embeddings are distributed representations of words, represented as real-valued, dense, and low-dimensional vectors. Each dimension poten- tially describes syntactic or semantic properties of the word. Here we briefly describe the three types of pre-trained embeddings that we use in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SENNA Embeddings</head><p>Collobert et al. (2011) present a unified NN archi- tecture for various NLP tasks (e.g., POS tagging, chunking, semantic role labeling, named entity recognition) with a window-based approach and a sentence-based approach (i.e., the input layer is a sentence). Each word in the input layer is represented by M features, each of which has an embedding vector associated with it in a lookup table. To give their network a better initializa- tion, they learn word embeddings using a non- probabilistic language model, which was trained on English Wikipedia for about 2 months. They released their 50-dimensional word embeddings (vocabulary size 130K) under the name SENNA. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Google Embeddings</head><p>Mikolov et al. (2013a) propose two log-linear models for computing word embeddings from large corpora efficiently: (i) a bag-of-words model CBOW that predicts the current word based on the context words, and (ii) a skip-gram model that pre- dicts surrounding words given the current word.</p><p>They released their pre-trained 300-dimensional word embeddings (vocabulary size 3M ) trained by the skip-gram model on part of Google news dataset containing about 100 billion words. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Amazon Embeddings</head><p>Since we work on customer reviews, which are less formal than Wikipedia and news, we have also trained domain-specific embeddings (vocabulary size 1M ) using the CBOW model of word2vec tool ( <ref type="bibr" target="#b23">Mikolov et al., 2013b</ref>) from a large cor- pus of Amazon reviews. <ref type="bibr">5</ref> The corpus contains 34, 686, 770 reviews (4.7 billion words) on Ama- zon products from June 1995 to <ref type="bibr">March 2013 (McAuley and</ref><ref type="bibr" target="#b19">Leskovec, 2013</ref>). For comparison with SENNA and Google, we learn word embed- dings of 50-and 300-dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present our experimental set- tings and results for the task of opinion target ex- traction from customer reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Datasets: In our experiments, we use the two review datasets provided by the SemEval-2014 task 4: aspect-based sentiment analysis evaluation campaign ( <ref type="bibr" target="#b27">Pontiki et al., 2014</ref>), namely the Laptop and the Restaurant datasets. <ref type="table" target="#tab_3">Table 2</ref> shows some basic statistics about the datasets. The majority of aspect terms have only one word, while about one third of them have multiple words. In both datasets, some sentences have no aspect terms and some have more than one aspect terms. We use the standard train:test split to compare our results with the SemEval best systems. In addition, we show a more general performance of our models on the two datasets based on 10-fold cross validation.  Evaluation Metric: The evaluation metric mea- sures the standard precision, recall and F 1 score based on exact matches. This means that a candi- date aspect term is considered to be correct only if it exactly matches with the aspect term annotated by the human. In all our experiments when com- paring two models, we use paired t-test on the F 1 scores to measure statistical significance and re- port the corresponding p-value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Laptop</head><p>CRF Baseline: We use a linear-chain CRF <ref type="bibr" target="#b15">(Lafferty et al., 2001</ref>) of order 2 as our baseline, which is the state-of-the-art model for opinion target ex- traction ( <ref type="bibr" target="#b27">Pontiki et al., 2014</ref>). Specifically, the CRF generates (binary) feature functions of order 1 and 2; see ( <ref type="bibr" target="#b7">Cuong et al., 2014</ref>) for higher or- der CRFs. The features used in the baseline model include the current word, its POS tag, its prefixes and suffixes between one to four characters, its po- sition, its stylistics (e.g., case, digit, symbol, al- phanumeric), and its context (i.e., the same fea- tures for the two preceding and the two following words). In addition to the hand-crafted features, we also include the three different types of word embeddings described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN Settings:</head><p>We pre-processed each dataset by lowercasing all words and spelling out each digit number as DIGIT. We then built the vocabulary from the training set by marking rare words with only one occurrence as UNKNOWN, and adding a PADDDING word to make context windows for boundary words.</p><p>To implement early stopping in SGD, we prepared a validation set by separating out randomly 10% of the available training data. The remaining 90% is used for training. The weights in the network were initialized by sampling from a small random uniform distribution U(−0.2, 0.2). The time step in the truncated BPTT was fixed to 6 based on the performance on the validation set; smaller values hurt the performance, while larger values showed no significant gains but increased the training time.</p><p>We use a fixed learning rate of 0.01, but we change the batch size depending on the sentence length following <ref type="bibr" target="#b20">Mesnil et al. (2013)</ref>. The net effect is a variable step size in the SGD. We run SGD for 30 epochs, calculate the F 1 score on the validation set after each epoch, and stop if the accuracy starts to decrease. The size of the context window and the hidden layer are empirically set based on the per- formance on the validation set. We experimented with the window size ∈ {1, 3, 5}, and found 3 to be the optimal on the validation set. The hidden layer sizes we experimented with are 50, 100, 150, and 200; we report the optimal values in <ref type="table" target="#tab_5">Table 3</ref> (see |h l | and |h r | columns).</p><p>Linguistic Features in RNNs: In addition to the neural features, we also explore the contribution of simple linguistic features in our RNN mod- els using the architecture described in Section 3.6. Specifically, we encode four POS tag classes (noun, adjective, verb, adverb) and BIO-tagged chunk information (NP, VP, PP, ADJP, ADVP) as binary features. We feed these extra features di- rectly to the output layer of the RNNs and learn their relative weights. Part-of-speech and phrasal information are arguably the most informative features for identifying aspect terms (i.e., aspect terms are generally noun phrases). BIO tags could be useful to find the right text spans (i.e., aspect terms are unlikely to violate phrasal boundaries). <ref type="table" target="#tab_5">Table 3</ref> presents our results of aspect term extrac- tion on the standard testset in F 1 scores. In <ref type="table" target="#tab_6">Table  4</ref>, we show the results on the whole datasets based on 10-fold cross validation. RNNs in <ref type="table" target="#tab_6">Table 4</ref> are trained using SENNA embeddings. We perform significance tests on the 10-fold results. In the fol- lowing, we highlight our main findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of Word Embeddings in CRF:</head><p>From the first group of results in <ref type="table" target="#tab_5">Table 3</ref>, we can observe that even though CRF uses a hand- ful of hand-designed features, including word embeddings still leads to sizable improvements on both datasets. The domain-specific Amazon embeddings (300 dim.) yield more general performance across the datasets, delivering the best gain of absolute 3.54% on the Laptop and the second best on the Restaurant dataset. Google embeddings give the best gain on the Restaurant dataset (absolute 3.08%). The contribution of embeddings in CRF is also validated by the 10-fold results in <ref type="table" target="#tab_6">Table 4</ref> (see first two rows), where SENNA embeddings yield significant improvements -absolute 1.47% on Laptop (p &lt; 0.03) and absolute 1.24% on Restaurant (p &lt; 0.01). This demonstrates that word embeddings offer generalizations that complement other strong features, and thus should be considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF vs. RNNs: When we compare the results of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Dim. |h l | Laptop |h r | Restaurant CRF <ref type="table" target="#tab_3">Base  - - 68.66  - 77.28  +SENNA  50  - 71.38  - 78.54  +Amazon  50  - 70.61  - 79.46  +Google  300  - 68.</ref>    RNNs with those of CRF in <ref type="table" target="#tab_5">Table 3</ref>, we see that most of our RNN models outperform CRF mod- els with the maximum absolute gains of 2.80% by LSTM-RNN+Feat. on Laptop and 1.70% by Bi- Elman-RNN+Feat. on Restaurant. What is re- markable is that RNNs without any hand-crafted features outperform feature-rich CRF models by a good margin -absolute maximum gains of 2.23% by Elman-RNN and 1.83% by Bi-LSTM-RNN on Laptop. When we compare their general per- formance on the 10-folds in <ref type="table" target="#tab_6">Table 4</ref>, we observe similar gains, maximum 5.88% on Laptop and 1.97% on Restaurant, which are significant with p &lt; 6 × 10 −6 on Laptop and p &lt; 2 × 10 −4 on Restaurant. These results demonstrate that RNNs as sequence labelers are more effective than CRFs for fine-grained opinion mining tasks. This can be attributed to RNN's ability to learn better features automatically and to capture long-range sequential dependencies between the output labels.</p><p>Comparison among RNN Models: A compari- son among the RNN models in <ref type="table" target="#tab_5">Table 3</ref> tells that Elman RNN generally outperforms Jordan RNN.</p><p>However, bi-directionality and LSTM do not pro- vide clear gains over the simple Elman RNN. In fact, bi-directionality hurts the performance in most cases. This finding contrasts the find- ing of <ref type="bibr" target="#b12">Irsoy and Cardie (2014)</ref> in opinion ex- pression detection task, where bi-directional El- man RNNs outperform their uni-directional coun- terparts. However, when we analyzed the data, we found it to be unsurprising because aspect terms are generally shorter than opinion expres- sions. For example, the average length of an aspect term in our Restaurant dataset is 1.4, where the average length of an expressive subjective expres- sion in their MPQA corpus is 3.3. Therefore, the information required to correctly identify aspect terms (e.g., hard disk) is already captured by the simple (as opposed to LSTM) unidirectional link and the context window covering the neighboring words. LSTM and Bi-directionality increase the number of parameters in the RNNs, which might contribute to overfitting on this specific task. <ref type="bibr">6</ref> As a partial solution to this problem, we experi- mented with a bi-directional Elman-RNN, where both directions share the same parameters. There- fore, the number of parameters remains the same as the uni-directional one. This modification im- proves the performance over the non-shared one slightly but not significantly. This demands for better modeling of the two sources of information rather than simple concatenation or sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions of Linguistic Features in RNNs:</head><p>Although our linguistic features are quite simple (i.e., POS tags and chunk), they give gains on both datasets when incorporated into Elman and LSTM RNNs. The maximum gains on the stan- dard testset <ref type="table" target="#tab_5">(Table 3</ref>) are 0.64% on Laptop and 1.96% on Restaurant for Bi-Elman, and 1.48% on Laptop and 1.58% on Restaurant for LSTM. Similar gains are also observed on the 10-folds in <ref type="table" target="#tab_6">Table 4</ref>, where the maximum gains are 1.25% on Laptop and 1.44% on Restaurant. These gains are significant with p &lt; 0.004 on Laptop and p &lt; 6 × 10 −5 on Restaurant. Linguistic features thus complement word embeddings in RNNs.</p><p>Importance of Fine-tuning in RNNs: Finally, in order to show the importance of fine-tuning the word embeddings in RNNs on our task, we present in <ref type="table" target="#tab_7">Table 5</ref> the performance of Elman and Jordan RNNs, when the embeddings are used as they are ('-tune'), and when they are fine-tuned ('+tune') on the task. The table also shows the contributions of pre-trained embeddings as compared to random initialization. Surprisingly, Amazon embeddings without fine-tuning deliver the worst performance, even lower than the Ran- dom initialization. We found that with Amazon embeddings the network gets stuck in a local minimum from the very first epoch.</p><p>Other pre-trained (untuned) embeddings improve over the Amazon and Random by providing better initialization. In most cases fine-tuning makes a big difference. For example, the absolute gains for fine-tuning SENNA embeddings in Elman RNN are 13.01% in Laptop and 4.11% in Restaurant. Remarkably, fine-tuning brings both Random and Amazon embeddings close to the best ones.</p><p>Comparison with SemEval-2014 Systems: When our RNN results are compared with the top performing systems in the SemEval-2014 (last two rows in <ref type="table" target="#tab_5">Table 3</ref>), we see that RNNs without using any linguistic features achieve the second best results on both Laptop and Restaurant datasets. Note that these RNNs only use word embeddings, while IHS RD and DLIREC use complex features like dependency relations, named entity, sentiment orientation of words, word cluster and many more in their CRF models, most of which are expensive to compute; see <ref type="bibr" target="#b34">(Toh and Wang, 2014;</ref><ref type="bibr" target="#b2">Chernyshevich, 2014)</ref>. The performance of our RNNs improves when they are given access to very simple features like POS tags and chunks, and LSTM-RNN+Feat. achieves the best results on the Laptop dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Direction</head><p>We presented a general class of discriminative models based on recurrent neural network (RNN) architecture and word embeddings, that can be successfully applied to fine-grained opinion min- ing tasks without any task-specific manual feature engineering effort. We used pre-trained word em- beddings from three external sources in different RNN architectures including Elman-type, Jordan- type, LSTM and their several variations.</p><p>Our results on the opinion target extraction task demonstrate that word embeddings improve the performance of both CRF and RNN models, how- ever, fine-tuning them in RNNs on the task gives the best results. RNNs outperform CRFs, even when they use word embeddings as the only fea- tures. Incorporating simple linguistic features into RNNs improves the performance further. Our best results with LSTM RNN outperform the top performing system on the Laptop dataset and achieve the second best on the Restaurant dataset in SemEval-2014 evaluation campaign. We made our code publicly available for research purposes.</p><p>In the future, we would like apply our models to other fine-grained opinion mining tasks includ- ing opinion expression detection and characteriz- ing the intensity and sentiment of the opinion ex- pressions. We would also like to explore to what extent these tasks can be jointly modeled in an RNN-based multi-task learning framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: (a) Bidirectional Elman-type RNN and (b) Linguistic features concatenated with the hidden layer output in Elman-type RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 t x 1 t x t y 1 t y 1 t y The hard disk is very t c × t i t f t o × × t h t x Input Gate Output Gate</head><label></label><figDesc></figDesc><table>The hard disk 
is very 

(b) Jordan-type RNN 

lstm 
lstm 
lstm 

t 

x 

Forget Gate 

Memory Cell 

LSTM 

(c) Long Short-Term Memory (LSTM) RNN 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Corpora statistics. 

4 https://code.google.com/p/word2vec/ 
5 https://snap.stanford.edu/data/web-Amazon.html 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>F 1 -score performance for CRF baselines, 
RNNs and SemEval'14 best systems on the stan-
dard Laptop and Restaturant testsets. |h l | and |h r | 
columns show the number of hidden units. 

1439 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 :</head><label>4</label><figDesc>10-fold cross validation results of the models on the two datasets. Elman-and LSTM-RNNs are trained using SENNA embeddings.</figDesc><table>System 
Dim. 
Laptop 
Restaurant 
Elman-RNN 
-tune +tune -tune +tune 
+SENNA 
50 
60.85 73.86 75.78 79.89 
+Amazon 
50 
15.51 74.43 22.85 80.37 
+Random 
50 
38.26 72.99 56.98 78.44 
+Google 
300 67.91 72.91 74.73 79.54 
+Amazon 300 15.51 73.67 22.85 79.82 
Jordan-RNN 
-tune +tune -tune +tune 
+SENNA 
50 
58.81 71.41 74.68 78.83 
+Amazon 
50 
15.51 73.21 22.85 79.01 
+Random 
50 
38.05 71.46 55.65 77.38 
+Google 
300 69.39 73.42 77.33 79.89 
+Amazon 300 15.51 72.43 22.85 78.30 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 : Effects of fine-tuning in Elman-RNN and Jordan-RNN.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> This is also known as one-hot vector representation.</note>

			<note place="foot" n="1"> th t h</note>

			<note place="foot" n="1"> t h b t h b 1 t h b (a) 1 t h t h</note>

			<note place="foot" n="1"> t h 1 t f t h t f</note>

			<note place="foot" n="3"> http://ronan.collobert.com/senna/</note>

			<note place="foot" n="6"> Bi-directional links double the number of parameters in RNNs.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are grateful to the anonymous reviewers for their insightful comments and suggestions to im-prove the paper. This research is affiliated with the Stanley Ho Big Data Decision Analytics Research Centre of The Chinese University of Hong Kong.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Identifying expressions of opinion in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Breck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2683" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">IHS R&amp;D Belarus: Cross-domain extraction of product features using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryna</forename><surname>Chernyshevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">309</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical sequential learning for extracting opinions and their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying sources of opinions with conditional random fields and extraction patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Patwardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Conditional random field with high-order dependencies for sequence labeling and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Viet Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><forename type="middle">Leong</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="981" to="1009" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards endto-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel lexicalized HMM-based learning framework for web opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hay</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Serial order: A parallel distributed processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in psychology</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="471" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional distributional semantics with long short term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the joint Conference on Lexical and Computational Semantics (*SEM)</title>
		<meeting>the joint Conference on Lexical and Computational Semantics (*SEM)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebret</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5542</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Word emdeddings through Hellinger PCA. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
		<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Investigation of recurrent-neuralnetwork architectures and learning methods for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3771" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Cernock`Cernock`y, and Sanjeev Khudanpur</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Statistical Language Models based on Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Brno University of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the design of LDA models for aspect-based opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM</title>
		<meeting>CIKM</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="803" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<title level="m">Machine Learning A Probabilistic Perspective</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Proceedings of the 8th International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Françoise</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fine-grained opinion mining using conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shabnam</forename><surname>Shariaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Moghaddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Mining Workshops (ICDMW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling online reviews with multi-grain topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="111" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DLIREC: Aspect term extraction and term polarity classification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenting</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">235</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of ACL</title>
		<meeting>the 48th Annual Meeting of ACL</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phrase-level sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Extracting opinion expressions with semi-Markov conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1335" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint inference for fine-grained opinion extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
