<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Binarized Neural Network Joint Model for Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<addrLine>3-5Hikaridai</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Keihanna Science City</orgName>
								<address>
									<postCode>619-0289</postCode>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Graduate School of Information Science</orgName>
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<postCode>630-0192</postCode>
									<settlement>Takayama</settlement>
									<region>Ikoma, Nara</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Binarized Neural Network Joint Model for Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the bina-rized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network translation models, which learn mappings over real-valued vector representations in high-dimensional space, have recently achieved large gains in translation accuracy ( <ref type="bibr" target="#b5">Hu et al., 2014;</ref><ref type="bibr" target="#b3">Devlin et al., 2014;</ref><ref type="bibr" target="#b12">Sundermeyer et al., 2014;</ref><ref type="bibr" target="#b1">Auli et al., 2013;</ref><ref type="bibr" target="#b11">Schwenk, 2012;</ref><ref type="bibr" target="#b13">Sutskever et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>.</p><p>Notably, <ref type="bibr" target="#b3">Devlin et al. (2014)</ref> proposed a neural network joint model (NNJM), which augments the n-gram neural network language model (NNLM) with an m-word source context window, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. While this model is effective, the computation cost of using it in a large-vocabulary SMT task is quite expensive, as probabilities need to be normalized over the entire vocabulary. To solve this problem, <ref type="bibr" target="#b3">Devlin et al. (2014)</ref> pre- sented a technique to train the NNJM to be self- normalized and avoided the expensive normaliza- tion cost during decoding. However, they also note that this self-normalization technique sacri- fices neural network accuracy, and the training process for the self-normalized neural network is very slow, as with standard maximum likelihood estimation (MLE).</p><p>To remedy the problem of long training times in the context of NNLMs, <ref type="bibr" target="#b14">Vaswani et al. (2013)</ref> used a method called noise contrastive estimation (NCE). Compared with MLE, NCE does not re- quire repeated summations over the whole vocab- ulary and performs nonlinear logistic regression to discriminate between the observed data and artifi- cially generated noise.</p><p>This paper proposes an alternative framework of binarized NNJMs (BNNJM), which are similar to the NNJM, but use the current target word not as the output, but as the input of the neural network, estimating whether the target word under exam- ination is correct or not, as shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word in- formation and processed in the hidden layers.</p><p>The BNNJM learns a simple binary classifier, given the context and target words, therefore it can be trained by MLE very efficiently. "Incor- rect" target words for the BNNJM can be gen- erated in the same way as NCE generates noise for the NNJM. We present a novel noise distribu- tion based on translation probabilities to train the NNJM and the BNNJM efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Network Joint Model</head><p>Let T = t |T | 1 be a translation of S = s |S| 1 . The NNJM ( <ref type="bibr" target="#b3">Devlin et al., 2014</ref>) defines the following probability,</p><formula xml:id="formula_0">P (T |S) = |T | i=1 P ti|s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 (1)</formula><p>where target word t i is affiliated with source word s a i . Affiliation a i is derived from the word align- ments using heuristics <ref type="bibr">1</ref> . To estimate these prob- abilities, the NNJM uses m source context words and n − 1 target history words as input to a neural network and performs estimation of unnormalized probabilities p (t i |C) before normalizing over all words in the target vocabulary V ,</p><formula xml:id="formula_1">P (t i |C) = p(t i |C) Z(C) Z (C) = t i ∈V p (t i |C)<label>(2)</label></formula><p>where C stands for source and target context words as in Equation 1. The NNJM can be trained on a word-aligned parallel corpus using standard MLE, but the cost of normalizing over the entire vocabulary to calcu- late the denominator in Equation 2 is quite large. <ref type="bibr" target="#b3">Devlin et al. (2014)</ref>'s self-normalization technique can avoid normalization cost during decoding, but not during training.</p><p>NCE can be used to train NNLM-style models ( <ref type="bibr" target="#b14">Vaswani et al., 2013</ref>) to reduce training times. NCE creates a noise distribution q (t i ), selects k noise samples t i1 , ..., t ik for each t i and introduces a random variable v which is 1 for training exam- ples and 0 for noise samples,</p><formula xml:id="formula_2">P (v = 1, t i |C) = 1 1+k · p(t i |C) Z(C) P (v = 0, t i |C) = k 1+k · q (t i )</formula><p>. NCE trains the model to distinguish training data from noise by maximize the conditional like- lihood,</p><formula xml:id="formula_3">L = log P (v = 1|C, ti) + k j=1 log P (v = 0|C, t ik ).</formula><p>The normalization cost can be avoided by using p (t i |C) as an approximation of P (t i |C). <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Binarized NNJM</head><p>In this paper, we propose a new framework of the binarized NNJM (BNNJM), which is similar to the NNJM but learns not to predict the next word given the context, but solves a binary classifica- tion problem by adding a variable v ∈ {0, 1} that stands for whether the current target word t i is cor- rectly/wrongly produced in terms of source con- text words s a i +(m−1)/2 a i −(m−1)/2 and target history words</p><formula xml:id="formula_4">t i−1 i−n+1 , P v|s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 , t i .</formula><p>The BNNJM is learned by a feed- forward neural network with m + n inputs</p><formula xml:id="formula_5">s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 , t i</formula><p>and two outputs for v = 1/0.</p><p>Because the BNNJM uses the current target word as input, the information about the current target word can be combined with the context word information and processed in the hidden lay- ers. Thus, the hidden layers can be used to learn the difference between correct target words and noise in the BNNJM, while in the NNJM the hid- den layers just contain information about context words and only the output layer can be used to dis- criminate between the training data and noise, giv- ing the BNNJM more power to learn this classifi- cation problem.</p><p>We can use the BNNJM probability in transla- tion as an approximation for the NNJM as below,</p><formula xml:id="formula_6">P t i |s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 ≈ P v = 1|s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 , t i .</formula><p>As a binary classifier, the gradient for a sin- gle example in the BNNJM can be calculated efficiently by MLE without it being necessary to calculate the softmax over the full vocabu- lary. On the other hand, we need to create "positive" and "negative" examples for the clas- sifier. Positive examples can be extracted di- rectly from the word-aligned parallel corpus as</p><formula xml:id="formula_7">s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 , t i</formula><p>; Negative examples can be generated for each positive example in the same way that NCE generates noise data as</p><formula xml:id="formula_8">s a i +(m−1)/2 a i −(m−1)/2 , t i−1 i−n+1 , t i</formula><p>, where t i ∈ V \ {t i }.  ing NNLMs with NCE,</p><formula xml:id="formula_9">q (t i ) = occur(t i ) t i ∈V occur(t i )</formula><p>where occur (t i ) stands for how many times t i occurs in the training corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Translation Model Noise</head><p>In this paper, we propose a noise distribution spe- cialized for translation models, such as the NNJM or BNNJM. <ref type="figure" target="#fig_1">Figure 2</ref> gives a Chinese-to-English parallel sentence pair with word alignments to demon- strate the intuition behind our method. Focusing on s a i =" ", this is translated into t i ="ar- range". For this positive example, UPD is allowed to sample any arbitrary noise, such as t i = "ba- nana". However, in this case, noise t i = "banana" is not useful for model training, as constraints on possible translations given by the phrase table en- sure that "" will never be translated into "ba- nana". On the other hand, noise t i = "arranges" and "arrangement" are both possible translations of "" and therefore useful training data, that we would like our model to penalize.</p><p>Based on this intuition, we propose the use of another noise distribution that only uses t i that are possible translations of s a i , i.e., t i ∈ U (s a i ) \ {t i }, where U (s a i ) contains all target words aligned to s a i in the parallel corpus. Because U (s a i ) may be quite large and con- tain many wrong translations caused by wrong alignments, "banana" may actually be included in U (" "). To mitigate the effect of un- common examples, we use a translation proba- bility distribution (TPD) to sample noise t i from U (s a i ) \ {t i } as follows,</p><formula xml:id="formula_10">q (t i |s a i ) = align(sa i ,t i ) t i ∈U (sa i ) align(sa i ,t i ) where align (s a i , t i</formula><p>) is how many times t i is aligned to s a i in the parallel corpus.</p><p>Note that t i could be unaligned, in which case we assume that it is aligned to a special null word. Noise for unaligned words is sampled according to the TPD of the null word. If several target/source words are aligned to one source/target word, we choose to combine these target/source words as a new target/source word. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setting</head><p>We evaluated the effectiveness of the proposed ap- proach for Chinese-to-English (CE), Japanese-to- English (JE) and French-to-English (FE) transla- tion tasks. The datasets officially provided for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used for the CE and JE tasks. The development and test sets were both provided for the CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set. Word segmentation was done by BaseSeg ( <ref type="bibr" target="#b16">Zhao et al., 2006</ref>) for Chinese and Mecab 4 for Japanese. For the FE language pair, we used stan- dard data for the WMT 2014 translation task. The training sets for CE, JE and FE tasks contain 1M, 3M and 2M sentence pairs, respectively.</p><p>For each translation task, a recent version of Moses HPB decoder ( <ref type="bibr" target="#b6">Koehn et al., 2007</ref>) with the training scripts was used as the baseline (Base). We used the default parameters for Moses, and a 5-gram language model was trained on the tar- get side of the training corpus using the IRSTLM Toolkit <ref type="bibr">5</ref> with improved Kneser-Ney smoothing. Feature weights were tuned by MERT <ref type="bibr" target="#b9">(Och, 2003)</ref>.</p><p>The word-aligned training set was used to learn the NNJM and the BNNJM. <ref type="bibr">6</ref> For both NNJM and BNNJM, we set m = 7 and n = 5. The NNJM was trained by NCE using UPD and TPD as noise distributions. The BNNJM was trained by stan- dard MLE using UPD and TPD to generate nega- tive examples.  <ref type="table">Table 2</ref>: Translation results. The symbol + and * represent significant differences at the p &lt; 0.01 level against Base and NNJM+UPD, respectively. Significance tests were conducted using bootstrap resampling <ref type="bibr" target="#b7">(Koehn, 2004</ref>). the whole neural network (not just the output layer like the NNJM) for each noise sample and thus noise computation is more expensive. However, for different epochs, we resampled the negative example for each positive example, so the BNNJM can make use of different negative examples. <ref type="table">Table 1</ref> shows how many epochs these two models needed and the training time for each epoch on a 10-core 3.47GHz Xeon X5690 machine. <ref type="bibr">7</ref> Trans- lation results are shown in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Discussion</head><p>We can see that using TPD instead of UPD as a noise distribution for the NNJM trained by NCE can speed up the training process signifi- cantly, with a small improvement in performance. But for the BNNJM, using different noise distribu- tions affects translation performance significantly. The BNNJM with UPD does not improve over the baseline system, likely due to the small num- ber of noise samples used in training the BNNJM, while the BNNJM with TPD achieves good per- formance, even better than the NNJM with TPD on the Chinese-to-English and French-to-English translation tasks.</p><p>From <ref type="table">Table 2</ref>, the NNJM does not improve translation performance significantly on the FE task. Note that the baseline BLEU for the FE S: (this) (movement) (continued) (until) (parasite) (by) (two) (tongues) 21 (each other) (contact) (where) (point) (touched) R: this movement is continued until the parasite is touched by the point where the two tongues 21 contact each other . T1: the mobile continues to the parasite from the two tongue 21 contacts the points of contact with each other . T2: this movement is continued until the parasite by two tongue 21 contact points of contact with each other .  <ref type="table">Table 4</ref>: Scores for different translations.</p><p>task is lower than CE and JE tasks, indicating that learning is harder for the FE task than CE and JE tasks. The validation perplexities of the NNJM with UPD for CE, JE and FE tasks are 4.03, 3.49 and 8.37. Despite these difficult learning circum- stances and lack of large gains for the NNJM, the BNNJM improves translations significantly for the FE task, suggesting that the BNNJM is more ro- bust to difficult translation tasks that are hard for the NNJM. <ref type="table" target="#tab_2">Table 3</ref> gives Chinese-to-English translation ex- amples to demonstrate how the BNNJM (with TPD) helps to improve translations over the NNJM (with TPD). In this case, the BNNJM helps to translate the phrase " " bet- ter. <ref type="table">Table 4</ref> gives translation scores for these two translations calculated by the NNJM and the BN- NJM. Context words are used for predictions but not shown in the table.</p><p>As can be seen, the BNNJM prefers T 2 while the NNJM prefers T 1 . Among these predictions, the NNJM and the BNNJM predict the translation for "" most differently. The NNJM clearly pre- dicts that in this case "" should be translated into "to" more than "until", likely because this exam- ple rarely occurs in the training corpus. However, the BNNJM prefers "until" more than "to", which demonstrates the BNNJM's robustness to less fre- quent examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis for JE Translation Results</head><p>Finally, we examine the translation results to ex- plore why the BNNJM with TPD did not outper- form the NNJM with TPD for the JE translation task, as it did for the other translation tasks. We found that using the BNNJM instead of the NNJM on the JE task did improve translation quality sig- nificantly for infrequent words, but not for fre- quent words.</p><p>First, we describe how we estimate translation quality for infrequent words. Suppose we have a test set S, a reference set R and a translation set T with I sentences,</p><formula xml:id="formula_11">Si (1 ≤ i ≤ I) , Ri (1 ≤ i ≤ I) , Ti (1 ≤ i ≤ I) T i contains J individual words, Wij ∈ W ords (Ti) T o (W ij ) is how many times W ij occurs in T i and R o (W ij ) is how many times W ij occurs in R i .</formula><p>The general 1-gram translation accuracy <ref type="bibr" target="#b10">(Papineni et al., 2002</ref>) is calculated as,</p><formula xml:id="formula_12">Pg = I i=1 J j=1 min(To(W ij ),Ro(Wij)) I i=1 J j=1 To(W ij )</formula><p>This general 1-gram translation accuracy does not distinguish word frequency.</p><p>We use a modified 1-gram translation accuracy that weights infrequent words more heavily,</p><formula xml:id="formula_13">P c = I i=1 J j=1 min(To(W ij ),Ro(W ij ))· 1 Occur ( W ij ) I i=1 J j=1 To(W ij )</formula><p>where Occur (W ij ) is how many times W ij oc- curs in the whole reference set. Note P c will not be 1 even in the case of completely accurate trans- lations, but it can approximately reflect infrequent word translation accuracy, since correct frequent word translations contribute less to P c . <ref type="table" target="#tab_4">Table 5</ref> shows P g and P c for different transla- tion tasks. It can be seen that the BNNJM im- proves infrequent word translation quality simi- larly for all translation tasks, but improves gen- eral translation quality less for the JE task than the other translation tasks. We conjecture that the rea- son why the BNNJM is less useful for frequent word translations on the JE task is the fact that the JE parallel corpus has less accurate function word alignments than other language pairs, as the  grammatical features of Japanese and English are quite different. 8 Wrong function word alignments will make noise sampling less effective and there- fore lower the BNNJM performance for function word translations. Although wrong word align- ments will also make noise sampling less effec- tive for the NNJM, the BNNJM only uses one noise sample for each positive example, so wrong word alignments affect the BNNJM more than the NNJM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Xu et al. <ref type="formula" target="#formula_1">(2011)</ref> proposed a method to use binary classifiers to learn NNLMs. But they also used the current target word in the output, similarly to NCE. The BNNJM uses the current target word as input, so the information about the current target word can be combined with the context word in- formation and processed in hidden layers. <ref type="bibr" target="#b8">Mauser et al. (2009)</ref> presented discriminative lexicon models to predict target words. They train a separate classifier for each target word, as these lexicon models use discrete representations of words and different classifiers do not share fea- tures. In contrast, the BNNJM uses real-valued vector representations of words and shares fea- tures, so we train one classifier and can use the similarity information between words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposes an alternative to the NNJM, the BNNJM, which learns a binary classifier that takes both the context and target words as input and combines all useful information in the hidden layers. We also present a novel noise distribution based on translation probabilities to train the BN- NJM efficiently. With the improved noise sam- pling method, the BNNJM can achieve compara- ble performance with the NNJM and even improve the translation results over the NNJM on Chinese- to-English and French-to-English translations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) the traditional NNJM and (b) the proposed BNNJM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A parallel sentence pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The number of noise samples for NCE was set to be 100. For the BNNJM, we used only one neg- ative example for each positive example in each training epoch, as the BNNJM needs to calculate</figDesc><table>CE 

JE 
FE 
E 
T 
E 
T 
E 
T 

NNJM 
UPD 20 22 
19 49 
20 28 
TPD 4 
6 
4 

BNNJM 
UPD 14 16 
12 34 
11 22 
TPD 11 
9 
9 

Table 1: Epochs (E) and time (T) in minutes per 
epoch for each task. 

CE 
JE 
FE 
Base 
32.95 
30.13 
24.56 

NNJM 
UPD 34.36+ 
31.30+ 24.68 
TPD 34.60+ 
31.50+ 24.80 

BNNJM 
UPD 32.89 
30.04 
24.50 
TPD 35.05+* 31.42+ 25.84+* 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Translation examples. Here, S: source; 
R: reference; T 1 uses NNJM; T 2 uses BNNJM. 

NNJM BNNJM 
− &gt;the 
1.681 -0.126 
− &gt;mobile 
-4.506 -3.758 
− &gt;continues 
-1.550 -0.130 
− &gt;to 
2.510 -0.220 
SUM 
-1.865 -4.236 
− &gt;this 
-2.414 -0.649 
− &gt;movement 
-1.527 -0.200 
null− &gt;is 
0.006 -0.055 
− &gt;continued 
-0.292 -0.249 
− &gt;until 
-6.846 -0.186 
SUM 
-11.075 -1.341 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 : 1-gram precisions and improvements.</head><label>5</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> If ti aligns to exactly one source word, ai is the index of this source word; If ti aligns to multiple source words, ai is the index of the aligned word in the middle; If ti is unaligned, they inherit its affiliation from the closest aligned word. 2 The theoretical properties of self-normalization techniques, including NCE and Devlin et al. (2014)&apos;s method, are investigated by Andreas and Klein (2015).</note>

			<note place="foot" n="3"> The processing for multiple alignments helps sample more useful negative examples for TPD, and had little effect on the translation performance when UPD was used as the noise distribution for the NNJM and the BNNJM in our preliminary experiments. 4 http://sourceforge.net/projects/mecab/files/ 5 http://hlt.fbk.eu/en/irstlm 6 Both the NNJM and the BNNJM had one hidden layer, 100 hidden nodes, input embedding dimension 50, output embedding dimension 50. A small set of training data was used as validation data. The training process was stopped when validation likelihood stopped increasing.</note>

			<note place="foot" n="7"> The decoding time for the NNJM and the BNNJM were similar, since the NNJM trained by NCE uses p (ti|C) as an approximation of P (ti|C) without normalization and the BNNJM only needs to be normalized over two output neurons.</note>

			<note place="foot" n="8"> Infrequent words are usually content words and frequent words are usually function words.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When and why are log-linear models self-normalizing?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="244" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint language and translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1044" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and robust neural network joint models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rabih</forename><surname>Zbib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1370" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Overview of the patent machine translation task at the NTCIR-9 workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting</title>
		<meeting>The 9th NII Test Collection for IR Systems Workshop Meeting</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimum translation modeling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="20" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Extending statistical machine translation with discriminative and trigger-based lexicon models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saša</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous space translation models for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computational Linguistics : Posters</title>
		<meeting>International Conference on Computational Linguistics : Posters</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1071" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Translation modeling with bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Alkhouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joern</forename><surname>Wuebker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoding with largescale neural language models improves translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinggong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1387" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient subsampling for training complex language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asela</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1128" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An improved Chinese word segmentation system with conditional random field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="162" to="165" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
