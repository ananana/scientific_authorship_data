<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Auto-Encoding Dictionary Definitions into Consistent Word Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
							<email>tom.bosc@umontreal.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">Université de Montréal</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
							<email>pascal.vincent@umontreal.ca</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Auto-Encoding Dictionary Definitions into Consistent Word Embeddings</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1522" to="1532"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1522</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Monolingual dictionaries are widespread and semantically rich resources. This paper presents a simple model that learns to compute word embeddings by processing dictionary definitions and trying to reconstruct them. It exploits the inherent recursivity of dictionaries by encouraging consistency between the representations it uses as inputs and the representations it produces as outputs. The resulting embeddings are shown to capture semantic similarity better than regular distributional methods and other dictionary-based methods. In addition, the method shows strong performance when trained exclusively on dictionary data and generalizes in one shot.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dense, low-dimensional, real-valued vector repre- sentations of words known as word embeddings have proven very useful for NLP tasks <ref type="bibr" target="#b43">(Turian et al., 2010)</ref>. They can be learned as a by-product of solving a particular task <ref type="bibr" target="#b8">(Collobert et al., 2011</ref>). Alternatively, one can pretrain generic embed- dings based on co-occurrence counts or using an unsupervised criterion such as predicting nearby words ( <ref type="bibr" target="#b2">Bengio et al., 2003;</ref><ref type="bibr" target="#b28">Mikolov et al., 2013</ref>). These methods implicitly rely on the distributional hypothesis <ref type="bibr" target="#b15">(Harris, 1954;</ref><ref type="bibr" target="#b38">Sahlgren, 2008)</ref>, which states that words that occur in similar contexts tend to have similar meanings.</p><p>It is common to study the relationships captured by word representations in terms of either simi- larity or relatedness ( . "Coffee" is related to "cup" as coffee is a beverage often drunk in a cup, but "coffee" is not similar to "cup" in that coffee is a beverage and cup is a container. Methods relying on the distributional hypothesis often capture relatedness very well, reaching hu- man performance, but fare worse in capturing sim- ilarity and especially in distinguishing it from re- latedness ( .</p><p>It is useful to specialize word embeddings to fo- cus on either relation in order to improve perfor- mance on specific downstream tasks. For instance, <ref type="bibr" target="#b23">Kiela et al. (2015)</ref> report that improvements on relatedness benchmarks also yield improvements on document classification. In the other direction, embeddings learned by neural machine translation models capture similarity better than distributional unsupervised objectives ( <ref type="bibr" target="#b17">Hill et al., 2014</ref>).</p><p>There is a wealth of methods that postprocess embeddings to improve or specialize them, such as retrofitting <ref type="bibr" target="#b9">(Faruqui et al., 2014</ref>). On similarity benchmarks, they are able to reach correlation co- efficients close to inter-annotator agreements. But these methods rely on additional resources such as paraphrase databases <ref type="bibr" target="#b46">(Wieting et al., 2016</ref>) or graphs of lexical relations such as synonymy, hy- pernymy, and their converse .</p><p>Rather than relying on such curated lexical re- sources that are not readily available for the ma- jority of languages, we propose a method capa- ble of improving embeddings by leveraging the more common resource of monolingual dictionar- ies. <ref type="bibr">1</ref> Lexical databases such as WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998</ref>) are often built from dictionary defi- nitions, as was proposed earlier by <ref type="bibr" target="#b0">Amsler (1980)</ref>. We propose to bypass the process of explicitly building a lexical database -during which infor- mation is structured but information is also lost - and instead directly use its detailed source: dictio- nary definitions. The goal is to obtain better rep- resentations for more languages with less effort.</p><p>The ability to process new definitions is also de- sirable for future natural language understanding systems. In a dialogue, a human might want to ex- plain a new term by explaining it in his own words, and the chatbot should understand it. Similarly, question-answering systems should also be able to grasp definitions of technical terms that often oc- cur in scientific writing.</p><p>We expect the embedding of a word to rep- resent its meaning compactly. For interpretabil- ity purposes, it would be desirable to be able to generate a definition from that embedding, as a way to verify what information it captured. Case in point: to analyse word embeddings, <ref type="bibr" target="#b33">Noraset et al. (2017)</ref> used RNNs to produce definitions from pretrained embeddings, manually annotated the errors in the generated definitions, and found out that more than half of the wrong definitions fit either the antonyms of the defined words, their hypernyms, or related but different words. This points in the same direction as the results of in- trinsic evaluations of word embeddings: lexical relationships such as lexical entailment, similar- ity and relatedness are conflated in these embed- dings. It also suggests a new criterion for evalu- ating word representations, or even learning them: they should contain the necessary information to reproduce their definition (to some degree). In this work, we propose a simple model that exploits this criterion. The model consists of a definition au- toencoder: an LSTM processes the definition of a word to yield its corresponding word embedding. Given this embedding, the decoder attempts to re- construct the bag-of-words representation of the definition. Importantly, to address and leverage the recursive nature of dictionaries -the fact that words that are used inside a definition have their own associated definition -we train this model with a consistency penalty that ensures proxim- ity of the embeddings produced by the LSTM and those that are used by the LSTM.</p><p>Our approach is self-contained: it yields good representations when trained on nothing but dic- tionary data. Alternatively, it can also leverage existing word embeddings and is then especially apt at specializing them for the similarity relation. Finally, it is also extremely data-efficient, as it per- mits to create representations of new words in one shot from a short definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Setting and motivation</head><p>We suppose that we have access to a dictionary that maps words to one or several definitions. Def- initions themselves are sequences of words. Our training criterion is built on the following princi- ple: we want the model to be able to recover the definition from which the representation was built. This objective should produce similar embeddings for words which have similar definitions. Our hy- pothesis is that this will help capture semantic sim- ilarity, as opposed to relatedness. Reusing the pre- vious example, "coffee" and "cup" should get dif- ferent representations in virtue of having very dif- ferent definitions, while "coffee" and "tea" should get similar representations as they are both defined as beverages and plants.</p><p>We chose to compute a single embedding per word in order to avoid having to disambiguate word senses. Indeed, word sense disambiguation remains a challenging open problem with mixed success on downstream task applications <ref type="bibr" target="#b31">(Navigli, 2009)</ref>. Also, recent papers have shown that a sin- gle word vector can capture polysemy and that having several vectors per word is not strictly nec- essary ( <ref type="bibr" target="#b25">Li and Jurafsky, 2015</ref>) <ref type="bibr" target="#b47">(Yaghoobzadeh and Schütze, 2016)</ref>. Thus, when a word has several definitions, we concatenate them to produce a sin- gle embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Autoencoder model</head><p>Let V D be the set of all words that are used in definitions and V K the set of all words that are defined. We let w ∈ V K be a word and D w = (D w,1 , . . . , D w,T ) be its definition, where D w,t is the index of a word in vocabulary V D . We en- code such a definition D w by processing it with an LSTM <ref type="bibr" target="#b20">(Hochreiter and Schmidhuber, 1997)</ref>.</p><p>The LSTM is parameterized by Ω and a matrix E of size |V D | × m, whose i th row E i contains an m-dimensional input embedding for the i th word of V D . These input embeddings can either be learned by the model or be fixed to a pretrained embedding. The last hidden state computed by this LSTM is then transformed linearly to yield an m-dimensional definition embedding h. Thus the encoder whose parameters are θ = {E, Ω, W, b} computes this embedding h as</p><formula xml:id="formula_0">h = f θ (D w ) = W LSTM E,Ω (D w ) + b.</formula><p>The subsequent decoder can be seen as a condi- tional language model trained by maximum likeli- hood to regenerate definition D w given definition embedding h = f θ (D w ). We use a simple con- ditional unigram model with a linear parametriza- tion θ = {E , b } where E is a |V D | × m matrix and b is a bias vector. <ref type="bibr">2</ref> We maximize the log-probability of definition D w under that model:</p><formula xml:id="formula_1">log p θ (D w |h) = t log p θ (D w,t |h) = t log e E D w,t ,h +b D w,t k e E k ,h+b k (1)</formula><p>where , denotes an ordinary dot product. We call E the output embedding matrix. The basic au- toencoder training objective to minimize over the dictionary can then be formulated as</p><formula xml:id="formula_2">J r (θ , θ) = − w∈V K log p θ (D w |f θ (D w )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Consistency penalty</head><p>We introduced 3 different embeddings: a) defini- tion embeddings h, produced by the definition en- coder, are the embeddings we are ultimately in- terested in computing; b) input embeddings E are used by the encoder as inputs; c) output embed- dings E are compared to definition embeddings to yield a probability distribution over the words in the definition. We propose a soft weight-tying <ref type="bibr">2</ref> We have tried using a LSTM decoder but it didn't yield good representations. It might overfit because the set of dic- tionary definitions is small. Also, using teacher forcing, we condition on ground-truth words, making it easier to predict the next words. More work is needed to address these issues. scheme that brings the input embeddings closer to the definition embeddings. We call this term a consistency penalty because its goal is to to en- sure that the embeddings used by the encoder (in- put embeddings) and the embeddings produced by the encoder (definition embeddings) are consistent with each other. It is implemented as</p><formula xml:id="formula_3">J p (θ) = w∈V D ∩V K d(E w , f θ (D w ))</formula><p>where d is a distance. In our experiments, we choose d to be the Euclidian distance. The penalty is only applied to some words because V D = V K . Indeed, some words are defined but are not used in definitions and some words are used in definitions but not defined. In particular, inflected words are not defined. To balance the two terms, we intro- duce two hyperparameters λ, α ≥ 0 and the com- plete objective is</p><formula xml:id="formula_4">J(θ , θ) = αJ r (θ , θ) + λJ p (θ).</formula><p>We call the model CPAE, for Consistency Pe- nalized AutoEncoder when α &gt; 0 and λ &gt; 0 (see <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="bibr">3</ref> The consistency penalty is a cheap proxy for dealing with the circularity found in dictionary definitions. We want the embeddings of the words in definitions to be compositionally built from their definition as well. The recursive process of fetching definitions of words in definitions does not terminate, because all words are defined using other words. To counter that, our model uses input embeddings that are brought closer to definition embeddings and vice versa in an asynchronous manner.</p><p>Moreover, if λ is chosen large enough, then E w ≈ f θ (D w ) after optimisation. This means that the definition embedding for w is close enough to the corresponding input embedding to be used by the encoder for producing other definition em- beddings for other words. In that case, the model could enrich its vocabulary by computing embed- dings for new words and consistently reusing them as inputs for defining other words.</p><p>Finally, the consistency penalty can be used to leverage pretrained embeddings and bootstrap the learning process. For that purpose, the encoder's input embeddings E can be fixed to pretrained em- beddings. These provide targets to the encoder but also helps the encoder to produce better definition embeddings in virtue of using input embeddings that already contain meaningful information.</p><p>To summarize, the consistency penalty has sev- eral motivations. Firstly, it deals with the fact that the recursive process of building representation of words out of definitions does not terminate. Sec- ondly, it is a way to enrich the vocabulary with new words dynamically. Finally, it is a way to in- tegrate prior knowledge in the form of pretrained embeddings.</p><p>In order to study the two terms of the objec- tive in isolation, we introduce two special cases. When λ = 0 and α &gt; 0, the model reduces to AE for Autoencoder. When α = 0 and λ &gt; 0, we retrieve Hill's model, as presented by . <ref type="bibr">4</ref> Hill's model is simply a recurrent en- coder that uses pretrained embeddings as targets so it only makes sense in the case we use fixed pretrained embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related work 3.1 Extracting lexical knowledge from dictionaries</head><p>There is a long history of attempts to extract and structure the knowledge contained in dictionaries.</p><p>Amsler (1980) studies the possibility of automat- ically building taxonomies out of dictionaries, re- lying on the syntactic and lexical regularities that definitions display. One relation is particularly straightforward to identify: it is the is a relation that translates to hypernymy. Dictionary defini- tions often contain a genus which is the hyper- nym of the defined word, as well as a differentia which differentiates the hypernym from the de- fined word. For example, the word "hostage" is defined as "a prisoner who is held by one party to insure that another party will meet specified terms", where "prisoner" is the genus and the rest is the differentia. To extract such relations, early works by <ref type="bibr" target="#b7">Chodorow et al. (1985)</ref> and <ref type="bibr" target="#b5">Calzolari (1984)</ref> use string matching heuristics. <ref type="bibr" target="#b3">Binot and Jensen (1987)</ref> operate at the syntactic parse level to detect these relations. Whether based on the string rep- resentation or the parse tree of a definition, these rule-based systems have helped to create large lex- ical databases. We aim to reduce the manual labor involved in designing the rules and directly obtain- ing representations from raw definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improving word embeddings using lexical resources</head><p>Postprocessing methods for word embeddings use lexical resources to improve already trained word embeddings irrespective of how they were ob- tained. When it is used with fixed pretrained em- beddings, our method can be seen as a postpro- cessing method.</p><p>Postprocessing methods typically have two terms for trading off conservation of distributional information that is brought by the original vec- tors with the new information from lexical re- sources. There are two main ways to preserve dis- tributional information: Attract-Repel , retrofitting  and our method control the distance between the original vector and the postprocessed vector so that the new vector does not drift too far away from the original vector. Counter-Fitting <ref type="bibr" target="#b30">(Mrkši´cMrkši´c et al., 2016</ref>) and dict2vec <ref type="bibr" target="#b42">(Tissier et al., 2017)</ref> ensure that the neighbourhood of a vector in the original space is roughly the same as the neighbourhood in the new space.</p><p>Finally, methods differ by the nature of the lexical resources they use. To our knowledge, dict2vec is the only technique that uses dictio- naries. Other postprocessing methods use vari- ous data from WordNet: sets of synonyms and sometimes antonyms, hypernyms, and hyponyms. For instance, Lexical Entailment Attract-Repel (LEAR) uses all of these . Other methods rely on paraphrase databases (Wi- eting et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dictionaries and word embeddings</head><p>We now turn to the most relevant works that in- volve dictionaries and word embeddings. <ref type="bibr">Dict2vec (Tissier et al., 2017</ref>) combines the word2vec skip-gram objective (predicting all the words that appear in the context of a target word) with a cost for predicting related words. These re- lated words either form strong pairs or weak pairs with the target word. Strong pairs have a greater influence in the cost. They are pairs of words that are in the neighbourhood of the target word in the original embedding, as well as pairs of words for which the definitions make reference to each other. Weak pairs are pairs of words where only one word appears in the definition of the other. Un- like dict2vec, our method can be used as either a standalone or a postprocessing method (when used with pretrained embeddings). It also focuses on handling and leveraging the recursivity found in dictionary definitions with the consistency penalty whereas dict2vec ignores this aspect of the struc- ture of dictionaries. Besides dict2vec,  train neu- ral language models to predict a pretrained word embedding given a definition. Their goal was to learn a general-purpose sentence encoder use- ful for downstream tasks. <ref type="bibr" target="#b33">Noraset et al. (2017)</ref> propose the task of generating definitions based on word embeddings for interpretability purposes. Our model unifies these two approaches into an autoencoder. However, we have a different goal: that of creating or improving word representa- tions. Their methods assume that pretrained em- beddings are available to provide either targets or inputs, whereas our model is unsupervised, and the use of pretrained embeddings is optional. <ref type="bibr" target="#b1">Bahdanau et al. (2017)</ref> present a related model that produces embeddings from definitions such that it improves performance on a downstream task. By contrast our approach is used either stand-alone or as as a postprocessing step, to pro- duce general-purpose embeddings at a lesser com- putational cost. The core novelty is the way we leverage the recursive structure of dictionaries.</p><p>Finally, Herbelot and Baroni (2017) also aim at learning representations for word embeddings in a few shots. The method consists of fine-tuning word2vec hyperparameters and can learn in one or several passes, but it is not specifically designed to handle dictionary definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We experiment on English to benefit from the many evaluation benchmarks available. The dic- tionary we use is that of WordNet <ref type="bibr" target="#b11">(Fellbaum, 1998)</ref>. WordNet contains graphs of linguistic re- lations such as synonymy, antonymy, hyponymy, etc. but also definitions. We emphasize that our method trains exclusively on the definitions and is thus applicable to any electronic dictionary.</p><p>However, in order to evaluate the quality of em- beddings on unseen definitions, WordNet relations comes in handy: we use the sets of synonyms to split the dictionary into a train set and a test set, as explained in Section 7. Moreover, WordNet has a wide coverage and high quality, so we do not need to aggregate several dictionaries as done by <ref type="bibr" target="#b42">Tissier et al. (2017)</ref>. Finally, WordNet is explic- itly made available for research purposes, there- fore we avoid technical and legal difficulties asso- ciated with crawling proprietary online dictionar- ies.</p><p>We do not include part of speech tags that go with definitions. WordNet does not contain func- tion words but contains homonyms of function words. We filter these out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Similarity and relatedness benchmarks</head><p>Evaluating the learned representations is a com- plex issue <ref type="bibr" target="#b10">(Faruqui et al., 2016)</ref>. Indeed, different evaluation methods yield different rankings of em- beddings: there is no single embedding that out- performs others on all tasks ( <ref type="bibr" target="#b39">Schnabel et al., 2015)</ref> and thus no single best evaluation method.</p><p>We focus on intrinsic evaluation methods. In particular, we study how different models trade off similarity and relatedness. We use benchmarks which consist of pairs of words scored according to some criteria. They vary in terms of annotation guidelines, number of annotators, selection of the words, etc. To evaluate our embeddings, we score each pair by computing the cosine similarity be- tween the corresponding word vectors. Then the predicted scores and the ground truth are ranked and the correlation between the ranks is measured by Spearman's ρ. We leave aside analogy predic- tion benchmarks as they suffer from many prob- lems <ref type="bibr" target="#b26">(Linzen, 2016;</ref><ref type="bibr" target="#b36">Rogers et al., 2017</ref>).</p><p>We adopt one of the methods proposed by <ref type="bibr" target="#b10">Faruqui et al. (2016)</ref> and use separate datasets for model selection. We choose the development set to be the development set of SimVerb3500 (Gerz et al., 2016) and MEN ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>), the only benchmarks with a standard train/test split.</p><p>We justified our emphasis on the similarity re- lation in Section 1: capturing this relation remains a challenge, and we hypothesize that dictionary data should improve representations in that re- spect. The model selection procedure reflects that we want embeddings specialized in similarity. To do that, we set the validation loss as a weighted mean which weights SimVerb twice as MEN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>The objective function presented in section 2 gives us 3 different models: CPAE, AE, and Hill's model. The objective of CPAE comprises the sum of the objective of Hill's model and of AE. We compare the CPAE model to both of these to eval- uate the individual contribution of the two terms to the performance. In addition, when we use exter- nal corpora to pretrain embeddings, we compare these models to dict2vec and retrofitting. The hy- perparameter search is described in Appendix C.</p><p>The test benchmarks for the similarity relation includes SimLex999 ( ) and more particularly SimLex333, a challenging subset of SimLex999 which contains only highly related pairs but in which similarity scores vary a lot. For relatedness, we use MEN ( <ref type="bibr" target="#b4">Bruni et al., 2014</ref>), RG <ref type="bibr" target="#b37">(Rubenstein and Goodenough, 1965</ref>), WS353 ( <ref type="bibr" target="#b12">Finkelstein et al., 2001</ref>), SCWS ( <ref type="bibr" target="#b21">Huang et al., 2012), and</ref><ref type="bibr">MTurk (Radinsky et al., 2011;</ref><ref type="bibr" target="#b14">Halawi et al., 2012</ref>). The evaluation is carried out by a modified version of the Word Embeddings Bench- marks project. <ref type="bibr">5</ref> Conveniently, all these bench- marks contain mostly lemmas, so we do not suffer too much from the problem of missing words. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results in the dictionary-only setting</head><p>In the first evaluation round, we train models only using a single monolingual dictionary. This allows us to check our hypothesis that dictionaries con- tain information for capturing the similarity rela- tion between words.</p><p>Our baselines are regular distributional mod- els: <ref type="bibr">GloVe (Pennington et al., 2014</ref>) and word2vec ( <ref type="bibr" target="#b28">Mikolov et al., 2013)</ref>. They are trained on the concatenation of defined words with their defini- tions.</p><p>Such a formatting introduces spurious co- occurrences that do not otherwise appear in free text. But these baselines are not designed for dictionaries and cannot deal with their particular structure.</p><p>We compare these models to the autoencoder model without (AE) and with (CPAE) the consis- tency penalty. In this setting, we cannot use Hill's model as it requires pretrained embeddings as tar- gets. We also trained an additional CPAE model with pretrained word2vec embeddings trained on the concatenated definitions. The results are pre- sented in <ref type="table">Table 1.</ref> GloVe is outperformed by word2vec by a large margin so we ignore this model in later experi- ments. Word2vec captures more relatedness than CPAE (+10.7 on MEN-t, +13.5 on MT, +13.2 on WS353) but less similarity than CPAE. The differ- ence in the nature of the relations captured is ex- emplified by the scores on SimLex333. This sub- set of SimLex999 focuses on pairs of words that are very related but that can be either similar or dissimilar. On this subset, CPAE fares better than word2vec (+13.1).</p><p>The consistency penalty improves performance on every dataset. This penalty provides targets to the encoder, but these targets are themselves learned and change during the learning process. The exact dynamics of the system are unknown. It can be seen as a regularizer because it puts strong weight-sharing constraints on both types of em- beddings. It also resembles bootstrapping in re- inforcement learning, which consists of building estimates of values functions on top of over esti- mates <ref type="bibr" target="#b40">(Sutton and Barto, 1998</ref>).</p><p>The last model is the CPAE model that uses the word2vec embeddings pretrained on the dic- tionary data. This combination not only equals other models on some benchmarks but outper- forms them, sometimes by a large margin (+6.3 on SimLex999 and +7.5 on SimVerb3500 com- pared to CPAE, +6.1 on SCWS, +5.4 on MT compared to word2vec). Thus, the two kinds of algorithms are complementary through the differ- ent relationships that they capture best. The pre- training helps in two different ways, by providing quality input embeddings and targets to the en- coder. The pretrained word2vec targets are already remarkably good. That is why the chosen con- sistency penalty coefficient selected is very high (λ = 64). The model can pay a small cost and deviate from the targets in order to encode infor- mation about the definitions.</p><p>To sum up, dictionary data contains a lot of data relevant to modeling the similarity relation- ship. Autoencoder based models learn different relationships than regular distributional methods. The consistency penalty is a very helpful prior and regularizer for dictionary data, as it always helps,  <ref type="table">Table 1</ref>: Positive effect of the consistency penalty and word2vec pretraining. Spearman's correlation coefficient ρ × 100 on benchmarks. Without pretraining, autoencoders (AE and CPAE) improve on similarity benchmarks while capturing less relatedness than distributional methods. The consistency penalty (CPAE) helps even without pretrained targets. Our method, combined with pretrained embeddings on the same dictionary data (CPAE-P), significantly improves on every benchmark.</p><note type="other">-P (λ = 64) 44.1 65.1 45.8 30.9 42.3 72.0 60.4 63.8 61.5 61.3</note><p>regardless of what relationship we focus on. Fi- nally, our model can drastically improve embed- dings that were trained on the same data but with a different algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Improving pretrained embeddings</head><p>We have seen that CPAE with pretraining is very efficient. But does this result generalizes to other kind of pretraining data? To answer this ques- tion, we experiment using embeddings pretrained on the first 50 million tokens of a Wikipedia dump, as well as the entire Wikipedia dump. We com- pare our method to existing postprocessing meth- ods such as dict2vec and retrofitting, which also aims at improving embeddings with external lexi- cal resources. Retrofitting, which operates on graphs, is not tailored for dictionary data, which consists in pairs of words along with their definitions. We build a graph where nodes are words and edges between nodes correspond to the presence of one of the words into the definition of another. Obviously, we lose word order in the process.</p><p>The results for the small corpus are presented in <ref type="table" target="#tab_2">Table 2</ref>. By comparing <ref type="table" target="#tab_2">Table 2</ref> with <ref type="table">Table  1</ref>, we see that word2vec does worse on similar- ity than when trained on dictionary data, but bet- ter on relatedness. Both dict2vec and retrofitting improve with regards to word2vec on similarity benchmarks and seem roughly on par. However, dict2vec fails to improve on relatedness bench- marks, whereas retrofitting sometimes improves (as in RG, MEN, and MT), sometimes equals (SCWS) and does worse <ref type="bibr">(353)</ref>.</p><p>We do an ablation study by comparing Hill's model and AE with CPAE. Recall that Hill's model lacks the reconstruction cost while AE lacks the consistency penalty. Firstly, CPAE al- ways improves over AE. Thus, we confirm the re- sults of the previous section on the importance of the consistency penalty. In that setting, it is more obvious why this penalty helps, as it now pro- vides pretrained targets to the encoder. Secondly, CPAE improves over Hill on all similarity bench- marks by a large margin (+12.2 on SL999, +13.7 on SL333, +16.1 on SV3500). It is sometimes slightly worse on relatedness benchmarks (−3.3 on MEN-t, −5.6 on MT), other times better or equal. We conclude that both terms of the CPAE objective matter.</p><p>We see identical trends when using the full Wikipedia dump. As expected, CPAE can still improve over even higher quality embeddings by roughly the same margins. The results are pre- sented in Appendix D.</p><p>Remarkably, the best model among all our ex- periments is CPAE in <ref type="table">Table 1</ref> and uses only the dictionary data. This supports our hypothesis that dictionaries contain similarity-specific infor- mation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Generalisation on unseen definitions</head><p>A model that uses definitions to produce word representations is appealing because it could be extremely data-efficient. Unlike regular distribu- tional methods which iteratively refine their repre- sentation as occurrences accumulate, such a model could output a representation in one shot. We now evaluate CPAE in a setting where some definitions are not seen during training.</p><p>The dictionary is split into train, validation (for early stopping) and test splits. The algorithm for splitting the dictionary puts words in batches. It ensures two things: firstly, that words which share at least one definition are in the same batch, and secondly, that each word in a batch is associated   with all its definitions. We can then group batches to build the training and the test sets such that the test set does not contain synonyms of words from the other sets. We sort the batches by the num- ber of distinct definitions they contain. We use the largest batch returned by the algorithm as the training set: it contains mostly frequent and pol- ysemous words. The validation and the test sets, on the contrary, contain many multiword expres- sions, proper nouns, and rarer words. More details are given in Appendix B.1. We train CPAE only on the train split of the dictionary, with randomly initialized input embed- dings. <ref type="table" target="#tab_3">Table 3</ref> presents the same correlation co- efficients as in the previous tables but also dis- tinguishes between two subsets of the pairs: the pairs for which all the definitions were seen during training (train) and the pairs for which at least one word was defined in the test set (test). Unfortu- nately, there are not enough pairs of words which both appear in the test set to be able to compute significant correlations. On small-sized bench- marks, correlation coefficients are sometimes not significant so we do not report them (when p-value &gt; 0.01).</p><p>The scores of CPAE on the test pairs are quite correlated with the ground truth: except on Sim- Lex999 and SCWS, there is no drop in correlation coefficients between the two sets. The scores of Hill's model follow similar trends, but are lower on every benchmark so we do not report them. This shows that recurrent encoders are able to generalize and produce coherent embeddings as a function of other embeddings in one pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and future work</head><p>We have focused on capturing the similarity rela- tion. It is a challenging task which we have pro- posed to solve using dictionaries, as definitions seem to encode the relevant kind of information.</p><p>We have presented an alternative for learning word embeddings that uses dictionary definitions. As a definition autoencoder, our approach is self- contained, but it can alternatively be used to im- prove pretrained embeddings, and includes Hill's model ( ) as a special case.</p><p>In addition, our model leverages the inherent re- cursivity of dictionaries via a consistency penalty, which yields significant improvements over the vanilla autoencoder.</p><p>Our method outperforms dict2vec and retrofitting on similarity benchmarks by a quite large margin. Unlike dict2vec, our method can be used as a postprocessing method which does not require going through the original pretraining corpus, it has fewer hyperparameters, and it generalises to new words.</p><p>We see several directions for future work. Firstly, more work is needed to evaluate the rep- resentations on other languages and tasks.</p><p>Secondly, solving downstream tasks requires representations for the inflected words as well. We have set aside this issue by focusing on bench- marks involving lemmas. To address it in future work, we might want to split word representa- tions into a lexical and a morphological part. With such a split representation, we could postprocess only the lexical component, and all the words, whether inflected or not, would benefit from this. This seems desirable for postprocessing methods in general and would make them more suitable for synthetic languages.</p><p>Thirdly, dictionary defines every sense of words, so we could produce one embedding per sense <ref type="bibr" target="#b6">(Chen et al., 2014</ref>) ( <ref type="bibr" target="#b22">Iacobacci et al., 2015)</ref>. This requires potentially complicated modifica- tions to our model as we would need to disam- biguate senses inside each definition. However, some class of words might benefit a lot from such representations, for example words that can be used as different parts of speech.</p><p>Lastly, a more speculative direction could be to study iterative constructions of the set of em- beddings. As our algorithm can generalize in one shot, we could start the training with a small set of words and their definitions and iteratively broaden the vocabulary and refine the representations with- out retraining the model. This could be useful in discovering a set of semantic primes from which one can define all the other words <ref type="bibr" target="#b45">(Wierzbicka, 1996)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the CPAE model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Improving pretrained embeddings computed on a small corpus. Spearman's correlation coefficient 
ρ × 100 on benchmarks. All methods use pretrained embeddings. All methods (except maybe Hill) manage 
to improve the embeddings. Retrofitting outperforms dict2vec and efficiently specializes for relatedness. CPAE 
outperforms AE and allows to trade off relatedness for similarity. 

Similarity 
Relatedness 
999 333 SV-t 353 MEN SCWS MT 
All 
36.5 27.7 36.9 43.7 48.8 
53.2 
41.5 
Train 40.0 28.5 36.7 46.9 53.4 
57.1 
42.9 
Test 
27.5 25.4 38.5 42.5 44.1 
40.3 
39.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>One pass generalisation. Spearman's correlation coefficient ρ×100 on benchmarks. The model is CPAE 
(without pretrained embeddings). All: all pairs in the benchmarks. Train: pairs for which both words are in the 
training or validation set. Test: pairs which contain at least one word in the test set. Correlation is lower for test 
pairs but remains strong (ρ &gt; 0.3): the model has good generalisation abilities. 

</table></figure>

			<note place="foot" n="1"> See Appendix A for a list of online monolingual dictionaries.</note>

			<note place="foot" n="3"> Our implementation is available at https://github.com/tombosc/cpae</note>

			<note place="foot" n="4"> It is not exactly their model as we use Euclidian distance instead of the cosine distance or the ranking loss. They also explore several variants where the input embeddings are learned, which we didn&apos;t find to produce any improvement. We haven&apos;t experimented with the ranking loss, but the cosine distance does not seem to improve over Euclidian. Finally, they also use a simple encoder that averages word vectors, which we found to be inferior.</note>

			<note place="foot" n="5"> Original project available at https://github.com/kudkudak/ word-embeddings-benchmarks, modified version distributed with our code. 6 Missing words are not removed from the dataset, but they are assigned a null vector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank NSERC and Facebook for financial sup-port, Calcul Canada for computational resources, Dzmitry Bahdanau and Stanisław Jastrz˛ ebski for contributing to the code on which the implemen-tation is based, the developers of Theano (Theano Development Team, 2016), <ref type="bibr">Blocks and Fuel (van Merriënboer et al., 2015</ref>) as well as Laura Ball for proofreading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The structure of the merriam-webster pocket dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">Alfred</forename><surname>Amsler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Learning to compute word embeddings on the fly. CoRR, abs/1706.00286</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantic expert using an online standard dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Binot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international joint conference on Artificial intelligence</title>
		<meeting>the 10th international joint conference on Artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="709" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam-Khanh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res.(JAIR)</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting patterns in a lexical data base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nicoletta Calzolari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1984" />
			<biblScope unit="page" from="170" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified model for word sense representation and disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting semantic hierarchies from a large on-line dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">J</forename><surname>Martin S Chodorow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heidorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 23rd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="299" to="304" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4166[cs].ArXiv:1411.4166</idno>
		<title level="m">Retrofitting Word Vectors to Semantic Lexicons</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02276</idno>
		<title level="m">Problems with evaluation of word embeddings using word similarity tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simverb-3500: A largescale evaluation set of verb similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00869</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zellig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06556</idno>
		<title level="m">High-risk learning: acquiring new word vectors from tiny data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Embedding word similarity with neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coline</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6448</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00548</idno>
		<title level="m">Learning to understand phrases by embedding the dictionary</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sensembed: Learning sense embeddings for word and relational similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Iacobacci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taher</forename><surname>Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Specializing word embeddings for similarity or relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2044" to="2048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Do multi-sense embeddings improve natural language understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Issues in evaluating semantic spaces using word analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tal Linzen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07736</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Blocks and fuel: Frameworks for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bart Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1506.00619</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Leviant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Gaši´cgaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Counter-fitting word vectors to linguistic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><forename type="middle">M</forename><surname>Gaši´cgaši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Rojas-Barahona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Hsien</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="142" to="148" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="217" to="250" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Definition Modeling: Learning to Define Word Embeddings in Natural Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanapon</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Birnbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3259" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A word at a time: computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Blondin-Massé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevan</forename><surname>Harnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Odile</forename><surname>Marcotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Chicoisne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Gargouri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0911.5703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World wide web</title>
		<meeting>the 20th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The (Too Many) Problems of Analogical Reasoning with Word Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017)</title>
		<meeting>the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The distributional hypothesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Italian journal of linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="54" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Theano Development Team. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dict2vec: Learning Word Embeddings using Lexical Dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Tissier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Specialising word vectors for lexical entailment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Mrkši´cmrkši´c</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06371</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Wierzbicka</surname></persName>
		</author>
		<title level="m">Semantics: Primes and universals: Primes and universals</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02789</idno>
		<title level="m">Charagram: Embedding words and sentences via character n-grams</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Intrinsic subspace evaluation of word embedding representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
