<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content Explorer: Recommending Novel Entities for a Document Writer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
							<email>mlukasik@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
							<email>zens@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">Content Explorer: Recommending Novel Entities for a Document Writer</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3371" to="3380"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3371</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However, the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper, we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a user study to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the deep learning models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An important part of writing about some topic is researching relevant background material, which can be a challenging and tedious task. If the au- thor has a clear idea what to write about, web search engines provide an excellent tool for re- trieving information. However, the author often first has to spend time discovering which topics are related, i.e., she needs to conduct exploratory search <ref type="bibr" target="#b31">(White and Roth, 2009)</ref>. This is challeng- ing with traditional keyword-based search engines like Google, which are tailored to providing the information that a user is explicitly searching for <ref type="bibr" target="#b20">(Marie, 2014)</ref>.</p><p>Exploratory search poses a challenge in the writing process. It has been shown that the explo- ration phase elicits strong negative feelings in stu- dents writing essays <ref type="bibr" target="#b27">(Smith and Campbell, 1999;</ref><ref type="bibr" target="#b17">Kuhlthau, 1990)</ref> and that students consider the re- search activity to be at the cost of other pending commitments ( <ref type="bibr" target="#b27">Smith and Campbell, 1999</ref>). More- over, exploratory search is more cognitively de- manding than lookup search tasks <ref type="bibr" target="#b20">(Marie, 2014)</ref>. Automatic tools tailored to exploratory search for new content could significantly alleviate the diffi- culty the writer faces in the research phase.</p><p>Intuitively, the tool should suggest topics for further research which are relevant and interest- ing. Hence the suggestions need to be related to the document and should not be too obvious. For instance, a student writing about the monarch but- terfly might benefit from a suggestion milkweed as it is both relevant and interesting. On the other hand, insect would be a rather poor suggestion as it is too obvious. To summarize, the key chal- lenge we set out to address is: For a given piece of text, what are the related topics and how impor- tant are they? In Section 3.2 we describe a user study where annotators evaluated usefulness of en- tities, which we then use to identify characteristics of good suggestions.</p><p>It is worth mentioning that systems for provid- ing automatic recommendations of entities to a document writer have recently been introduced by the industry, including Google Explore in Docs <ref type="bibr">1</ref> and Microsoft Researcher 2 . The systems are pro- prietary and their design is not published, making the comparison not possible. To the best of our knowledge this is the first documented attempt to address the problem of recommending novel enti- ties to a document writer. We hope this paper will attract the interest of the research community and inspire future work.</p><p>The contributions of this paper are:</p><p>1. Formulating the problem of recommending future entities to a document writer.</p><p>2. Conducting a user study to identify what topic suggestions users find useful.</p><p>3. Formulating the problem and defining an au- tomatic evaluation metric, both motivated by the user study.</p><p>4. Evaluating state of the art approaches to ex- treme multi-label classification.</p><p>5. Demonstrating how a modified loss function helps improve over state of the art neural net- work models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recommending topics to a document writer can be viewed in the context of different fields which we discuss below.</p><p>Exploratory Search White and Roth (2009) de- fine exploratory search as "information seeking problem context that is open-ended, persistent, and multi-faceted" and "information-seeking pro- cesses that are opportunistic, iterative, and multi- tactical". Research on exploratory search focuses on supporting a user in the interactive and iterative process of seeking for information and includes: designing better interfaces, visualization of search results, clustering of results, supporting serendipi- tous discoveries, supporting different user profiles <ref type="bibr" target="#b20">(Marie, 2014)</ref>. Instead, in this work we aim to fully automate discovery of relevant and interest- ing topics to the document writer. Moreover, the input in our case is an initial portion of a docu- ment written by a user rather than a query or a sin- gle entity, as is usually the case in the information retrieval setups.</p><p>Extreme Multi-label Classification Extreme multi-label classification (XML) is an instance of a multi-label classification problem (i.e., where multiple labels can be assigned to a single example at the same time) under a large label space. There are multiple works investigating XML, including random forest (RF) ( <ref type="bibr" target="#b26">Prabhu and Varma, 2014;</ref><ref type="bibr" target="#b12">Jain et al., 2016</ref>) and embedding ( <ref type="bibr" target="#b2">Bhatia et al., 2015)</ref> approaches. These methods rely on bag of words feature representation, ignoring the sequential na- ture of text. Neural networks (NNs) have been successful in modeling NLP tasks through their ability to learn structure, however have not been widely studied for XML problems. <ref type="bibr" target="#b7">Covington et al. (2016)</ref> applied NNs to YouTube video recom- mendation, and <ref type="bibr" target="#b18">Liu et al. (2017)</ref> showed a convo- lutional neural network architecture to outperform strong baselines on a range of NLP tasks. The importance of label weighting for recommending rare items has not been considered in the NNs for extreme multi-label classification, which is a cen- tral problem in our task, as in other tasks with ex- treme label spaces ( <ref type="bibr" target="#b12">Jain et al., 2016)</ref>. In Section 5 we describe the state of the art RF and NN ap- proaches to XML.</p><p>Entity Retrieval and Tagging Assigning enti- ties to an input has been considered, however in different contexts. In tag recommendation, a text is summarized with a set of entities <ref type="bibr" target="#b28">(Song et al., 2011)</ref> and in entity search one needs to answer queries about entities against a set of documents and entities <ref type="bibr" target="#b6">(Cheng et al., 2007)</ref>. These tasks are different as no new content is predicted. The Related Entity Finding (REF) challenge of the Text Retrieval Conference (TREC) considered a task where given a source entity, a relation and a target type, a target entity needs to be identi- fied satisfying the required relation ( <ref type="bibr" target="#b1">Balog et al., 2010)</ref>. This is a different problem since no spec- ification for relations between the entities is pro- vided in the input document from a user.</p><p>The work on semantic relatedness of entities ( <ref type="bibr" target="#b22">Milne and Witten, 2008</ref>) is different since, as explained in the introduction, good entity recom- mendations are not necessarily those which are se- mantically related to entities from the user text. <ref type="bibr" target="#b4">Bordino et al. (2013)</ref> considered retrieving enti- ties related to a query in the question answering scenario. The authors did not set the problem in a supervised learning setup and instead find entities closest in terms of similarity of documents con- taining them. In contrast, in Section 3.2 we justify a supervised learning setup of the task.</p><p>The TREC Complex Answer Retrieval Track (CAR) 3 is a challenge where based on a document outline, related text passages and entities are re- trieved ( <ref type="bibr" target="#b9">Dietz et al., 2017)</ref>. In this formulation it is assumed that a general outline of what a document author intends to cover is given, thus providing a clear guidance for what is relevant. Instead, our input is an initial part of a document and we seek to find novel entities based on the input.</p><p>Recommending Rare Items Information re- trieval applications emphasized the importance of retrieving rare labels rather than common ones which are likely to be already known to a user. Baeza-Yates and Ribeiro-Neto (1999) de- fined novelty of a set of recommendations as the proportion of unknown items to the user, a challenging definition to work with when user's knowledge is unknown ( <ref type="bibr" target="#b11">Hurley and Zhang, 2011</ref>). <ref type="bibr" target="#b4">Bordino et al. (2013)</ref> explored the problem of re- trieving serendipitous results when retrieving an- swers to queries. The authors built an information retrieval system based on finding entities most of- ten co-occurring with a query entity and employed IDF (inverse document frequency) for filtering out overly generic answers. Also many other works employ IDF for rewarding rare items ( <ref type="bibr" target="#b33">Zhou et al., 2010;</ref><ref type="bibr" target="#b30">Vargas and Castells, 2011;</ref><ref type="bibr" target="#b32">Wu et al., 2014;</ref><ref type="bibr" target="#b12">Jain et al., 2016)</ref>. Here we take a supervised learn- ing approach to the entity recommendation prob- lem, demonstrate the usefulness of IDF scoring in the context of our problem with a user study, and utilize IDF in the evaluation metric.</p><p>Hurley and Zhang (2011) define novelty of an item in the context of the set of relevant recom- mendations using an average dissimilarity from other items in the set. Similarly, the Maximal Marginal Relevance metric evaluates a set of re- trieved items in an information retrieval problem by rewarding the diversity of the set <ref type="bibr" target="#b5">(Carbonell and Goldstein, 1998</ref>). Note this is a different no- tion of novelty from that considered in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Investigating the Problem</head><p>In this section we investigate the problem formu- lation and what it means for an entity to be a useful suggestion for a user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Let a document d be represented as a sequence of entities (e 1 , . . . , e |d| ) = E. We partition these en- tities into two sets C d and F d : those which oc- cur in the first h sentences (in our case h = 10) and those which don't. Hence,</p><formula xml:id="formula_0">C d ∩ F d = ∅ and C d ∪ F d = E.</formula><p>We are not interested in retrieving all entities that could occur in the future, because it is not feasible to provide all such recommendations to a user. Instead, we focus on ranking the target en- tities, and selecting the most relevant k to suggest, which is a standard practice in retrieval tasks (Jain et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">What Entities are Good Suggestions?</head><p>In this section we describe a user study we ran for testing the hypotheses about which entities con- stitute useful recommendations. In particular, we test whether what a user writes next in a document is actually considered to be a good recommenda- tion by the raters and whether IDF score correlates with how useful an entity is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation Dataset We consider 1000</head><p>Wikipedia documents for a human evaluation study. For each document, let E denote the set of all entities that occur in the document and E p de- note entities occurring in the initial passage p. We found 5 most co-occurring entities from E \ E p 4 with the entities from E p across sentences from a large scale web documents corpus (the dataset de- scribed later in Section 4). Annotators were then asked to rate entity suggestions against an input passage p in terms of how useful they are in the process of continuing to write the document. Each entity was rated by 3 annotators with a score rang- ing from 1 to 5 and the total number of annotators was 845. To evaluate agreement among raters, we employ the Intraclass Coefficient for consistency <ref type="bibr">5 (ICC;</ref><ref type="bibr" target="#b21">(McGraw and Wong, 1996)</ref>) for two-way random effects model with the effects correspond- ing to a rater and a rated entity. We found the aver- age score ICC to be equal ICC(C, 3) = 0.69. We refer the reader to the supplemental material for details about how ICC was applied.</p><p>Evaluating the Supervised Setup We found that the mean rating for entities which actually oc- cur in the future is 3.24 ± 0.83, whereas the mean rating for the other entities is 2.59±0.93. Running an independent T-test for comparing means be- tween the two groups yields a p-value p &lt; 0.001. This supports the supervised learning setup of en- tity suggestion, where we split documents from a corpus into two parts, and unseen entities from later parts form labels for the initial parts.</p><p>Does IDF Correlate with Usefulness? One of the requirements we set for the entity recommen- dation is the interestingness of the entities. As reported in Section 2, one popular approach to recommending rare items in information retrieval tasks is weighting their utility by their IDF score ( <ref type="bibr" target="#b4">Bordino et al., 2013;</ref><ref type="bibr" target="#b12">Jain et al., 2016)</ref>. Let us analyze in the document writing setup how useful users perceive entities which score high in IDF. To this end, we measure how the ground truth reviewer (the average rating given by the human annotators to an item) correlates against the base- line rating B and how it correlates against the IDF weighted rating W. Here, the baseline rating B simply rates an entity with 0 if it does not oc- cur in the future of the document and with 1 if it does occur in the future of the document. The IDF weighted rating W rates an entity with its IDF score multiplied by the score returned by the re- viewer B. We employ ICC(A, 1) to find the agree- ments between pairs of ratings. We find ICC(A, 1) between the ground truth reviewer and B to be 0.30, and between the ground truth reviewer and W to be 0.42. This shows that entities with high IDF scores are perceived as more useful. Note the potential space for improvement by finding a metric which would yield even higher correlation against the human rating than IDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Automatic Evaluation</head><p>Recall that we aim at rewarding entities which are both relevant (i.e., within the target set of enti- ties) and interesting (i.e., are not obvious, as the entity insect was for the monarch butterfly exam- ple from Section 1). Since we care only about the k highest ranked recommendations from the sys- tem, a potentially useful metric for evaluation is prec@k = 1 k k j=1 I {p j ∈F d } , where p j are the pre- dictions. Even though prec@k captures relevancy, it fails to distinguish between generic and specific entities. Documents tend to contain many entities in the target set, some of which are generic (e.g. insect). Prec@k scores all entities the same and rewards predictions of generic entities occurring across almost all web documents (e.g. Internet), which are arguably not interesting.</p><p>We propose a metric based on cumulative gain <ref type="bibr" target="#b13">(Järvelin and Kekäläinen, 2002</ref>), where we use IDF for scoring the relevance of labels:</p><formula xml:id="formula_1">CG-IDF@k = k j=1 I {p j ∈F d } IDF(p j ).</formula><p>To facilitate interpretability, we use a normalized version of CG-IDF@k, the normalized cumulative gain (NCG-IDF@k). NCG-IDF@k is obtained by dividing CG-IDF@k by the maximum sum of IDF scores of k entities from the target set.</p><p>Why would IDF help make entities more inter- esting? As shown by the user study in Section 3.2 IDF correlates with how relevant an entity is per- ceived by a user. Moreover, as reported in Sec- tion 2, IDF is widely used for boosting rare, novel items ( <ref type="bibr" target="#b4">Bordino et al., 2013;</ref><ref type="bibr" target="#b12">Jain et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We consider the problem of recommending top- ics that an author writing a document might be interested in writing about next. In Section 3.2 we used a human evaluation study to support a supervised learning setup for this problem, where what is written later in a document is deemed to be a good recommendation for an initial part. The Web is a rich source of documents which facili- tates construction of large datasets. Below we de- scribe details of how we construct the dataset com- posed of web documents and report basic statistics thereof.</p><p>Construction We collected a dataset of 10M web documents with high pagerank ( <ref type="bibr" target="#b24">Page et al., 1998</ref>) scores across the Web as of November 2017. We ran entity recognition and linking to the Freebase knowledge graph using the Google NLP cloud. <ref type="bibr">6</ref> Moreover, only the 100K most frequent entities are kept. <ref type="bibr">7</ref> We randomly select 10K doc- uments for the test set and 10K for the validation set which we use for hyperparameter tuning.</p><p>Statistics As shown in <ref type="figure" target="#fig_0">Figure 1</ref> the document frequencies for the 100K most frequent entities from the dataset follow a power law distribution, with a small number of very frequent entities and many infrequent ones. The average number of documents per target entity is 4694.95, which is 3 orders of magnitude smaller than the maximum frequency. We found that the average number of future entities per document is 96, and the average number of input entities is 10. In <ref type="figure" target="#fig_1">Figure 2</ref> we show the percentage of times a context entity is followed by a particular target entity (row and column, re- spectively). Notice the matrix is asymmetric. For  instance, USA is more likely to be followed by Re- search than the other way around.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Baselines and Models</head><p>In this section we describe the models that we ap- plied to the entity recommendation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Linear Models</head><p>Let us represent input entities via n-hot vector representation, i.e., vector entries corresponding to entities present in the input set are set to one, whereas other entries are set to zero. We consider linear models, where such an input vector is mul- tiplied by a square weight matrix of size #entities × #entities, and a resulting vector contains scores for predicted entities. There are different possibil- ities for filling the entries of the weight matrix. We considered multiple options:</p><p>1. N(C, F ) obtained by putting the raw co- occurrences of the context entity C (corre- sponding to a row) and the future entity F (corresponding to a column).</p><p>2. P(F |C) obtained by normalizing the N(C, F ) matrix row-wise.</p><p>3. PPMI(C, F ) (positive pointwise mutual in- formation) aims to show how much more likely an entity F is to occur for a context C compared to observing them independently <ref type="bibr" target="#b14">(Jurafsky and Martin, 2000</ref>). PPMI is a pop- ular preprocessing step on the co-occurrence matrix before applying dimensionality reduc- tion, such as SVD (see Section 5.2) (Herbelot and Vecchi, 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Matrix Factorization</head><p>Matrix factorization methods (MFM) are among the most popular approaches for recommendation systems ( <ref type="bibr" target="#b16">Koren et al., 2009)</ref>. This approach works similar to the linear model, except that the re- sulting matrix is decomposed into a sequence of smaller matrices, the product of which approxi- mates the linear model. This approach reduces the number of parameters, which speeds predic- tions, saves memory, and may improve robustness to overfitting. We employ SVD and reduce the rank of the resulting matrix of the linear model to 100. MFM is applied analogously to the linear model, namely a vector encoding input entities is mapped into a vector with scores of target entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Random Forests</head><p>FastXML FastXML ( <ref type="bibr" target="#b26">Prabhu and Varma, 2014</ref>) is one of the most popular approaches to extreme multi-label classification. Each tree in this random forest model is grown recursively by splitting each node by a separating hyperplane. The hyperplane weight vector is chosen by optimizing for nDCG score (a non-differentiable loss which poses issues in the NN framework) and is additionally regular- ized with 1 norm to induce sparsity. Each leaf node contains a probability distribution over la- bels. At prediction time, the distributions from reached leaf nodes across the trees are aggregated and a final ranking of entities is created.</p><p>PFastreXML PFastreXML ( <ref type="bibr" target="#b12">Jain et al., 2016)</ref> builds on the FastXML model by introducing two modifications. First, the nDCG loss function is replaced by a propensity weighted nDCG (in our case, the IDF weighted nDCG), resulting in rare items ranked higher. Second, the final ranking is re-ranked using tail label classifiers, which aims at further improvements in how highly rare items are scored.</p><p>Experimental Setup The scale of our dataset is bigger than that reported by <ref type="bibr" target="#b12">Jain et al. (2016)</ref> and consequently the size of the generated random forests tends to be very large. Therefore, apart from keeping most of the hyperparameters as re- ported by <ref type="bibr" target="#b12">Jain et al. (2016)</ref>, in order to limit the size of generated trees, we modify two hyperpa- rameters: the maximum number of labels per leaf node is set to 50 (instead of 10), and the maxi- mum number of training examples per leaf node is set to 50 (instead of 10). Even under such a setup, the size of the generated models is around 150GB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Neural models</head><p>Recently NN models have been applied to ex- treme classification. In this section we discuss the YouTube neural model used for recommending YouTube videos ( <ref type="bibr" target="#b7">Covington et al., 2016</ref>) and next the XML-CNN model which has been demon- strated to achieve competitive results on a few NLP tasks ( <ref type="bibr" target="#b18">Liu et al., 2017</ref>).</p><p>The YouTube Model A NN model (depicted in <ref type="figure" target="#fig_2">Figure 3</ref>(a)) has been successfully applied for the YouTube recommendation problem <ref type="bibr" target="#b7">(Covington et al., 2016</ref>). In the model, the input entities are embedded into a latent dimensionality V . The embeddings are then summed, and the resulting vector is passed through a number of layers, where in each layer an input vector is passed through matrix multiplication and a rectified linear unit (RELU) <ref type="bibr" target="#b23">(Nair and Hinton, 2010)</ref>. Afterwards, the resulting vector is converted into the space of di- mensionality equal to the number of entities via an output layer. This way, scores are obtained over the entity space, which are used to choose the highest scored entities for predictions. The loss function is the cross-entropy (CE) between the soft-maxed activations and a uniform distribu- tion over the target entities.</p><p>XML-CNN Convolutional neural networks (CNNs) have been successfully applied to a range of NLP problems <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr" target="#b3">Bitvai and Cohn, 2015)</ref>. Recently <ref type="bibr" target="#b18">Liu et al. (2017)</ref> demonstrated how CNNs can be effective for XML problems. We depict their architecture in <ref type="figure" target="#fig_2">Figure 3</ref>(b). The input sequence is embedded in V dimensions, and passed through a convo- lutional layer, a fully connected layer and an output layer. After the convolutional layer the authors employ dynamic pooling which helps retain information about where in the input sequence the convolutions got triggered. The loss function is binary cross entropy (BCE), which considers labels individually rather than jointly. BCE is given by the formula BCE(p, y) = j=M j=1 y j log(σ(p j )) + (1 − y j ) log(1 − σ(p j )), where σ is the sigmoid function. Lastly, a hidden bottleneck layer is used, which is motivated by introducing better generalization. In the experi- ments we did not find the dynamic max pooling to be beneficial, and instead we found max pooling with a higher number of filters (512 compared to 32 in ( <ref type="bibr" target="#b18">Liu et al., 2017)</ref>) to be better, keeping the number of parameters.</p><p>Modified Loss Function A potential problem with CE and BCE is that they reward all enti- ties equally. However, as demonstrated in Sec- tion 3.2, some entities are more valuable than oth- ers. Since we would like to promote interest- ing entities over generic ones, we consider an al- ternative training loss function to BCE and CE which incorporates the IDF scores of target en- tities. We use CE, but instead of comparing the soft-maxed activations to a uniform distribution over target entities, we compare against normal- ized IDF scores from the training set. We call this loss function CE-IDF. When using CE-IDF with XML-CNN, we found that adding 2 norm of the weights and the cross-entropy regulariza- tion ( <ref type="bibr" target="#b25">Pereyra et al., 2017</ref>) helps prevent the model from overfitting. We select the hyperparameters controlling these two regularization terms using a held out validation set. This contribution is analo- gous to that of PFastreXML over FastXML due to <ref type="bibr" target="#b12">Jain et al. (2016)</ref>, where weighting the loss func- tion in random forests by label propensity scores helps achieve better propensity weighted results.</p><p>Experimental Setup To regularize the networks we use a 50% dropout rate ( <ref type="bibr" target="#b29">Srivastava et al., 2014</ref>). We set the dimensionality of the embed- dings to 1000, and the hidden layer size to 512. The hyperparameters for XML-CNN are set as reported by <ref type="bibr" target="#b18">Liu et al. (2017)</ref>, except for the 2 and cross-entropy regularization hyperparameters  which were selected on the validation set. Note we optimize all parameters, including the entity embeddings, on the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In <ref type="table">Table 1</ref> we report results from the experiments on the 10M web documents dataset for prec and NDCG-IDF metrics for k = 1, 3, 5, 7, limiting k to small values as is common in the recommenda- tion problems from large sets of items ( <ref type="bibr" target="#b12">Jain et al., 2016)</ref>. The p(F ) baseline always predicts entities according to their frequency over the training set. It can be viewed as maximum likelihood estimate (MLE) for the model which is only composed of a bias vector (i.e., input entities are ignored). No- tice the relatively high performance when the most popular entities are taken. For example, in 36.76% of cases entity Research (the most popular future entity from the corpus) is in the future of the docu- ment (as can be also seen from <ref type="figure" target="#fig_0">Figure 1</ref>, where the entity Research corresponds to the leftmost point of the graph). This constitutes a high value, as the vocabulary consists of 100K entities.</p><p>Among the linear models, P(F |C) yields the highest scores, significantly outperforming the baselines. PPMI(F |C) model yields relatively high NCG-IDF scores (although in most cases lower than P(F |C)), and very low precision scores. Notice that the SVD methods are consis- tently worse than the linear models. This shows that no additional generalization is gained when lowering the number of parameters of the linear models. When experimenting with higher ranks for SVD decomposition we found the performance increases, but does not improve over the linear models.</p><p>NNs improve over linear models according to both NCG-IDF and prec scores. This is espe- cially apparent for NCG-IDF, where the relative improvement is very significant. XML-CNN is in all cases better than the Youtube model, which shows how utilizing more linguistic structure than simply bag of entities is helpful in the NN frame- work. Both Youtube and XML-CNN models with a modified loss function improve over the basic NN models in terms of the NCG-IDF metrics, showing that a simple adjustment of a loss func- tion in the NN framework can lead to more rare input predictions XML-CNN base XML-CNN IDF FastXML PFastreXML</p><p>NASA recently released a study suggesting that the Antarctic Ice Sheet is gaining more ice than it is losing -a finding that at first blush seems to contradict the idea of global warming.  entities being recommended. This comes at the cost of lowering prec@k scores, which however correlates with user judgments to a lesser extent, as we showed in Section 3.2. The Random Forest models turn out to be the most competitive. Notice that no linguistic structure is captured in FastXML models, only the bags of entities. This is in con- trast with XML-CNN approach which looks at lo- cal contexts of feature entities. FastXML performs particularly well on the precision scores, which however is not necessarily useful, as demonstrated in the examples discussed later.</p><p>Last, we inspect the sizes of the different mod- els reported in the rightmost column of <ref type="table">Table 1</ref>. Model size is an important factor to consider in practical applications, e.g. when deploying a sys- tem on the device. PFastreXML model takes 150GB, by far the most of all methods, resulting in its capability in recommending tail entities. The linear models take 4.7GB related to the fact that in the full 100K × 100K co-occurrence matrix ap- proximately 11% of entries are non-zero. Apply- ing SVD matrix decomposition helps reduce this size significantly. The NN models take around 2GB, significantly less than the random forests.</p><p>Analysis To demonstrate the usefulness of the models, in <ref type="table" target="#tab_2">Table 2</ref> we report top 4 entity pre- dictions from XML-CNN and FastXML models for a few example inputs from the test set. No- tice how predictions from XML-CNN base are more generic than from XML-CNN IDF. In par- ticular, for the first input related to Antarctic Ice Sheet gaining ice entities Earth and research are recommended. The relevance of them is clear, however their usefulness is doubtful due to how obvious to the writer they may be. Due to of- ten making such safe predictions XML-CNN base scores higher in precision than XML-CNN IDF. XML-CNN IDF instead makes more specific rec- ommendations, such as sea level rise or temper- ature for the first input. This shows how much more beneficial scoring high in NDCG-IDF com- pared to precision is. An analogous phenomenon takes place between FastXML and PFastreXML -despite FastXML achieving very high precision scores, the predictions tend to be less interesting.</p><p>When comparing PFastreXML results against XML-CNN IDF, the results become even more specific. For the first input the entities such as glaciology or Greenland ice sheet are recom- mended. For the third input about economet- rics and machine learning, both XML-CNN base and FastXML predict a generic entity informa- tion, XML-CNN IDF predicts more specific statis- tics, and PFastreXML recommends R program- ming language, a popular statistics toolkit for statisticians and mathematicians with support for machine learning. The usefulness of PFastreXML predictions is particularly profound for the last example about gene biology, where all other ap- proaches back off to very generic entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper we introduced the problem of entity recommendation for a document writer. Good en- tity recommendations need to be both relevant and interesting, which we motivated with a user study. In particular, we showed how entities which users write in the document are considered as good rec- ommendations and how IDF score correlates with how useful an entity is considered to be. We cor- roborated this with example predictions, showing how models scoring higher in metrics weighted by IDF provide more interesting suggestions.</p><p>The problem of recommending content to docu- ment writers has recently been addressed by indus- try with tools like Google Explore in Docs and Mi- crosoft Researcher, however the systems are pro- prietary and the methods have not been published. In particular, this work is the first to formalize the problem and provide insights about it to the re- search community. We hope that this work will inspire further research on recommending novel content to document writers.</p><p>Many avenues for further work can be identi- fied, including: finding a better metric capturing novelty of entities, analyzing the influence of the size of the input passage to quality of predictions, and experimenting with new models. Moreover, recent work has considered incorporating knowl- edge graph information for better use of entity fea- tures ( <ref type="bibr" target="#b8">Dalton et al., 2014;</ref><ref type="bibr" target="#b19">Liu et al., 2018)</ref>. This could also be explored for better feature represen- tation in our problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Percentage of documents for which each target entity occurs.</figDesc><graphic url="image-1.png" coords="5,82.91,62.81,196.44,132.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A heatmap showing what percentage of times a context entity (row) is followed by a particular target entity (column).</figDesc><graphic url="image-2.png" coords="5,72.00,250.50,218.27,178.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Neural network architectures applied to the entity recommendation problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Example inputs and corresponding top 4 entity predictions from the models.</figDesc><table></table></figure>

			<note place="foot" n="1"> http://bit.ly/2f6CVeR 2 http://bit.ly/29XaPAJ</note>

			<note place="foot" n="3"> http://trec-car.cs.unh.edu</note>

			<note place="foot" n="4"> I.e., excluding entities occurring in the passage. 5 Intraclass Coefficient is an approach to evaluate interrater reliability when ratings are organized into groups, as in our case, where entities are grouped into passages against which they are scored.</note>

			<note place="foot" n="6"> http://tinyurl.com/h246dnz 7 100K entities are kept due to the challenges in handling larger output spaces by the models. Scaling to larger output spaces is possible but requires modifications which we leave for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Kishore Papineni and Shankar Kumar for inspiring and useful discus-sions which greatly impacted the shape of this work. We also thank the anonymous reviewers for their insightful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berthier</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>AddisonWesley Longman Publishing Co., Inc</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2010 entity track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjen</forename><forename type="middle">P</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Nineteenth Text REtrieval Conference</title>
		<meeting>The Nineteenth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sparse local embeddings for extreme multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kush</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purushottam</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-linear text regression with a deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Bitvai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="180" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Penguins in sweaters, or serendipitous entity search on user-generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelena</forename><surname>Mejova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounia</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</title>
		<meeting>the 22Nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98</title>
		<meeting>the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;98<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Supporting entity search: A largescale prototype search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chen-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;07</title>
		<meeting>the 2007 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1144" to="1146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for YouTube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems, RecSys &apos;16</title>
		<meeting>the 10th ACM Conference on Recommender Systems, RecSys &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Entity query feature expansion using knowledge base links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval, SIGIR &apos;14</title>
		<meeting>the 37th International ACM SIGIR Conference on Research &amp;#38; Development in Information Retrieval, SIGIR &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TREC complex answer retrieval overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manisha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Twenty-Sixth Text REtrieval Conference</title>
		<meeting>The Twenty-Sixth Text REtrieval Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building a shared world: mapping distributional to modeltheoretic semantic spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Maria</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Novelty and diversity in top-n recommendation-analysis and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>14:1- 14:30</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Internet Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extreme multi-label loss functions for recommendation, tagging, ranking &amp; other missing label applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="935" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cumulated gain-based evaluation of ir techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalervo</forename><surname>Järvelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaana</forename><surname>Kekäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="422" to="446" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech and Language Processing: An Introduction to Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics, and Speech Recognition</title>
		<meeting><address><addrLine>Upper Saddle River, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall PTR</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inside the search process: Information seeking from the users perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Carol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuhlthau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">42</biblScope>
			<biblScope unit="page" from="361" to="371" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning for extreme multi-label text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Entity-duet neural ranking: Understanding the role of knowledge graph semantics in neural information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1805.07591</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Linked data based exploratory search. Theses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">Marie</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Université Nice Sophia Antipolis</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forming inferences about some intraclass correlation coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="46" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An effective, low-cost measure of semantic relatedness obtained from Wikipedia links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International World Wide Web Conference</title>
		<meeting>the 7th International World Wide Web Conference<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="161" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1701.06548</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashoteja</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manik</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The impact of students&apos; approaches to essay writing on the quality of their essays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assessment &amp; Evaluation in Higher Education</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">327</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic tag recommendation algorithms for social recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Web</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rank and relevance in novelty and diversity metrics for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saúl</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Castells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM Conference on Recommender Systems, RecSys &apos;11</title>
		<meeting>the Fifth ACM Conference on Recommender Systems, RecSys &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploratory Search: Beyond the Query-Response Paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ryen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Resa</forename><forename type="middle">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On improving aggregate recommendation diversity and novelty in folksonomy-based social systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijian</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personal Ubiquitous Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1855" to="1869" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Solving the apparent diversity-accuracy dilemma of recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Kuscsik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matúš</forename><surname>Medo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">Rushton</forename><surname>Wakeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4511" to="4515" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
