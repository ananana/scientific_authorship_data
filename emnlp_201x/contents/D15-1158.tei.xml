<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Computer Science Department Bar</orgName>
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Ilan University Ramat-Gan</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Dependency Parsing using Bilexical Contextual Features from Auto-Parsed Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a semi-supervised approach to improve dependency parsing accuracy by using bilexical statistics derived from auto-parsed data. The method is based on estimating the attachment potential of head-modifier words, by taking into account not only the head and modifier words themselves, but also the words surrounding the head and the modifier. When integrating the learned statistics as features in a graph-based parsing model, we observe nice improvements in accuracy when parsing various English datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We are concerned with semi-supervised depen- dency parsing, namely how to leverage large amounts of unannotated data, in addition to anno- tated Treebank data, to improve dependency pars- ing accuracy. Our method (Section 2) is based on parsing large amounts of unannotated text us- ing a baseline parser, extracting word-interaction statistics from the automatically parsed corpus, and using these statistics as the basis of additional parser features. The automatically-parsed data is used to acquire statistics about lexical interactions, which are too sparse to estimate well from any realistically-sized Treebank. Specifically, we at- tempt to infer a function assoc(head, modif er) measuring the "goodness" of head-modifier rela- tions ("how good is an arc in which black is a mod- ifier of jump"). A similar approach was taken by <ref type="bibr" target="#b3">Chen et al. (2009)</ref> and Van Noord et al. <ref type="bibr">(2007)</ref>. We depart from their work by extending the scoring to include a wider lexical context. That is, given the sentence fragment in <ref type="figure" target="#fig_0">Figure 1</ref>, we score the (incorrect) dependency arc (black, jump) based on the triplets (the black fox, will jump over). Learn- ing a function between word triplets raises an extreme data sparsity issue, which we deal with by decomposing the interaction between triplets to a sum of interactions between word pairs. The decomposition we use is inspired by recent work in word-embeddings and dense vector rep- resentations ( <ref type="bibr" target="#b16">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b18">Mnih and Kavukcuoglu, 2013)</ref>. Indeed, we initially hoped to leverage the generalization abilities associated with vector-based representations. However, we find that in our setup, reverting to direct count- based statistics achieve roughly the same results (Section 3).</p><p>Our derived features improve the accuracy of a first-order dependency parser by 0.75 UAS points (absolute) when evaluated on the in-domain WSJ test-set, obtaining a final accuracy of 92.32 UAS for a first-order parser. When comparing to the strong baseline of using Brown-clusters based fea- tures ( <ref type="bibr" target="#b9">Koo et al., 2008)</ref>, we find that our triplets- based method outperform them by over 0.27 UAS points. This is in contrast to previous works (e.g. ( <ref type="bibr" target="#b0">Bansal et al., 2014)</ref>) in which improvements over using Brown-clusters features were achieved only by adding to the cluster-based features, not by re- placing them. As expected, combining both our features and the brown-cluster features result in some additional gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Our departure point is a graph-based parsing model ( <ref type="bibr" target="#b14">McDonald et al., 2005</ref>):</p><formula xml:id="formula_0">parse(x) = argmax y∈Y(x) score(x, y) score(x, y) = w · Φ(x, y) = part∈y w · φ(x, part)</formula><p>Given a sentence x we look for the highest-scoring parse tree y in the space Y(x) of valid dependency trees over x. The score of a tree is determined by a linear model parameterized by a weights vector w, and a feature function Φ(x, y). To make the search <ref type="table">Table 1</ref>: All features are binary indicators. x and y are token indices. S ij is estimated from auto-parsed corpora as described in the text. The values of S(·, ·) are in the range (0, 1), which is split by bin into 10 equally-spaced intervals. dist is the signed and binned sentence-distance between x and y. pos(x) is the part of speech of token x.</p><formula xml:id="formula_1">Features in φ ij lex (x, y) bin(S ij (x, y)) bin(S ij (x, y)) • dist(x,y) bin(S ij (x, y)) • pos(x) • pos(y) bin(S ij (x, y)) • pos(x) • pos(y) • dist(x,y)</formula><p>• indicates a concatenation of features.</p><p>tractable, the feature function is decomposed into local feature functions over tree-parts φ(x, part). The features in φ are standard graph-based depen- dency parsing features, capturing mostly structural information from the parse tree.</p><p>We extend the scoring function by adding an ad- ditional term capturing the strength of lexical as- sociation between head word h and modifier word m in each dependency arc:</p><formula xml:id="formula_2">score(x, y) = part∈y w · φ(x, part) + (h,m)∈y assoc(h, m)</formula><p>The association function assoc is also modeled as a linear model assoc(h, m) = w lex ·φ lex (h, m). While the weights w lex are trained jointly with w based on supervised training data, the features in φ lex do not look at h and m directly, but instead rely on a quantity S(h, m) that reflects the "good- ness" of the arc (h, m). The quantity S(h, m) ranges between 0 and 1, and is estimated based on large quantities of auto-parsed data. Given a value for S(h, m), φ lex is composed of indicator functions indicating the binned ranges of S(h, m), possibly conjoined with information such as the binned surface distance between the tokens h and m and their parts of speech. The complete spec- ification of φ lex we use is shown in <ref type="table">Table 1</ref> (the meaning of the ij indices will be discussed in Sec- tion 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Estimating S(h, m)</head><p>One way of estimating S(h, m), which was also used in <ref type="bibr" target="#b3">(Chen et al., 2009)</ref>, is using rank statistics. Let D be the list of observed (h, m) pairs sorted by their frequencies, and let rank(h, m) be the index of the pair (h, m) in D. We now set:</p><formula xml:id="formula_3">S RANK (h, m) = rank(h, m) |D|</formula><p>While effective, this approach has two related shortcomings: first, it requires storing counts for all the pairs (h, m) appearing in the auto-parsed data, resulting in memory requirement that scales quadratically with the vocabulary size. Second, even with very large auto-parsed corpora many plausible head-modifier pairs are likely to be un- observed.</p><p>An alternative way of estimating S(h, m) that does not require storing all the observed pairs and that has a potential of generalizing beyond the seen examples is using a log-bilinear embedding model similar to the skip-gram model presented by <ref type="bibr" target="#b17">Mikolov et al. (2013b)</ref> to embed word pairs such that compatible pairs receive high scores. The model assigns two disjoint sets of d-dimensional continuous latent vectors, u and v, where u h ∈ R d is an embedding of a head word h, and v m ∈ R d is an embedding of a modifier word m. The embed- ding is done by trying to optimize the following corpus-wide objective that is maximizing the dot product of the vectors of observed (h, m) pairs and minimizing the dot product of vectors of random h and m pairs. Formally:</p><formula xml:id="formula_4">h,m∈C ln (σ (u h · vm)) − k i=1 E m i ∼ Pm ln (σ (u h · vm i ))</formula><p>where σ(x) = 1/(1 + e −x ), and k is the number of negative samples, drawn from the corpus-based Unigram distribution P m . For further details, see <ref type="bibr" target="#b17">(Mikolov et al., 2013b;</ref><ref type="bibr" target="#b8">Goldberg and Levy, 2014</ref>). We then take: 1</p><formula xml:id="formula_5">S EMBED (h, m) = σ(u h · v m )</formula><p>In contrast to the counts based method, this model is able to estimate the strength of a pair of words even if the pair did not appear in the corpus due to sparsity. Finally, <ref type="bibr" target="#b11">Levy and Goldberg (2014b)</ref> show that the skip-grams with negative-sampling model de- scribed above achieves its optimal solution when u h · v m = P M I(h, m) − log k. This gives rise to another natural way of estimating S:</p><formula xml:id="formula_6">m −1 m 0 m +1 . . . h −1 h 0 h +1 the black fox</formula><p>. . . will jump over </p><formula xml:id="formula_7">S PMI (h, m) = σ(P M I(h, m)) = p(h,m) p(h,m)+p(h)p(m)</formula><p>where p(h, m), p(h) and p(m) are unsmoothed maximum-likelihood estimates based on the auto- parsed corpus.</p><p>Like S RANK and unlike S EMBED , S PMI requires storing statistics for all observed word pairs, and is not able of generalizing beyond (h, m) pairs seen in the auto-parsed data. However, as we see in Section 3, this method performing similarly in practice, suggesting that the generalization ca- pabilities of the embedding-based method do not benefit the parsing task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adding additional context</head><p>Estimating the association between a pair of words is effective. However, we would like to go a step further and take into account also the context in which these words occur. Specifically, our com- plete model attempts to estimate the association between word trigrams centered around the head and the modifier words.</p><p>A naive solution that defines each trigram as its own vocabulary item will increase the vocab- ulary size by two orders of magnitude and re- sult in severe data sparsity. An alternative so- lution would be to associate each word in the triplet (h −1 , h 0 , h +1 ) with its own unique vocab- ulary item, and likewise for modifier words. In the embeddings-based model, this results in 6 vec-</p><formula xml:id="formula_8">tor sets u −1 , u 0 , u +1 , v −1 , v 0 , v +1 , where v −1</formula><p>dog , for example, represents the word "dog" when it ap- pears to the left of the modifier word, and u +1 walk the word "walk" when it appears to the right of the head word. <ref type="bibr">2</ref> This amounts to only a 3- fold increase in the required vocabulary size. We then model the strength of association between h −1 h 0 h +1 and m −1 m 0 m +1 as a weighted sum of pairwise interactions: 3</p><formula xml:id="formula_9">assoc(h −1 h 0 h +1 , m −1 m 0 m +1 ) = 1 i=−1 1 j=−1 α ij assoc ij (h i , m j )</formula><p>As before, the pairwise association measure assoc ij (h i , m j ) is modeled as a linear model:</p><formula xml:id="formula_10">assoc ij (h i , m j ) = w ij lex · φ ij lex (h i , m j )</formula><p>Where φ ij lex is again defined in terms of a goodness function S ij (x, y). For example, S −1,+1 (the, over) corresponds to the goodness of a head-modifier arc where the word to the left of the head word is "the" and the word to the right of the modifier word is "over". S ij (h i , m j ), the goodness of the arc induced by the pair (h i , m j ), can be estimated by either S RANK , S EMBED or S PMI as before. For example, in the embeddings model we set S ij (x, y) = σ(u i x · v j y ). We update the bilexical features to include con- text as explained above. Instead of learning α and w lex coefficients separately, we absorb the α ij terms into w lex , learning both at the same time:</p><formula xml:id="formula_11">assoc(h −1 h 0 h +1 , m −1 m 0 m +1 ) = i∈{−1,0,1} j∈{−1,0,1} α ij · assoc ij (h i , m j ) = i∈{−1,0,1} j∈{−1,0,1} α ij · w ij lex · φ ij lex (h i , m j ) = i∈{−1,0,1} j∈{−1,0,1} w ij lex · φ ij lex (h i , m j )</formula><p>Finally, the parser selects a dependency tree which maximizes the following:   </p><formula xml:id="formula_12">w · Φ(x, y) =</formula><formula xml:id="formula_13">+ u 0 y + u +1 z ) · (v −1 a + v 0 b + v +1 c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head><p>Data Our experiments are based on the Penn Treebank (PTB) <ref type="bibr" target="#b12">(Marcus et al., 1993</ref>) as well as the Google Web Treebanks (LDC2012T13), cov- ering both in-domain and out-of-domain scenar- ios. We use the Stanford-dependencies represen- tation ( <ref type="bibr" target="#b6">de Marneffe and Manning, 2008)</ref>. All the constituent-trees are converted to Stanford- dependencies based on the settings of Version 1.0 of the Universal Treebank ( <ref type="bibr" target="#b15">McDonald et al., 2013)</ref>. <ref type="bibr">4</ref> These are based on the Stanford Depen- dencies converter but use some non-default flags, and change some of the dependency labels. All of the models are trained on section 2-21 of the WSJ portion of the PTB. For in-domain data, we evaluate on sections 22 (Dev) and 23 (Test). All of the parameter tuning were performed on the Dev set, and we report test-set numbers only for the "most interesting" configurations. For out-of- domain data, we use the Brown portion of the PTB (Brown), as well as the test-sets of different do- mains available in the Google Web Treebank: An- swers, Blogs, Emails, Reviews and Newsgroups. All trees have automatically assigned part-of- speech tags, assigned by the TurboTagger POS- tagger. <ref type="bibr">5</ref> The train-set POS-tags were derived in a 10-fold jacknifing, and the different test datasets receive tags from a tagger trained on sections 2-21.</p><p>For auto-parsed data, we parse the text of the BLLIP corpus <ref type="bibr" target="#b1">(Charniak, 2000</ref>) using our base- line parser. This is the same corpus used for deriv- ing Brown clusters for use as features in ( <ref type="bibr" target="#b9">Koo et al., 2008)</ref>. We use the clusters provided by Terry Koo <ref type="bibr">6</ref> . Parsing accuracy is measured by unlabeled attachment score (UAS) excluding punctuations. Implementation Details We focus on first-order parsers, as they are the most practical graph- based parsers in terms of running time in real- istic parsing scenarios. Our base model is a re- implementation of a first-order projective Graph- based parser <ref type="bibr" target="#b14">(McDonald et al., 2005</ref>), which we extend to support the semi-supervised φ lex fea- tures. The parser is trained for 10 iterations of online-training with passive-aggressive updates <ref type="bibr" target="#b5">(Crammer et al., 2006</ref>). For the Brown-cluster fea- tures, we use the feature templates described by ( <ref type="bibr" target="#b9">Koo et al., 2008;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>).</p><p>The embedding vectors are trained using the freely available word2vecf software 7 , by con- joining each word with its relative position (-1, 0 or 1) and treating the head words as "words" and the modifier words to be "contexts". The words are embedded into 300-dimensional vectors. All code and vectors will be available at the first au- thor's website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. The second block (HM) compares the baseline parser to a parser including the assoc(h, m) lexical com- ponent, using various ways of computing s(h, m). We observe a clear improvement above the base- line from using the lexical component across all domains. The different estimation methods per- form very similar to each other.</p><p>In the third block (TRIP) we switch to the triplet-based lexical association. With S RANK , there is very little advantage over looking at just pairs. However, with S EMBED or S PMI the improve- ment of using the triplet-based method over using just the head-modifier pairs is clear. The counting- based PMI method performs on par with the Em- bedding based approximation of it.</p><p>The second line of the first block (Base+Brown) represents the current state-of-the-art in semi- supervised training of graph-based parsing: using Brown-cluster derived features ( <ref type="bibr" target="#b9">Koo et al., 2008;</ref><ref type="bibr" target="#b0">Bansal et al., 2014</ref>). The Brown-derived features provide similar (sometimes larger) gains to using our HM method, and substantially smaller gains than our TRIP method. To the best of our knowl- edge, we are the first to show a semi-supervised method that significantly outperforms the use of Brown-clusters without using Brown-clusters as a component.</p><p>As expected, combining our features and the Brown-based features provide an additional im- provement, as can be seen in the last block of <ref type="table" target="#tab_1">Table  2</ref> (Base+Brown+TRIP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Semi-supervised approaches to dependency pars- ing can be roughly categorized into two groups: those that use unannotated data and those that use automatically-parsed data. Our proposed method falls in the second group.</p><p>Among the words that use unannotated data, the dominant approach is to derive either word clus- ters ( <ref type="bibr" target="#b9">Koo et al., 2008)</ref> or word vectors <ref type="bibr" target="#b2">(Chen and Manning, 2014</ref>) based on unparsed data, and use these as additional features for a supervised pars- ing model. While the word representations used in such methods are not specifically designed for the parsing task, they do provide useful features for parsing, and in particular the method of ( <ref type="bibr" target="#b9">Koo et al., 2008)</ref>, relying on features derived using the Brown-clustering algorithm, provides very com- petitive state-of-the-art results. To the best of our knowledge, we are the first to show a substantial improvement over using Brown-clustering derived features without using Brown-cluster features as a component.</p><p>Among the words that use auto-parsed data, a dominant approach is self-training ( <ref type="bibr" target="#b13">McClosky et al., 2006</ref>), in which a parser A (possibly an en- semble) is used to parse large amounts of data, and a parser B is then trained over the union of the gold data and the auto-parsed data produced by parser A. In the context of dependency-parsing, successful uses of self-training require parser A to be stronger than parser B ( <ref type="bibr" target="#b20">Petrov et al., 2010)</ref> or use a selection criteria for training only on high- quality parses produced by parser A ( <ref type="bibr" target="#b22">Sagae and Tsujii, 2007;</ref><ref type="bibr" target="#b24">Weiss et al., 2015)</ref>. In contrast, our work uses the same parser (modulo the feature-set) for producing the auto-parsed data and for train- ing the final model, and does not employ a high- quality parse selection criteria when creating the auto-parsed corpus. It is possible that high-quality parse selection can improve our proposed method even further.</p><p>Works that derive features from auto-parsed data include <ref type="bibr" target="#b21">(Sagae and Gordon, 2009;</ref><ref type="bibr" target="#b0">Bansal et al., 2014)</ref>. Such works assign a representation (ei- ther cluster or vector) for individual word in the vocabulary based on their syntactic behavior. In contrast, our learned features are designed to cap- ture interactions between words. As discussed in sections <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref>, most similar to ours is the work of <ref type="bibr" target="#b3">(Chen et al., 2009;</ref><ref type="bibr" target="#b23">Van Noord, 2007</ref>). We extend their approach to take into account not only direct word-word interactions but also the lexical surroundings in which these interactions occur.</p><p>Another recent approach that takes into account various syntactic interactions was recently intro- duced by , who propose to learn to embed complex features that are being used in a graph-based parser based on other features they co-occur with in auto-parsed data. Similar to our approach, the embedded features are then used as additional features in a conventional graph-based model. The approaches are to a large extent com- plementary, and could be combined.</p><p>Finally, our work adds additional features to a graph-based parser which is based on a linear- model. Recently, progress in dependency parsing has been made by introducing non-linear, neural- network based models ( <ref type="bibr" target="#b19">Pei et al., 2015;</ref><ref type="bibr" target="#b2">Chen and Manning, 2014;</ref><ref type="bibr" target="#b24">Weiss et al., 2015;</ref><ref type="bibr" target="#b7">Dyer et al., 2015;</ref><ref type="bibr" target="#b25">Zhou et al., 2015)</ref>. Adapting our approach to work with such models is an interesting research direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the bilexical information including context. When scoring the (incorrect) arc between h 0 and m 0 , we take into account also the surrounding words h −1 , h +1 , m −1 and m +1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ij lex · φ ij lex (h i , m j ) 3 In the word embeddings literature, it is common to rep- resent a word triplet as the sum of the individual component vectors, resulting in ux,y,z · v a,b,c = (u −1 x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Expanding the terms will result in a very similar formulation to our proposal, but we allow the extra flexibility of associating a different strength αij with each pairwise term.</figDesc><table>Dev 

Test 
Brown Answers Blogs Email News Reviews 
Baseline 
91.97 91.57 86.86 
81.58 
84.88 79.75 82.59 83.22 
Base + Brown 
92.16 92.05 87.16 
81.96 
85.46 80.27 83.02 83.56 
Base + HM(S RANK ) 
92.16 91.74 87.01 
82.06 
85.36 80.29 82.72 83.62 
Base + HM(S EMBED ) 
92.29 91.98 87.04 
81.97 
85.34 79.93 82.76 83.33 
Base + HM(S PMI ) 
92.35 92.00 87.14 
82.20 
85.65 80.34 82.83 83.81 
Base + TRIP(S RANK ) 
92.23 91.91 87.02 
82.31 
85.59 80.50 83.30 83.79 
Base + TRIP(S EMBED ) 
92.38 92.27 87.15 
82.34 
85.71 80.41 83.21 83.68 
Base + TRIP(S PMI ) 
92.61 92.32 87.29 
82.58 
85.88 80.43 83.57 84.18 
Base + Brown + TRIP(S RANK ) 
92.43 92.33 87.23 
82.60 
85.75 80.57 83.48 84.00 
Base + Brown + TRIP(S EMBED ) 92.51 92.45 87.33 
82.42 
86.06 80.44 83.40 83.87 
Base + Brown + TRIP(S PMI ) 
92.70 92.40 87.42 
82.74 
86.08 80.72 83.78 84.26 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parsing accuracies (UAS, excluding puctuation) of the different models on various corpora. All models are trained 

on the PTB training set. Dev and Test are sections 22 and 23. Brown is the Brown portion of the PTB. The other columns 
correspond to the test portions of the Google Web Treebanks. Automatic POS-tags are assigned in all cases. HM indicates using 
assoc(h, m) and TRIP using assoc(h −1 h 0 h 1 , m −1 m 0 m 1 ). + BROWN indicate using features based on Brown clustering. 

</table></figure>

			<note place="foot" n="1"> The embedding we derive are very similar to the ones described in (Levy and Goldberg, 2014a; Bansal et al., 2014), and which were used by Bansal et al.(2014) for deriving semisupervised parsing features. An important difference from these previous work is that after training, they keep only one set of vectors (u or v) and ignore the other, basing the features on the derived vector representations. In contrast, we keep both sets of vectors and are interested in the association measure induced by the dot product u h · vm.</note>

			<note place="foot" n="2"> Sentences are padded by special sentence-boundary symbols.</note>

			<note place="foot" n="4"> https://github.com/ryanmcd/ uni-dep-tb/raw/master/universal_ treebanks_v1.0.tar.gz 5 http://www.ark.cs.cmu.edu/ TurboParser/</note>

			<note place="foot" n="6"> http://people.csail.mit.edu/maestro/ papers/bllip-clusters.gz 7 http://www.bitbucket.org/yoavgo/ word2vecf</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We presented a semi-supervised method for de-pendency parsing and demonstrated its effective-ness on a first-order graph-based parser. Taking into account not only the (head,modifier) word-pair but also their immediate surrounding words add a clear benefit to parsing accuracy.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bllip 1987-89 wsj corpus release 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<imprint>
			<publisher>Philadelphia</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving dependency parsing with subtrees from auto-parsed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Jun&amp;apos;ichi Kazama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="570" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature embedding for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shai Shalev-Shwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passive-aggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The stanford typed dependencies representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, CrossParser &apos;08</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transitionbased dependency parsing with stack long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">word2vec explained: deriving Mikolov et al.&apos;s negativesampling word-embedding method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3722</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple semi-supervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="595" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependencybased word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="302" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural word embeddings as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Ryan T Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvonne</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Quirmbachbrundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Täckström</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An effective neural network model for graph-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Uptraining for accurate deterministic question parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP2010</title>
		<meeting>of EMNLP2010</meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clustering words by syntactic similarity improves dependency parsing of predicate-argument structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWPT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dependency parsing and domain adaptation with LR models and parser ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun&amp;apos;ichi</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL-2007 Shared Task</title>
		<meeting>of CoNLL-2007 Shared Task</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using self-trained bilexical preferences to improve disambiguation accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertjan</forename><surname>Van Noord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Parsing Technologies</title>
		<meeting>the 10th International Conference on Parsing Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A neural probabilistic structured-prediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
