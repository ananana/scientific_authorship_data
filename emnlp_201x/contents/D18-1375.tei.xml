<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Genre-Aware Attention Model to Improve the Likability Prediction of Books</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y-Gómez</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Systems and Computer Engineering Department</orgName>
								<orgName type="institution">Universidad Nacional</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">University of Houston</orgName>
								<orgName type="institution" key="instit2">Instituto Nacional de Astrofısica Optica y Electronica</orgName>
								<address>
									<settlement>Puebla</settlement>
									<country key="MX">Mexico</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Genre-Aware Attention Model to Improve the Likability Prediction of Books</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3381" to="3391"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3381</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Likability prediction of books has many uses. Readers, writers, as well as the publishing industry , can all benefit from automatic book lik-ability prediction systems. In order to make reliable decisions, these systems need to assimilate information from different aspects of a book in a sensible way. We propose a novel multimodal neural architecture that incorporates genre supervision to assign weights to individual feature types. Our proposed method is capable of dynamically tailoring weights given to feature types based on the characteristics of each book. Our architecture achieves competitive results and even outper-forms state-of-the-art for this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Book likability prediction is an important but chal- lenging task. It can be a valuable resource for supporting buying decisions. The experience of choosing a book can be daunting for readers, con- sidering the overwhelming number of books being published. On the other hand, being able to predict how a book will fare in the market has relevant economic value for the publishing industry in or- der to increase their revenue. The current process is guided by humans, but this is error-prone, very subjective, and a non-scalable process.</p><p>An alternative to the human-guided process is to design a reliable automatic system that predicts the likability of books. Such a system, we argue, must be able to take into account all of the many aspects involved in the eventual success of a book. These include not only the topic of the book and the writ- ing style of the author, but in the case of creative writing, also include elements such as creativity, plot structure, and the flow of sentiments <ref type="bibr">(Hall, 2012;</ref><ref type="bibr">Archer and Jockers, 2016;</ref>. Other relevant aspects in- fluencing readers' interest for a book could be the cover and the title of the book.</p><p>We believe that in addition to the ability to in- corporate the different aspects, it is equally impor- tant to have a robust mechanism that gives higher weight to the most relevant aspects, while at the same time disregards the noisy or redundant as- pects. Traditionally, this is achieved by searching through multiple feature combination experiments for an optimal combination of different feature types <ref type="bibr" target="#b20">(Yang and Pedersen, 1997;</ref><ref type="bibr">Forman, 2003)</ref>. The main problem with these methods is that they are time-consuming and too rigid. The resulting feature types are fixed for every document. In some books, the style of the author may contribute more than the specific topic, whereas the reverse may be true for other books. These methods lack the ability to dynamically assign weights to differ- ent features based on the characteristics of a par- ticular test instance. Most likely, a more flexible scheme that adjusts feature weights based on the current book, can lead to better results. This paper attempts to solve this problem by in- troducing a novel method that is capable of au- tomatically combining information from different aspects and learning to weight them dynamically for each book in order to improve likability predic- tion. Our method also extends the attention model to incorporate domain specific information like the genre of books. As far as we know, we are the first to use genre supervision while computing atten- tion weights and to use them in the field of feature importance. There are many potentially relevant aspects of books that make them likable by read- ers. Here we focus on different textual modalities, like the lexical, stylistic, syntactic, and neural rep- resentations, along with the visual modality from book covers. Our main contributions in this paper are as follows:</p><p>• We propose a novel neural architecture, which incorporates genre supervision for computing attention weights to learn the im- portance of hand-engineered and deep learn- ing features coming from different modalities for predicting the likability of books.</p><p>• We show through our results that an adaptive combination of features with the genre-aware attention model performs better than strong baselines and also outperforms state-of-the- art.</p><p>• We present visualizations that increase inter- pretability of our results and also demonstrate the advantages of our model.</p><p>Along with these contributions, we also show that book cover images contain sufficient informa- tion by themselves to perform likability classifica- tion, although their contribution becomes negligi- ble in the presence of strong textual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>We propose a model that we call Genre-Aware At- tention model (GA), which dynamically weights features coming from different aspects of a book by using genre supervision. We first feed our textual and visual features through a non-linear layer to train higher feature representations. We then use our genre-aware attention model to com- pute appropriate weights for these feature repre- sentations. The motivation to add genre infor- mation comes from our previous work showing that adding genre classification as an auxiliary task to success prediction improved results <ref type="bibr" target="#b7">(Maharjan et al., 2017)</ref>. Moreover, it is also reasonable to ex- pect that different genres should have different sets of features that are more relevant when trying to predict whether readers will like the book. For in- stance, in Science Fiction, the theme may be more relevant than say, in Drama, where the characters and their interactions or their struggles might be more relevant for likeability prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Features</head><p>For our features, we build on the work by <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref> that provides a comprehen- sive exploration of different hand-crafted features and neural representations. They showed that a combination of writing density (WR) (distribution The source code and data for this paper can be down- loaded from https://github.com/sjmaharjan/ genre_aware_attention of word, character, sentences, and paragraphs), Book2Vec, and recurrent neural network represen- tations (RNN) works well for books. Similar to their work, our textual features consist of word, character, and typed character n-grams <ref type="bibr" target="#b12">(Sapkota et al., 2015)</ref>, syntactic features, sentiment and sentic concepts and scores (SCS) <ref type="bibr">(Cambria et al., 2014</ref>), style-related WR and readabil- ity (R), and neural representations learned using Word2Vec ( <ref type="bibr" target="#b9">Mikolov et al., 2013</ref>), Doc2Vec and RNN. We consider these categories of the textual features as different modalities or sources since they capture different aspects of a book and are generated by different processes. In addition to these features, we also add visual information ex- tracted from the book covers. To extract the visual features, we rely on state-of-the-art visual feature extractor methods like VGG <ref type="bibr" target="#b13">(Simonyan and Zisserman, 2014</ref>) and Resnet ( <ref type="bibr">He et al., 2016)</ref>, ini- tialized with the weights trained on the Imagenet dataset.  <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of our Genre-Aware Attention model. Let X be a collec- tion of books. For a book xxX, let x 1 , x 2 , . . . , x n be the feature representations from the different textual modalities and the visual modality. Since these features have different dimensions, we first pass them through a non-linear layer to project them into a space with the same dimension us- ing Equation 1. This will allow us to perform a weighted average of features from different modalities according to their importance:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Genre-Aware Attention Model</head><formula xml:id="formula_0">h i = selu(W h x i + b h ) (1)</formula><p>where i is the index of the modality whose feature representation is fed into the network, W h is the weight matrix, x i is the input feature vector for the ith modality, b h is the bias, and selu <ref type="bibr" target="#b3">(Klambauer et al., 2017</ref>) is the activation function. All of these feature vectors from different modalities may not be equally important to the final represen- tation and in turn to the likability prediction task. We use the genre-aware attention mechanism to learn the importance of each of these features to- wards our task and aggregate them to get the final representation. The final book representation r is the weighted sum of h i vectors:</p><formula xml:id="formula_1">r = i α i h i (2)</formula><p>where α i are the weights measuring the impor- tance of the different modalities. The GA model combines the genre vector gR dg (d g being the di- mension of the genre vector) while computing the α weights. The α i weights are computed as fol- lows:</p><formula xml:id="formula_2">α i = exp(score(h i , g)) i exp(score(h i , g))<label>(3)</label></formula><p>and the score(.) function is defined as:</p><formula xml:id="formula_3">score(h i , g) = v T selu(W a h i +W g g+b a ) (4)</formula><p>where, W a and W g are the weight matrices and v is the weight vector. The addition of W g g in- corporates genre supervision. These parameters are shared across all modalities. This will pre- vent parameter explosion that is likely to occur when the number of modalities is high, which is the case for us. To further investigate the effect of the genre, we also experiment by concatenat- ing the genre vector g to the final weighted aver- aged vectors from different modalities r to obtain r; g. The dotted line from genre vector g repre- sents this in <ref type="figure" target="#fig_0">Figure 1</ref>. We then use a non-linear layer with sigmoid activation to project the book representation (either r or the concatenation r; g) to class probabilities.</p><formula xml:id="formula_4">ˆ p = σ(W c r + b c ) (5)</formula><p>where, W c is the weight matrix and b c is the bias vector. Finally, we train the network by minimizing the binary cross entropy loss using Adam ( <ref type="bibr" target="#b2">Kingma and Ba, 2015)</ref>.</p><formula xml:id="formula_5">L = − i p i logˆplogˆ logˆp i (6)</formula><p>where, p i andˆpandˆ andˆp i are true labels and predictions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>We experiment with the dataset collected by <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref>. The dataset consists of books from eight different genres: Detective Mystery, Drama, Fiction, Historical Fiction, Love Stories, Poetry, Science Fiction, and Short Stories. These books have been reviewed by at least ten review- ers. Based on the average rating received by the books on Goodreads 1 , they labeled the books into two categories: Successful and Unsuccessful. The collection has a total of 1,003 books. However, the dataset did not include book covers. We aug- mented this dataset by downloading the covers from Goodreads. Since this dataset only contains publicly available books, all of them were pub- lished over 100 years ago. Some of the books only had the title of the book on a plain background as their cover images on Goodreads. We manu- ally searched for these books with Google Image Search and found the actual covers for most of them. However, even after an exhaustive search, we were unable to obtain proper covers for 21 books. We did not remove these books from the dataset for the sake of comparison with <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>We used the same train and test folds as used by <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref> for all of our experi- ments. The dataset consists of 349 books belong- ing to the Unsuccessful class and 654 books be- longing to the Successful class. Since the dataset is imbalanced, they as well as we use weighted F1- score to evaluate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>The most naive baseline will be to predict the majority class for all test instances. This majority class baseline yields a weighted F1-score of 50.6% for the likability classification task. This baseline will help to understand whether our proposed model is actually learning from the data at all. Apart from this, we compare with the results from Maharjan et al. <ref type="formula">(2017)</ref> and we also define several other baselines to validate the superiority of our proposed model. All of the baseline methods are listed below:</p><p>Mah'17: The current state-of-the-art for this dataset by <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref>. They have sev- eral results on various combinations of textual fea- tures.</p><p>Mah'17+Vis: This method is the extension of the Mah'17 method with the addition of visual fea- tures. Similar to them, we use the SVM classifier under two settings: Single-task (ST) and Multi- task (MT). In ST, we simply predict the likability of books. In MT, along with predicting likability, we also predict genre simultaneously. This exper- iment will allow us to make a direct comparison with Mah'17 regarding the effect of adding visual modalities.</p><p>Concatenation: Similar to GA, we first feed the features from different modalities through a non- linear layer each having the same number of neu- rons. We then concatenate them to obtain the final representation for a book. We send this represen- tation to a sigmoid layer for success prediction.</p><p>Average Pooling: Instead of concatenation, we take an average of the features after passing them through the non-linear layer. This is also compara- ble to an attention model assigning equal weights to all modalities. Attention: We use a multilayer perceptron to learn the appropriate weights for each of the features from different modalities. This method is sim- ilar to our proposed method, except that we do not use genre information for computing the at- tention weights. We compute the score(.) as</p><formula xml:id="formula_6">v T selu(W a h i + b a )</formula><p>, without the genre informa- tion. This experiment will help us understand the importance of genre in computing weights for the feature types. Bilinear Model: We combine the non-linear transformed modalities h 1 , . . . , h n using a bilinear form</p><formula xml:id="formula_7">(h i T W b h j + b b ),</formula><p>where pairs of these modalities and combine each of them using a bilinear form. The final book vector is the concatenation of the resulting vectors from each of these pairs. Bilinear models are used in the visual question answering community to fuse visual and textual information ( <ref type="bibr">Fukui et al., 2016)</ref>. This experiment will help us understand how our proposed model compares with other state-of-the-art multimodal approaches.</p><formula xml:id="formula_8">W b R k×d h i ×d h</formula><p>For all these models as well, we also performed additional experiments by concatenating the genre vector g with the final representations r obtained from each of these models to study the signifi- cance of including genre explicitly for likability prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>For the experiments involving the SVM classifier, we tuned the C hyper-parameter with values {1e- 4, . . . , 1e4} by performing three-fold grid search over the training data and then used the best hyper- parameters to train the final model. For the neu- ral network experiments, we first separated 20% of the training data as a validation set and tuned dropout rates {0.2, 0.4, 0.5}, different weights ini- tialization schemes {Glorot Uniform ( <ref type="bibr">Glorot and Bengio, 2010)</ref>, LeCun Uniform ( <ref type="bibr" target="#b5">LeCun et al., 1998</ref>)}, learning rate with Adam {1e-4, . . . , 1e-1}, number of hidden neurons in different layers {100, 200}, and batch size {1, 4, 8} with early stopping criteria. We initialized the genre embeddings with orthogonal vectors. <ref type="table" target="#tab_2">Table 1</ref> shows and compares our results with dif- ferent baselines. We experimented with both low performing as well as high performing features and their combinations as found by <ref type="bibr" target="#b7">Maharjan et al. (2017)</ref>. We obtained the best weighted F1-score of 75.4% with our proposed GA+Genre concate- nation model. This is 4.2% and 8.7% above the corresponding results reported by Mah'17 with their MT and ST settings, respectively. We also see a significant* improvement of 6.5% (over MT) and 22.2% (over ST) when using RNN fea- tures with our proposed method as compared to Mah'17. These results support the superiority of our method in learning high-quality book repre- sentations than Mah'17's state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The results also show that it is beneficial to use at least some form of attention over just Average Pooling. This suggests that using all available fea- tures without regards to their individual contribu- tion towards the task at hand can actually worsen the performance. Our proposed model is capable *We used the McNemar significance test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mah'17</head><p>Mah  of assigning importance to these features and the results clearly show that this works to our benefit. The results also demonstrate the added advantage of using genre supervision while computing fea- ture weights. There is a considerable improvement in the performance over the Attention method af- ter taking the genre information into account using our GA method. We suspect that the genre meta- information is helping to learn more specialized weights based on the genre of the books. With the neural baseline methods like Concate- nation and Average pooling, we do not always see improvement in performance after combining the genre information with the final book repre- sentation. Apart from these two, the combination of genre information does improve the results for other methods. The Bilinear and Attention meth- ods seem to be able to utilize this information well. However, none of these methods are capable of do- ing better than our method. GA and GA+Genre concatenation models always achieve the best per- formance for all experiments. This also illustrates the latent power of our method to better exploit do- main information like genre for performance im- provement.</p><note type="other">'17+Vis Concatenation Average Pooling Bilinear Attention Genre Attention</note><formula xml:id="formula_9">Features ST (SVM) MT (SVM) ST (SVM) MT (SVM) - + Genre - + Genre - + Genre - + Genre - + Genre</formula><p>Another interesting finding is that with the ad- dition of multiple modalities, the performance of Bilinear methods degrades to the majority class baseline <ref type="table" target="#tab_2">(Table 1</ref>, last row). This may be due to parameter explosion with the increase in the num- ber of modalities. However, our method is able to selectively weight the feature sources and dis- count the effect of redundant and irrelevant fea- tures to obtain the best performance, even with a larger number of modalities. In short, we see that our proposed method is able to cope with feature pollution and parameter explosion.</p><p>Next, we investigate the addition of visual infor- mation with the textual information for the likabil- ity prediction of books. Under the ST setting with SVMs, we see that the low performing textual fea- tures are benefited significantly by the addition of visual features, sometimes even outperforming the MT setting <ref type="table" target="#tab_2">(Table 1,</ref>  <ref type="figure" target="#fig_0">rows 1-4)</ref>. However, the vi- sual features are not able to contribute much when combined with strong textual features that were al- ready performing well. On the other hand, for the MT setting, the performance decreases for most of the feature combinations with the addition of the visual modality. We suspect that book covers are not very helpful at predicting genre and thus the MT setting does not do well with additional visual features.</p><p>Visual Results: Our next set of experiments con- siders only the visual information for books' lika- bility prediction. Even though we do believe that this current corpus might not be ideal for using cover features, we believe it is still interesting to explore whether the current book covers have suf- ficient information to perform likability classifica- tion with reasonable accuracy. We used VGG and Resnet to extract features from book cover images. We replaced the top layers by a dense layer of 256 neurons, and a classification layer (eight neurons with softmax for genre classification and one neu- ron with sigmoid activation for success classifica- tion). We also added a dropout layer in between the dense and the classification layer. The layers were initialized with weights trained on the Ima- genet dataset.   <ref type="table" target="#tab_4">Table 2</ref> shows the results with only the visual features for likability and genre classification of books under the ST and MT settings. We ob- tain the highest weighted F1-score of 61.8% and 25.9% for likability and genre classification tasks, respectively. With the neural experimental setup, we get similar performance under the ST and the MT settings for both tasks. We also experimented with transferring the visual feature vectors to the SVM classifier under the ST and the MT settings. We saw a decrease in performance under the MT settings with both the VGG and Resnet features <ref type="table" target="#tab_4">(Table 2</ref>, last two rows). This is the opposite of the Mah'17 results for the textual features as seen in <ref type="table" target="#tab_2">Table 1</ref>. The reason behind this may be due to the fact that the textual features are better at both the likability and the genre classification tasks in- dividually, whereas the visual features are not as good as the textual features for the genre classifi- cation task. <ref type="bibr">Iwana et al. (2016)</ref> also concluded that genre classification with book covers is a difficult task as book covers have images with few visual features or ambiguous features.</p><p>These results also empirically verify the de- crease in performance for the MT settings with the addition of visual features for likability pre- diction. Although these results are significantly lower (p&lt;0.001*) than our best results, they are still better than the majority baseline (50.6% and 10.7% for success and genre classification tasks, respectively). These results support our hypothe- sis that the books' cover images correlate with the likability of books. Also, they dictate for the need of extracting other features that consider different aspects of books.  given by the best model to the different feature types for the books in the test set. The purpose of this visualization is to understand which as- pects of a book are deemed to be more important by the model. The figure shows that most of the weights are assigned to the Char 5-gram and the RNN representations. The results in <ref type="table" target="#tab_2">Table 1</ref> also support that RNN features are indeed one of the most important features. The contribution of the visual representations is negligible in the presence of strong textual features. The results in <ref type="table" target="#tab_2">Table 1</ref> also validate this finding. These two textual fea- tures also dominate over the other weaker textual features. In the same way, as for the visual fea- tures, we see negligible weights assigned to the other textual features as well. Our model seems to have learned that the Char 5-gram and the RNN features can cover the information given by the rest of the features. The Char 5-gram feature is capable of capturing the content, topic, and style of a text and as such might be able to cover the Unigram and Sentic Concepts features. Likewise, the Book2Vec features may be non-essential in the presence of the RNN representations. The model is reducing redundant information that does not aid the classification task and instead might just add noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Attention Weights Visualization</head><p>In order to validate that features given the top weights by our model are indeed the best features for the task, we ran an experiment with only the Char 5-gram and the RNN features. We were able to obtain a weighted F1-score of 73.6% with just these two features. This score is close to the best score of 75.4%, showing that these features are indeed good features for the task. Also, note that our model was able to figure out this feature set automatically, while using traditional methods would have entailed performing multiple experi- ments (2 n − 1 experiments, where n is the number of feature types) which is often times not possi- ble to do exhaustively. There is still an extra boost when using the whole feature set rather than us- ing just the Char 5-gram and the RNN features. Since our method tailors the feature weights to each book and its genre as well, the boost likely comes from the presence of other visual and tex- tual features, which at least for some books must be informative.</p><p>We just saw that only two out of all feature types are given most of the weights. However, the re- sults in <ref type="table" target="#tab_2">Table 1</ref> show that even without these fea-  <ref type="figure">Figure 3</ref> plots the av- erage attention weights for a model with Sentic Concepts and Scores, Writing Density, Typed n- grams, and Visual features' combination. We see that the weights now shift to Typed n-grams, and Sentic Concepts and Scores. The topic and con- tent captured through Sentic concepts and the style with Typed n-grams prove important. These fea- tures capture different aspects of books and are not strongly correlated with one another. Our model is capable of figuring out that in the absence of Char 5-gram, which encompasses all this information, these other features need to be made more promi- nent. We can also see that the model knows three different feature types to capture the same amount of information as captured by the two best ones from before.  From the figure, it is evident that different genres respond differently to each feature type. Compar- ing the two models, we see that Char 5-gram ac- tivates similarly to Typed n-gram, and RNN simi- larly to Sentic concepts for different genres.   We took the books that were misclassified when we used the visual features only but were correctly predicted after the combination with the textual di- mensions. As expected, we found that the books without proper covers were misclassified by vi- sual features. But upon addition of other textual features, they were correctly classified. <ref type="figure" target="#fig_7">Figure 6</ref> shows the cover image of two of such books. The fact that the cover has no images with just plain background, and title, leaves little information for the visual modality. Similarly, we also analyzed the books that were correctly classified by visual features only and misclassified when textual fea- tures were added. <ref type="figure" target="#fig_8">Figure 7</ref> shows two such books. Both the cover image and the title (present in the cover) of these two books seem to be interesting and are very likely to attract a reader's attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Prior works have shown that stylistic traits to be useful features to predict success of books <ref type="bibr">(Ashok et al., 2013;</ref><ref type="bibr" target="#b16">Underwood and Sellers, 2016;</ref><ref type="bibr" target="#b7">Maharjan et al., 2017)</ref>. <ref type="bibr">Ashok et al. (2013)</ref> used stylis- tic features extracted using the first 1K sentences from books to classify highly successful litera- ture from less successful literature. van <ref type="bibr">Cranenburgh and Bod (2017)</ref> used lexical and rich syn- tactic tree features to distinguish the degrees of high and less literary novels. <ref type="bibr" target="#b6">Louis and Nenkova (2013)</ref>    also showed that modeling sequential flow of emotions across entire books improves lika- bility prediction of books. <ref type="bibr">Iwana et al. (2016)</ref> used neural networks to learn relationships be- tween book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works' hand-engineered and deep learning features but differs in a way how these features are combined to produce a meaningful book represen- tations.</p><p>The attention mechanism ( <ref type="bibr">Bahdanau et al., 2014)</ref> has been successfully applied in enhanc- ing the document representation for several text classification <ref type="bibr" target="#b21">(Zhang et al., 2016;</ref><ref type="bibr" target="#b18">Wang et al., 2016b</ref>), sentiment classification ( <ref type="bibr">Kar et al., 2017;</ref><ref type="bibr">Nguyen and Shirai, 2015;</ref><ref type="bibr" target="#b17">Wang et al., 2016a</ref>), question answering ( <ref type="bibr" target="#b15">Tan et al., 2015;</ref><ref type="bibr">Chen et al., 2016a;</ref><ref type="bibr">Hermann et al., 2015)</ref>, named entity recog- nition ( <ref type="bibr">Bharadwaj et al., 2016;</ref><ref type="bibr">Aguilar et al., 2017</ref>), summarization <ref type="bibr" target="#b11">(Rush et al., 2015)</ref>, image- captioning ( <ref type="bibr" target="#b19">Xu et al., 2015)</ref> tasks. <ref type="bibr" target="#b22">Zhang et al. (2017)</ref> used summary vectors and position vectors while computing the attention weights for the slot filling problem. <ref type="bibr">Chen et al. (2016b)</ref> applied user preferences and product characteristics as atten- tions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user in- formation was much more effective in enhancing the document representations than the product in- formation. Similar to their idea, we fuse the genre information while computing attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>We present a novel method to fuse the information coming from different modalities using a genre- aware attention mechanism to predict the likability of books. We showed that our proposed method outperforms strong baselines and state-of-the-art by learning to distinguish the important features from irrelevant or redundant ones. Other methods either suffered from feature pollution or parame- ter explosion and yielded low performance. Along with this, our results also showed that the book cover images by themselves also have sufficient information to perform success prediction. How- ever, the difficulty in predicting genre from book covers decreased the performance in multi-task settings with additional visual features. We also used different visualizations to support our find- ings and improve interpretability of our model. As future work, we will extend the proposed method to include components that learn weights for indi- vidual feature elements and not only the entire fea- ture type. This could likely result in higher quality multimodal representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Genre-Aware Attention Model.</figDesc><graphic url="image-1.png" coords="2,307.28,382.55,218.27,176.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Feature importance for the feature combination: All best handcrafted, RNN, and visual (RNN = Recurrent Neural Networks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 shows</head><label>2</label><figDesc>Figure 2 shows the average attention weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average attention weights with respect to genre for the best features from two models.</figDesc><graphic url="image-2.png" coords="7,72.00,517.21,218.27,137.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 further</head><label>4</label><figDesc>Figure 4 further breaks down the attention weights by genre for RNN and Char 5-gram, and Typed n-grams and Sentic Scores and Concepts. From the figure, it is evident that different genres respond differently to each feature type. Compar</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Feature importance as assigned by attention weights for two most important features for six different books: A=The Count of Monte Cristo, B=The Scouts of the Valley, C=The Daughter of the Commandant, D=The Northern Light , E=The Great Secret, F=House of the Seven Gables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 shows</head><label>5</label><figDesc>Figure 5 shows the feature importance for the Char 5-gram and RNN feature types for six different books having different attention weights for the two features. This validates our assumption that the model is able to dynamically learn and assign weights to different modalities, not only according to the genre but also according to the characteristics of each book. The high variance of attention weights for the top features in Figures 2 and 3 also support this claim. This gives an edge to our model and helps it excel over all other methods.</figDesc><graphic url="image-4.png" coords="7,446.17,532.62,62.36,93.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Books misclassified by visual features but correctly classified when textual features are added.</figDesc><graphic url="image-3.png" coords="7,332.79,532.92,62.37,93.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Books misclassified by visual and text features' combination but correctly classified when only visual features are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>defined genre-specific and general features to predict the article quality in science journal- ism domain. Maharjan et al. (2017) compared their work with Ashok et al. (2013) and presented a new dataset for the book success prediction task. Their multitask approach with the combination of deep representations and hand-crafted features im- proved the classification results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Weighted F1-scores(%) for different multimodal methods for books' likability classification task 
(ST=Single Task, MT=Multitask, SCS=Sentic Concepts and Scores, WR=Writing Density, RNN=Recurrent Neu-
ral Network Representations, + Genre= genre embedding g concatenated with the final book vector r). Our base-
lines and proposed method include visual features as well. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Weighted F1-scores(%) for visual features for 
likability and genre classification of books with Single 
Task (ST) and Multitask (MT) settings. 

</table></figure>

			<note place="foot" n="1"> https://www.goodreads.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was partially funded by the National Science Foundation under award 1462141. We would like to thank the anonymous reviewers, John Arevalo, Deepthi Mave, and Prasha Shrestha for reviewing the paper and providing helpful comments and suggestions. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gustavo Aguilar, Suraj Maharjan, Adrian Pastor</head><p>López Monroy, and Thamar Solorio. 2017. A multi- task approach for named entity recognition in so- cial media data. Andreas van Cranenburgh and Rens Bod. 2017. A data-oriented model of literary language. In Pro- ceedings of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics: Volume 1, Long Papers, pages 1228-1238, Valencia, Spain. Association for Computational Lin- guistics.</p><p>George Forman.</p><p>2003. An extensive empirical study of feature selection metrics for text classi- fication. Journal of machine learning research, 3(Mar):1289-1305.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
	</analytic>
	<monogr>
		<title level="m">Semantic Evaluation (SemEval-2017)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="877" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Folksonomication: Predicting tags for movies from plot synopses using emotion flow encoded neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2879" to="2891" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>abs/1706.02515</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical evaluation of various deep learning architectures for bi-sequence classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Raykar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2762" to="2773" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop</title>
		<meeting><address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="9" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What makes writing great? first experiments on article quality prediction in the science journalism domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-task approach to predict likability of books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Maharjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1217" to="1227" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Letting emotions flow: Success prediction by modeling the flow of emotions in books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Suraj Maharjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="259" to="265" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR), Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Not all character ngrams are created equal: A study in authorship attribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Upendra</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Lstm-based deep learning models for non-factoid answer selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04108</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The longue durée of literary prestige</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Sellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Language Quarterly</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="344" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspectlevel sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiaoyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A bilingual attention network for code-switched emotion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1624" to="1634" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection in text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">O</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="412" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tweet sarcasm detection using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2449" to="2460" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
