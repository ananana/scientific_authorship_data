<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why Neural Translations are the Right Length</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute &amp; Computer Science Department</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
							<email>dyuret@ku.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Engineering</orgName>
								<orgName type="institution">Koç University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Why Neural Translations are the Right Length</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2278" to="2282"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The neural encoder-decoder framework for machine translation <ref type="bibr" target="#b6">(Neco and Forcada, 1997;</ref><ref type="bibr" target="#b3">Castaño and Casacuberta, 1997;</ref><ref type="bibr" target="#b9">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2014;</ref><ref type="bibr" target="#b5">Luong et al., 2015)</ref> provides new tools for addressing the field's difficult challenges. In this framework <ref type="figure" target="#fig_0">(Figure 1</ref>), we use a recurrent neural net- work (encoder) to convert a source sentence into a dense, fixed-length vector. We then use another re- current network (decoder) to convert that vector into a target sentence. In this paper, we train long short- term memory (LSTM) neural units <ref type="bibr" target="#b4">(Hochreiter and Schmidhuber, 1997</ref>) trained with back-propagation through time <ref type="bibr" target="#b10">(Werbos, 1990)</ref>.</p><p>A remarkable feature of this simple neural MT (NMT) model is that it produces translations of the right length. When we evaluate the system on previ- ously unseen test data, using BLEU ( <ref type="bibr" target="#b8">Papineni et al., 2002</ref>), we consistently find the length ratio between MT outputs and human references translations to be very close to 1.0. Thus, no brevity penalty is in- curred. This behavior seems to come for free, with- out special design.</p><p>By contrast, builders of standard statistical MT (SMT) systems must work hard to ensure correct length. The original mechanism comes from the IBM SMT group, whose famous Models 1-5 in- cluded a learned table (y|x), with x and y being the lengths of source and target sentences <ref type="bibr" target="#b1">(Brown et al., 1993</ref>). But they did not deploy this table when decoding a foreign sentence f into an English sen- tence e; it did not participate in incremental scoring and pruning of candidate translations. As a result ( <ref type="bibr" target="#b2">Brown et al., 1995)</ref>:</p><p>"However, for a given f, if the goal is to discover the most probable e, then the product P(e) P(f|e) is too small for long English strings as compared with short ones. As a result, short English strings are im- properly favored over longer English strings. This tendency is counteracted in part by the following modification: Replace P(f|e) with c length(e) · P(f|e) for some empirically chosen constant c. This modifi- cation is treatment of the symptom rather than treat- ment of the disease itself, but it offers some tempo- rary relief. The cure lies in better modeling."</p><p>More temporary relief came from Minimum Error-Rate Training (MERT) <ref type="bibr" target="#b7">(Och, 2003)</ref>, which au- tomatically sets c to maximize BLEU score. MERT also sets weights for the language model P(e), trans- lation model P(f|e), and other features. The length feature combines so sensitively with other features that MERT frequently returns to it as it revises one weight at a time.</p><p>NMT's ability to correctly model length is re- markable for these reasons:</p><p>• SMT relies on maximum BLEU training to ob- tain a length ratio that is prized by BLEU, while NMT obtains the same result through generic maximum likelihood training.</p><p>• Standard SMT models explicitly "cross off" source words and phrases as they are translated, so it is clear when an SMT decoder has finished translating a sentence. NMT systems lack this explicit mechanism.</p><p>• SMT decoding involves heavy search, so if one MT output path delivers an infelicitous ending, another path can be used. NMT decoding ex- plores far fewer hypotheses, using a tight beam without recombination. In this paper, we investigate how length regulation works in NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Toy Problem for Neural MT</head><p>We start with a simple problem in which source strings are composed of symbols a and b. The goal of the translator is simply to copy those strings. Training cases look like this:</p><formula xml:id="formula_0">a a a b b a &lt;EOS&gt; → a a a b b a &lt;EOS&gt; b b a &lt;EOS&gt; → b b a &lt;EOS&gt; a b a b a b a a &lt;EOS&gt; → a b a b a b a a &lt;EOS&gt; b b a b b a b b a &lt;EOS&gt; → b b a b b a b b a &lt;EOS&gt;</formula><p>The encoder must summarize the content of any source string into a fixed-length vector, so that the decoder can then reconstruct it. 1 With 4 hidden LSTM units, our NMT system can learn to solve this problem after being trained on 2500 randomly chosen strings of lengths up to 9. <ref type="bibr">2 3</ref> To understand how the learned system works, we encode different strings and record the resulting LSTM cell values. Because our LSTM has four hid- den units, each string winds up at some point in four-dimensional space. We plot the first two dimensions (unit 1 and unit 2 ) in the left part of <ref type="figure" target="#fig_2">Figure 2</ref>, and we plot the other two dimensions (unit 3 and unit 4 ) in the right part. There is no dimension reduction in these plots. Here is what we learn:</p><p>• unit 1 records the approximate length of the string. Encoding a string of length 7 may gen- erate a value of -6.99 for unit 1 .</p><p>• unit 2 records the number of b's minus the num- ber of a's, thus assigning a more positive value to b-heavy strings. It also includes a +1 bonus if the string ends with a.</p><p>• unit 3 records a prefix of the string. If its value is less than 1.0, the string starts with b. Other- wise, it records the number of leading a's.</p><p>• unit 4 has a more diffuse function. If its value is positive, then the string consists of all b's (with a possible final a). Otherwise, its value corre- lates with both negative length and the prepon- derance of b's. For our purposes, unit 1 is the interesting one. The behavior of unit 1 shows that the translator in- corporates explicit length regulation. It also explains two interesting phenomena:</p><p>• When asked to transduce previously-unseen strings up to length 14, the system occasionally makes a mistake, mixing up an a or b. How- ever, the output length is never wrong. shows the encoded strings in dimensions described by the cell states of LSTM unit1 (x-axis) and unit2 (y-axis). unit1 learns to record the length of the string, while unit2 records whether there are more b's than a's, with a +1 bonus for strings that end in a.</p><p>The right plot shows the cell states of LSTM unit3 (x-axis) and unit4 (y-axis). unit3 records how many a's the string begins with, while unit4 correlates with both length and the preponderance of b's. Some text labels are omitted for clarity.</p><p>• When we ask the system to transduce very long strings, beyond what it has been trained on, its output length may be slightly off. For example, it transduces a string of 28 b's into a string of 27 b's. This is because unit 1 is not incremented and decremented by exactly 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Full-Scale Neural Machine Translation</head><p>Next we turn to full-scale NMT. We train on data from the WMT 2014 English-to-French task, consisting of 12,075,604 sentence pairs, with 303,873,236 tokens on the English side, and 348,196,030 on the French side. We use 1000 hid- den LSTM units. We also use two layers of LSTM units between source and target. <ref type="bibr">5</ref> After the LSTM encoder-decoder is trained, we send test-set English strings through the encoder portion. Every time a word token is consumed, we record the LSTM cell values and the length of the when the translation is completely wrong, the length is still cor- rect (anonymous). <ref type="bibr">5</ref>  string so far. Over 143,379 token observations, we investigate how the LSTM encoder tracks length. With 1000 hidden units, it is difficult to build and inspect a heat map analogous to <ref type="figure" target="#fig_3">Figure 3</ref>. Instead, we seek to predict string length from the cell values, using a weighted, linear combination of the 1000 LSTM cell values. We use the least-squares method to find the best predictive weights, with resulting R 2 values of 0.990 (for the first layer, closer to source text) and 0.981 (second layer). So the entire network records length very accurately.</p><p>However, unlike in the toy problem, no single unit tracks length perfectly. The best unit in the second layer is unit 109 , which correlates with R 2 =0.894.</p><p>We therefore employ three mechanisms to locate k Best subset of LSTM's 1000 units R <ref type="table" target="#tab_1">2  1 109  0.894  2 334, 109  0.936  3 334, 442, 109  0.942  4 334, 442, 109, 53</ref> 0.947 <ref type="bibr">5 334, 442, 109, 53, 46 0.951 6 334, 442, 109, 53, 46, 928 0.953 7 334, 442, 109, 53, 46, 433, 663</ref> 0.955  Encoding Decoding</p><p>Unit 109 Unit 334 log P(&lt;EOS&gt;) a subset of units responsible for tracking length. We select the top k units according to: (1) individual R 2 scores, (2) greedy search, which repeatedly adds the unit which maximizes the set's R 2 value, and (3) beam search. <ref type="table">Table 1</ref> shows different subsets we ob- tain. These are quite predictive of length. <ref type="table" target="#tab_1">Table 2</ref> shows how R 2 increases as beam search augments the subset of units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mechanisms for Decoding</head><p>For the toy problem, <ref type="figure" target="#fig_3">Figure 3</ref> (middle part) shows how the cell value of unit 1 moves back to zero as the target string is built up. It also shows (lower part) how the probability of target word &lt;EOS&gt; shoots up once the correct target length has been achieved. MT decoding is trickier, because source and tar- get strings are not necessarily the same length, and target length depends on the words chosen. <ref type="figure" target="#fig_5">Figure 4</ref> shows the action of unit 109 and unit 334 for a sample sentence. They behave similarly on this sentence, but not identically. These two units do not form a simple switch that controls length-rather, they are high-level features computed from lower/previous states that contribute quantitatively to the decision to end the sentence. <ref type="figure" target="#fig_5">Figure 4</ref> also shows the log P(&lt;EOS&gt;) curve, where we note that the probability of outputting &lt;EOS&gt; rises sharply (from 10 −8 to 10 −4 to 0.998), rather than gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We determine how target length is regulated in NMT decoding. In future work, we hope to determine how other parts of the translator work, especially with reference to grammatical structure and transforma- tions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The encoder-decoder framework for neural machine translation (NMT) (Sutskever et al., 2014). Here, a source sentence C B A (fed in reverse as A B C) is translated into a target sentence W X Y Z. At each step, an evolving real-valued vector summarizes the state of the encoder (left half) and decoder (right half).</figDesc><graphic url="image-1.png" coords="2,72.00,57.82,425.21,87.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig- ure 3 shows the progression of "a b a b b b" as it gets encoded (top figure), then decoded (bottom two fig- ures). During encoding, the value of unit 1 decreases by approximately 1.0 each time a letter is read. Dur- ing decoding, its value increases each time a letter is written. When it reaches zero, it signals the decoder to output &lt;EOS&gt;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>abbb</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The progression of LSTM state as the recurrent network encodes the string "a b a b b b". Columns show the inputs over time and rows show the outputs. Red color indicates positive values, and blue color indicates negative. The value of unit1 decreases during the encoding phase (top figure) and increases during the decoding phase (middle figure). The bottom figure shows the decoder's probability of ending the target string (&lt;EOS&gt;).</figDesc><graphic url="image-4.png" coords="4,107.55,407.66,143.28,61.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Action of translation unit109 and unit334 during the encoding and decoding of a sample sentence. Also shown is the softmax log-prob of output &lt;EOS&gt;.</figDesc><graphic url="image-2.png" coords="4,96.30,193.16,153.39,87.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Sets of k units chosen by beam search to optimally 

track length in the NMT encoder. These units are from the 

LSTM's second layer. 

</table></figure>

			<note place="foot" n="1"> We follow Sutskever et al. (2014) in feeding the input string backwards to the encoder. 2 Additional training details: 100 epochs, 100 minibatch size, 0.7 learning rate, 1.0 gradient clipping threshold. 3 We use the toolkit: https://github.com/isi-nlp/Zoph RNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by ARL/ARO (W911NF-10-1-0533), DARPA (HR0011-15-C-0115), and the Scientific and Technological Research Council of Turkey (T ¨ UB ˙ ITAK) (grants 114E628 and 215E201).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Method and system for natural language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">451</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A connectionist approach to machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Casacuberta</surname></persName>
		</author>
		<editor>EUROSPEECH</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lstm can solve hard long time lag problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="473" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Asynchronous translations with recurrent neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forcada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Neural Networks</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2535" to="2540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minimum error rate training in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
