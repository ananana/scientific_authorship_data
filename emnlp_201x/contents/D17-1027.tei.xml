<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Jian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Xin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="286" to="291"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Word embeddings have attracted much attention recently. Different from alphabetic writing systems, Chinese characters are often composed of subcharacter components which are also semantically informative. In this work, we propose an approach to jointly embed Chinese words as well as their characters and fine-grained subcharacter components. We use three likelihoods to evaluate whether the context words, characters, and components can predict the current target word, and collected 13,253 subcharacter components to demonstrate the existing approaches of decomposing Chinese characters are not enough. Evaluation on both word similarity and word analogy tasks demonstrates the superior performance of our model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed word representation represents a word as a vector in a continuous vector space and can better uncover both the semantic and syntactic in- formation over traditional one-hot representations. It has been successfully applied to many down- stream natural language processing (NLP) tasks as input features, such as named entity recog- nition <ref type="bibr" target="#b3">(Collobert et al., 2011</ref>), text classification ( <ref type="bibr" target="#b4">Joulin et al., 2016)</ref>, sentiment analysis ( <ref type="bibr" target="#b16">Tang et al., 2014)</ref>, and question answering ( <ref type="bibr" target="#b20">Zhou et al., 2015</ref>). Among many embedding methods <ref type="bibr" target="#b0">(Bengio et al., 2003;</ref><ref type="bibr" target="#b9">Mnih and Hinton, 2009</ref>), CBOW and Skip-Gram models are very popular due to their simplicity and efficiency, making it feasi- ble to learn good embeddings of words from large scale training corpora ( <ref type="bibr">Mikolov et al., 2013b,a)</ref>.</p><p>Despite the success and popularity of word em- beddings, most of the existing methods treat each word as the minimum unit, which ignores the mor- phological information of words. Rare words can- not be well represented when optimizing a cost function related to a rare word and its contexts. To address this issue, some recent studies ( <ref type="bibr" target="#b6">Luong et al., 2013;</ref><ref type="bibr" target="#b11">Qiu et al., 2014;</ref><ref type="bibr" target="#b13">Sun et al., 2016a;</ref><ref type="bibr" target="#b17">Wieting et al., 2016)</ref> have investigated how to ex- ploit morphemes or character n-grams to learn bet- ter embeddings of English words.</p><p>Different from other alphabetic writing systems such as English, written Chinese is logosyllabic, i.e., a Chinese character can be a word on its own or part of a polysyllabic word 1 . The characters them- selves are often composed of subcharacter com- ponents which are also semantically informative. The subword items of Chinese words, including characters and subcharacter components, contain rich semantic information. The characters com- posing a word can indicate the semantic mean- ing of the word and the subcharacter components, such as radicals and components themselves being a character, composing a character can indicate the semantic meaning of the character. The compo- nents of characters can be roughly divided into two types: semantic component and phonetic compo- nent. The semantic component indicates the mean- ing of a character while the phonetic component indicates the sound of a character. For example, (water) is the semantic component of charac- ters (lake) and (sea), (horse) is the pho- netic component of characters (mother) and (scold) where both and are pronounced sim- ilar to .</p><p>Leveraging the subword information such as characters and subcharacter components can en- hance Chinese word embeddings with internal morphological semantics. Some methods have been proposed to incorporate the subword infor-mation for Chinese word embeddings. <ref type="bibr" target="#b15">Sun et al. (2014)</ref> and <ref type="bibr" target="#b5">Li et al. (2015)</ref> proposed methods to enhance Chinese character embeddings with rad- icals based on C&amp;W model <ref type="bibr" target="#b2">(Collobert and Weston, 2008</ref>) and word2vec models ( <ref type="bibr" target="#b7">Mikolov et al., 2013a</ref>,b) respectively. <ref type="bibr" target="#b1">Chen et al. (2015)</ref> used Chinese characters to improve Chinese word em- beddings and proposed the CWE model to jointly learn Chinese word and character embeddings. <ref type="bibr" target="#b18">Xu et al. (2016)</ref> extended the CWE model by ex- ploiting the internal semantic similarity between a word and its characters in a cross-lingual man- ner. To combine both the radical-character and character-word compositions, <ref type="bibr" target="#b19">Yin et al. (2016)</ref> proposed a multi-granularity embedding (MGE) model based on the CWE model, which represents the context as a combination of surrounding words, surrounding characters, and the radicals of the tar- get word. Particularly, they developed a dictionary of 20,847 characters and 296 radicals.</p><p>However, all the above approaches still missed a lot of fine-grained components in Chinese charac- ters. Formally and historically, radicals are char- acter components used to index Chinese charac- ters in dictionaries. Although many of the rad- icals are also semantic components, a character has only one radical, which cannot fully uncover the semantics and structure of the character. Be- sides over 200 radicals, there are more than 10,000 components which are also semantically mean- ingful or phonetically useful. For example, Chi- nese character (illuminate, reflect, mirror, pic- ture) has one radical (the corresponding tra- ditional Chinese radical is , meaning fire) and three other components, i.e., (sun), (knife), and (mouth). <ref type="bibr" target="#b12">Shi et al. (2015)</ref> proposed us- ing WUBI input method to decompose the Chinese characters into components. However, WUBI in- put method uses rules to group Chinese characters into meaningless clusters which can fit the alpha- bet based keyboard. The semantics of the compo- nents are not straightforwardly meaningful.</p><p>In this work, we present a model to jointly learn the embeddings of Chinese words, charac- ters, and subcharacter components. The learned Chinese word embeddings can leverage the ex- ternal context co-occurrence information and in- corporate rich internal subword semantic informa- tion. Experiments on both word similarity and word analogy tasks demonstrate the effectiveness of our model over previous works. The code and data are available at https://github.com/ HKUST-KnowComp/JWE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Joint Learning Word Embedding</head><p>In this section, we introduce our joint learning word embedding model (JWE), which combines words, characters, and subcharacter components information. Our model is based on CBOW model ( <ref type="bibr" target="#b7">Mikolov et al., 2013a</ref>). JWE uses the average of context word vectors, the average of context char- acter vectors, and the average of context subchar- acter vectors to predict the target word, and uses the sum of these three prediction losses as the ob- jective function.</p><formula xml:id="formula_0">INPUT PROJECTION OUTPUT w i+1 w i1 c i1 c i+1 s i1 s i+1</formula><p>s i w i We denote D as the training corpus, W = (w 1 , w 2 , · · · , w N ) as the vocabulary of words, C = (c 1 , c 2 , · · · , c M ) as the vocabulary of char- acters, S = (s 1 , s 2 , · · · , s K ) as the vocabulary of subcharacters, and T as the context window size respectively. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, JWE aims to maximize the sum of log-likelihoods of three predictive conditional probabilities for a tar- get word w i :</p><formula xml:id="formula_1">L(w i ) = 3 ∑ k=1 log P (w i |h i k ),<label>(1)</label></formula><p>where h i 1 , h i 2 , h i 3 are the composition of context words, context characters, context subcharacters respectively. Let v w i , v c i , v s i be the "input" vec- tors of word w i , character c i , and subcharacter s i respectively, ˆ v w i be the "output" vectors of word w i . The conditional probability is defined by the softmax function as follows:</p><formula xml:id="formula_2">p(w i |h i k ) = exp(h T i k ˆ v w i ) ∑ N j=1 exp(h T i k ˆ v w j ) , k = 1, 2, 3,<label>(2)</label></formula><p>where h i 1 is the average of the "input" vectors of words in the context, i.e.:</p><formula xml:id="formula_3">h i 1 = 1 2T ∑ −T ≤j≤T,j̸ =0 v w i+j .<label>(3)</label></formula><p>Similarly, h i 2 is the average of characters' "input" vectors in the context, h i 3 is the average of sub- characters' "input" vectors in the context or in the target word or all of them. Given a corpus D, JWE maximizes the overall log likelihood:</p><formula xml:id="formula_4">L(D) = ∑ w i ∈D L(w i ),<label>(4)</label></formula><p>where the optimization follows the implementa- tion of negative sampling used in CBOW model ( <ref type="bibr" target="#b7">Mikolov et al., 2013a</ref>). This objective function is different from that of MGE ( <ref type="bibr" target="#b19">Yin et al., 2016</ref>). For a target word w i , the objective function of MGE is almost equivalent to maximizing P (w i |h i 1 + h i 2 + h i 3 ). During the backpropagation, the gradients of h i 1 , h i 2 , h i 3 can be different in our model while they are always same in MGE, so the gradients of the embeddings of words, characters, subcharacter components can be different in our model while they are same in MGE. Thus, the representations of words, charac- ters, and subcharacter components are decoupled and can be better trained in our model. A sim- ilar decoupled objective function is used in ( <ref type="bibr" target="#b13">Sun et al., 2016a</ref>) to learn English word embeddings and phrase embeddings. Our model differs from theirs in that we combine the subwords of both the context words and target word to predict the target word while they use the morphemes of the target English word to predict it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We quantitatively evaluate the quality of word em- beddings learned by our model on word similarity evaluation and word analogy tasks. For all models, we used the same parameter set- tings. We fixed the word vector dimension to be 200, the window size to be 5, the training iteration to be 100, the initial learning rate to be 0.025, and the subsampling parameter to be 10 −4 . Words with frequency less than 5 were ignored during training. We used 10-word negative sampling for optimiza- tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Word Similarity</head><p>This task evaluates the embedding's ability of un- covering the semantic relatedness of word pairs. We select two different Chinese word similarity datasets, wordsim-240 and wordsim-296 provided by <ref type="bibr" target="#b1">(Chen et al., 2015</ref>) for evaluation. There are 240 pairs of Chinese words in wordsim-240 and 296 pairs of Chinese words in wordsim-296. Both datasets contain human-labeled similarity scores for each word pair. There is a word in wordsim- 296 that did not appear in the training corpus, so we removed this from the gold-standard to pro- duce wordsim-295. All words in wordsim-240 ap- peared in the training corpus. The similarity score for a word pair is computed as the cosine simi- larity of their embeddings generated by the learn- ing model. We compute the Spearman correlation ( <ref type="bibr" target="#b10">Myers et al., 2010</ref>) between the human-labeled scores and similarity scores computed by embed- dings. The evaluation results of our model and baseline methods on wordsim-240 and wordsim- 295 are shown in <ref type="table">Table 1</ref>.</p><p>From the results, we can see that JWE substan- tially outperforms CBOW, CWE, and MGE on the two word similarity datasets. JWE can bet- ter leverage the rich morphological information in Chinese words than CWE and MGE. It shows the benefits of decoupling the representation of words, characters, and subcharacter components as op- posed to employing concatenation, sum, or aver- age on all of them as the context.</p><p>We also observe that JWE with only characters can get competitive results on the word similarity task compared to JWE with characters and sub- characters. The reason may be that characters are enough to provide additional semantic information for computing the similarities of many word pairs in the two datasets. For example, the similarity of (law, statute) and (lawyer) in wordsim- 295 can be directly inferred from the shared char- acter (law, rule).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word Analogy</head><p>This task examines the quality of word embedding by its capacity of discovering linguistic regularities between pairs of words. For example, for a tuple like " (Rome): (Italy):: (Berlin):</p><p>(Germany)", the model can answer correctly if the nearest vector representation to vec() -vec() + vec() is vec() among all words except from , , and . More generally, given an analogy tuple "a : b :: c : d," the model answers the analogy question "a : b :: c :?" by finding x in the vocabulary such that arg max x̸ =a,x̸ =b,x̸ =c</p><formula xml:id="formula_5">cos( ⃗ b − ⃗ a + ⃗ c, ⃗ x).</formula><p>We use accuracy as the evaluation metric. In this  <ref type="table">Table 2</ref>: Results on word analogy reasoning. The configurations are the same of the ones used in <ref type="table">Ta- ble 1.</ref> task, we use the Chinese word analogy dataset in- troduced by <ref type="bibr" target="#b1">(Chen et al., 2015)</ref>, which consists of 1,124 tuples of words and each tuple contains 4 words, coming from three different categories: "Capital" (677 tuples), "State" (175 tuples), and "Family" (272 tuples). Our training corpus covers all the testing words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>The results in <ref type="table">Table 2</ref> show that JWE outper- forms the baselines on all categories' word anal- ogy tasks. Different from the results on the word similarity task, JWE with components consistently performs better than JWE with radicals and JWE without either radicals or components. It demon- strates the necessary of delving deeper into fine- grained components for complex semantic reason- ing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Case Studies</head><p>In addition to evaluating the benefits of incorpo- rating subword information for Chinese word em-beddings, it would be interesting to see the rela- tionships of the embeddings of words, characters, and subcharacter components as they are embed- ded into a same continuous vector space.   We evaluate the embeddings' abilities of uncov- ering the semantic relatedness of words, charac- ters, and subcharacter components through case studies. The similarities between them are com- puted by the cosine similarities of their embed- dings. Take two Chinese character (photo- graph) and (river) as examples, we list their closest words in <ref type="table" target="#tab_2">Table 3</ref>. We can see that most of the closest words are semantically related to the corresponding character.</p><p>We further take the component (illness) as an example and list its closest characters and words in <ref type="table" target="#tab_3">Table 4</ref>. All of the closest characters and words are semantically related to the component (ill- ness). Most of them have the component (ill- ness). (suffer), (swelling), and (pa- tients) do not have the component (illness), but they are also semantically related to (illness). It shows that JWE does not overuse the component information but leverages both the external con- text co-occurrence information and internal sub- word morphological information well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Future Work</head><p>In this paper, we propose a model to jointly learn the embeddings of Chinese words, characters, and subcharacter components. Our approach makes full use of subword information to enhance Chi- nese word embeddings. Experiments show that our model substantially outperforms the baseline methods on Chinese word similarity computation and Chinese word analogy reasoning, and demon- strate the benefits of incorporating fine-grained components compared to just using characters.</p><p>There could be several directions to be explored for future work. First, we use the average oper- ation to integrate the subcharacter components as the context to predict the target word. The struc- ture of Chinese characters and the positions of components in the character may be considered to fully leverage the component information of Chi- nese characters. Second, for any target word, we simply use word context, character context, and subcharacter context to predict it and do not distin- guish compositional words and non-compositional words. To solve this problem, attention models may be used to adaptively assign weights to word context, character context, and subcharacter con- text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of JWE. w i is the target word. w i−1 and w i+1 are the left word and right word of w i respectively. c i−1 and c i+1 represent the characters in the context. s i−1 and s i+1 represent the subcharacters in the context, s i represents the subcharacters of the target word w i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Closest words of characters (photo-
graph) and (river). 

Component 
(illness) 

Closest 
characters 

(cure) (symptom) 
(pain) (sore) 
(suffer) (itch) 
(infantile malnutrition) 
(disease) (swelling) 

Closest 
words 

(cure) (symptom) 
(recurrence) (pain) 
(symptom) 
(abdominal pain) 
(patients) (epilepsy) 
(disease) (therapy) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Closest characters and closest words of 
the component (illness). 

</table></figure>

			<note place="foot" n="1"> https://en.wikipedia.org/wiki/Written_ Chinese</note>

			<note place="foot" n="3"> http://thulac.thunlp.org/ 4 http://tool.httpcn.com/zi/ 5 https://code.google.com/p/word2vec/ 6 https://github.com/Leonard-Xu/CWE 7 We used the source code provided by the author. Our experimental results of baselines are different from that in MGE paper because we used a 1GB corpus while they used a 500MB corpus and we fixed the training iteration while they tried the training iteration in range [5, 200] and chose the best.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views expressed are those of the au-thors and do not reflect the official policy or posi-tion of the Department of Defense or the U.S. Gov-ernment. We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint learning of character and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1236" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Component-enhanced chinese character embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="829" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Research design and statistical analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Jerome L Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">Frederick</forename><surname>Well</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Routledge</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-learning of word representations and morpheme representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Radical embedding: Delving deeper to chinese radicals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehua</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inside out: Two jointly predictive models for word representations and phrase representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2821" to="2827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Thulac: An efficient lexical analyzer for chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Radical-enhanced chinese character embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Charagram: Embedding words and sentences via character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1504" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improve chinese word embeddings by exploiting internal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanhuan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1041" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-granularity chinese word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongchao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="981" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning continuous word embedding with metadata for question retrieval in community question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
