<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving the Transformer Translation Model with Document-Level Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Laboratory of Intelligent Technology and Systems Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Laboratory of Intelligent Technology and Systems Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Laboratory of Intelligent Technology and Systems Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">#</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence State Key Laboratory of Intelligent Technology and Systems Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Sogou Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving the Transformer Translation Model with Document-Level Context</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">533</biblScope>
							<biblScope unit="page" from="533" to="542"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and de-coder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past several years have witnessed the rapid de- velopment of neural machine translation (NMT) <ref type="bibr" target="#b11">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, which investigates the use of neural networks to model the translation process. Showing re- markable superiority over conventional statisti- cal machine translation (SMT), NMT has been recognized as the new de facto method and is widely used in commercial MT systems ( <ref type="bibr" target="#b10">Wu et al., 2016)</ref>. A variety of NMT models have been pro- posed to map between natural languages such as RNNencdec ( <ref type="bibr" target="#b11">Sutskever et al., 2014</ref>), RNNsearch ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>), <ref type="bibr">ConvS2S (Gehring et al., 2017)</ref>, and Transformer ( <ref type="bibr">Vaswani et al., 2017</ref>). Among them, the Transformer model has achieved state-of-the-art translation performance. The ca-pability to minimize the path length between long- distance dependencies in neural networks con- tributes to its exceptional performance.</p><p>However, the Transformer model still suffers from a major drawback: it performs translation only at the sentence level and ignores document- level context. Document-level context has proven to be beneficial for improving translation perfor- mance, not only for conventional <ref type="bibr">SMT (Gong et al., 2011;</ref><ref type="bibr">Hardmeier et al., 2012</ref>), but also for NMT ( <ref type="bibr">Wang et al., 2017;</ref><ref type="bibr">Tu et al., 2018)</ref>. <ref type="bibr" target="#b1">Bawden et al. (2018)</ref> indicate that it is important to ex- ploit document-level context to deal with context- dependent phenomena which are problematic for machine translation such as coreference, lexical cohesion, and lexical disambiguation.</p><p>While document-level NMT has attracted in- creasing attention from the community in the past two years ( <ref type="bibr">Jean et al., 2017;</ref><ref type="bibr" target="#b6">Kuang et al., 2017;</ref><ref type="bibr">Tiedemann and Scherrer, 2017;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr" target="#b7">Maruf and Haffari, 2018;</ref><ref type="bibr" target="#b1">Bawden et al., 2018;</ref><ref type="bibr">Tu et al., 2018;</ref><ref type="bibr">Voita et al., 2018)</ref>, to the best of our knowledge, only one existing work has en- deavored to model document-level context for the Transformer model ( <ref type="bibr">Voita et al., 2018)</ref>. Previous approaches to document-level NMT have concen- trated on the RNNsearch model ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. It is challenging to adapt these approaches to Transformer because they are designed specifi- cally for RNNsearch.</p><p>In this work, we propose to extend the Trans- former model to take advantage of document- level context. The basic idea is to use multi- head self-attention ( <ref type="bibr">Vaswani et al., 2017</ref>) to com- pute the representation of document-level context, which is then incorporated into the encoder and decoder using multi-head attention. Since large- scale document-level parallel corpora are usually hard to acquire, we propose to train sentence- level model parameters on sentence-level paral- lel corpora first and then estimate document-level model parameters on document-level parallel cor- pora while keeping the learned original sentence- level Transformer model parameters fixed. Our approach has the following advantages:</p><p>1. Increased capability to capture context: the use of multi-head attention, which signifi- cantly reduces the path length between long- range dependencies, helps to improve the ca- pability to capture document-level context;</p><p>2. Small computational overhead: as all newly introduced modules are based on highly par- allelizable multi-head attention, there is no significant slowdown in both training and de- coding;</p><p>3. Better use of limited labeled data: our ap- proach is capable of maintaining the superi- ority over the sentence-level counterpart even when only small-scale document-level paral- lel corpora are available.</p><p>Experiments show that our approach achieves an improvement of 1.96 and 0.89 BLEU points over Transformer on Chinese-English and French- English translation respectively by exploiting document-level context. It also outperforms a state-of-the-art cache-based method ( <ref type="bibr" target="#b6">Kuang et al., 2017</ref>) adapted for Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach 2.1 Problem Statement</head><p>Our goal is to enable the Transformer translation model ( <ref type="bibr">Vaswani et al., 2017</ref>) as shown in <ref type="figure" target="#fig_0">Figure  1</ref>(a) to exploit document-level context.</p><p>Formally, let X = x (1) , . . . , x (k) , . . . , x (K) be a source-language document composed of K source sentences. We use</p><formula xml:id="formula_0">x (k) = x (k) 1 , . . . , x (k) i , . . . , x (k) I</formula><p>to denote the k-th source sentence containing I words. x (k) i denotes the i-th word in the k-th source sentence.</p><p>Likewise, the corre- sponding target-language document is denoted by Y = y (1) , . . . , y (k) , . . . , y (K) and y (k) = y</p><formula xml:id="formula_1">(k) 1 , . . . , y (k) j , . . . , y (k)</formula><p>J represents the k-th target sentence containing J words. y (k) j denotes the j-th word in the k-th target sentence. We assume that X, Y constitutes a parallel document and each</p><formula xml:id="formula_2">x (k) , y (k) forms a parallel sentence.</formula><p>Therefore, the document-level translation prob- ability is given by</p><formula xml:id="formula_3">P (Y|X; θ) = K k=1 P (y (k) |X, Y &lt;k ; θ),<label>(1)</label></formula><p>where Y &lt;k = y (1) , . . . , y (k−1) is a partial trans- lation.</p><p>For generating y (k) , the source document X can be divided into three parts: (1) the k-th source sen- tence X =k = x (k) , (2) the source-side document-level context on the left X &lt;k = x (1) , . . . , x (k−1) , and (3) the source-side document-level context on the right X &gt;k = x (k+1) , . . . , x <ref type="bibr">(K)</ref> . As the lan- guages used in our experiments (i.e., Chinese and English) are written left to right, we omit X &gt;k for simplicity.</p><p>We also omit the target-side document-level context Y &lt;k due to the translation error propaga- tion problem ( <ref type="bibr">Wang et al., 2017)</ref>: errors made in translating one sentence will be propagated to the translation process of subsequent sentences. Inter- estingly, we find that using source-side document- level context X &lt;k , which conveys the same infor- mation with Y &lt;k , helps to compute better repre- sentations on the target side (see <ref type="table">Table 8</ref>).</p><p>As a result, the document-level translation prob- ability can be approximated as</p><formula xml:id="formula_4">P (Y|X; θ) ≈ K k=1 P (y (k) |X &lt;k , x (k) ; θ),<label>(2)</label></formula><formula xml:id="formula_5">= K k=1 J j=1 P (y (k) j |X &lt;k , x (k) , y (k) &lt;j ; θ),<label>(3)</label></formula><p>where y</p><formula xml:id="formula_6">(k) &lt;j = y (k) 1 , . . . , y (k) j−1 is a partial transla- tion.</formula><p>In this way, the document-level translation model can still be defined at the sentence level without sacrificing efficiency except that the source-side document-level context X &lt;k (or con- text for short) is taken into account.</p><p>In the following, we will introduce how to rep- resent the context (Section 2.2), how to integrate the context (Section 2.3), and how to train the model especially when only limited training data is available (Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-level Context Representation</head><p>As document-level context often includes several sentences, it is important to capture long-range dependencies and identify relevant information. We use multi-head self-attention ( <ref type="bibr">Vaswani et al., 2017)</ref> to compute the representation of document- level context because it is capable of reducing the maximum path length between long-range depen- dencies to O(1) (Vaswani et al., 2017) and deter- mining the relative importance of different loca- tions in the context ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. Be- cause of this property, multi-head self-attention has proven to be effective in other NLP tasks such as constituency parsing <ref type="bibr" target="#b5">(Kitaev and Klein, 2018</ref>).</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), we use a self-attentive encoder to compute the representation of X &lt;k . The input to the self-attentive encoder is a se- quence of context word embeddings, represented as a matrix. Suppose X &lt;k is composed of M source words: X &lt;k = x 1 , . . . , x m , . . . , x M . We use x m ∈ R D×1 to denote the vector representa- tion of x m that is the sum of word embedding and positional encoding ( <ref type="bibr">Vaswani et al., 2017)</ref>. There- fore, the matrix representation of X &lt;k is given by</p><formula xml:id="formula_7">X c = [x 1 ; . . . ; x M ],<label>(4)</label></formula><p>where X c ∈ R D×M is the concatenation of all vector representations of all source contextual words.</p><p>The self-attentive encoder is composed of a stack of N c identical layers. Each layer has two sub-layers. The first sub-layer is a multi-head self- attention:</p><formula xml:id="formula_8">A (1) = MultiHead(X c , X c , X c ),<label>(5)</label></formula><p>where A (1) ∈ R D×M is the hidden state calcu- lated by the multi-head self-attention at the first layer, MultiHead(Q, K, V) is a multi-head self- attention function that takes a query matrix Q, a key matrix K, and a value matrix V as inputs. In this case, Q = K = V = X c . This is why it is called self-attention. Please refer to ( <ref type="bibr">Vaswani et al., 2017</ref>) for more details. Note that we follow <ref type="bibr">Vaswani et al. (2017)</ref> to use residual connection and layer normalization in each sub-layer, which are omitted in the presenta- tion for simplicity. For example, the actual output of the first sub-layer is:</p><formula xml:id="formula_9">LayerNorm(A (1) + X c ).<label>(6)</label></formula><p>The second sub-layer is a simple, position-wise fully connected feed-forward network:</p><formula xml:id="formula_10">C (1) = FNN(A (1) ·,1 ); . . . ; FNN(A (1) ·,M )<label>(7)</label></formula><p>where C (1) ∈ R D×M is the annotation of X &lt;k af- ter the first layer, A</p><p>·,m ∈ R D×1 is the column vec- tor for the m-th contextual word, and FNN(·) is a position-wise fully connected feed-forward net- work ( <ref type="bibr">Vaswani et al., 2017)</ref>.</p><p>This process iterates N c times as follows:</p><formula xml:id="formula_12">A (n) = MultiHead C (n−1) , C (n−1) , C (n−1) ,<label>(8)</label></formula><formula xml:id="formula_13">C (n) = FNN(A (n) ·,1 ); . . . ; FNN(A (n) ·,M ) ,<label>(9)</label></formula><p>where A (n) and C (n) (n = 1, . . . , N c ) are the hid- den state and annotation at the n-th layer, respec- tively. Note that C (0) = X c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document-level Context Integration</head><p>We use multi-head attention to integrate C (Nc) , which is the representation of X &lt;k , into both the encoder and the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Integration into the Encoder</head><p>Given the k-th source sentence x (k) , we use x (k) i ∈ R D×1 to denote the vector representation of the i- th source word x (k) i , which is a sum of word em- bedding and positional encoding. Therefore, the initial matrix representation of</p><formula xml:id="formula_14">x (k) is X = [x (k) 1 ; . . . ; x (k) I ],<label>(10)</label></formula><p>where X ∈ R D×I is the concatenation of all vec- tor representations of source words. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), we follow ( <ref type="bibr">Vaswani et al., 2017)</ref> to use a stack of N s identical lay- ers to encode x (k) . Each layer consists of three sub-layers. The first sub-layer is a multi-head self- attention:</p><formula xml:id="formula_15">B (n) = MultiHead S (n−1) , S (n−1) , S (n−1) ,<label>(11)</label></formula><p>where S (0) = X. The second sub-layer is con- text attention that integrates document-level con- text into the encoder:</p><formula xml:id="formula_16">D (n) = MultiHead B (n) , C (Nc) , C (Nc) .<label>(12)</label></formula><p>The third sub-layer is a position-wise fully con- nected feed-forward neural network:</p><formula xml:id="formula_17">S (n) = FNN(D (n) ·,1 ); . . . ; FNN(D (n) ·,I ) ,<label>(13)</label></formula><p>where S (n) ∈ R D×I is the representation of the source sentence x (k) at the n-th layer (n = 1, . . . , N s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Integration into the Decoder</head><p>When generating the j-th target word y </p><formula xml:id="formula_18">Y = [y (k) 0 , . . . , y (k) j−1 ],<label>(14)</label></formula><p>where y (k) 0 ∈ R D×1 is the vector representation of a begin-of-sentence token and Y ∈ R D×j is the concatenation of all vectors.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), we follow ( <ref type="bibr">Vaswani et al., 2017)</ref> to use a stack of N t identical layers to compute target-side representations. Each layer is composed of four sub-layers. The first sub-layer is a multi-head self-attention:</p><formula xml:id="formula_19">E (n) = MultiHead T (n−1) , T (n−1) , T (n−1) ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_20">T (0) = Y .</formula><p>The second sub-layer is con- text attention that integrates document-level con- text into the decoder:</p><formula xml:id="formula_21">F (n) = MultiHead E (n) , C (Nc) , C (Nc) .<label>(16)</label></formula><p>The third sub-layer is encoder-decoder attention that integrates the representation of the corre- sponding source sentence:</p><formula xml:id="formula_22">G (n) = MultiHead F (n) , S (Ns) , S (Ns) .<label>(17)</label></formula><p>The fourth sub-layer is a position-wise fully con- nected feed-forward neural network:</p><formula xml:id="formula_23">T (n) = FNN(G (n) ·,1 ); . . . ; FNN(G (n) ·,j ), ,<label>(18)</label></formula><p>where T (n) ∈ R D×j is the representation at the n-th layer (n = 1, . . . , N t ). Note that T (0) = Y . Finally, the probability distribution of generat- ing the next target word y (k) j is defined using a softmax layer:</p><formula xml:id="formula_24">P (y (k) j |X &lt;k , x (k) , y (k) &lt;j ; θ) ∝ exp(W o T (Nt) ·,j ) (19)</formula><p>where W o ∈ R |Vy|×D is a model parameter, V y is the target vocabulary, and T (Nt) ·,j ∈ R D×1 is a column vector for predicting the j-th target word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Context Gating</head><p>In our model, we follow <ref type="bibr">Vaswani et al. (2017)</ref> to use residual connections (He et al., 2016) around each sub-layer to shortcut its input to its output:</p><formula xml:id="formula_25">Residual(H) = H + SubLayer(H),<label>(20)</label></formula><p>where H is the input of the sub-layer. While residual connections prove to be effective for building deep architectures, there is one poten- tial problem for our model: the residual connec- tions after the context attention sub-layer might increase the influence of document-level context X &lt;k in an uncontrolled way. This is undesirable because the source sentence x (k) usually plays a more important role in target word generation.</p><p>To address this problem, we replace the residual connections after the context attention sub-layer with a position-wise context gating sub-layer:</p><formula xml:id="formula_26">Gating(H) = λH + (1 − λ)SubLayer(H). (21)</formula><p>The gating weight is given by</p><formula xml:id="formula_27">λ = σ(W i H + W s SubLayer(H)),<label>(22)</label></formula><p>where σ(·) is a sigmoid function, W i and W s are model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Given a document-level parallel corpus D d , the standard training objective is to maximize the log- likelihood of the training data:</p><formula xml:id="formula_28">ˆ θ = argmax θ X,Y∈D d log P (Y|X; θ) .<label>(23)</label></formula><p>Unfortunately, large-scale document-level par- allel corpora are usually unavailable, even for resource-rich languages such as English and Chi- nese.</p><p>Under small-data training conditions, document-level NMT is prone to underperform sentence-level NMT because of poor estimates of low-frequency events.</p><p>To address this problem, we adopt the idea of freezing some parameters while tuning the re- maining part of the model ( <ref type="bibr">Jean et al., 2015;</ref><ref type="bibr">Zoph et al., 2016</ref>). We propose a two-step training strat- egy that uses an additional sentence-level paral- lel corpus D s , which can be larger than D d . We divide model parameters into two subsets: θ = θ s ∪ θ d , where θ s is a set of original sentence- level model parameters (highlighted in blue in <ref type="figure" target="#fig_0">Figure 1</ref>(b)) and θ d is a set of newly-introduced document-level model parameters (highlighted in red in <ref type="figure" target="#fig_0">Figure 1(b)</ref>).</p><p>In the first step, sentence-level parameters θ s are estimated on the combined sentence-level par- allel corpus</p><formula xml:id="formula_29">D s ∪ D d : 2 ˆ θ s = argmax θs x,y∈Ds∪D d log P (y|x; θ s ). (24)</formula><p>Note that the newly introduced modules (high- lighted in red in <ref type="figure" target="#fig_0">Figure 1(b)</ref>) are inactivated in this step. P (y|x; θ s ) is identical to the original Transformer model, which is a special case of our model.</p><p>In the second step, document-level parameters θ d are estimated on the document-level parallel corpus D d only:</p><formula xml:id="formula_30">ˆ θ d = argmax θ d X,Y∈D d log P (Y|X; ˆ θ s , θ d ).<label>(25)</label></formula><p>Our approach is also similar to pre-training which has been widely used in NMT ( <ref type="bibr" target="#b10">Shen et al., 2016;</ref><ref type="bibr">Tu et al., 2018</ref>). The major difference is that our approach keepsˆθkeepsˆ keepsˆθ s fixed when estimating θ d to prevent the model from overfitting on the rela- tively smaller document-level parallel corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>We evaluate our approach on Chinese-English and French-English translation tasks. In Chinese- English translation task, the training set contains 2M Chinese-English sentence pairs with 54.8M Chinese words and 60.8M English words. <ref type="bibr">3</ref> The document-level parallel corpus is a subset of the full training set, including 41K documents with 940K sentence pairs. On average, each document in the training set contains 22.9 sentences. We use the NIST 2006 dataset as the development set and the <ref type="bibr">NIST 2002</ref><ref type="bibr">NIST , 2003</ref><ref type="bibr">NIST , 2004</ref><ref type="bibr">NIST , 2005</ref><ref type="bibr">NIST , 2008</ref> datasets as test sets. The development and test sets contain 588 documents with 5,833 sentences. On average, each document contains 9.9 sentences.</p><p>In French-English translation task, we use the IWSLT bilingual training data ( <ref type="bibr" target="#b8">Mauro et al., 2012</ref>) which contains 1,824 documents with 220K sen- tence pairs as training set. For development and testing, we use the IWSLT 2010 development and test sets, which contains 8 documents with 887 sentence pairs and 11 documents with 1,664 sen- tence pairs respectively. The evaluation metric for both tasks is case-insensitive BLEU score as cal- culated by the multi-bleu.perl script.</p><p>In preprocessing, we use byte pair encoding ( <ref type="bibr" target="#b9">Sennrich et al., 2016</ref>) with 32K merges to seg- ment words into sub-word units for all languages. For the original Transformer model and our ex- tended model, the hidden size is set to 512 and the # sent.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effect of Context Length</head><p>We first investigate the effect of context length (i.e., the number of preceding sentences) on our approach. As shown in <ref type="table">Table 1</ref>, using two pre- ceding source sentences as document-level context achieves the best translation performance on the development set. Using more preceding sentences does not bring any improvement and increases computational cost. This confirms the finding of <ref type="bibr">Tu et al. (2018)</ref> that long-distance context only has limited influence. Therefore, we set the number of preceding sentences to 2 in the following experi- ments. <ref type="bibr">5</ref>  <ref type="table" target="#tab_1">Table 2</ref> shows the effect of self-attention layer number for computing representations of document-level context (see Section 2.2) on trans- lation quality. Surprisingly, using only one self- attention layer suffices to achieve good perfor- mance. Increasing the number of self-attention layers does not lead to any improvements. There- fore, we set N c to 1 for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effect of Self-Attention Layer Number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Comparison with Previous Work</head><p>In Chinese-English translation task, we compare our approach with the following previous meth- ods:</p><p>1. ( <ref type="bibr">Wang et al., 2017)</ref>: using a hierarchical RNN to integrate document-level context into the RNNsearch model. They use a document- level parallel corpus containing 1M sentence pairs. <ref type="table" target="#tab_3">Table 3</ref> gives the BLEU scores re- ported in their paper.</p><p>2. ( <ref type="bibr" target="#b6">Kuang et al., 2017)</ref>: using a cache which stores previous translated words and topi- cal words to incorporate document-level con- text into the RNNsearch model. They use a document-level parallel corpus containing 2.8M sentence pairs. <ref type="table" target="#tab_3">Table 3</ref> gives the BLEU scores reported in their paper. We also use the same training data (i.e., 2M sentence pairs) and the same two- step training strategy to estimate sentence- and document-level parameters separately.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, using the same data, our approach achieves significant improvements over the original Transformer model ( <ref type="bibr">Vaswani et al., 2017</ref>) (p &lt; 0.01). The gain on the concate- nated test set (i.e., "All") is 1.96 BLEU points. It also outperforms the cache-based method ( <ref type="bibr" target="#b6">Kuang et al., 2017</ref>) adapted for Transformer significantly (p &lt; 0.01), which also uses the two-step train- ing strategy. <ref type="table" target="#tab_4">Table 4</ref> shows that our model also outperforms Transformer by 0.89 BLEU points on French-English translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Model MT06 MT02 MT03 MT04 MT05 MT08 All ( <ref type="bibr">Wang et al., 2017</ref>   </p><formula xml:id="formula_31">&gt; = &lt; Human 1 24% 45% 31%</formula><p>Human 2 20% 55% 25% Human 3 12% 52% 36% Overall 19% 51% 31% <ref type="table">Table 5</ref>: Subjective evaluation of the comparison be- tween the original Transformer model and our model. "&gt;" means that Transformer is better than our model, "=" means equal, and "&lt;" means worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Subjective Evaluation</head><p>We also conducted a subjective evaluation to vali- date the benefit of exploiting document-level con- text. All three human evaluators were asked to compare the outputs of the original Transformer model and our model of 20 documents contain- ing 198 sentences, which were randomly sampled from the test sets. <ref type="table">Table 5</ref> shows the results of subjective evalu- ation. Three human evaluators generally made consistent judgements. On average, around 19% of Transformer's translations are better than that of our model, 51% are equal, and 31% are worse. This evaluation confirms that exploiting document-level context helps to improve transla- tion quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Evaluation of Efficiency</head><p>We evaluated the efficiency of our approach. It takes the original Transformer model about 6.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Decoding Transformer 41K 872 this work 31K 364 hours to converge during training and the training speed is 41K words/second. The decoding speed is 872 words/second. In contrast, it takes our model about 7.8 hours to converge in the second step of training. The training speed is 31K words/second. The decoding speed is 364 words/second. Therefore, the training speed is only reduced by 25% thanks to the high parallelism of multi-head attention used to incorporate document-level con- text. The gap is larger in decoding because target words are generated in an autoregressive way in Transformer. <ref type="table" target="#tab_7">Table 7</ref> shows the effect of the proposed two- step training strategy. The first two rows only use sentence-level parallel corpus to train the origi- nal Transformer model (see Eq. 24) and achieve BLEU scores of 39.53 and 45.97. The third row only uses the document-level parallel corpus to di- rectly train our model (see Eq. 23) and achieves a BLEU score of 36.52. The fourth and fifth rows use the two-step strategy to take advantage of both sentence-and document-level parallel corpora and achieve BLEU scores of 40.22 and 47.93, respec- tively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Effect of Two-Step Training</head><p>We find that document-level NMT achieves much worse results than sentence-level NMT (i.e., 36.52 vs. 39.53) when only small-scale document- level parallel corpora are available. Our two-step training method is capable of addressing this prob- lem by exploiting sentence-level corpora, which sent.</p><p>doc.   <ref type="table">Table 8</ref>: Effect of context integration. "none" means that no document-level context is integrated, "encoder" means that the document-level context is integrated only into the encoder, "decoder" means that the document- level context is integrated only into the decoder, and "both" means that the context is integrated into both the encoder and the decoder.  leads to significant improvements across all test sets. <ref type="table">Table 8</ref> shows the effect of integrating document- level context to the encoder and decoder (see Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Effect of Context Integration</head><p>It is clear that integrating document-level context into the encoder (Eq. 12) brings significant improvements (i.e., 45.97 vs. 47.51). Similarly, it is also beneficial to inte- grate document-level context into the decoder (Eq. 16). Combining both leads to further improve- ments. This observation suggests that document- level context does help to improve Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Effect of Context Gating</head><p>As shown in <ref type="table" target="#tab_9">Table 9</ref>, we also validated the effec- tiveness of context gating (see Section 2.3.3). We find that replacing residual connections with con- text gating leads to an overall improvement of 0.38 BLEU point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10">Analysis</head><p>We use an example to illustrate how document- level context helps translation <ref type="table">(Table 10)</ref>. In order to translate the source sentence, NMT has to disambiguate the multi-sense word "yun- dong", which is actually impossible without the document-level context. The exact meaning of "rezhong" is also highly context dependent. For- tunately, the sense of "yundong" can be in- ferred from the word "saiche" (car racing) in the document-level context and "rezhong" is the antonym of "yanjuan" (tired of). This example shows that our model learns to resolve word sense ambiguity and lexical cohesion problems by inte- grating document-level context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Developing document-level models for machine translation has been an important research direc- tion, both for conventional SMT <ref type="bibr" target="#b4">(Gong et al., 2011;</ref><ref type="bibr">Hardmeier et al., 2012;</ref><ref type="bibr">Xiong et al., 2013a,b;</ref><ref type="bibr" target="#b2">Garcia et al., 2014</ref>) and NMT ( <ref type="bibr">Jean et al., 2017;</ref><ref type="bibr" target="#b6">Kuang et al., 2017;</ref><ref type="bibr">Tiedemann and Scherrer, 2017;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr" target="#b7">Maruf and Haffari, 2018;</ref><ref type="bibr" target="#b1">Bawden et al., 2018;</ref><ref type="bibr">Tu et al., 2018;</ref><ref type="bibr">Voita et al., 2018)</ref>.</p><p>Most existing work on document-level NMT has focused on integrating document-level con- text into the RNNsearch model (Bahdanau et al.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>· · ·ziji ye yinwei queshao jingzheng duishou er dui saiche youxie yanjuan shi· · · Source wo rengran feichang rezhong yu zhexiang yundong. Reference I'm still very fond of the sport. Transformer I am still very enthusiastic about this movement. Our work I am still very keen on this sport. <ref type="table">Table 10</ref>: An example of Chinese-English translation. In the source sentence, "yundong" (sport or political move- ment) is a multi-sense word and "rezhong" (fond of) is an emotional word whose meaning is dependent on its context. Our model takes advantage of the words "saiche" (car racing) and "yanjuan" (tired of) in the document- level context to translate the source words correctly.</p><p>2015). These approaches can be roughly divided into two broad categories: computing the repre- sentation of the full document-level context <ref type="bibr">(Jean et al., 2017;</ref><ref type="bibr">Tiedemann and Scherrer, 2017;</ref><ref type="bibr">Wang et al., 2017;</ref><ref type="bibr" target="#b7">Maruf and Haffari, 2018;</ref><ref type="bibr">Voita et al., 2018)</ref> and using a cache to memorize most rel- evant information in the document-level context ( <ref type="bibr" target="#b6">Kuang et al., 2017;</ref><ref type="bibr">Tu et al., 2018)</ref>. Our approach falls into the first category. We use multi-head at- tention to represent and integrate document-level context. <ref type="bibr">Voita et al. (2018)</ref> also extended Transformer to model document-level context, but our work is dif- ferent in modeling and training strategies. The ex- perimental part is also different. While <ref type="bibr">Voita et al. (2018)</ref> focus on anaphora resolution, our model is able to improve the overall translation quality by integrating document-level context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a method for exploiting document-level context inside the state-of-the-art neural translation model Transformer. Exper- iments on Chinese-English and French-English translation tasks show that our method is able to improve over Transformer significantly. In the fu- ture, we plan to further validate the effectiveness of our approach on more language pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) The original Transformer translation model (Vaswani et al., 2017) and (b) the extended Transformer translation model that exploits document-level context. The newly introduced modules are highlighted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>We follow Vaswani et al. (2017) to offset the target word embeddings by one position, resulting in the following matrix representation of y (k) &lt;j :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3</head><label></label><figDesc>. (Vaswani et al., 2017): the state-of-the-art NMT model that does not exploit document- level context. We use the open-source toolkit THUMT (Zhang et al., 2017) to train and evaluate the model. The training dataset is our sentence-level parallel corpus containing 2M sentence pairs. 4. (Kuang et al., 2017)*: adapting the cache- based method to the Transformer model. We implement it on top of the open-source toolkit THUMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>2 3 MT06 49.38 49.69 49.49 Table 1: Effect of context length on translation quality. The BLEU scores are calculated on the development set.</figDesc><table># Layer MT06 
1 
49.69 
2 
49.38 
3 
49.54 
4 
49.59 
5 
49.31 
6 
49.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Effect of self-attention layer number (i.e., N c ) 
on translation quality. The BLEU scores are calculated 
on the development set. 

filter size is set to 2,048. The multi-head atten-
tion has 8 individual attention heads. We set N = 
N s = N t = 6. In training, we use Adam (Kingma 
and Ba, 2015) for optimization. Each mini-batch 
contains approximately 24K words. We use the 
learning rate decay policy described by Vaswani 
et al. (2017). In decoding, the beam size is set 
to 4. We use the length penalty (Wu et al., 2016) 
and set the hyper-parameter α to 0.6. We use four 
Tesla P40 GPUs for training and one Tesla P40 
GPU for decoding. We implement our approach 
on top of the open-source toolkit THUMT (Zhang 
et al., 2017). 4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>2017) Transformer 48.09 48.63 47.54 47.79 48.34 38.31 45.97 (Kuang et al., 2017)* Transformer 48.14 48.97 48.05 47.91 48.53 38.38 46.37 this work Transformer 49.69 50.96 50.21 49.73 49.46 39.69 47.93</figDesc><table>) 
RNNsearch 
37.76 
-
-
-
36.89 27.57 
-
(Kuang et al., 2017) 
RNNsearch 
-
34.41 
-
38.40 32.90 31.86 
-
(Vaswani et al., </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with previous works on Chinese-English translation task. The evaluation metric is case-
insensitive BLEU score. (Wang et al., 2017) use a hierarchical RNN to incorporate document-level context into 
RNNsearch. (Kuang et al., 2017) use a cache to exploit document-level context for RNNsearch. (Kuang et al., 
2017)* is an adapted version of the cache-based method for Transformer. Note that "MT06" is not included in 
"All". 

Method 
Dev 
Test 
Transformer 29.42 35.15 
this work 
30.40 36.04 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison with Transformer on French-
English translation task. The evaluation metric is case-
insensitive BLEU score. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Evaluation of training and decoding speed. 
The speed is measured in terms of word/second (wps). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Effect of two-step training. "sent." denotes sentence-level parallel corpus and "doc." denotes document-
level parallel corpus. 

Integration MT06 MT02 MT03 MT04 MT05 MT08 
All 
none 
48.09 48.63 47.54 47.79 48.34 38.31 45.97 
encoder 
48.88 50.30 49.34 48.81 49.75 39.55 47.51 
decoder 
49.10 50.31 49.83 49.35 49.29 39.07 47.48 
both 
49.69 50.96 50.21 49.73 49.46 39.69 47.93 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 9 :</head><label>9</label><figDesc>Effect of context gating.</figDesc><table></table></figure>

			<note place="foot" n="2"> It is easy to create a sentence-level parallel corpus from D d .</note>

			<note place="foot" n="3"> The training set consists of sentence-level parallel corpora LDC2002E18, LDC2003E07, LDC2003E14, news part of LDC2004T08 and document-level parallel corpora LDC2002T01, LDC2004T07, LDC2005T06, LDC2005T10, LDC2009T02, LDC2009T15, LDC2010T03.</note>

			<note place="foot" n="4"> https://github.com/thumt/THUMT 5 If there is no preceding sentence, we simply use a single begin-of-sentence token.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating discourse phenomena in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Document-level machine translation with word vector models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva Martínez</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">Espãna</forename><surname>Bonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluíz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno>abs/1705.03122</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cache-based document-level statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxian</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cache-based document-level neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1711.11221</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Document context neural machine translation with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameen</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cettolo</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girardi</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EAMT</title>
		<meeting>EAMT</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
