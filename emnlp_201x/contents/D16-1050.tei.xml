<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Variational Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Duan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Provincial Key Laboratory for Computer Information Processing Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<postCode>215006</postCode>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Variational Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="521" to="530"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Models of neural machine translation are often from a discriminative family of encoder-decoders that learn a conditional distribution of a target sentence given a source sentence. In this paper, we propose a variational model to learn this conditional distribution for neu-ral machine translation: a variational encoder-decoder model that can be trained end-to-end. Different from the vanilla encoder-decoder model that generates target translations from hidden representations of source sentences alone, the variational model introduces a continuous latent variable to explicitly model underlying semantics of source sentences and to guide the generation of target translations. In order to perform efficient posterior inference and large-scale training, we build a neural posterior approximator conditioned on both the source and the target sides, and equip it with a reparameterization technique to estimate the variational lower bound. Experiments on both Chinese-English and English-German translation tasks show that the proposed variational neural machine translation achieves significant improvements over the vanilla neural machine translation baselines.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of us- ing a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than * Corresponding author phrase-or syntax-based statistical machine transla- tion (SMT) that typically has a huge phrase/rule ta- ble. Due to these advantages over traditional SMT system, NMT has recently attracted growing inter- ests from both deep learning and machine transla- tion community <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b23">Sutskever et al., 2014;</ref><ref type="bibr" target="#b15">Luong et al., 2015a;</ref><ref type="bibr" target="#b16">Luong et al., 2015b;</ref><ref type="bibr" target="#b17">Meng et al., 2015;</ref><ref type="bibr" target="#b24">Tu et al., 2016)</ref>.</p><p>Current NMT models mainly take a discrimi- native encoder-decoder framework, where a neu- ral encoder transforms source sentence x into dis- tributed representations, and a neural decoder gen- erates the corresponding target sentence y according to these representations 1 ( <ref type="bibr" target="#b23">Sutskever et al., 2014;</ref>). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the atten- tion mechanism ( ) to iden- tify semantic alignments between source and target words. Due to potential errors in these alignments, the attention-based context vector may be insuffi- cient to capture the entire meaning of a source sen- tence, hence resulting in undesirable translation phe- nomena ( <ref type="bibr" target="#b24">Tu et al., 2016)</ref>.</p><p>Unlike the vanilla encoder-decoder framework, we model underlying semantics of bilingual sen- tence pairs explicitly. We assume that there exists a continuous latent variable z from this underlying semantic space. And this variable, together with x, amssymb amsmath guides the translation process, i.e. p <ref type="bibr">(y|z, x)</ref>. With this assumption, the original conditional probability evolves into the following formulation:</p><formula xml:id="formula_0">p(y|x) = z p(y, z|x)d z = z p(y|z, x)p(z|x)d z (1)</formula><p>This brings in the benefits that the latent variable z can serve as a global semantic signal that is com- plementary to the attention-based context vector for generating good translations when the model learns undesirable attentions. However, although this la- tent variable enables us to explicitly model under- lying semantics of translation pairs, the incorpora- tion of it into the above probabilistic model has two challenges: 1) the posterior inference in this model is intractable; 2) large-scale training, which lays the ground for the data-driven NMT, is accordingly problematic.</p><p>In order to address these issues, we propose a vari- ational encoder-decoder model to neural machine translation (VNMT), motivated by the recent suc- cess of variational neural models ( ). <ref type="figure" target="#fig_0">Figure 1</ref> illus- trates the graphic representation of VNMT. As deep neural networks are capable of learning highly non- linear functions, we employ them to fit the latent- variable-related distributions, i.e. the prior and pos- terior, to make the inference tractable. The former is modeled to be conditioned on the source side alone p θ (z|x), because the source and target part of a sen- tence pair usually share the same semantics so that the source sentence should contain the prior infor- mation for inducing the underlying semantics. The latter, instead, is approximated from all observed variables q φ (z|x, y), i.e. both the source and the tar- get sides. In order to efficiently train parameters, we apply a reparameterization technique ) on the vari- ational lower bound. This enables us to use standard stochastic gradient optimization for training the pro- posed model. Specifically, there are three essential components in VNMT (The detailed architecture is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>):</p><p>• A variational neural encoder transforms source/target sentence into distributed repre- sentations, which is the same as the encoder of NMT ( ) (see section 3.1).</p><p>• A variational neural inferer infers the repre- sentation of z according to the learned source representations (i.e. p θ (z|x)) together with the target ones (i.e. q φ (z|x, y)), where the repa- rameterization technique is employed (see sec- tion 3.2).</p><p>• And a variational neural decoder integrates the latent representation of z to guide the genera- tion of target sentence (i.e. p(y|z, x)) together with the attention mechanism (see section 3.3).</p><p>Augmented with the posterior approximation and reparameterization, our VNMT can still be trained end-to-end. This makes our model not only effi- cient in translation, but also simple in implementa- tion. To train our model, we employ the conven- tional maximum likelihood estimation. Experiments on both Chinese-English and English-German trans- lation tasks show that VNMT achieves significant improvements over several strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background: Variational Autoencoder</head><p>This section briefly reviews the variational autoen- coder (VAE) ( . Given an observed variable x, VAE in- troduces a continuous latent variable z, and assumes that x is generated from z, i.e.,</p><formula xml:id="formula_1">p θ (x, z) = p θ (x|z)p θ (z)<label>(2)</label></formula><p>where θ denotes the parameters of the model. p θ (z) is the prior, e.g, a simple Gaussian distribution. p θ (x|z) is the conditional distribution that models the generation procedure, typically estimated via a deep non-linear neural network. Similar to our model, the integration of z in Eq. (2) imposes challenges on the posterior inference as well as large-scale learning. To tackle these prob- lems, VAE adopts two techniques: neural approxi- mation and reparameterization. Neural Approximation employs deep neural net- works to approximate the posterior inference model q φ (z|x), where φ denotes the variational parame- ters. For the posterior approximation, VAE regards q φ (z|x) as a diagonal Gaussian N (µ, diag(σ 2 )), and parameterizes its mean µ and variance σ 2 with deep neural networks.</p><formula xml:id="formula_2">reparameterization h z h ′ z h ′ e log σ 2 µ h f s 3 s 2 s 1 s 0 y 0 y 1 y 2 y 3 ⊕ α 2,1 α 2,2 α 2,3 α 2,4 (a) Variational Neural Encoder (c) Variational Neural Decoder (b) Variational Neural Inferer mean-pooling − → h 1 ← − h 1 − → h 2 ← − h 2 − → h 3 ← − h 3 ← − h 4 − → h 4 x 4 x 3 x 2 x 1 mean-pooling y 1 y 2 y 3 − → h 3 ← − h 3 − → h 2 ← − h 2 − → h 1 ← − h 1 h e</formula><p>Reparameterization reparameterizes z as a func- tion of µ and σ, rather than using the standard sampling method. In practice, VAE leverages the "location-scale" property of Gaussian distribution, and uses the following reparameterization:</p><formula xml:id="formula_3">˜ z = µ + σ (3)</formula><p>where is a standard Gaussian variable that plays a role of introducing noises, and denotes an element-wise product. With these two techniques, VAE tightly incor- porates both the generative model p θ (x|z) and the posterior inference model q φ (z|x) into an end-to- end neural network. This facilitates its optimiza- tion since we can apply the standard backpropaga- tion to compute the gradient of the following varia- tional lower bound: </p><formula xml:id="formula_4">L VAE (θ, φ; x) = − KL(q φ (z|x)||p θ (z)) +E q φ (z|x) [log p θ (x|z)] ≤ log p θ (x)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Variational Neural Machine Translation</head><p>Different from previous work, we introduce a latent variable z to model the underlying semantic space as a global signal for translation. Formally, given the definition in Eq. <ref type="formula" target="#formula_12">(1)</ref> and Eq. <ref type="formula" target="#formula_4">(4)</ref>, the varia- tional lower bound of VNMT can be formulated as follows:</p><formula xml:id="formula_5">L VNMT (θ, φ; x, y) = −KL(q φ (z|x, y)||p θ (z|x)) +E q φ (z|x,y) [log p θ (y|z, x)]<label>(5)</label></formula><p>where p θ (z|x) is our prior model, q φ (z|x, y) is our posterior approximator, and p θ (y|z, x) is the de- coder with the guidance from z. Based on this formulation, VNMT can be decomposed into three components, each of which is modeled by a neu- ral network: a variational neural inferer that models p θ (z|x) and q φ (z|x, y) (see part (b) in <ref type="figure" target="#fig_1">Figure 2</ref>), a variational neural decoder that models p θ (y|z, x) (see part (c) in <ref type="figure" target="#fig_1">Figure 2</ref>), and a variational neural encoder that provides distributed representations of a source/target sentence for the above two modules (see part (a) in <ref type="figure" target="#fig_1">Figure 2</ref>). Following the information flow illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, we describe part (a), (b) and (c) successively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Variational Neural Encoder</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), the variational neural en- coder aims at encoding an input sequence (w 1 , w 2 , . . . , w T ) into continuous vectors. In this paper, we adopt the encoder architecture proposed by , which is a bidirectional RNN with a forward and backward RNN. The forward RNN reads the sequence from left to right while the backward RNN in the opposite direction (see the parallel arrows in <ref type="figure" target="#fig_1">Figure 2</ref> (a)):</p><formula xml:id="formula_6">− → h i = RNN( − → h i−1 , E w i ) ← − h i = RNN( ← − h i+1 , E w i )<label>(6)</label></formula><p>where E w i ∈ R dw is the embedding for word w i , and − → h i , ← − h i are hidden states generated in two direc- tions. Following , we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing long-distance depen- dencies.</p><p>We further concatenate each pair of hidden states at each time step to build a set of annotation vec-</p><formula xml:id="formula_7">tors (h 1 , h 2 , . . . , h T ), h T i = − → h T i ; ← − h T i</formula><p>. In this way, each annotation vector h i encodes information about the i-th word with respect to all the other sur- rounding words in the sequence. Therefore, these annotation vectors are desirable for the following modeling.</p><p>We use this encoder to represent both the source sentence {x i } T f i=1 and the target sentence {y i } Te</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i=1</head><p>(see the blue color in <ref type="figure" target="#fig_1">Figure 2</ref>). Accordingly, our encoder generates both the source annotation vec-</p><formula xml:id="formula_8">tors {h i } T f i=1 ∈ R 2d f and the target annotation vec- tors {h i } Te i=1 ∈ R 2de .</formula><p>The source vectors flow into the inferer and decoder while the target vectors the posterior approximator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Variational Neural Inferer</head><p>A major challenge of variational models is how to model the latent-variable-related distributions. In VNMT, we employ neural networks to model both the prior p θ (z|x) and the posterior q φ (z|x, y), and let them subject to a multivariate Gaussian distri- bution with a diagonal covariance structure. <ref type="bibr">2</ref> As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, these two distributions mainly differ in their conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Neural Posterior Approximator</head><p>Exactly modeling the true posterior p(z|x, y) ex- actly usually intractable. Therefore, we adopt an approximation method to simplify the posterior in- ference. Conventional models typically employ the mean-field approaches. However, a major limitation of this approach is its inability to capture the true posterior of z due to its oversimplification. Follow- ing the spirit of VAE, we use neural networks for better approximation in this paper, and assume the approximator has the following form:</p><formula xml:id="formula_9">q φ (z|x, y) = N (z; µ(x, y), σ(x, y) 2 I) (7)</formula><p>The mean µ and s.d. σ of the approximate poste- rior are the outputs of neural networks based on the observed variables x and y as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b).</p><p>Starting from the variational neural encoder, we first obtain the source-and target-side representa- tion via a mean-pooling operation over the annota- tion vectors, i.e.</p><formula xml:id="formula_10">h f = 1 T f T f i h i , h e = 1 Te Te i h i .</formula><p>With these representations, we perform a non-linear transformation that projects them onto our con- cerned latent semantic space:</p><formula xml:id="formula_11">h z = g(W (1) z [h f ; h e ] + b (1) z )<label>(8)</label></formula><p>where</p><formula xml:id="formula_12">W (1) z ∈ R dz×2(d f +de) , b<label>(1)</label></formula><p>z ∈ R dz is the pa- rameter matrix and bias term respectively, d z is the dimensionality of the latent space, and g(·) is an element-wise activation function, which we set to be tanh(·) throughout our experiments.</p><p>In this latent space, we obtain the abovementioned Gaussian parameters µ and log σ 2 through linear re- gression:</p><formula xml:id="formula_13">µ = W µ h z + b µ , log σ 2 = W σ h z + b σ (9)</formula><p>where µ, log σ 2 are both d z -dimension vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Neural Prior Model</head><p>Different from the posterior, we model (rather than approximate) the prior as follows:</p><formula xml:id="formula_14">p θ (z|x) = N (z; µ (x), σ (x) 2 I)<label>(10)</label></formula><p>We treat the mean µ and s.d. σ of the prior as neural functions of source sentence x alone. This is sound and reasonable because bilingual sentences are se- mantically equivalent, suggesting that either y or x is capable of inferring the underlying semantics of sentence pairs, i.e., the representation of latent vari- able z.</p><p>The neural model for the prior p θ (z|x) is the same as that (i.e. Eq (8) and <ref type="formula">(9)</ref>) for the posterior q φ (z|x, y), except for the absence of h e . Besides, the parameters for the prior are independent of those for the posterior.</p><p>To obtain a representation for latent variable z, we employ the same technique as the Eq. (3) and repa- rameterized it as h z = µ + σ , ∼ N (0, I). Dur- ing decoding, however, due to the absence of target sentence y, we set h z to be the mean of p θ (z|x), i.e., µ . Intuitively, the reparameterization bridges the gap between the generation model p θ (y|z, x) and the inference model q φ <ref type="figure">(z|x, y)</ref>. In other words, it connects these two neural networks. This is impor- tant since it enables the stochastic gradient optimiza- tion via standard backpropagation.</p><p>We further project the representation of latent variable h z onto the target space for translation:</p><formula xml:id="formula_15">h e = g(W (2) z h z + b (2) z )<label>(11)</label></formula><p>where h e ∈ R d e . The transformed h e is then in- tegrated into our decoder. Notice that because of the noise from , the representation h e is not fixed for the same source sentence and model parameters. This is crucial for VNMT to learn to avoid overfit- ting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Variational Neural Decoder</head><p>Given the source sentence x and the latent variable z, our decoder defines the probability over transla- tion y as a joint probability of ordered conditionals:</p><formula xml:id="formula_16">p(y|z, x) = Te j=1 p(y j |y &lt;j , z, x)<label>(12)</label></formula><p>where p(y j |y &lt;j ,z,</p><formula xml:id="formula_17">x) = g (y j−1 , s j−1 , c j )</formula><p>The feed forward model g (·) (see the yellow arrows in <ref type="figure" target="#fig_1">Figure 2</ref>) and context vector c j = i α ji h i (see the "⊕" in <ref type="figure" target="#fig_1">Figure 2)</ref> are the same as ( ). The difference between our decoder and Bahdanau et al.'s decoder (2014) lies in that in ad- dition to the context vector, our decoder integrates the representation of the latent variable, i.e. h e , into the computation of s j , which is denoted by the bold dashed red arrow in <ref type="figure" target="#fig_1">Figure 2 (c)</ref>. </p><note type="other">, the hidden state s j in our decoder is cal- culated by 3</note><formula xml:id="formula_18">s j = (1 − u j ) s j−1 + u j ˜ s j , ˜ s j = tanh(W E y j + U [r j s j−1 ] + Cc j + V h e ) u j = σ(W u E y j + U u s j−1 + C u c j + V u h e ) r j = σ(</formula><note type="other">W r E y j + U r s j−1 + C r c j + V r h e ) Here, r j , u j , ˜ s j denotes the reset gate, update gate and candidate activation in GRU respectively, and E y j ∈ R dw is the word embedding for target word.</note><formula xml:id="formula_19">W, W u , W r ∈ R de×dw , U, U u , U r ∈ R de×de , C, C u , C r ∈ R de×2d f , and V, V u , V r ∈ R de×d</formula><p>e are parame- ter weights. The initial hidden state s 0 is initialized in the same way as  (see the arrow to s 0 in <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>In our model, the latent variable can affect the rep- resentation of hidden state s j through the gate be- tween r j and u j . This allows our model to access the semantic information of z indirectly since the pre- diction of y j+1 depends on s j . In addition, when the model learns wrong attentions that lead to bad con- text vector c j , the semantic representation h e can help to guide the translation process .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Training</head><p>We use the Monte Carlo method to approximate the expectation over the posterior in Eq. (5), i.e.</p><formula xml:id="formula_20">E q φ (z|x,y) [·] 1 L L l=1 log p θ (y|x, h (l) z ),</formula><p>where L is the number of samples. The joint training objective for a training instance (x, y) is defined as follows:</p><formula xml:id="formula_21">L(θ, φ) −KL(q φ (z|x, y)||p θ (z|x)) + 1 L L l=1 Te j=1 log p θ (y j |y &lt;j , x, h (l) z ) (13)</formula><p>where h (l) z = µ + σ (l) and (l) ∼ N (0, I) The first term is the KL divergence between two Gaussian distributions which can be computed and differentiated without estimation (see ) for details). And the second term is the approximate expectation, which is also dif- ferentiable. Suppose that L is 1 (which is used in our experiments), then our second term will be de- generated to the objective of conventional NMT. In- tuitively, VNMT is exactly a regularized version of  <ref type="table" target="#tab_4">Table 1</ref>: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We highlight the best results in bold for each test set. "↑/⇑": significantly better than Moses (p &lt; 0.05/p &lt; 0.01); "+/++": significantly better than GroundHog (p &lt; 0.05/p &lt; 0.01);</p><p>NMT, where the introduced noise increases its ro- bustness, and reduces overfitting. We verify this point in our experiments.</p><p>Since the objective function in Eq. <ref type="formula" target="#formula_12">(13)</ref> is differ- entiable, we can optimize the model parameter θ and variational parameter φ jointly using standard gradi- ent ascent techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>To evaluate the effectiveness of the proposed VNMT, we conducted experiments on both Chinese- English and English-German translation tasks. Our Chinese-English training data 4 consists of 2.9M sen- tence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German train- ing data <ref type="bibr">5</ref> consists of 4.5M sentence pairs with 116M English words and 110M German words <ref type="bibr">6</ref> . We used the newstest2013 (3000 sentences) as the develop- ment set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) metric to evaluate translation quality, and paired bootstrap sampling <ref type="bibr" target="#b13">(Koehn, 2004</ref>) for signif- icance test.</p><p>We compared our model against two state-of-the- art SMT and NMT systems:</p><p>• Moses ( <ref type="bibr" target="#b12">Koehn et al., 2007)</ref>: a phrase-based SMT system.</p><p>• GroundHog ( ): an attention-based NMT system.</p><p>Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by set- ting h z to µ . For Moses, we adopted all the default settings ex- cept for the language model. We trained a 4-gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM 7 toolkit with modified Kneser-Ney smoothing. Im- portantly, we used all words in the vocabulary.</p><p>For GroundHog, we set the maximum length of training sentences to be 50 words, and pre- served the most frequent 30K (Chinese-English) and 50K (English-German) words as both the source and target vocabulary , covering approximately 98.9%/99.2% and 97.3%/93.3% on the source and target side of the two parallel corpora respectively . All other words were represented by a specific to- ken "UNK". Following , we set d w = 620, d f = 1000, d e = 1000, and M = 80. All other settings are the same as the default config- uration (for RNNSearch). During decoding, we used the beam-search algorithm, and set beam size to 10.</p><p>For VNMT, we initialized its parameters with the trained RNNSearch model. The settings of our model are the same as that of GroundHog, except for some parameters specific to VNMT. Following VAE, we set the sampling number L = 1. Addi- tionally, we set d e = d z = 2d f = 2000 according to preliminary experiments. We used the Adadelta algorithm for model training with ρ = 0.95. With regard to the source and target encoders, we shared their recurrent parameters but not word embeddings.</p><p>We implemented our VNMT based on Ground- Hog <ref type="bibr">8</ref>     GPU. In one hour, GroundHog processes about 1100 batches, while our VNMT processes 630 batches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Chinese-English Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Long Sentences</head><p>We further testify VNMT on long sentence transla- tion where the vanilla NMT usually suffers from at- tention failures ( <ref type="bibr" target="#b24">Tu et al., 2016;</ref><ref type="bibr" target="#b1">Bentivogli et al., 2016)</ref>. We believe that the global latent variable can play an important role on long sentence translation.</p><p>Our first experiment is carried out on 6 disjoint groups according to the length of source sentences in our test sets. <ref type="figure" target="#fig_4">Figure 3</ref> shows the BLEU scores of two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Specif- ically, on the final group with the longest source sentences, our VNMT obtains the biggest improve- ment (3.55 BLEU points). Overall, these obvious improvements on all groups in terms of the length of source sentences indicate that the global guidance from the latent variable benefits our VNMT model. Our second experiment is carried out on a syn- thetic dataset where each new source sentence is a concatenation of neighboring source sentences in the original test sets. As a result, the average length of source sentences in the new dataset (&gt; 50) is almost twice longer than the original one. Trans- lation results is summarized in <ref type="table" target="#tab_2">Table 2</ref>, where our VNMT obtains significant improvements on all new test sets. This further demonstrates the advantage of introducing the latent variable. <ref type="table" target="#tab_3">Table 3</ref> shows the results on English-German trans- lation. We also provide several existing NMT sys-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on English-German Translation</head><formula xml:id="formula_22">Source 两 国 官员 确定 了 今后 会谈 的 日程 和 模式 , 建立 起 进行 持续 对话 的 机制 , 此举 标 志 着 巴 印 对话 进程 在 中断 两 年 后 重新 启动 , 为 两 国 逐步 解决 包括 克什米 尔 争 端 在内 的 所有 悬而未决 的 问题 奠定 了 基础 , 体现 了 双方 可贵 的 和平 诚意 。</formula><p>Reference the officials of the two countries have established the mechanism for continued dialogue down the road, including a confirmed schedule and model of the talks. this symbolizes the restart of the dialogue process between pakistan and india after an interruption of two years and has paved a foundation for the two countries to sort out gradually all the questions hanging in the air, including the kashmir dispute. it is also a realization of their precious sincerity for peace.</p><p>Moses officials of the two countries set the agenda for future talks , and the pattern of a continuing dialogue mechanism . this marks a break in the process of dialogue between pakistan and india , two years after the restart of the two countries including kashmir dispute to gradually solve all the outstanding issues have laid the foundation of the two sides showed great sincerity in peace .</p><p>GroundHog the two countries have decided to set up a mechanism for conducting continuous dialogue on the agenda and mode of the talks . this indicates that the ongoing dialogue between the two countries has laid the foundation for the gradual settlement of all outstanding issues including the dispute over kashmir .</p><p>VNMT the officials of the two countries set up a mechanism for holding a continuous dialogue on the agenda and mode of the future talks, and this indicates that the ongoing dialogue between pakistan and india has laid a foundation for resolving all outstanding issues , including the kashmir disputes , and this serves as a valuable and sincere peace sincerity . tems that use the same training, development and testing data. The results show that VNMT signifi- cantly outperforms GroundHog and achieves a sig- nificant gain of 0.73 BLEU points (p &lt; 0.01). With unknown word replacement ( <ref type="bibr" target="#b8">Jean et al., 2015;</ref><ref type="bibr" target="#b15">Luong et al., 2015a</ref>), VNMT reaches the performance level that is comparable to the previous state-of-the- art NMT results. <ref type="table" target="#tab_5">Table 4</ref> shows a translation example that helps un- derstand the advantage of VNMT over NMT . As the source sentence in this example is long (more than 40 words), the translation generated by Moses is relatively messy and incomprehensible. In con- trast, translations generated by neural models (both GroundHog and VNMT) are much more fluent and comprehensible. However, there are essential differ- ences between GroundHog and our VNMT. Specifi- cally, GroundHog does not translate the phrase "官 员" at the beginning of the source sentence. The translation of the clause "体现 了 双方 可贵 的 和 平 诚意 。" at the end of the source sentence is com- pletely lost. In contrast, our VNMT model does not miss or mistake these fragments and can convey the meaning of entire source sentence to the target side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Translation Analysis</head><p>From these examples, we can find that although attention networks can help NMT trace back to rel- evant parts of source sentences for predicting tar- get translations, capturing the semantics of entire sentences still remains a big challenge for neural machine translation. Since NMT implicitly models variable-length source sentences with fixed-size hid- den vectors, some details of source sentences (e.g., the red sequence of words in <ref type="table" target="#tab_5">Table 4</ref>) may not be encoded in these vectors at all. VNMT seems to be able to capture these details through a latent vari- able that explicitly model underlying semantics of source sentences. The promising results suggest that VNMT provides a new mechanism to deal with sen- tence semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Neural Machine Translation</head><p>Neural machine translation starts from the sequence to sequence learning, where <ref type="bibr" target="#b23">Sutskever et al. (2014)</ref> employ two multilayered Long Short-Term Memory (LSTM) models that first encode a source sentence into a single vector and then decode the translation word by word until a special end token is gener- ated. In order to deal with issues caused by encoding all source-side information into a fixed-length vec- tor,  introduce attention-based</p><p>NMT that aims at automatically concentrating on relevant source parts for predicting target words dur- ing decoding. The incorporation of attention mech- anism allows NMT to cope better with long sen- tences, and makes it really comparable to or even superior to conventional SMT. Following the success of attentional NMT, a num- ber of approaches and models have been proposed for NMT recently, which can be grouped into differ- ent categories according to their motivations: deal- ing with rare words or large vocabulary ( <ref type="bibr" target="#b8">Jean et al., 2015;</ref><ref type="bibr" target="#b16">Luong et al., 2015b;</ref><ref type="bibr" target="#b21">Sennrich et al., 2015)</ref>, learning better attentional structures ( <ref type="bibr" target="#b15">Luong et al., 2015a</ref>), integrating SMT techniques ( <ref type="bibr" target="#b3">Cheng et al., 2015;</ref><ref type="bibr" target="#b6">Feng et al., 2016;</ref><ref type="bibr" target="#b24">Tu et al., 2016)</ref>, memory network ( <ref type="bibr" target="#b17">Meng et al., 2015)</ref>, etc. All these models are designed within the discriminative encoder-decoder framework, leaving the explicit ex- ploration of underlying semantics with a variational model an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variational Neural Model</head><p>In order to perform efficient inference and learn- ing in directed probabilistic models on large-scale dataset,  as well as  introduce variational neural networks. Typically, these models utilize an neural inference model to approximate the intractable pos- terior, and optimize model parameters jointly with a reparameterized variational lower bound using the standard stochastic gradient technique. This ap- proach is of growing interest due to its success in various tasks.  revisit the approach to semi- supervised learning with generative models and fur- ther develop new models that allow effective gen- eralization from a small labeled dataset to a large unlabeled dataset. <ref type="bibr" target="#b5">Chung et al. (2015)</ref> incorporate latent variables into the hidden state of a recurrent neural network, while <ref type="bibr" target="#b7">Gregor et al. (2015)</ref> combine a novel spatial attention mechanism that mimics the foveation of human eyes, with a sequential varia- tional auto-encoding framework that allows the it- erative construction of complex images. Very re- cently, <ref type="bibr" target="#b18">Miao et al. (2015)</ref> propose a generic varia- tional inference framework for generative and con- ditional models of text.</p><p>The most related work is that of <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>, where they develop a variational autoen- coder for unsupervised generative language model- ing. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although varia- tional neural models have been widely used in NLP tasks and the variational decoding has been investi- gated for SMT ( <ref type="bibr" target="#b14">Li et al., 2009)</ref>, the adaptation and utilization of variational neural model to neural ma- chine translation, to the best of our knowledge, has never been investigated before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and repa- rameterize the variational lower bound. This en- ables our model to be an end-to-end neural network that can be optimized through the stochastic gradi- ent algorithms. Comparing with the conventional attention-based NMT, our model is better at trans- lating long sentences. It also greatly benefits from a special regularization term brought with this la- tent variable. Experiments on Chinese-English and English-German translation tasks verified the effec- tiveness of our model. In the future, since the latent variable in our model is at the sentence level, we want to explore more fine-grained latent variables for neural ma- chine translation, such as the Recurrent Latent Vari- able <ref type="bibr">Model (Chung et al., 2015)</ref>. We are also inter- ested in applying our model to other similar tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of VNMT as a directed graph. We use solid lines to denote the generative model p θ (z|x)p θ (y|z, x), and dashed lines to denote the variational approximation q φ (z|x) to the intractable posterior p(z|x, y). Both variational parameters φ and generative model parameters θ are learned jointly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Neural architecture of VNMT. We use blue, gray and red color to indicate the encoder-related (x, y), underlying semantic (z) and decoder-related (y) representation respectively. The yellow lines show the flow of information employed for target word prediction. The dashed red line highlights the incorporation of latent variable z into target prediction. f and e represent the source and target language respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>KL</head><label></label><figDesc>(Q||P ) is the Kullback-Leibler divergence be- tween Q and P . Intuitively, VAE can be considered as a regularized version of the standard autoencoder. It makes use of the latent variable z to capture the variations in the observed variable x.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Formally</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BLEU scores on different groups of source sentences in terms of their length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>34.50 ++ 33.78 ++ 36.72 ⇑++ 30.92 ⇑++ 24.41 ↑++ 32.07</head><label></label><figDesc></figDesc><table>System 

MT05 
MT02 
MT03 
MT04 
MT06 
MT08 
AVG 
Moses 
33.68 34.19 
34.39 
35.34 
29.20 
22.94 
31.21 
GroundHog 
31.38 33.32 
32.59 
35.05 
29.80 
22.82 
30.72 
VNMT w/o KL 
31.40 33.50 
32.92 
34.95 
28.74 
22.07 
30.44 
VNMT 
32.25 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>. Both NMT systems are trained on a Telsa K40</head><label></label><figDesc></figDesc><table>System 

MT05 MT02 MT03 MT04 MT06 MT08 
GroundHog 
18.23 22.20 
20.19 
21.67 
19.11 
13.41 
VNMT 
21.31 26.02 
23.78 
25.81 
21.81 
15.59 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : BLEU scores on the new dataset. All improvements are significant at p &lt; 0.01.</head><label>2</label><figDesc></figDesc><table>System 
Architecture 
BLEU 
Existing end-to-end NMT systems 
Jean et al. (2015) 
RNNSearch 
16.46 
Jean et al. (2015) 
RNNSearch + unk replace 
18.97 
Jean et al. (2015) 
RNNsearch + unk replace + large vocab 
19.40 
Luong et al. (2015a) LSTM with 4 layers + dropout + local att. + unk replace 20.90 
Our end-to-end NMT systems 

this work 
RNNSearch 
16.40 
VNMT 
17.13 ++ 
VNMT + unk replace 
19.58 ++ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>BLEU scores on the English-German translation task. 

5 
15 
25 
35 
45 
55 
20 

23 

26 

29 

32 

35 

Sentence Length 

BLEU Scores 

GroundHog 

Our VNMT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 summarizes</head><label>1</label><figDesc></figDesc><table>the BLEU scores of different 
systems on the Chinese-English translation tasks. 
Clearly VNMT significantly improves translation 
quality in terms of BLEU on most cases, and ob-
tains the best average results that gain 0.86 and 1.35 
BLEU points over Moses and GroundHog respec-
tively. Besides, without the KL objective, VNMT 
w/o KL obtains even worse results than GroundHog. 
These results indicate the following two points: 1) 
explicitly modeling underlying semantics by a latent 
variable indeed benefits neural machine translation, 
and 2) the improvements of our model are not from 
enlarging the network. 

https://github.com/DeepLearnXMU/VNMT. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Translation examples of different systems. We highlight important parts in red color.</figDesc><table></table></figure>

			<note place="foot" n="1"> In this paper, we use bold symbols to denote variables, and plain symbols to denote their values. Without specific statement, all variables are multivariate.</note>

			<note place="foot" n="2"> The reasons of choosing Gaussian distribution are twofold: 1) it is a natural choice for modeling continuous variables; 2) it belongs to the family of &quot;location-scale&quot; distributions, which is required for the following reparameterization.</note>

			<note place="foot" n="3"> We omit the bias term for clarity.</note>

			<note place="foot" n="4"> This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT&apos;14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/</note>

			<note place="foot" n="7"> http://www.speech.sri.com/projects/srilm/download.html 8 Our code is publicly available at</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors were supported by National Nat-ural Science Foundation of China (Grant Nos 61303082, 61672440, 61622209 and 61403269), Natural Science Foundation of Fujian Province (Grant No. 2016J05161), Natural Science Founda-tion of Jiangsu Province (Grant No. BK20140355), and Research fund of the Provincial Key Laboratory for Computer Information Processing Technology in Soochow University (Grant No. KJS1520). We also thank the anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural versus Phrase-Based Machine Translation Quality: a Case Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Federico</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generating Sentences from a Continuous Space</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<title level="m">Agreement-based Joint Training for Bidirectional Attention-based Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>abs/1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AutoEncoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Ondřej Bojar, Alexandra Constantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational decoding for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="593" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
		<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Deep Memory-based Architecture for Sequence-toSequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<title level="m">Neural Variational Inference for Text Processing</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-08" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Minimum Risk Training for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Coverage-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1601.04811</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
