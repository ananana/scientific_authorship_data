<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Capsule Networks with Dynamic Routing for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyang</forename><surname>Lei</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Graduate School at Shenzhen</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soufei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Capsule Networks with Dynamic Routing for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3110" to="3119"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3110</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this study, we explore capsule networks with dynamic routing for text classification. We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain &quot;background&quot; information or have not been successfully trained. A series of experiments are conducted with capsule networks on six text classification benchmarks. Capsule networks achieve competitive results over the compared baseline methods on 4 out of 6 datasets, which shows the effectiveness of capsule networks for text classification. We additionally show that capsule networks exhibit significant improvement when transfer single-label to multi-label text classification over the competitors. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for text modeling 1 .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modeling articles or sentences computationally is a fundamental topic in natural language process- ing. It could be as simple as a keyword/phrase matching problem, but it could also be a nontrivial problem if compositions, hierarchies, and struc- tures of texts are considered. For example, a news article which mentions a single phrase "US elec- tion" may be categorized into the political news with high probability. But it could be very diffi- cult for a computer to predict which presidential candidate is favored by its author, or whether the author's view in the article is more liberal or more conservative.</p><p>Earlier efforts in modeling texts have achieved limited success on text categorization using a sim- ple bag-of-words classifier <ref type="bibr" target="#b7">(Joachims, 1998;</ref><ref type="bibr" target="#b14">McCallum et al., 1998)</ref>, implying understanding the meaning of the individual word or n-gram is a necessary step towards more sophisticated mod- els. It is therefore not a surprise that distributed representations of words, a.k.a. word embeddings, have received great attention from NLP commu- nity addressing the question "what" to be modeled at the basic level ( <ref type="bibr" target="#b15">Mikolov et al., 2013;</ref><ref type="bibr" target="#b19">Pennington et al., 2014)</ref>. In order to model higher level concepts and facts in texts, an NLP researcher has to think cautiously the so-called "what" question: what is actually modeled beyond word meanings. A common approach to the question is to treat the texts as sequences and focus on their spatial patterns, whose representatives include convolu- tional neural networks (CNNs) <ref type="bibr" target="#b9">(Kim, 2014;</ref><ref type="bibr" target="#b29">Zhang et al., 2015;</ref><ref type="bibr" target="#b2">Conneau et al., 2017)</ref> and long short- term memory networks (LSTMs) <ref type="bibr" target="#b24">(Tai et al., 2015;</ref><ref type="bibr" target="#b16">Mousa and Schuller, 2017)</ref>. Another common ap- proach is to completely ignore the order of words but focus on their compositions as a collection, whose representatives include probabilistic topic modeling ( <ref type="bibr" target="#b0">Blei et al., 2003;</ref><ref type="bibr" target="#b13">Mcauliffe and Blei, 2008)</ref> and Earth Mover's Distance based model- ing ( <ref type="bibr" target="#b10">Kusner et al., 2015;</ref><ref type="bibr" target="#b26">Ye et al., 2017)</ref>.</p><p>Those two approaches, albeit quite different from the computational perspective, actually fol- low a common measure to be diagnosed regarding their answers to the "what" question. In neural network approaches, spatial patterns aggregated at lower levels contribute to representing higher level concepts. Here, they form a recursive process to articulate what to be modeled. For example, CNN builds convolutional feature detectors to extract lo- cal patterns from a window of vector sequences and uses max-pooling to select the most promi- nent ones. It then hierarchically builds such pat- tern extraction pipelines at multiple levels. Being a spatially sensitive model, CNN pays a price for the inefficiency of replicating feature detectors on a grid. As argued in ( <ref type="bibr" target="#b21">Sabour et al., 2017)</ref>, one has to choose between replicating detectors whose size grows exponentially with the number of di- mensions, or increasing the volume of the labeled training set in a similar exponential way. On the other hand, methods that are spatially insensitive are perfectly efficient at the inference time regard- less of any order of words or local patterns. How- ever, they are unavoidably more restricted to en- code rich structures presented in a sequence. Im- proving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue.</p><p>A recent method called capsule network intro- duced by <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> possesses this at- tractive potential to address the aforementioned is- sue. They introduce an iterative routing process to decide the credit attribution between nodes from lower and higher layers. A metaphor (also as an argument) they made is that human visual system intelligently assigns parts to wholes at the infer- ence time without hard-coding patterns to be per- spective relevant. As an outcome, their model could encode the intrinsic spatial relationship be- tween a part and a whole constituting viewpoint invariant knowledge that automatically general- izes to novel viewpoints. In our work, we follow a similar spirit to use this technique in modeling texts. Three strategies are proposed to stabilize the dynamic routing process to alleviate the distur- bance of some noise capsules which may contain "background" information such as stop words and the words that are unrelated to specific categories. We conduct a series of experiments with capsule networks on top of the pre-trained word vectors for six text classification benchmarks. More im- portantly, we show that capsule networks achieves significant improvement when transferring single- label to multi-label text classifications over the compared baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Methodology</head><p>Our capsule network, depicted in <ref type="figure">Figure 1</ref>, is a variant of the capsule networks proposed in <ref type="bibr" target="#b21">Sabour et al. (2017)</ref>. It consists of four layers: n- gram convolutional layer, primary capsule layer, convolutional capsule layer, and fully connected capsule layer. In addition, we explore two capsule frameworks to integrate these four components in different ways. In the rest of this section, we elab- orate the key components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">N -gram Convolutional Layer</head><p>This layer is a standard convolutional layer which extracts n-gram features at different positions of a sentence through various convolutional filters.</p><p>Suppose x ∈ R L×V denotes the input sentence representation where L is the length of the sen- tence and V is the embedding size of words. Let x i ∈ R V be the V -dimensional word vector cor- responding to the i-th word in the sentence. Let W a ∈ R K 1 ×V be the filter for the convolution op- eration, where K 1 is the N -gram size while sliding over a sentence for the purpose of detecting fea- tures at different positions. A filter W a convolves with the word-window x i:i+K 1 −1 at each possible position (with stride of 1) to produce a column fea- ture map m a ∈ R L−K 1 +1 , each element m a i ∈ R of the feature map is produced by</p><formula xml:id="formula_0">m a i = f (x i:i+K 1 −1 • W a + b 0 ) (1)</formula><p>where • is element-wise multiplication, b 0 is a bias term, and f is a nonlinear activate function (i.e., ReLU). We have described the process by which one feature is extracted from one filter.</p><p>Hence, for a = 1, . . . , B, totally B filters with the same N -gram size, one can generate B feature maps which can be rearranged as</p><formula xml:id="formula_1">M = [m 1 , m 2 , ..., m B ] ∈ R (L−K 1 +1)×B (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Primary Capsule Layer</head><p>This is the first capsule layer in which the cap- sules replace the scalar-output feature detectors of CNNs with vector-output capsules to preserve the instantiated parameters such as the local order of words and semantic representations of words. Suppose p i ∈ R d denotes the instantiated pa- rameters of a capsule, where d is the dimension of the capsule. Let W b ∈ R B×d be the filter shared in different sliding windows. For each matrix multi- plication, we have a window sliding over each N - gram vector denoted as M i ∈ R B , then the corre- sponding N -gram phrases in the form of capsule are produced with</p><formula xml:id="formula_2">p i = (W b ) T M i . The filter W b multiplies each N -gram vector in {M i } L−K 1 +1 i=1</formula><p>with stride of 1 to produce a</p><formula xml:id="formula_3">B X C X d K1 X V X B K2 X C X D X d X d H X E X d X d Conv1 PrimaryCaps ConvCaps C B D 1 K2 = d-dimension FCCaps E Flatten PrimCaps K2 x C x D x d D x d ConvCaps Capsule Probability (L−K1-K2+2) x D (L−K1+1) x C K2 x C x D x d x d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matrix Transformation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing</head><p>Local Spatial Routing (PrimCaps&gt;ConvCaps)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Routing Routing</head><p>Figure 1: The Architecture of Capsule network for text classification. The processes of dynamic routing between consecutive layers are shown in the bottom.</p><formula xml:id="formula_4">column-list of capsules p ∈ R (L−K 1 +1)×d , each capsule p i ∈ R d in the column-list is computed as p i = g(W b M i + b 1 ) (3)</formula><p>where g is nonlinear squash function through the entire vector, b 1 is the capsule bias term. For all C filters, the generated capsule feature maps can be rearranged as</p><formula xml:id="formula_5">P = [p 1 , p 2 , ..., p C ] ∈ R (L−K 1 +1)×C×d , (4)</formula><p>where totally (L − K 1 + 1) × C d-dimensional vectors are collected as capsules in P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Child-Parent Relationships</head><p>As argued in ( <ref type="bibr" target="#b21">Sabour et al., 2017)</ref>, capsule net- work tries to address the representational limita- tion and exponential inefficiencies of convolutions with transformation matrices. It allows the net- works to automatically learn child-parent (or part- whole) relationships constituting viewpoint invari- ant knowledge that automatically generalizes to novel viewpoints. In this paper, we explore two different types of transformation matrices to generate prediction vector (vote) ˆ u j|i ∈ R d from its child capsule i to the parent capsule j. The first one shares weights W t 1 ∈ R N ×d×d across child capsules in the layer below, where N is the number of parent capsules in the layer above. Formally, each corresponding vote can be computed by:</p><formula xml:id="formula_6">ˆ u j|i = W t 1 j u i + ˆ b j|i ∈ R d (5)</formula><p>where u i is a child-capsule in the layer below andˆb andˆandˆb j|i is the capsule bias term.</p><p>In the second design, we replace the shared weight matrix W t 1 j with non-shared weight ma- trix W t 2 i,j , where the weight matrices W t 2 ∈ R H×N ×d×d and H is the number of child capsules in the layer below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic Routing</head><p>The basic idea of dynamic routing is to construct a non-linear map in an iterative manner ensuring that the output of each capsule gets sent to an ap- propriate parent in the subsequent layer:</p><formula xml:id="formula_7">ˆ u j|i ∈ R d i=1,...,H,j=1...,N → v j ∈ R d N j=1</formula><p>.</p><p>For each potential parent, the capsule network can increase or decrease the connection strength by dynamic routing, which is more effective than the primitive routing strategies such as max-pooling in CNN that essentially detects whether a feature is present in any position of the text, but loses spatial information about the feature. We explore three strategies to boost the accuracy of routing process by alleviating the disturbance of some noisy cap- sules:</p><p>Orphan Category Inspired by <ref type="bibr" target="#b21">Sabour et al. (2017)</ref>, an additional "orphan" category is added to the network, which can capture the "back- ground" information of the text such as stop words and the words that are unrelated to specific cat- egories, helping the capsule network model the child-parent relationship more efficiently. Adding "orphan" category in the text is more effective than in image since there is no single consistent "back- ground" object in images, while the stop words are consistent in texts such as predicate "s", "am" and pronouns "his", "she".</p><p>Leaky-Softmax We explore Leaky-Softmax <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> in the place of standard soft- max while updating connection strength between the children capsules and their parents. Despite the orphan category in the last capsule layer, we also need a light-weight method between two consecutive layers to route the noise child cap- sules to extra dimension without any additional parameters and computation consuming.</p><p>Coefficients Amendment We also attempt to use the probability of existence of child capsules in the layer below to iteratively amend the con- nection strength as Eq.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Dynamic Routing Algorithm</head><formula xml:id="formula_8">1 procedure ROUTING(ˆ u j|i , ˆ a j|i , r, l)</formula><p>2 Initialize the logits of coupling coefficients</p><formula xml:id="formula_9">b j|i = 0 3 for r iterations do 4</formula><p>for all capsule i in layer l and capsule j in layer l + 1:</p><formula xml:id="formula_10">c j|i = ˆ a j|i · leaky-softmax(b j|i ) 5</formula><p>for all capsule j in layer l + 1:</p><formula xml:id="formula_11">v j = g( i c j|î u j|i ), a j = |v j | 6</formula><p>for all capsule i in layer l and capsule j in layer l + 1:</p><formula xml:id="formula_12">b j|i = b j|i + ˆ u j|i · v j 7 return v j ,a j</formula><p>Given each prediction vectorûvectorˆvectorû j|i and its prob- ability of existencê a j|i , wherê a j|i = ˆ a i , each it- erative coupling coefficient of connection strength c j|i is updated by</p><formula xml:id="formula_13">c j|i = ˆ a j|i · leaky-softmax(b j|i )<label>(6)</label></formula><p>where b j|i is the logits of coupling coefficients. Each parent capsule v j in the layer above is a weighted sum over all prediction vectorsûvectorsˆvectorsû j|i :</p><formula xml:id="formula_14">v j = g( i c j|î u j|i ), a j = |v j | (7)</formula><p>where a j is the probabilities of parent capsules, g is nonlinear squash function <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> through the entire vector. Once all of the parent capsules are produced, each coupling coefficient b j|i is updated by:</p><formula xml:id="formula_15">b j|i = b j|i + ˆ u j|i · v j<label>(8)</label></formula><p>For simplicity of notation, the parent capsules and their probabilities in the layer above are denoted as v, a = Routing(ˆ u)</p><p>wherê u denotes all of the child capsules in the layer below, v denotes all of the parent-capsules and their probabilities a.</p><p>Our dynamic routing algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Convolutional Capsule Layer</head><p>In this layer, each capsule is connected only to a local region K 2 × C spatially in the layer be- low. Those capsules in the region multiply trans- formation matrices to learn child-parent relation- ships followed by routing by agreement to produce parent capsules in the layer above.</p><p>Suppose W c 1 ∈ R D×d×d and W c 2 ∈ R K 2 ×C×D×d×d denote shared and non-shared weights, respectively, where K 2 · C is the number of child capsules in a local region in the layer be- low, D is the number of parent capsules which the child capsules are sent to. When the transforma- tion matrices are shared across the child capsules, each potential parent-capsulê u j|i is produced byû</p><formula xml:id="formula_17">byˆbyû j|i = W c 1 j u i + ˆ b j|i (10)</formula><p>wherê b j|i is the capsule bias term, u i is a child capsule in a local region K 2 × C and W c 1 j is the j th matrix in tensor W c 1 . Then, we use routing- by-agreement to produce parent capsules feature maps totally (L−K 1 −K 2 +2)×D d-dimensional capsules in this layer. When using the non-shared weights across the child capsules, we replace the transformation matrix W c 1 j in Eq. <ref type="formula">(10)</ref> with W c 2 j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Fully Connected Capsule Layer</head><p>The capsules in the layer below are flattened into a list of capsules and fed into fully connected capsule layer in which capsules are multiplied by transformation matrix W d 1 ∈ R E×d×d or W d 2 ∈ R H×E×d×d followed by routing-by-agreement to produce final capsule v j ∈ R d and its probability a j ∈ R for each category. Here, H is the number of child capsules in the layer below, E is the num- ber of categories plus an extra orphan category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">The Architectures of Capsule Network</head><p>We explore two capsule architectures (denoted as Capsule-A and Capsule-B) to integrate these four </p><formula xml:id="formula_18">B x C x d PrimCap B x C x d PrimCap B x C x d PrimCap K2 x C x D x d x d ConvCap K2 x C x D x d x d ConvCap K2 x C x D x d x d ConvCap H x E x d x d FCCap H x E x d x d FCCap H x E x d x d FCCap</formula><p>Capsule-B Each capsule has 16-dimensional (d = 16) in- stantiated parameters and their length (norm) can describe the probability of the existence of cap- sules. The capsule layers are connected by the transformation matrices, and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism.</p><p>The basic structure of Capsule-B is similar to Capsule-A except that we adopt three parallel net- works with filter windows (N ) of 3, 4, 5 in the N -gram convolutional layer (see <ref type="figure" target="#fig_0">Figure 2)</ref>. The final output of the fully connected capsule layer is fed into the average pooling to produce the final re- sults. In this way, Capsule-B can learn more mean- ingful and comprehensive text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Datasets</head><p>In order to evaluate the effectiveness of our model, we conduct a series of experiments on six bench- marks including: movie reviews (MR) <ref type="bibr" target="#b18">(Pang and Lee, 2005</ref>), Stanford Sentiment Treebankan exten- sion of MR (SST-2) <ref type="bibr" target="#b22">(Socher et al., 2013)</ref>, Subjec- tivity dataset (Subj) ( <ref type="bibr" target="#b17">Pang and Lee, 2004)</ref>, TREC question dataset (TREC) ( <ref type="bibr" target="#b12">Li and Roth, 2002</ref>), cus- tomer review (CR) ( <ref type="bibr" target="#b6">Hu and Liu, 2004</ref>), and AG's news corpus ( <ref type="bibr" target="#b2">Conneau et al., 2017</ref>). These bench- marks cover several text classification tasks such as sentiment classification, question categoriza- tion, news categorization. The detailed statistics are presented in <ref type="table">Table 1</ref>  <ref type="table">Table 1</ref>: Characteristics of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>In the experiments, we use 300-dimensional word2vec ( <ref type="bibr" target="#b15">Mikolov et al., 2013</ref>) vectors to ini- tialize embedding vectors. We conduct mini-batch with size 50 for AG's news and size 25 for other datasets. We use Adam optimization algorithm with 1e-3 learning rate to train the model. We use 3 iteration of routing for all datasets since it opti- mizes the loss faster and converges to a lower loss at the end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline methods</head><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantitative Evaluation</head><p>In our experiments, the evaluation metric is classi- fication accuracy. We summarize the experimental results in <ref type="table" target="#tab_3">Table 2</ref>. From the results, we observe that the capsule networks achieve best results on 4 out of 6 benchmarks, which verifies the effec- tiveness of the capsule networks. In particular, our model substantially and consistently outperforms   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>To analyze the effect of varying different compo- nents of our capsule architecture for text classifica- tion, we also report the ablation test of the capsule- B model in terms of using different setups of the capsule network. The experimental results are summarized in <ref type="table" target="#tab_7">Table 5</ref>. Generally, all three pro- posed dynamic routing strategies contribute to the effectiveness of Capsule-B by alleviating the dis- turbance of some noise capsules which may con- tain "background" information such as stop words and the words that are unrelated to specific cate- gories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Single-Label to Multi-Label Text Classification</head><p>Capsule network demonstrates promising perfor- mance in single-label text classification which as- signs a label from a predefined set to a text (see <ref type="table" target="#tab_3">Ta- ble 2</ref>). Multi-label text classification is, however, a more challenging practical problem. From single- label to multi-label (with n category labels) text classification, the label space is expanded from n to 2 n , thus more training is required to cover the whole label space. For single-label texts, it is prac- tically easy to collect and annotate the samples. However, the burden of collection and annotation for a large scale multi-label text dataset is gener- ally extremely high. How deep neural networks (e.g., CNN and LSTM) best cope with multi-label text classification still remains a problem since ob- taining large scale of multi-label dataset is a time- consuming and expensive process. In this section, we investigate the capability of capsule network on multi-label text classification by using only the single-label samples as training data. With fea- ture property as part of the information extracted by capsules, we may generalize the model better to multi-label text classification without an over extensive amount of labeled data.</p><p>The evaluation is carried on the Reuters-21578 dataset <ref type="bibr" target="#b11">(Lewis, 1992)</ref>. This dataset consists of 10,788 documents from the Reuters financial newswire service, where each document contains either multiple labels or a single label. We re- process the corpus to evaluate the capability of capsule networks of transferring from single-label to multi-label text classification. For dev and train- ing, we only use the single-label documents in the Reuters dev and training sets. For testing, Reuters- Multi-label only uses the multi-label documents in testing dataset, while Reuters-Full includes all documents in test set. The characteristics of these two datasets are described in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Following <ref type="bibr" target="#b23">(Sorower, 2010)</ref>, we adopt Micro Averaged Precision (Precision), Micro Averaged Recall (Recall) and Micro Averaged F1 scores (F1) as the evaluation metrics for multi-label text classification. Any of these scores are firstly com- puted on individual class labels and then averaged over all classes, called label-based measures. In addition, we also measure the Exact Match Ratio (ER) which considers partially correct prediction as incorrect and only counts fully correct samples.</p><p>The experimental results are summarized in <ref type="table" target="#tab_6">Ta- ble 4</ref>. From the results, we can observe that the capsule networks have substantial and significant improvement in terms of all four evaluation met- rics over the compared baseline methods on the test sets in both Reuters-Multi-label and Reuters- Full datasets. In particular, larger improvement   is achieved on Reuters-Multi-label dataset which only contains the multi-label documents in the test set. This is within our expectation since the cap- sule network is capable of preserving the instanti- ated parameters of the categories trained by single- label documents. The capsule network has much stronger transferring capability than the conven- tional deep neural networks. In addition, the good results on Reuters-Full also indicate that the cap- sule network has robust superiority over competi- tors on single-label documents.</p><note type="other">Reuters-Multi-label Reuters-Full ER</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Connection Strength Visualization</head><p>To visualize the connection strength between cap- sule layers clearly, we remove the convolutional capsule layer and make the primary capsule layer followed by the fully connected capsule layer di- rectly, where the primary capsules denote N-gram phrases in the form of capsules. The connection strength shows the importance of each primary capsule for text categories, acting like a parallel attention mechanism. This should allow the cap- sule networks to recognize multiple categories in the text even though the model is trained on single- label documents.</p><p>Due to space reasons, we choose a multi- label document from Reuters-Multi-label test set whose category labels (i.e., Interest Rates and Money/Foreign Exchange) are correctly predicted (fully correct) by our model with high confidence (p &gt; 0.8) to report in <ref type="table" target="#tab_8">Table 6</ref>. The category- specific phrases such as "interest rates" and "for- eign exchange" are highlighted with red color. We use the tag cloud to visualize the 3-gram phrases for Interest Rates and Money/Foreign Exchange categories. The stronger the connection strength, the bigger the font size. From the results, we ob- serve that capsule networks can correctly recog- nize and cluster the important phrases with respect to the text categories. The histograms are used to show the intensity of connection strengths be- tween primary capsules and the fully connected capsules, as shown in <ref type="table" target="#tab_8">Table 6</ref> (bottom line). Due to space reasons, five histograms are demon- strated. The routing procedure correctly routes the votes into the Interest Rates and Money/Foreign Exchange categories.</p><p>To experimentally verify the convergence of the routing algorithm, we also plot learning curve to show the training loss over time with different it- erations of routing. From <ref type="figure" target="#fig_3">Figure 3</ref>, we observe that the Capsule-B with 3 or 5 iterations of routing optimizes the loss faster and converges to a lower loss at the end than the capsule network with 1 it- eration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Early methods for text classification adopted the typical features such as bag-of-words, n-grams, and their TF-IDF features ( <ref type="bibr" target="#b28">Zhang et al., 2008</ref>) as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U.K. MONEY RATES FIRM ON LAWSON STERLING TARGETS Interest Rates Money/Foreign Exchange</head><p>Interest rates on the London money market were slightly firmer on news U.K. Chancellor of the Exchequer Nigel Lawson had stated target rates for sterling against the dollar and mark, dealers said. They said this had come as a surprise and expected the targets, 2.90 marks and 1.60 dlrs, to be promptly tested in the foreign exchange markets. Sterling opened 0.3 points lower in trade weighted terms at 71.3. Dealers noted the chancellor said he would achieve his goals on sterling by a combination of intervention in currency markets and interest rates. Operators feel the foreign exchanges are likely to test sterling on the downside and that this seems to make a fall in U.K. Base lending rates even less likely in the near term, dealers said. The feeling remains in the market, however, that fundamental factors have not really changed and that a rise in U.K. Interest rates is not very likely. The market is expected to continue at around these levels, reflecting the current 10 pct base rate level, for some time.</p><p>The key three months interbank rate was 1/16 point firmer at 10 9-7/8 pct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Orphan</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mergers/Acquisitions Money/Foreign Exchange</head><p>Trade Interest Rates   input of machine learning algorithms such as sup- port vector machine (SVM) <ref type="bibr" target="#b7">(Joachims, 1998)</ref>, lo- gistic regression <ref type="bibr" target="#b3">(Genkin et al., 2007)</ref>, naive Bayes (NB) <ref type="bibr" target="#b14">(McCallum et al., 1998</ref>) for classification. However, these models usually heavily relied on laborious feature engineering or massive extra lin- guistic resources.</p><p>Recent advances in deep neural networks and representation learning have substantially im- proved the performance of text classification tasks. The dominant approaches are recurrent neural net- works, in particular <ref type="bibr">LSTMs and CNNs. (Kim, 2014</ref>) reported on a series of experiments with CNNs trained on top of pre-trained word vectors for sentence-level classification tasks. The CNN models improved upon the state of the art on 4 out of 7 tasks. ( <ref type="bibr" target="#b29">Zhang et al., 2015</ref>) offered an empirical exploration on the use of character-level convolutional networks (Convnets) for text classi- fication and the experiments showed that Convnets outperformed the traditional models. ( <ref type="bibr" target="#b8">Joulin et al., 2016)</ref> proposed a simple and efficient text classi- fication method fastText, which could be trained on a billion words within ten minutes. ( <ref type="bibr" target="#b2">Conneau et al., 2017)</ref> proposed a very deep convolutional networks (with 29 convolutional layers) for text classification. ( <ref type="bibr" target="#b24">Tai et al., 2015)</ref> generalized the LSTM to the tree-structured network topologies (Tree-LSTM) that achieved best results on two text classification tasks.</p><p>Recently, a novel type of neural network is pro- posed using the concept of capsules to improve the representational limitations of CNN and RNN. <ref type="bibr" target="#b5">Hinton et al. (2011)</ref> firstly introduced the con- cept of "capsules" to address the representational limitations of CNNs and RNNs. Capsules with transformation matrices allowed networks to au- tomatically learn part-whole relationships. Conse- quently, <ref type="bibr" target="#b21">Sabour et al. (2017)</ref> proposed capsule net- works that replaced the scalar-output feature de- tectors of CNNs with vector-output capsules and max-pooling with routing-by-agreement. The cap- sule network has shown its potential by achiev- ing a state-of-the-art result on MNIST data. Un- like max-pooling in CNN, however, Capsule net-work do not throw away information about the precise position of the entity within the region. For lowlevel capsules, location information is place- coded by which capsule is active. ( <ref type="bibr" target="#b25">Xi et al., 2017)</ref> further tested out the application of capsule net- works on CIFAR data with higher dimensionality. ( <ref type="bibr" target="#b4">Hinton et al., 2018)</ref> proposed a new iterative rout- ing procedure between capsule layers based on the EM algorithm, which achieves significantly bet- ter accuracy on the smallNORB data set. ( <ref type="bibr" target="#b27">Zhang et al., 2018</ref>) generalized existing routing methods within the framework of weighted kernel density estimation. To date, no work investigates the per- formance of capsule networks in NLP tasks. This study herein takes the lead in this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we investigated capsule networks with dynamic routing for text classification. Three strategies were proposed to boost the performance of the dynamic routing process to alleviate the dis- turbance of noisy capsules. Extensive experiments on six text classification benchmarks show the ef- fectiveness of capsule networks in text classifi- cation. More importantly, capsule networks also show significant improvement when transferring single-label to multi-label text classifications over the co baseline methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Two architectures of capsule networks. components in different ways, as depicted in Figure 2. Capsule-A starts with an embedding layer which transforms each word in the corpus to a 300-dimensional (V = 300) word vector, followed by a 3-gram (K 1 = 3) convolutional layer with 32 filters (B = 32) and a stride of 1 with ReLU non-linearity. All the other layers are capsule layers starting with a B × d primary capsule layer with 32 filters (C = 32), followed by a 3 × C × d × d (K 2 = 3) convolutional capsule layer with 16 filters (D = 16) and a fully connected capsule layer in sequence. Each capsule has 16-dimensional (d = 16) instantiated parameters and their length (norm) can describe the probability of the existence of capsules. The capsule layers are connected by the transformation matrices, and each connection is also multiplied by a routing coefficient that is dynamically computed by routing by agreement mechanism. The basic structure of Capsule-B is similar to Capsule-A except that we adopt three parallel networks with filter windows (N ) of 3, 4, 5 in the N-gram convolutional layer (see Figure 2). The final output of the fully connected capsule layer is fed into the average pooling to produce the final results. In this way, Capsule-B can learn more meaningful and comprehensive text representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the experiments, we evaluate and compare our model with several widely used baseline methods including: LSTM/Bi-LSTM (Cho et al., 2014), tree-structured LSTM (Tree-LSTM) (Tai et al., 2015), LSTM regularized by linguistic knowl- edge (LR-LSTM) (Qian et al., 2016), CNN- rand/CNN-static/CNN-non-static (Kim, 2014), very deep convolutional network (VD-CNN) (Conneau et al., 2017), and character-level convo- lutional network (CL-CNN) (Zhang et al., 2015).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training loss of Capsule-B on ReutersMulti-label dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.</head><label></label><figDesc></figDesc><table>Dataset 
Train Dev 
Test Classes Classification Task 

MR 
8.6k 
0.9k 
1.1k 
2 
review classification 
SST-2 
8.6k 
0.9k 
1.8k 
2 
sentiment analysis 
Subj 
8.1k 
0.9k 
1.0k 
2 
opinion classification 
TREC 
5.4k 
0.5k 
0.5k 
6 
question categorization 
CR 
3.1k 
0.3k 
0.4k 
2 
review classification 
AG's news 108k 12.0k 7.6k 
4 
news categorization 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Comparisons of our capsule networks and baselines on six text classification benchmarks.</head><label>2</label><figDesc></figDesc><table>Dataset 
Train Dev Test Description 

Reuters-Multi-label 5.8k 
0.6k 0.3k only multi-label data in test 
Reuters-Full 
5.8k 
0.6k 3.4k full data in test 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Characteristics of Reuters-21578 corpus. 

the simple deep neural networks such as LSTM, 
Bi-LSTM and CNN-rand by a noticeable margin 
on all the experimental datasets. Capsule net-
work also achieves competitive results against the 
more sophisticated deep learning models such as 
LR-LSTM, Tree-LSTM, VC-CNN and CL-CNN. 
Note that Capsule-B consistently performs better 
than Capsule-A since Capsule-B allows to learn 
more meaningful and comprehensive text repre-
sentation. For example, a combination of N-gram 
convolutional layer with filter windows of {3,4,5} 
can capture the 3/4/5-gram features of the text 
which play a crucial role in text modeling. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparisons of the capability for transferring from single-label to multi-label text classification 
on Reuters-Multi-label and Reuters-Full datasets. For fair comparison, we use margin-loss for our model 
and other baselines. 

Iteration Accuracy 

Capsule-B + Sabour's routing 
3 
81.4 

Capsule-B + our routing 
1 
81.4 
Capsule-B + our routing 
3 
82.3 
Capsule-B + our routing 
5 
81.6 
w/o Leaky-softmax 
3 
81.7 
w/o Orphan Category 
3 
81.9 
w/o Amendent Coeffient 
3 
82.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Ablation study of Capsule-B on MR 
dataset. The standard routing is routing-by-
agreement algorithm without leaky-softmax and 
orphan category in the last capsule layer. More 
ablations are discussed in Appendix. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Visualization of connection strength between primary capsules and the FC capsules by 3-gram 
phrases cloud and histogram of the their intensities. x axis denotes primary capsules (3-gram phrases) 
selected for demonstration, y axis denotes intensity of connection strength. The results are retrieved from 
Capsule-B trained with 3 routing iterations. The category-specific key-phrases in red color in raw text 
(first column) are annotated manually for reference. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale bayesian logistic regression for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Madigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="304" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Matrix capsules with em routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJWLfGWRb" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual meeting on Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From word embeddings to document distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An evaluation of phrasal and clustered representations on a text categorization task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-98 workshop on learning for text categorization</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">752</biblScope>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextual bidirectional long short-term memory recurrent neural network language models: A generative approach to sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Mousa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd annual meeting on association for computational linguistics</title>
		<meeting>the 43rd annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Linguistically regularized lstms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03949</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A literature survey on algorithms for multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sorower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">18</biblScope>
			<pubPlace>Corvallis</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Oregon State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Selina</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03480</idno>
		<title level="m">Capsule network performance on complex data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Determining gains acquired from word embedding quantitatively using discrete distribution clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10807</idno>
		<title level="m">Fast dynamic routing based on weighted kernel density estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tfidf, lsi and multi-word in information retrieval and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taketoshi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijin</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="108" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
