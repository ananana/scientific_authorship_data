<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Addressing Troublesome Words in Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Addressing Troublesome Words in Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="391" to="400"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>391</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>One of the weaknesses of Neural Machine Translation (NMT) is in handling low-frequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memory-enhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) based on the encoder-decoder architecture becomes the new state-of-the-art due to distributed representation and end-to-end learning ( <ref type="bibr" target="#b3">Cho et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b14">Junczys-Dowmunt et al., 2016;</ref><ref type="bibr" target="#b7">Gehring et al., 2017;</ref><ref type="bibr" target="#b23">Vaswani et al., 2017)</ref>.</p><p>However, the current NMT is a global model that maximizes the performance on the overall data and has problems in handling low-frequency words and ambiguous words 1 , we refer these words as troublesome words and define them in Section 3.1.</p><p>Some previous work attempt to tackle the trans- lation problem of low-frequency words. <ref type="bibr" target="#b21">Sennrich et al. (2016)</ref> propose to decompose the words into subwords which are used as translation units so  that the low-frequency words can be represented by frequent subword sequences. <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> and <ref type="bibr" target="#b6">Feng et al. (2017)</ref> try to incorporate a translation lexicon into NMT in order to obtain the correct translation of low-frequency words. However, the former method still faces the low- frequency problem of subwords. And the latter one has a drawback that they use lexicons with- out considering specific contexts. <ref type="figure" target="#fig_1">Fig. 1</ref> shows an example, in which "aerkate" is an infrequent word and the baseline NMT incorrectly translates it into a pronoun "he". Incorporation of bilin- gual lexicon rectifies the mistake but wrongly con- verts "chengzhang" into an incorrect target word "growth" since an entry "(chengzhang, growth)" in the bilingual lexicon is somewhat wrongly used without taking the contexts into account. Further- more, these two kinds of methods mainly focus on low-frequency words that are just a part of the troublesome words.</p><p>In this paper, we categorize the words (includ- ing infrequent words and ambiguous words) which are difficult to translate as troublesome words and propose a novel memory-augmented framework to address them. Our method first investigates dif- ferent strategies to define the troublesome words. Then, these words and their contexts in the train- ing data are memorized with a contextual mem- ory which is finally accessed dynamically during decoding to solve the translation problem of the troublesome words.</p><p>Specifically, we first decode all the source sen- tences of the bilingual training data with baseline NMT and define the troublesome source words according to the distance between the predicted words and the gold words. The troublesome words associated with their hidden contextual represen- tations are stored in a memory which memorizes the correct translation and the corresponding con- textual information. During decoding, we acti- vate the contextual memory when we encounter the troublesome words and employ the contextual similarity between the test sentence and the mem- ory to determine appropriate target words. We test our methods on Chinese-to-English and English- to-German translation tasks. The experimental re- sults demonstrate that the translation performance can be significantly improved and a large portion of troublesome words can be correctly translated. The contributions are listed as follows:</p><p>1) We are the first to define and handle the trou- blesome words in neural machine translation.</p><p>2) We propose to memorize not only the bilin- gual lexicons but also their contexts with a contex- tual memory.</p><p>3) We design a dynamic approach to correctly translate the troublesome words by combining the contextual memory and the NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>NMT contains an encoder and a decoder. The encoder transforms a source sentence</p><formula xml:id="formula_0">X = {x 1 , x 2 , ..., x T x } into a set of context vectors C = (h m 1 , h m 2 , ..., h m T x )</formula><p>by using m stacked Long Short Term Memory (LSTM) layers <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> . h m j is the hidden state of the top layer in encoder. The bottom layer of en- coder is a bi-direction LSTM layer to collect the context from the left side and right side.</p><p>The decoder generates one target word at a time by computing p N i (y i |y &lt;i , C) as follows:</p><formula xml:id="formula_1">p N i (y i |y &lt;i , C) = sof tmax(W y i z i + b s ) (1)</formula><p>where z i is the attention output:</p><formula xml:id="formula_2">z i = tanh(W z [z m i ; c i ])<label>(2)</label></formula><p>c i can be calculated as follows:</p><formula xml:id="formula_3">c i = T x j=1 a ij h m i (3)</formula><p>where a i,j is the attention weight:</p><formula xml:id="formula_4">a i,j = h m j z m i j h m j z m i<label>(4)</label></formula><p>where z m i is the hidden state of the top layer in decoder. More detailed introduction can be found in ( <ref type="bibr" target="#b17">Luong et al., 2015)</ref>.</p><p>Notation. In this paper, we denote the whole source vocabulary by V S = {s m } |V S | m=1 and target vocabulary by V T = {t n } |V T | n=1 , where s m is the source word and t n is the target word. We denote a source sentence by X and a target sentence by Y . Each source word in X is denoted by x j . Each target word in Y is denoted by y i . Accordingly, a target word can be denoted not only by t n , but also by y i . This does not contradict. t n means this target word is the n th word in vocabulary V T , and y i means this target word is the i th word in sentence Y . Similarly, we denote a source word by s m and x j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method Description</head><p>Our method contains three parts: 1) definition and detection of the troublesome words (Section 3.1); 2) contextual memory construction (Section 3.2); and 3) hybrid approach combining contextual memory and baseline NMT model (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Troublesome Word Definition</head><p>Generally speaking, troublesome words are those that are difficult to translate for the baseline NMT system BN M T . <ref type="figure" target="#fig_2">Fig. 2</ref> shows the main process to detect the troublesome words. Given each training sentence pair (X, Y ), BN M T decodes the source sentence X and outputs the predicted probability of each gold target word p N i (y i ). We call y i an ex- ception if p N i (y i ) satisfies the predefined excep- tion criteria introduced below. The source word x j is an exception (a candidate troublesome word) if (x j , y i ) is an entry in the word alignment A 2 . Suppose x j appears N times in the training data and there are M exceptions among all its aligned gold target words. Then, the exception rate r(x j ) will be M/N . Definition: x j is a troublesome word if r(x j ) &gt; in which is a predefined threshold.</p><p>Exception Criteria. As discussed before, we need an exception criterion to measure whether a gold target word is an exception or not. In this pa- per, we investigate three exception criteria. Here, we introduce each of them through a toy exam- ple shown in <ref type="figure">Fig. 3</ref>, in which the source sentence is X = {x 1 , x 2 , x 3 } and the gold target sentence is Y = {y 1 , y 2 , y 3 }. The left shows the proba- bility distribution of all target vocabulary p N i (V T ) at each decoding step i, where the probability of the gold target word is highlighted in yellow. The right shows the word alignments between X and Y .</p><p>1) Absolute Criterion. A gold target word y i is an exception if its predicted probability p N i (y i ) is lower than a predefined threshold, namely p N i (y i ) &lt; p 0 . In <ref type="figure">Fig. 3</ref>, p N i (y i ) at each decod- ing step is respectively 0.8, 0.31 and 0.2. If we set p 0 = 0.5, p N 2 (y 2 ) and p N 3 (y 3 ) are lower than threshold p 0 . x 1 and x 3 are both exceptions ac- cording to the alignments.</p><p>2) Gap Criterion. For this criterion, we utilize the predicted probability gap between the gold tar- get word and the top one. Specifically, the gap can be calculated by:</p><formula xml:id="formula_5">g(y i ) = max(p N i (V T )) − p N i (y i )<label>(5)</label></formula><p>where max(p N i (V T )) is the top one in the prob- ability distribution at the i th decoding step. y i is an exception if g(y i ) &gt; g 0 . In <ref type="figure">Fig. 3</ref>, the largest predicted probabilities at each decoding step max(p N i (V T )) are respectively 0.8, 0.35 and 0.75. Thus, the gap is 0.0, 0.04 and 0.55. If g 0 = 0.1, x 3 is an exception since g(y 3 ) &gt; g 0 and x 3 aligns to y 3 . Figure 3: A toy example to show the process: if p N i (y i ) (left) satisfies the predefined exception criteria and x j aligns to y i , then x j is an exception. <ref type="figure">Fig. 3</ref>, the ranking of each gold tar- get word is 1, 3 and 2. If we set rank 0 = 2, then rank(y 2 ) = 3 &gt; rank 0 and x 1 is an exception due to the alignment between x 1 and y 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Arthur: alc percent in fo</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Ranking Criterion. This criterion is based on the ranking of p</head><formula xml:id="formula_6">N i (y i ) in p N i (V T ) (denoted by rank(y i )). If rank(y i ) &gt; rank 0 , then y i is an exception. In</formula><p>Using the above exception criteria and the def- inition of troublesome words, we can detect all the source-side troublesome words in the bilingual training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextual Memory Construction</head><p>For a troublesome word, we now introduce how to build a contextual memory M to store its transla- tion knowledge. Specifically, the contextual mem- ory contains five elements:</p><formula xml:id="formula_7">s m , t n , c(s m , t n ), p L (s m , t n ), r(s m )<label>(6)</label></formula><p>each of them is described as follows:</p><p>• s m is a troublesome source word.</p><p>• t n is a gold target word for s m .</p><p>• c(s m , t n ) is the context of lexicon pair (s m , t n ). Here, we use the hidden states of encoder h j to represent the context, since it contains the information from left (</p><formula xml:id="formula_8">− → h j ) and right ( ← − h j ).</formula><p>Note that when we traverse the training data and memorize the contexts of all troublesome words, there must be many cases in which the same pair (s m , t n ) appears in different contexts. In order to reduce the memory size and fuse different contexts of a same lexicon pair, we merge these memories by averaging the contexts. Assume there are K different contexts for (s m , t n ), and they  are denoted by h k (s m , t n ). The average con- text of (s m , t n ) can be calculated by:</p><formula xml:id="formula_9">c(s m , t n ) = K k=1 h k (s m , t n ) K<label>(7)</label></formula><p>Note that the context here is defined on the source side.</p><p>• p L (s m , t n ) is the lexicon translation proba- bility. It is the average of source-to-target and target-to-source probabilities calculated through maximum likelihood estimation on word alignments.</p><p>• r(s m ) is the exception rate of s m introduced in Section 3.1 and it can indicate the transla- tion difficulty of a source word. We will use r(s m ) to determine the dynamic weights of contextual memories in Section 4.</p><p>Noise Reduction. As we know, the training data and word alignments are not perfect and may introduce noise to the contextual memory. To re- duce the noise, we employ two strategies.</p><p>1) To improve the quality of the alignments A, we derive the alignment results from source-to- target and target-to-source, respectively. We only save the alignments which exist in both directions.</p><p>2) We eliminate the lexicon pairs whose trans- lation probabilities are too small. For a lexicon pair (s m , t n ), if its lexicon translation probability is smaller than 0.01, we treat this lexicon pair as a noisy sample and eliminate it from our memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integrating Contextual Memory into NMT</head><p>In this section, we integrate the contextual mem- ory into NMT to handle troublesome words. The overall framework is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref> and the integration process can be divided into four steps:</p><p>Step 1. Given a test sentence X, the first step is to find the troublesome words in X and collect corresponding local memories from the global contextual memory M. For each source word x j , we retrieve from M if it is a troublesome word and obtain the local memory as follows:</p><formula xml:id="formula_10">x j , t n , c(x j , t n ), p L (x j , t n ), r(x j )<label>(8)</label></formula><p>Step 2. The next step is to measure the con- textual similarity between the context in the test sentence X and the context in M. For the trou- blesome word x j ∈ X, we still use the encoder hidden state h j to represent the context in X.</p><formula xml:id="formula_11">The corresponding context in M is c(x j , t n ) in Eq. (8).</formula><p>Here, we use a feed-forward network to measure this similarity 3 :</p><formula xml:id="formula_12">d j (t n ) = sigmoid(v T d * tanh(W h * h j + W c * c(x j , t n )))<label>(9)</label></formula><p>where v d , W h and W c are learnable parameters. The sigmoid function guarantees the similarity score is in the range (0, 1). This similarity d j (t n ) will determine whether or not to adopt the target translation word t n in M.</p><p>Step 3. The next task is calculating the prob- ability p M i (t n ) of t n at each decoding step i. p M i (t n ) is the probability predicted by the contex- tual memory M and is calculated by:</p><formula xml:id="formula_13">p M i (t n ) = T x j a i,j * d j (t n ) * p L (x j , t n ) (10)</formula><p>where a i,j is the attention weight, d j (t n ) is the context similarity in Eq. <ref type="formula" target="#formula_12">(9)</ref>, and p L (x j , t n ) is the lexicon translation probability.</p><p>Step 4. The final task is to combine the mem- ory predicted probability (p M i in Eq. <ref type="formula">(10)</ref>) and the NMT predicted one (p N i in Eq. <ref type="formula">(1)</ref>). Here, we propose a dynamic strategy to balance these two probabilities:</p><formula xml:id="formula_14">p F i (t n ) = λ i * p M i (t n ) + (1 − λ i ) * p N i (t n ) (11)</formula><p>where p F i (t n ) is the final probability of the tar- get word t n , λ i is the dynamic weight to adjust the contribution from the memory and NMT. Here we explain the reason why we apply the dynamic manner. Recall that for each source troublesome word s m , we calculate its exception rate (similar to error rate). If a troublesome word has a lower exception rate, indicating that this source word is easier to be translated for the neural model. In this case, p N i is more reliable. Thus we design the dy- namic weight λ i according to the exception rate r(x j ):</p><formula xml:id="formula_15">λ i = sigmoid(β γ * γ i ) γ i = T x j a i,j * r(x j )<label>(12)</label></formula><p>where β γ is a learnable parameter. From Eq. (12), the dynamic weight λ i is determined by both of the attention weight a i,j , and the exception rate r(x j ).</p><p>Training the parameters. As discussed above, our method contains some parameters (v d , W h , W c and β γ ) to be learned. We denote the pa- rameters introduced by our method by θ M and the parameters in NMT by θ N . To make it efficient, given the aligned training data</p><formula xml:id="formula_16">D = X (d) , Y (d) |D| d=1</formula><p>, we keep θ N unchanged and op- timize θ M by maximizing the following objective function.</p><formula xml:id="formula_17">L(θ M ) = 1 |D| |D| d=1 Ty i log p F i (y (d) i ; θ M ) (13)</formula><p>where p F i can be calculated by Eq. (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head><p>We test the proposed methods on Chinese-to- English (CH-EN) and English-to-German <ref type="figure">(EN</ref>  <ref type="bibr">4</ref> to implement all described methods. In all experiments, the en- coder and decoder include two stacked LSTM lay- ers. The word embedding dimension and the size of hidden layers are both set to 1,000. The mini- batch size is set to 128. We discard the training sentence pairs whose length exceeds 100. We run a total of 20 iterations for all translation tasks. We test all methods based on two granularities: words and sub-words. For word granularity, we limit the vocabulary to 30K (CH-EN) and 50K (EN-DE) for both the source and target languages. For sub- word granularity, we use the BPE method <ref type="bibr" target="#b21">(Sennrich et al., 2016</ref>) to merge 30K (CH-EN) and 32K (EN-DE) steps. The beam size is set to 12. We use case-insensitive 4-gram BLEU ( <ref type="bibr" target="#b19">Papineni et al., 2002</ref>) for translation quality evaluation.</p><p>We compare our method with other relevant methods as follows:  <ref type="formula" target="#formula_2">(2016)</ref>'s method in the test phase. We denote this system by Arthur(test). In second way, we allow Baseline to be retrained by <ref type="bibr" target="#b0">Arthur et al. (2016)</ref>'s method, and denote the system by Arthur(train+test). We replicate the Arthurs work using the bias method with the hyper parameter being set to 0.001 as re- ported in their paper.</p><p>3) X+MEM: It is our proposed memory aug- ment method for any neural model X, in which we define the troublesome word by using the gap cri- terion with threshold g 0 = 0.1. We set threshold = 0.05, which is fine-tuned in validation set. It means if the exception rate of a source word ex- ceeds 0.05, we treat this word as a troublesome word.  <ref type="table" target="#tab_3">Table 1</ref>: The main results of CH-EN translation. shows the BLEU points improvement of system "X+MEM" than system X. "*" indicates that system "X+MEM" is statistically significant better (p &lt; 0.05) than system X and " †" indicates p &lt; 0.01.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results on CH-EN Translation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Our methods vs. Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Sub-words</head><p>We also test the proposed method when the translation unit is sub-word. The baseline and our method using sub-word as translation unit are respectively denoted by Baseline(sub- word) and Baseline(sub-word)+MEM. The re- sults are shown in row 4 and row 7. From the results, Baseline(sub-word)+MEM outperforms Baseline(sub-word) by 1.01 BLEU points, indi- cating that adopting sub-words as translation units still faces the problem of troublesome tokens, and our method could alleviate this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Our Method vs. Method Using Translation Lexicon</head><p>We also compare our method with <ref type="bibr" target="#b0">Arthur et al. (2016)</ref>'s method which incorporates a translation lexicon into NMT. Here, the comparison is con- ducted in two ways based on whether the baseline neural model is fixed or retrained. Fixed Baseline. Comparing Arthur(test) (row 2 in <ref type="table" target="#tab_3">Table 1</ref>) and Baseline+MEM (row 5 in Ta- ble 1), we can see that our proposed method can surpass Arthur(test) with 1.05 BLEU points. As there are three differences between our methods and Arthur(test), we take the following experi- ments to evaluate the effect of each difference.</p><p>The first difference is that our memory only Arthur: alcatel says sales growth nearly 30 percent in fourth quarter of last year Baseline+MEM: alcatel says sales grew nearly 30 percent in fourth quarter of last year  stores the lexicon pairs for troublesome words, while Arthur(test) utilizes all the available lexi- con pairs. We implement another system which is similar to Arthur(test), except that we only utilize the troublesome lexicon pairs. We denote the sys- tem by Tword. The results are reported in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>From the results, we can find that Tword obtains better translation results than Arthur(test) while using much fewer lexicon pairs (125K vs. 938K).</p><p>The second difference is that we take the con- text into consideration. When we add the con- text on the basis of Tword (denoted by +Con- text), it further improves the baseline system by 1.03 BLEU points, indicating the importance of the context. <ref type="figure" target="#fig_6">Fig.5</ref> shows the mentioned exam- ple in Section 1, in which Arthur(test) trans- lates chengzhang into a wrong target word growth, while Baseline+MEM could overcome this mis- take with the help of the context modeling.</p><p>We also implement another system, in which we build the contextual memory for all source words. We denote the system by All+Context and the re- sults are reported in <ref type="table" target="#tab_5">Table 2</ref>. As shown in <ref type="table" target="#tab_5">Table  2</ref>, All+Context surpasses Arthur(test) with 0.75 BLEU points while at the cost of 6.4G memory footprint and 1.829s time consuming. However, if we only build the contextual memory for the troublesome words, comparing to All+Context, there is only a slighter BLEU points decline (40.19 vs. 40.23) while sharply reduces memory size to 893M and decoding time to 0.511s, showing that our strategy of only building the contextual mem- ory for troublesome words is effective.</p><p>The final difference is that we employ the dy- namic strategy to balance between NMT and the contextual memory. When we employ this dy- namic strategy (denoted by +Dynamic), the im- provement can further reach 1.37 BLEU points.</p><p>Retrained Baseline. In the second compari- son, we allow the baseline model to be retrained by Arthur's method (Arthur(train+test)). We then implement our method using Arthur(train+test) as baseline (denoted by Arthur(train+test)+MEM). Comparing the results of these two methods in <ref type="table" target="#tab_3">Table 1</ref> (line 3 and 6), our method is still effective on the retrained model. The average gains are 0.92 BLEU points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effects of Different Exception Criteria</head><p>In our method, we investigate three exception cri- teria to define the troublesome words. The fol- lowing experiment is conducted to compare their performances. For fairness, the comparison of the three criteria is conducted under the same num- ber of contextual memory, which can be achieved by adjusting the respective thresholds (p 0 , g 0 and rank 0 ). The results are reported in <ref type="figure" target="#fig_7">Fig. 6</ref>, in which the x axis represents the size of contextual memory, the y axis denotes BLEU score, and the numbers in the bracket from left to right are the   <ref type="table">Table 4</ref>: The manual analysis on the word level. Col- umn Tword shows the number of troublesome words in sentence. Column Error shows the number of er- rors made by Baseline when translates the troublesome words. The number (ratio) of rectification caused by our method is reported in column Rectify. Column Deterio shows the number of deterioration (the orig- inal translation is correct, while our method produces the incorrect translation) caused by our method.</p><p>respective thresholds of gap, absolute and rank- ing. As shown in <ref type="figure" target="#fig_7">Fig. 6</ref>, all the three criteria can improve the translation quality. When the mem- ory size is relatively small, absolute criterion per- forms best. With the size increases, the gap crite- rion achieves a higher performance than others. Note that our current criteria only consider one single factor. The combination of different criteria may be more beneficial, and we leave this as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results on Low-Frequency Words and Ambiguous Words</head><p>We further analyze our method on specific trou- blesome words, such as low-frequency words and ambiguous words. Here, we use the following def- inition in our analysis.</p><p>Low-frequency words: The words whose fre- quency is lower than 100.</p><p>Ambiguous words: Assume a word s m con- tains K candidate translations with a probabil- ity p L k . If the entropy of probability distribution</p><formula xml:id="formula_18">− K k=1 p L k logp L k &gt; E 0 ( E 0 = 1.5</formula><p>in this paper), we treat this word as an ambiguous word.</p><p>Therefore, the sentences containing trouble- some words can be divided into four different parts: 1) sentences which contain both low- frequency and ambiguous words (Low+Amb,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Rectify Deterio Arthur(test) 51 17 Baseline+MEM 70 11 <ref type="table">Table 5</ref>: The numbers of rectification (Rectify) and deterioration (Deterio) caused by different models. 986 sentences), 2) sentences which contain low- frequency words but no ambiguous words (Low, 1427 sentences). 3) sentences which contain am- biguous words while do not contain low-frequency words (Amb, 1301 sentences). 4) Other sentences (Others, 832 sentences). The results are reported in <ref type="table" target="#tab_7">Table 3</ref>. From this table, we observe that our proposed method improves the translation quality on all kinds of sentences. Low+Amb performs best (Low the second), indicating that our method is most effective in dealing with low-frequency words. The improvement on Amb is 0.81 BLEU points, showing that our method can also well han- dle the ambiguous words. We also conduct a manual analysis to figure out how many troublesome words could be rectified by our method. We randomly select 200 testing sentences, and count the following three numbers: 1) the number of troublesome words in the sen- tence (Tword), 2) the number of mistakes pro- duced by Baseline (Error), 3) the number (ra- tio) of rectification using our method (Rectify). 4) The number of deterioration caused by our method (Deterio). The statistics are reported in <ref type="table">Table 4</ref>. From the results, we can get similar conclusions that our method is most effective on low-frequency and ambiguous words with the rectification rate 50.8% and 41.7% respectively.</p><p>We can notice that the proposed method also produces 11 deterioration cases (Deterio) when rectifying the troublesome words. As a compar- ison, we also count the total rectification and dete- rioration numbers of Arthur(test). The results are reported in <ref type="table">Table 5</ref>. These results show that our method could rectify more words (51 vs. 70) with less deterioration (17 vs. 11) than Arthur(test).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results on EN-DE Translation</head><p>We also test our method on EN-DE translation and the results are reported in <ref type="table">Table 6</ref>. We can see that our method is still effective on EN-DE translation. Specifically, when the translation unit is word, the proposed method improves the baseline by 1.13 BLEU points. The improvement is 0.76 BLEU points when the translation unit is sub-word.  <ref type="table">Table 6</ref>: The results on EN-DE translation. "*" in- dicates that it is statistically significantly better (p &lt; 0.05) than system X and " †" indicates p &lt; 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>The related work can be divided into three cate- gories and we describe each of them as follows:</p><p>Neural Turing Machine for NMT. Our idea is first inspired by the Neural Turing Machine (NTM) ( <ref type="bibr" target="#b8">Graves et al., 2014</ref><ref type="bibr" target="#b9">Graves et al., , 2016</ref> and mem- ory network <ref type="bibr">(Weston et al., 2014)</ref>. ( <ref type="bibr" target="#b25">Wang et al., 2017a</ref>) used special NTM memory to extend the decoder in the attention-based NMT. In their method, the memory is used to provide tempo- rary information from source to assist the decod- ing process. In contrast, our work uses memory to store contextual knowledge in the training data.</p><p>Smaller translation granularity. Our work is also inspired by the other studies to deal with the low-frequency and ambiguous <ref type="bibr">words (Vickrey et al., 2005;</ref><ref type="bibr" target="#b31">Zhai et al., 2013;</ref><ref type="bibr" target="#b20">Rios et al., 2017;</ref><ref type="bibr" target="#b2">Carpuat and Wu, 2007;</ref>). Among them, the most relevant is the work that decom- poses the low-frequency words into smaller gran- ularities, e.g, hybrid word-character model <ref type="bibr" target="#b16">(Luong and Manning, 2016)</ref>, sub-word model <ref type="bibr" target="#b21">(Sennrich et al., 2016</ref>) or word piece model ( . These methods mainly focus on low- frequency words that are just a subset of the trou- blesome words. Furthermore, our experimental re- sults show that even using a smaller translation unit, the NMT model still faces the problem of troublesome tokens and our method could allevi- ate this problem.</p><p>Combining SMT and NMT. Our ideas are also inspired by the work which combines SMT and NMT. Earlier studies were mostly based on the SMT framework, and have been deeply discussed by the review paper in <ref type="bibr" target="#b32">Zhang and Zong (2015)</ref>. Later, the researchers transfer to NMT framework, e.g. ( <ref type="bibr" target="#b26">Wang et al., 2017b;</ref><ref type="bibr" target="#b35">Zhou et al., 2017;</ref><ref type="bibr" target="#b22">Tu et al., 2016;</ref><ref type="bibr" target="#b18">Mi et al., 2016;</ref><ref type="bibr" target="#b11">He et al., 2016;</ref><ref type="bibr" target="#b4">Dahlmann et al., 2017;</ref><ref type="bibr">Wang et al., 2017c,d;</ref><ref type="bibr" target="#b10">Gu et al., 2018;</ref><ref type="bibr" target="#b34">Zhao et al., 2018)</ref>. The most relevant studies are <ref type="bibr" target="#b0">Arthur et al. (2016)</ref> and <ref type="bibr" target="#b6">Feng et al. (2017)</ref>. They incorporate the lexi- con pairs into NMT to improve the translation quality. There are three differences between our method and theirs. First, we only utilize the lex- icon pairs for the troublesome words, rather than using all lexicon pairs. Second, we take contextual information into consideration for memory con- struction. Third, we design a dynamic strategy to balance the memory and NMT. The experiments show the superiority of our proposed methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>To address troublesome words in NMT, we have proposed a novel memory-enhanced framework. We first define and detect the troublesome words, then construct a contextual memory to store the translation knowledge and finally access the con- textual memory dynamically to correctly trans- late the troublesome words. The extensive ex- periments on Chinese-to-English and English-to- German translation tasks demonstrate that our method significantly outperforms the strong base- line models in translation quality, especially in handling the troublesome words.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Source: 阿尔卡特 宣称 去年 第四 季 销售 成长 近 百分之三十 Pinyin: aerkate cheng qunian disi ji xiaoshou chengzhang jin baifenzhisanshi Reference: alcatel says sales in fourth quarter last year grew nearly 30 % NMT: he said sales grew nearly 30 percent in fourth quarter of last year NMT+LexiconTable: alcatel said sales growth nearly 30 percent in fourth quarter of last year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The NMT model produces a wrong translation for the low-frequency word "aerkat". While introducing an external lexicon table without contextual information, the model incorrectly translates the ambiguous word "chengzhang" into "growth".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The main process to detect an exception.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of contextual memoryaugmented NMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 )</head><label>1</label><figDesc>Baseline: It is the baseline NMT system with global attention (Luong et al., 2015; Zoph and Knight, 2016; Jean et al., 2015). 2) Arthur: It is the state-of-the-art method which incorporates discrete translation lexicons into NMT (Arthur et al., 2016). We implement Arthur et al. (2016)'s method in two different ways. In the first way, we fix the Baseline un- changed, and utilize Arthur et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Source: 阿尔卡特 宣称 去年 第四 季 销售 成长 近 百分之三十 Pinyin: aerkate cheng qunian disi ji xiaoshou chengzhang jin baifenzhisanshi Reference: alcatel says sales in fourth quarter last year grew nearly 30 % Baseline: he said sales grew nearly 30 percent in fourth quarter of last year Arthur: alcatel says sales growth nearly 30 percent in fourth quarter of last year Baseline+MEM: alcatel says sales grew nearly 30 percent in fourth quarter of last year</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example to show that considering context could produce a better translation result. In the example, Arthur(test) translates chengzhang into a wrong target word growth, while Baseline+MEM could overcome this mistake with the help of the context modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The comparison of different criteria. The gap criterion outperforms others with the increase of the memory size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 reports04 † 44.65 * 42.19 † 42.59 † 32.05 * 40.90 +0.92 7 Baseline(sub-word)+MEM 44.98 † 45.51 † 43.93 † 43.95 † 33.33 † 42.34 +1.01</head><label>1</label><figDesc>the main translation results of CH- EN translation. We first compare Baseline+MEM with Baseline. As shown in row 1 and row 5 in Ta- ble 1, Baseline+MEM can improve over Baseline on all test datasets, and the average improvement is 1.37 BLEU points. The results show that our method could significantly outperform the base- line model.</figDesc><table># Model 
03 
04 
05 
06 
08 
Avg. 

1 Baseline 
41.01 
42.94 
40.31 
40.57 
30.96 
39.16 
-
2 Arthur(test) 
41.34 
43.31 
40.79 
40.84 
31.11 
39.48 
-
3 Arthur(train+test) 
41.88 
43.75 
41.16 
41.63 
31.47 
39.98 
-
4 Baseline(sub-word) 
43.93 
44.74 
42.46 
43.01 
32.53 
41.33 
-
5 Baseline+MEM 
42.74  † 43.94  † 42.15  † 41.94  † 31.86  † 40.53 +1.37 
6 Arthur(train+test)+MEM 
43.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The effects of lexicon pairs only contain-
ing troublesome words (Tword), context and dynamic 
weights. Column #Pairs shows the number of lexicon 
pairs. Column Time shows the average decoding time 
(seconds) of per sentence. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 : The BLEU score on different kinds of sen- tences. Low denotes low frequency words, Amb de</head><label>3</label><figDesc></figDesc><table>-
</table></figure>

			<note place="foot" n="1"> In this work, we consider a source word is ambiguous if it has multiple translations with high entropy of probability distribution.</note>

			<note place="foot" n="2"> The word alignments A is extracted using the fast-align tool (Dyer et al., 2013) on the bilingual training data with both source-to-target and target-to-source directions.</note>

			<note place="foot" n="3"> In our preliminary experiment, we also try cosine distance to measure this similarity, while the performance of cosine distance is lower than the current feed-forward network method.</note>

			<note place="foot" n="4"> https://github.com/isi-nlp/Zoph_RNN. We extend this toolkit with global attention.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research <ref type="bibr">work</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incorporating discrete translation lexicons into neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1557" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR 2015</title>
		<meeting>ICLR 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2007</title>
		<meeting>EMNLP 2007</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Fethi Bougares Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation leveraging phrase-based models in a hybrid search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Dahlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Matusov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Petrushkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Khadivi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1422" to="1431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple, fast, and effective reparameterization of ibm model 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chahuneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory-augmented neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Abel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1401" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03317</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabskabarwi´nskabarwi´nska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">Gómez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page" from="471" to="476" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Search engine guided nonparametric neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with smt features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongjun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="124" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Is neural machine translation ready for deployment? a case study on 30 translation directions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01108</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards zero unknown word in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2016</title>
		<meeting>IJCAI 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Achieving open vocabulary neural machine translation with hybrid word-character models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1054" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2015</title>
		<meeting>EMNLP 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coverage embedding model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baskaran</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Ittycheriah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="955" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improving word sense disambiguation in neural machine translation with sense embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Mascarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In In proceedings of WMT</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coverage-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
		<meeting>ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.03317</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word-sense disambiguation for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vickrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Biewald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of ACL 2005</title>
		<meeting>ACL 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Memory-enhanced decoder for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural machine translation advised by statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Translating phrases in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of EMNLP 2017</title>
		<meeting>EMNLP 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1432" to="1442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards neural machine translation with partially aligned corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Chengqing Zong, and Zhengshan Xue</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint/>
	</monogr>
<note type="report_type">Bordes. 2014. Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Handling ambiguities of bilingual predicate-argument structures for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of ACL 2013</title>
		<meeting>ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1127" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep neural networks in machine translation: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2016</title>
		<meeting>EMNLP 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Phrase table as recommendation memory for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of IJCAI 2018</title>
		<meeting>IJCAI 2018</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4609" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural system combination for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
