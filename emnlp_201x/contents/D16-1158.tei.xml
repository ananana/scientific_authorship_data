<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Length bias in Encoder Decoder Models and a Case for Global Conditioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Sountsov Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">IIT Bombay</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
							<email>sunita@iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="department">IIT Bombay</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Length bias in Encoder Decoder Models and a Case for Global Conditioning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1516" to="1525"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Encoder-decoder networks are popular for modeling sequences probabilistically in many applications. These models use the power of the Long Short-Term Memory (LSTM) architecture to capture the full dependence among variables, unlike earlier models like CRFs that typically assumed conditional independence among non-adjacent variables. However in practice encoder-decoder models exhibit a bias towards short sequences that surprisingly gets worse with increasing beam size. In this paper we show that such phenomenon is due to a discrepancy between the full sequence margin and the per-element margin enforced by the locally conditioned training objective of a encoder-decoder model. The discrepancy more adversely impacts long sequences, explaining the bias towards predicting short sequences. For the case where the predicted sequences come from a closed set, we show that a globally conditioned model alleviates the above problems of encoder-decoder models. From a practical point of view, our proposed model also eliminates the need for a beam-search during inference, which reduces to an efficient dot-product based search in a vector-space.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we investigate the use of neural net- works for modeling the conditional distribution Pr(y|x) over sequences y of discrete tokens in re- sponse to a complex input x, which can be another * Work done while visiting Google Research on a leave from IIT <ref type="bibr">Bombay.</ref> sequence or an image. Such models have applica- tions in machine translation ( <ref type="bibr" target="#b8">Sutskever et al., 2014</ref>), image captioning ( <ref type="bibr" target="#b8">Vinyals et al., 2015)</ref>, response generation in emails ( <ref type="bibr" target="#b5">Kannan et al., 2016)</ref>, and conversations <ref type="bibr">(Khaitan, 2016;</ref><ref type="bibr" target="#b8">Vinyals and Le, 2015;</ref>).</p><p>The most popular neural network for probabilis- tic modeling of sequences in the above applications is the encoder-decoder (ED) network <ref type="bibr" target="#b8">(Sutskever et al., 2014)</ref>. A ED network first encodes an input x into a vector which is then used to initialize a re- current neural network (RNN) for decoding the out- put y. The decoder RNN factorizes Pr(y|x) using the chain rule as j Pr(y j |y 1 , . . . , y j−1 , x) where y 1 , . . . , y n denote the tokens in y. This factoriza- tion does not entail any conditional independence assumption among the {y j } variables. This is un- like earlier sequence models like CRFs ( <ref type="bibr" target="#b6">Lafferty et al., 2001</ref>) and MeMMs ( <ref type="bibr">McCallum et al., 2000</ref>) that typically assume that a token is independent of all other tokens given its adjacent tokens. Modern-day RNNs like LSTMs promise to capture non-adjacent and long-term dependencies by summarizing the set of previous tokens in a continuous, high-dimensional state vector. Within the limits of parameter capacity allocated to the model, the ED, by virtue of exactly factorizing the token sequence, is consistent.</p><p>However, when we created and deployed an ED model for a chat suggestion task we observed sev- eral counter-intuitive patterns in its predicted outputs. Even after training the model over billions of exam- ples, the predictions were systematically biased to- wards short sequences. Such bias has also been seen in translation ( ). Another curious phenomenon was that the accuracy of the predictions sometimes dropped with increasing beam-size, more than could be explained by statistical variations of a well-calibrated model ( <ref type="bibr" target="#b7">Ranzato et al., 2016)</ref>.</p><p>In this paper we expose a margin discrepancy in the training loss of encoder-decoder models to ex- plain the above problems in its predictions. We show that the training loss of ED network often under- estimates the margin of separating a correct sequence from an incorrect shorter sequence. The discrepancy gets more severe as the length of the correct sequence increases. That is, even after the training loss con- verges to a small value, full inference on the training data can incur errors causing the model to be under- fitted for long sequences in spite of low training cost. We call this the length bias problem.</p><p>We propose an alternative model that avoids the margin discrepancy by globally conditioning the P (y|x) distribution. Our model is applicable in the many practical tasks where the space of allowed out- puts is closed. For example, the responses gener- ated by the smart reply feature of Inbox is restricted to lie within a hand-screened whitelist of responses W ⊂ Y ( <ref type="bibr" target="#b5">Kannan et al., 2016)</ref>, and the same holds for a recent conversation assistant feature of Google's Allo <ref type="bibr">(Khaitan, 2016)</ref>. Our model uses a second RNN encoder to represent the output as another fixed length vector. We show that our proposed encoder- encoder model produces better calibrated whole se- quence probabilities and alleviates the length-bias problem of ED models on two conversation tasks. A second advantage of our model is that inference is significantly faster than ED models and is guaran- teed to find the globally optimal solution. In contrast, inference in ED models requires an expensive beam- search which is both slow and is not guaranteed to find the optimal sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Length Bias in Encoder-Decoder Models</head><p>In this section we analyze the widely used encoder- decoder neural network for modeling Pr(y|x) over the space of discrete output sequences. We use y 1 , . . . , y n to denote the tokens in a sequence y. Each y i is a discrete symbol from a finite dictionary V of size m. Typically, m is large. The length n of a se- quence is allowed to vary from sequence to sequence even for the same input x. A special token EOS ∈ V is used to mark the end of a sequence. We use Y to denote the space of such valid sequences and θ to denote the parameters of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The encoder-decoder network</head><p>The Encoder-Decoder (ED) network represents Pr(y|x, θ) by applying chain rule to exactly factor- ize it as n t=1 Pr(y t |y 1 , . . . , y t−1 , x, θ). First, an en- coder with parameters θ x ⊂ θ is used to transform x into a d-dimensional real-vector v x . The network used for the encoder depends on the form of x - for example, when x is also a sequence, the encoder could be a RNN. The decoder then computes each Pr(y t |y 1 , . . . , y t−1 , v x , θ) as</p><formula xml:id="formula_0">Pr(y t |y 1 , . . . , y t−1 , v x , θ) = P (y t |s t , θ),<label>(1)</label></formula><p>where s t is a state vector implemented using a recur- rent neural network as</p><formula xml:id="formula_1">s t = v x if t = 0, RNN(s t−1 , θ E,y t−1 , θ R ) otherwise.<label>(2)</label></formula><p>where RNN() is typically a stack of LSTM cells that captures long-term dependencies, θ E,y ⊂ θ are pa- rameters denoting the embedding for token y, and θ R ⊂ θ are the parameters of the RNN. The function Pr(y|s, θ y ) that outputs the distribution over the m tokens is a softmax:</p><formula xml:id="formula_2">Pr(y|s, θ) = e sθ S,y e sθ S,1 + . . . + e sθ S,m ,<label>(3)</label></formula><p>where θ S,y ⊂ θ denotes the parameters for token y in the final softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Origin of Length Bias</head><p>The ED network builds a single probability distri- bution over sequences of arbitrary length. For an input x, the network needs to choose the highest probability y among valid candidate sequences of widely different lengths. Unlike in applications like entity-tagging and parsing where the length of the output is determined based on the input, in appli- cations like response generation valid outputs can be of widely varying length. Therefore, Pr(y|x, θ) should be well-calibrated over all sequence lengths. Indeed under infinite data and model capacity the ED model is consistent and will represent all sequence lengths faithfully. In practice when training data is finite, we show that the ED model is biased against long sequences. Other researchers (  have reported this bias but we are not aware of any analysis like ours explaining the reasons of this bias.</p><p>Claim 2.1. The training loss of the ED model under- estimates the margin of separating long sequences from short ones.</p><p>Proof. Let x be an input for which a correct out- put y + is of length and an incorrect output y − is of length 1. Ideally, the training loss should put a positive margin between y + and y − which is log Pr(y + |x) − log Pr(y − |x). Let us investigate if the maximum likelihood training objective of the ED model achieves that. We can write this objective as:</p><formula xml:id="formula_3">max θ log Pr(y + 1 |x, θ)+ j=2 log Pr(y + j |y + 1...j−1 , x, θ).</formula><p>(4) Only the first term in the above objective is in- volved in enforcing a margin between y + and y − because log Pr(</p><formula xml:id="formula_4">y + 1 |x) is maximized when log Pr(y − 1 |x) is correspondingly minimized. Let m L (θ) = log Pr(y + 1 |x, θ) − log Pr(y − 1 |x, θ)</formula><p>, the local margin from the first position and m R (θ) = j=2 log Pr(y + j |y + 1...j−1 , x, θ). It is easy to see that our desired margin between y + and y − is log Pr(y + |x) − log Pr(y − |x) = m L + m R . Let m g = m L + m R . Assuming two possible labels for the first position (m = 2) 1 , the training objective in Equation 4 can now be rewritten in terms of the margins as:</p><formula xml:id="formula_5">min θ log(1 + e −m L (θ) ) − m R (θ)</formula><p>We next argue that this objective is not aligned with our ideal goal of making the global margin m L + m R positive.</p><p>First, note that m R is a log probability which un- der finite parameters will be non-zero. Second, even though m L can take any arbitrary finite value, the training objective drops rapidly when m L is positive. When training objective is regularized and training data is finite, the model parameters θ cannot take <ref type="bibr">1</ref> For m &gt; 2, the objective will be upper bounded by</p><formula xml:id="formula_6">min θ log(1 + (m − 1)e −m L (θ) ) − mR(θ).</formula><p>The argument that follows remains largely unchanged very large values and the trainer will converge at a small positive value of m L . Finally, we show that the value of m R decreases with increasing sequence length. For each position j in the sequence, we add to m R log-probability of y + j . The maximum value of log Pr(y + j |y + 1...j−1 , x, θ) is log(1 − ) where is non-zero and decreasing with the magnitude of the parameters θ. In general, log Pr(y + j |y + 1...j−1 , x, θ) can be a much smaller negative value when the input x has multiple correct responses as is common in con- versation tasks. For example, an input like x ='How are you?', has many possible correct outputs: y ∈{'I am good', 'I am great', 'I am fine, how about you?', etc}. Let f j denote the relative frequency of output y + j among all correct responses with prefix y + 1...j−1 . The value of m R will be upper bounded as</p><formula xml:id="formula_7">m R ≤ j=2 log min(1 − , f j )</formula><p>This term is negative always and increases in mag- nitude as sequence length increases and the set of positive outpus have high entropy. In this situation, when combined with regularization, our desired mar- gin m g may not remain positive even though m L is positive. In summary, the core issue here is that since the ED loss is optimized and regularized on the lo- cal problem it does not control for the global, task relevant margin.</p><p>This mismatch between the local margin optimized during training and the global margin explains the length bias observed by us and others ( ). During inference a shorter sequence for which m R is smaller wins over larger sequences.</p><p>This mismatch also explains why increasing beam size leads to a drop in accuracy sometimes (Ran- zato et al., 2016) 2 . When beam size is large, we are more likely to dig out short sequences that have oth- erwise been separated by the local margin. We show empirically in Section 4.3 that for long sequences larger beam size hurts accuracy whereas for small sequences the effect is the opposite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Proposed fixes to the ED models</head><p>Many ad hoc approaches have been used to alleviate length bias directly or indirectly. Some resort to nor-malizing the probability by the full sequence length ( <ref type="bibr" target="#b3">Graves, 2013)</ref> whereas <ref type="bibr" target="#b0">(Abadie et al., 2014</ref>) proposes segmenting longer sentences into shorter phrases. (  conjectures that the length bias of ED models could be because of limited representation power of the encoder network. Later more powerful encoders based on attention achieved greater accuracy ( ) on long sequences. Attention can be viewed as a mechanism of improving the capacity of the local models, thereby making the local margin m L more definitive. But attention is not effective for all tasks -for example, <ref type="bibr" target="#b8">(Vinyals and Le, 2015)</ref> report that attention was not useful for conversation.</p><p>Recently ( <ref type="bibr" target="#b0">Bengio et al., 2015;</ref><ref type="bibr" target="#b7">Ranzato et al., 2016)</ref> propose another modification to the ED training ob- jective where the true token y j−1 in the training term log Pr(y j |y 1 , . . . , y j−1 ) is replaced by a sample or top-k modes from the posterior at position j − 1 via a careful schedule. Incidently, this fix also helps to indirectly alleviate the length bias problem. The sam- pling causes incorrect tokens to be used as previous history for producing a correct token. If earlier the incorrect token was followed by a low-entropy EOS token, now that state should also admit the correct token causing a decrease in the probability of EOS, and therefore the short sequence.</p><p>In the next section we propose our more direct fix to the margin discrepancy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Globally Conditioned Encoder-Encoder Models</head><p>We represent Pr(y|x, θ) as a globally conditioned model e s(y|x,θ) Z(x,θ) where s(y|x, θ) denotes a score for output y and Z(x, θ) denotes the shared normalizer. We show in Section 3.3 why such global condition- ing solves the margin discrepancy problem of the ED model. The intractable partition function in global conditioning introduces several new challenges dur- ing training and inference. In this section we discuss how we designed our network to address them.</p><p>Our model assumes that during inference the out- put has to be selected from a given whitelist of re- sponses W ⊂ Y. In spite of this restriction, the problem does not reduce to multi-class classification because of two important reasons. First, during train- ing we wish to tap all available input-output pairs including the significantly more abundant outputs that do not come from the whitelist. Second, the whitelist could be very large and treating each output sequence as an atomic class can limit generalization achievable by modeling at the level of tokens in the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling s(y|x, θ)</head><p>We use a second encoder to convert y into a vector v y of the same size as the vector v x obtained by encoding x as in a ED network. The parameters used to encode v x and v y are disjoint. As we are only interested in a fixed dimensional output, unlike in ED networks, we have complete freedom in choosing the type of network to use for this second encoder. For our experiments, we have chosen to use an RNN with LSTM cells. Experimenting with other network architectures, such as bidirectional RNNs remains an interesting avenue for future work. The score s(y|x, θ) is the dot-product between v y and v x . Thus our model is</p><formula xml:id="formula_8">Pr(y|x) = e v T x vy y ∈Y e v T x v y .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training and Inference</head><p>During training we use maximum likelihood to esti- mate θ given a large set of valid input-output pairs {(x 1 , y 1 ), . . . , (x N , y N )} where each y i belongs to Y which in general is much larger than W. Our main challenge during training is that Y is intractably large for computing Z. We decompose Z as</p><formula xml:id="formula_9">Z = e s(y|x,θ) + y ∈Y \y e s(y |x,θ) ,<label>(6)</label></formula><p>and then resort to estimating the last term using im- portance sampling. Constructing a high quality pro- posal distribution over Y \ y is difficult in its own right, so in practice, we make the following approxi- mations. We extract the most common T sequences across a data set into a pool of negative examples. We estimate the empirical prior probability of the se- quences in that pool, Q(y), and then draw k samples from this distribution. We take care to remove the true sequence from this distribution so as to remove the need to estimate its prior probability. During inference, given an input x we need to find argmax y∈W s(y|x, θ). This task can be performed  efficiently in our network because the vectors v y for the sequences y in the whitelist W can be pre- computed. Given an input x, we compute v x and take dot-product with the pre-computed vectors to find the highest scoring response. This gives us the optimal response. When W is very large, we can obtain an approximate solution by indexing the vectors v y of W using recent methods specifically designed for dot-product based retrieval (Guo et al., 2016).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Margin</head><p>It is well-known that the maximum likelihood train- ing objective of a globally normalized model is mar- gin maximizing <ref type="bibr" target="#b7">(Rosset et al., 2003)</ref>. We illustrate this property using our set up from Claim 2.1 where a correct output y + is of length and an incorrect output y − is of length 1 with two possible labels for each position (m = 2). The globally conditioned model learns a parameter per possible sequence and assigns the probability to each sequence using a softmax over those parame- ters. Additionally, we place a Gaussian prior on the parameters with a precision c. The loss for a positive example becomes:</p><formula xml:id="formula_10">L G (y + ) = − log e −θ y + y e −θ y + c 2 y θ 2 y ,</formula><p>where the sums are taken over all possible sequences.</p><p>We also train an ED model on this task. It also learns a parameter for every possible sequence, but assigns probability to each sequence using the chain rule. We also place the same Gaussian prior as above on the parameters. Let y j denote the first j tokens {y 1 , . . . , y j } of sequence y. The loss for a positive example for this model is then:</p><formula xml:id="formula_11">L L (y + ) = − j=1   log e −θ y + j y j e −θ y j + c 2 y j θ 2 y j    ,</formula><p>where the inner sums are taken over all sequences of length j.</p><p>We train both models on synthetic sequences gen- erated using the following rule. The first token is chosen to be '1' probability 0.6. If '1' is chosen, it means that this is a positive example and the remain- ing − 1 tokens are chosen to be '1' with probability 0.9 1 −1 . If a '0' is chosen as the first token, then that is a negative example, and the sequence generation does not go further. This means that there are 2 −1 unique positive sequences of length and one neg- ative sequence of length 1. The remaining possible sequences do not occur in the training or testing data. By construction the unbiased margin between the most probable correct example and the incorrect ex- ample is length independent and positive. We sample 10000 such sequences and train both models using Adagrad (Duchi et al., 2011) for 1000 epochs with a learning rate of 0.1, effectively to convergence. <ref type="figure" target="#fig_1">Figure 2</ref> shows the margin for both models (be- tween the most likely correct sequence and the incor- rect sequence) and the local margin for the ED model at the end of training. On the left panel, we used sequences with = 2 and varied the regularization constant c. When c is zero, both models learn the same global margin, but as it is increased the margin for the ED model decreases and becomes negative at c &gt; 0.2, despite the local margin remaining pos- itive and high. On the right panel we used c = 0.1 and varied . The ED model becomes unable to sep- arate the sequences with length above 2 with this regularization constant setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Tasks</head><p>We contrast the quality of the ED and encoder- encoder models on two conversational datasets: Open Subtitles and Reddit Comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Open Subtitles Dataset</head><p>The Open Subtitles dataset consists of transcrip- tions of spoken dialog in movies and television shows ( <ref type="bibr">Lison and Tiedemann, 2016)</ref>. We restrict our model- ing only to the English subtitles, of which results in 319 million utternaces. Each utterance is tokenized into word and punctuation tokens, with the start and end marked by the BOS and EOS tokens. We ran- domly split out 90% of the utterances into the training set, placing the rest into the validation set. As the speaker information is not present in this data set, we treat each utterance as a label sequence, with the preceding utterances as context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Reddit Comments Dataset</head><p>The Reddit Comments dataset is constructed from publicly available user comments on submissions on the Reddit website. Each submission is associated with a list of directed comment trees. In total, there are 41 million submissions and 501 million com- ments. We tokenize the individual comments in the same way as we have done with the utternaces in the Open Subtitles dataset. We randomly split 90% of the submissions and the associated comments into the training set, and the rest into the validation set. We use each comment (except the ones with no par- ent comments) as a label sequence, with the context sequence composed of its ancestor comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Whitelist and Vocabulary</head><p>From each dataset, we derived a dictionary of 20 thousand most commonly used tokens. Additionally, each dictionary contained the unknown token (UNK), BOS and EOS tokens. Tokens in the datasets which were not present in their associated vocabularies were replaced by the UNK token.</p><p>From each data set, we extracted 10 million most common label sequences that also contained at most 100 tokens. This set of sequences was used as the negative sample pool for the encoder-encoder models. For evaluation we created a whitelist W out of the 100 thousand most common sequences. We removed any sequence from this set that contained any UNK tokens to simplify inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Sequence Prediction Task</head><p>To evaluate the quality of these models, we task them to predict the true label sequence given its context. Due to the computational expense, we sub-sample the validation data sets to around 1 mil- lion context-label pairs. We additionally restrict the context-label pairs such that the label sequence is present in the evaluation set of common messages. We use recall@K as a measure of accuracy of the model predictions. It is defined as the fraction of test pairs where the correct label is within K most probable predictions according to the model. For encoder-encoder models we use an exhaustive search over the evaluation set of common messages. For ED models we use a beam search with width ranging from 1 to 15 over a token prefix trie constructed from the sequences in W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Structure and Training Procedure</head><p>The context encoder, label encoder and decoder are implemented using LSTM recurrent networks <ref type="bibr">(Hochreiter and Schmidhuber, 1997</ref>) with peephole connections ( <ref type="bibr">Sak et al., 2014</ref>). The context and label token sequences were mapped to embedding vectors using a lookup table that is trained jointly with the rest of the model parameters. The recurrent nets were unrolled in time up to 100 time-steps, with label sequences of greater length discarded and context sequences of greater length truncated.</p><p>The decoder in the ED model is trained by using the true label sequence prefix as input, and a shifted label sequence as output <ref type="bibr" target="#b8">(Sutskever et al., 2014</ref>). The partition function in the softmax over tokens is es- timated using importance sampling with a unigram distribution over tokens as the proposal distribution ( <ref type="bibr" target="#b4">Jean et al., 2014</ref>). We sample 512 negative examples from Q(y) to estimate the partition function for the encoder-encoder model. See <ref type="figure" target="#fig_0">Figure 1</ref> for connectiv- ity and network size details.</p><p>All models were trained using Adagrad <ref type="bibr">(Duchi et al., 2011</ref>) with an initial base learning rate of 0.1 which we exponentially decayed with a decade of 15 million steps. For stability, we clip the L2 norm of the gradients to a maximum magnitude of 1 as described in ( <ref type="bibr">Pascanu et al., 2012</ref>). All models are trained for 30 million steps with a mini-batch size of 64. The models are trained in a distributed manner on CPUs and NVidia GPUs using TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We first demonstrate the discrepancy between the local and global margin in the ED models as dis- cussed in Section 3.3. We used a beam size of 15 to get the top prediction from our trained ED mod- els on the test data and focussed on the subset for which the top prediction was incorrect. We measured local and global margin between the top predicted sequence (y − ) and the correct test sequence (y + ) as follows: Global margin is the difference in their full sequence log probability. Local margin is the differ- ence in the local token probability of the smallest position j where y − j = y + j , that is local margin is Pr(y + j |y + 1...j−1 , x, θ) − Pr(y − j |y + 1...j−1 , x, θ). Note the training loss of ED models directly compares only the local margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global margin is much smaller than local margin</head><p>In <ref type="figure" target="#fig_2">Figure 3</ref> we show the local and global margin as a 2D histogram with color luminosity denoting fre- quency. We observe that the global margin values are much smaller than the local margins. The prominent spine is for (y + , y − ) pairs differing only in a single position making the local and global margins equal. Most of the mass is below the spine. For a significant fraction of cases (27% for Reddit, and 21% for Sub- titles), the local margin is positive while the global margin is negative. That is, the ED loss for these sequences is small even though the log-probability of the correct sequence is much smaller than the log- probability of the predicted wrong sequence.</p><p>Beam search is not the bottleneck An interesting side observation from the plots in <ref type="figure" target="#fig_2">Figure 3</ref> is that more than 98% of the wrong predictions have a nega- tive margin, that is, the score of the correct sequence is indeed lower than the score of the wrong predic- tion. Improving the beam-width beyond 15 is not likely to improve these models since only in 1.9% and 1.7% of the cases is the correct score higher than the score of the wrong prediction. Margin discrepancy is higher for longer se- quences In <ref type="figure" target="#fig_3">Figure 4</ref> we show that this discrep- ancy is significantly more pronounced for longer sequences. In the figure we show the fraction of wrongly predicted sequences with a positive local margin. We find that as sequence length increases, we have more cases where the local margin is posi- tive yet the global margin is negative. For example, for the Reddit dataset half of the wrongly predicted sequences have a positive local margin indicating that the training loss was low for these sequences even though they were not adequately separated. Increasing beam size drops accuracy for long se- quences Next we show why this discrepancy leads to non-monotonic accuracies with increasing beam- size. As beam size increases, the predicted se- quence has higher probability and the accuracy is expected to increase if the trained probabilities are well-calibrated. In <ref type="figure" target="#fig_4">Figure 5</ref> we plot the number of correct predictions (on a log scale) against the length of the correct sequence for beam sizes of 1, 5, 10, and 15. For small sequence lengths, we indeed ob- serve that increasing the beam size produces more accurate results. For longer sequences (length &gt; 4) we observe a drop in accuracy with increasing the beam width beyond 1 for Reddit and beyond 5 for Subtitles.</p><p>Globally conditioned models are more accurate than ED models We next compare the ED model with our globally conditioned encoder-encoder (EE) model. In <ref type="figure" target="#fig_5">Figure 6</ref> we show the recall@K values for K=1, 3 and 5 for the two datasets for increasing length of correct sequence. We find the EE model is largely better that the ED model. The most in- teresting difference is that for sequences of length greater than 8, the ED model has a recall@5 of zero for both datasets. In contrast, the EE model manages  to achieve significant recall even at large sequence lengths.</p><p>Length normalization of ED models A common modification to the ED decoding procedure used to promote longer message is normalization of the pre- diction log-probability by its length raised to some power f ( <ref type="bibr" target="#b3">Graves, 2013)</ref>. We ex- perimented with two settings, f = 0.5 and 1.0. Our experiments show that while this indeed promotes longer sequences, it does so at the expense of reduc- ing the accuracy on the shorter sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this paper we showed that encoder-decoder mod- els suffer from length bias and proposed a fix us- ing global conditioning. Global conditioning has been proposed for other RNN-based sequence pre- diction tasks in ( <ref type="bibr" target="#b8">Yao et al., 2014)</ref> and <ref type="bibr" target="#b0">(Andor et al., 2016)</ref>. The RNN models that these work attempt to fix capture only a weak form of dependency among variables, for example they assume x is seen incre- mentally and only adjacent labels in y are directly dependent. As proved in (2016) these models are subject to label bias since they cannot represent a dis- tribution that a globally conditioned model can. Thus, their fix for global dependency is using a CRFs. Such  global conditioning will compromise a ED model which does not assume any conditional independence among variables. The label-bias proof of <ref type="formula" target="#formula_0">(2016)</ref> is not applicable to ED models because the proof rests on the entire input not being visible during output. Earlier illustrations of label bias of MeMMs in <ref type="bibr" target="#b1">(Bottou, 1991;</ref><ref type="bibr" target="#b6">Lafferty et al., 2001</ref>) also require local observations. In contrast, the ED model transitions on the entire input and chain rule is an exact factoriza- tion of the distribution. Indeed one of the suggestions in <ref type="bibr" target="#b1">(Bottou, 1991)</ref> to surmount label-bias is to use a fully connected network, which the ED model al- ready does.</p><p>Our encoder-encoder network is reminiscent of the dual encoder network in ( <ref type="bibr" target="#b7">Lowe et al., 2015)</ref>, also used for conversational response generation. A cru- cial difference is our use of importance sampling to correctly estimate the probability of a large set of candidate responses, which allows us to use the model as a standalone response generation system. Other differences include our model using separate sets of parameters for the two encoders, to reflect the assymetry of the prediction task. Lastly, we found it crucial for the model's quality to use multiple appro- priately weighed negative examples for every positive example during training.</p><p>( <ref type="bibr" target="#b7">Ranzato et al., 2016</ref>) also highlights limitations of the ED model and proposes to mix the ED loss with a sequence-level loss in a reinforcement learning framework under a carefully tuned schedule. Our method for global conditioning can capture sequence- level losses like BLEU score more easily, but may also benefit from a similar mixed loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that encoder-decoder models in the regime of finite data and parameters suffer from a length-bias problem. We have proved that this arises due to the locally normalized models insufficiently separating correct sequences from incorrect ones, and have verified this empirically. We explained why this leads to the curious phenomenon of decreasing accu- racy with increasing beam size for long sequences. Our proposed encoder-encoder architecture side steps this issue by operating in sequence probability space directly, yielding improved accuracy for longer se- quences.</p><p>One weakness of our proposed architecture is that it cannot generate responses directly. An interesting future work is to explore if the ED model can be used to generate a candidate set of responses which are then re-ranked by our globally conditioned model. Another future area is to see if the techniques for making Bayesian networks discriminative can fix the length bias of encoder decoder networks ( <ref type="bibr">Peharz et al., 2013;</ref><ref type="bibr" target="#b3">Guo et al., 2012</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural network architectures used in our experiments. The context encoder network is used for both encoder-encoder and encoder-decoder models to encode the context sequence ('A') into a vx. For the encoder-encoder model, label sequence ('B') are encoded into vy by the label encoder network. For the encoder-decoder network, the label sequence is decomposed using the chain rule by the decoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing final margins of ED model with a globally conditioned model on example dataset of Section 3.3 as a function of regularization constant c and message length .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Local margin versus global margin for incorrectly predicted sequences. The color luminosity is proportional to frequency.</figDesc><graphic url="image-1.png" coords="7,338.21,520.05,76.66,107.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fraction of incorrect predictions with positive local margin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of beam width on the number of correct predictions broken down by sequence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparing recall@1, 3, 5 for increasing length of correct sequence.</figDesc></figure>

			<note place="foot" n="2"> Figure 6 in the paper shows a drop in BLEU score by 0.5 as the beam size is increased from 3 to 10.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overcoming the curse of sentence length for neural machine translation using automatic segmentation. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [abadi</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence prediction with recurrent neural networks. In NIPS</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Une approche theorique de l&apos;apprentissage connexionniste: Applications a la recon&apos;naissance de la parole</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Universitede Paris XI</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation: Encoderdecoder approaches. CoRR, abs/1409.1259</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Adaptive subgradient methods for online learning and stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. ; Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><forename type="middle">F</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simcha</surname></persName>
		</author>
		<idno>abs/1207.1382. [Guo et al.2016</idno>
	</analytic>
	<monogr>
		<title level="m">AISTATS. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and Jürgen Schmidhuber</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Balint Miklos, Greg Corrado, László Lukács, Marina Ganea, Peter Young, and Vivek Ramavajjala</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Smart reply: Automated response suggestion for email</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
		<ptr target="http://googleresearch.blogspot.com/2016/05/chat-smarter-with-allo.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robert Peharz, Sebastian Tschiatschek, and Franz Pernkopf. 2013. The most generative maximum margin bayesian networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A diversity-promoting objective function for neural conversation models. CoRR, abs/1510.03055. [Lison and Tiedemann2016] Pierre Lison and Jörg Tiedemann</title>
		<editor>ICML. [Pascanu et al.2012] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio</editor>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>INTERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
		<idno>abs/1506.05869. [Vinyals et al.2015</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS. [Vinyals and Le2015] Oriol Vinyals and Quoc V. Le. 2015. A neural conversational model. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CVPR. Recurrent conditional random field for language understanding. In ICASSP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
