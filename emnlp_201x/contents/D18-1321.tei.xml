<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafang</forename><surname>Huang</surname></persName>
							<email>huangyafang@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn *</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<postCode>200240</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2923" to="2929"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2923</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Chinese pinyin input method engine (IME) converts pinyin into character so that Chinese characters can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chi-nese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chi-nese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in IMEs. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pinyin is the official romanization representation for Chinese and the P2C converting the inputted pinyin sequence to Chinese character sequence is the most basic module of all pinyin based IMEs.</p><p>Most of the previous research <ref type="bibr" target="#b5">(Chen, 2003;</ref><ref type="bibr" target="#b29">Zhang et al., 2006</ref>; <ref type="bibr" target="#b18">Lin and Zhang, 2008;</ref><ref type="bibr" target="#b6">Chen and Lee, 2000;</ref><ref type="bibr" target="#b16">Jiang et al., 2007;</ref><ref type="bibr" target="#b1">Cai et al., 2017a)</ref> for IME focused on the matching correspondence between pinyin syllables and Chinese characters. ( <ref type="bibr" target="#b26">Yang et al., 2012;</ref><ref type="bibr" target="#b15">Jia and Zhao, 2014;</ref> regarded the P2C as a translation between two languages and solved it in statistical or neural machine translation frame- work. The fundamental difference between ) work and ours is that our work is a fully end-to-end neural IME model with extra at- tention enhancement, while the former still works on traditional IME only with converted neural net- work language model enhancement. ( <ref type="bibr" target="#b28">Zhang et al., 2017</ref>) introduced an online algorithm to construct appropriate dictionary for P2C. All the above men- tioned work, however, still rely on a complete in- put pattern, and IME users have to input very long pinyin sequence to guarantee the accuracy of P2C module as longer pinyin sequence may receive less decoding ambiguity.</p><p>The Chinese IME is supposed to let user in- put Chinese characters with least inputting cost, i.e., keystroking, which indicates extra content predication from incomplete inputting will be ex- tremely welcomed by all IME users. ( <ref type="bibr" target="#b11">Huang et al., 2015)</ref> partially realized such an extra predication using a maximum suffix matching postprocess- ing in vocabulary after SMT based P2C to predict longer words than the inputted pinyin.</p><p>To facilitate the most convenience for such an IME, in terms of a sequence to sequence model as neural machine translation (NMT) between pinyin sequence and character sequence, we propose a P2C model with the entire previous inputted ut- terance confirmed by IME users being used as a part of the source input. When learning the type of the previous utterance varies from the previous sentence in the same article to the previous turn of utterance in a conversation, the resulting IME will make amazing predication far more than what the pinyin IME users actually input.</p><p>In this paper, we adopt the attention-based NMT  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the core of our P2C is based the attention-based neural machine trans- lation model that converts at word level. Still, we formulize P2C as a translation between pinyin and character sequences as shown in a traditional model in <ref type="figure" target="#fig_0">Figure 1</ref>(a). However, there comes a key difference from any previous work that our source language side includes two types of inputs, the current source pinyin sequence (noted as P ) as usual, and the extended context, i.e., target charac- ter sequence inputted by IME user last time (noted as C). As IME works dynamically, every time IME makes a predication according to a source pinyin input, user has to indicate the 'right answer' to output target character sequence for P2C model learning. This online work mode of IMEs can be fully exploited by our model whose work flow is shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b).</p><p>As introduced a hybrid source side input, our model has to handle document-wide translation by considering discourse relationship between two consecutive sentences. The most straightforward modeling is to simply concatenate two types of source inputs with a special token 'BC' as sepa- rator. Such a model is in <ref type="figure" target="#fig_0">Figure 1</ref>(c). However, the significant drawback of the model is that there are a slew of unnecessary words in the extended context (previous utterance) playing a noisy role in the source side representation.</p><p>To alleviate the noise issue introduced by the extra part in the source side, inspired by the work of ( <ref type="bibr" target="#b8">Dhingra et al., 2016;</ref><ref type="bibr">Pang et al., 2016;</ref><ref type="bibr">Zhang et al., 2018c,a,b;</ref><ref type="bibr" target="#b2">Cai et al., 2017b</ref>), our model adopts a gated-attention (GA) mechanism that performs multiple hops over the pinyin with the extended context as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(d). In order to ensure the correlation between each other, we build a parallel bilingual training cor- pus and use it to train the pinyin embeddings and the Chinese embeddings at once. We use two Bidirectional gated recurrent unit (BiGRU) ( <ref type="bibr" target="#b7">Cho et al., 2014</ref>) to get contextual representations of the source pinyin and context respectively, H p = BiGRU(P ), H c = BiGRU(C), where the repre- sentation of each word is formed by concatenating the forward and backward hidden states.</p><p>For each pinyin p i in H p , the GA module forms a word-specific representation of the con- text c i ∈ H c using soft attention, and then adopts element-wise product to multiply the context rep- resentation with the pinyin representation.</p><formula xml:id="formula_0">α i = sof tmax(H T c p i ), β i = Cα i , x i = p i β i , where is multiplication operator. The pinyin representatioñ H p = x 1 , x 2 , ..., x k</formula><p>is augmented by context representation and then sent into the encoder-decoder framework. The encoder is a bi-directional long short-term mem- ory (LSTM) network (Hochreiter and Schmidhu- ber, 1997). The vectorized inputs are fed to for- ward and backward LSTMs to obtain the internal representation of two directions. The output for each input is the concatenation of the two vec- tors from both directions. Our decoder based on the global attentional models proposed by <ref type="bibr" target="#b19">(Luong et al., 2015)</ref> to consider the hidden states of the en- coder when deriving the context vector. The prob- ability is conditioned on a distinct context vector for each target word. The context vector is com- puted as a weighted sum of previous hidden states. The probability of each candidate word as being the recommended one is predicted using a soft- max layer over the inner-product between source embeddings and candidate target characters. This work belongs to one of the first line which fully introduces end-to-end deep learning solution to the IME implementation following a series of our previous work ( <ref type="bibr" target="#b34">Zhu et al., 2018;</ref><ref type="bibr" target="#b24">Wu and Zhao, 2018;</ref><ref type="bibr" target="#b22">Qin et al., 2018;</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition of Incomplete Input for IME</head><p>The completeness in IME is actually uneasily well-defined as it is a relative concept for inputting procedure. Note that not until user types the re- turn key enter, user will not (usually) really make the input choice. Meanwhile, even though the en- tire/complete input can be strictly defined by the time when user types enter, user still can make de- cision at any time and such incompleteness cannot be well evaluated by all the current IME metrics. As the incomplete from is hard to simulate and it is diverse in types, we have to partially evaluate it in the following two ways 1 ,</p><p>The incomplete pinyin as abbreviation pinyin To compare with previous work directly, we fol- lowed ( <ref type="bibr" target="#b11">Huang et al., 2015</ref>) and focused on the ab- breviated pinyin (the consonant letter only) to per- form evaluation (i.e., tian qi to t q).</p><p>Take incomplete user input as the incomplete As IME works as an interactive system, it will al- ways give prediction only if users keep typing. If <ref type="bibr">1</ref> Our code is at https://github.com/YvonneHuang/gaIME user's input does not end with typing enter, we can regard the current input pinyin sequence is an in- complete one.   <ref type="table" target="#tab_1">Table 1</ref>. The relativity refers to total proportion of sentences that associate with contextual history at word level. For example, there are 65.8% of sen- tences of DC corpus have words appearing in the context. With character text available, the needed parallel corpus between pinyin and character texts is automatically created following the approach proposed by <ref type="bibr" target="#b26">(Yang et al., 2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets and Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PD</head><p>Our model was implemented using the Py- Torch 2 library, here is the hyperparameters we used: (a) the RNNs used are deep LSTM models, 3 layers, 500 cells, (c) 13 epoch training with plain SGD and a simple learning rate schedule -start with a learning rate of 1.0; after 9 epochs, halve the learning rate every epoch, (d) mini-batches are of size 64 and shuffled, (e) dropout is 0.3. Word embeddings are pre-trained by word2vec ( <ref type="bibr" target="#b20">Mikolov et al., 2013</ref>) toolkit on the adopted cor- pus and unseen words are assigned unique random vectors. (f) the gated attention layers size is 3, the hidden units number of BiGRU is 100.</p><p>Two metrics are used: Maximum Input Unit (MIU) accuracy ( <ref type="bibr" target="#b28">Zhang et al., 2017)</ref> and KeyStroke Score (KySS) .  The former measures the conversion accuracy of MIU, whose definition is the longest uninterrupted Chinese character sequence during inputting. As the P2C conversion aims to output a rank list of the corresponding character sequences candi- dates, the Top-K MIU accuracy means the pos- sibility of hitting the target in the first K predicted items. The KySS quantifies user experience by us- ing keystroke count. For an ideal IME with com- plete input, we have KySS = 1. An IME with higher KySS is supposed to perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DC PD Top-1 KySS Top-1 KySS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Definition</head><p>We considered the following baselines: (a) Google IME: the only commercial Chinese IME providing a debuggable API in the market now; (b) OMWA: online model for word acquisition proposed by <ref type="bibr" target="#b28">(Zhang et al., 2017)</ref>; (c) CoCat: an SMT based in- put method proposed by <ref type="bibr" target="#b11">(Huang et al., 2015</ref>) that supports incomplete pinyin inputs.</p><p>Three models with incomplete or complete in- puts will be evaluated: (a) Basic P2C, the basic P2C based on attention-NMT model; (b) Basic C2C, the basic C to C model based on Seq2Seq model; (b) Simple C+ P2C, the simple con- catenated P2C conversion model that concatenate context to pinyin representation; (c) Gated C+ P2C, our gated attention based context-enhanced pinyin-to-character model. Pinyin in model * has been actually set to abbreviation form when we say it goes to ( <ref type="bibr" target="#b11">Huang et al., 2015</ref>) incomplete def- inition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Result and Analysis</head><p>Effect of Gated Attention Mechanism <ref type="table" target="#tab_5">Table 3</ref> shows the Effect of gated attention mechanism. We compared models with Gated C+ P2C and Simple C+ P2C. The MIU accuracy of the P2C  model has over 10% improvement when changing the operate pattern of the extra information proves the effect of GA mechanism. The Gated C+ P2C achieves the best in DC corpus, suggesting that the gated-attention works extremely well for handling long and diverse context.  not surprise that straightforward concatenation strategy for source inputs performs poorly when the input pinyin is incomplete in DC corpus, due to obvious noise in too long context. The rela- tively small gap between the results of CoCat and CoCat indicate that statistical learning model may be helpful in obtaining some useful patterns from limited input. When the input statement contains adequacy information, the MIU accuracy of Gated C+ P2C system achieves more than 20% improve- ment in both corpora. However, we find that the KySS scores are much more close even with dif- ferent pinyin integrity, which indicates that user experience in terms of KySS are more hard im- proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of P2C modules with Different Input Forms</head><p>Instance Analysis We input a dialogue in won- der to how much of the contextual information is used when P2C module find the input pinyin is un- known. <ref type="figure" target="#fig_2">Figure 2</ref> demonstrates the effect of the gated attention mechanism on candidates offering and unknown word replacement. As shown in <ref type="figure" target="#fig_2">Fig- ure 2(a)</ref>, we find that our IME suggests a more suitable candidates to the user when user is ob- viously not consistent with what the model has learned previously, which shows our model ex- ceeds the Simple C+ P2C learning for maximally matching the inputted pinyin, but become capable of effectively resisting user pinyin input noise, and turns to learn potential language knowledge in pre- vious input history 3 .</p><p>As the ability predict user input from incom- plete pinyin cannot be covered by any current IME performance metrics, thus the reported re- sults yielded by our model actually underestimate our model performance to some extent. We illus- trate the empirical discoveries of <ref type="figure" target="#fig_2">Figure 2(b)</ref> to demonstrate the extra effect of our P2C system on such situation, which indicates that the gated- attention pattern has taken great advantage of con- textual information when given an unknown word. Or, namely, our model enables the incomplete in- put prediction though has to let it outside the cur- rent IME performance measurement. We display the attention visualization of <ref type="figure" target="#fig_2">Figure 2</ref>(b) in <ref type="figure" target="#fig_3">Figure  3</ref> for better reference to explain the effect extended context plays on the generation of target charac- ters.</p><p>Main Result Our model is compared to other models in <ref type="table" target="#tab_5">Table 3</ref>. So far, ( <ref type="bibr" target="#b11">Huang et al., 2015)</ref> and ( <ref type="bibr" target="#b28">Zhang et al., 2017</ref>) reported the state-of-the- art results among statistical models. We list the top-5 accuracy contrast to all baselines with top- 10 results, and the comparison indicates the no- ticeable advancement of our P2C model. To our surprise, the top-5 result on PD of our best Gated C+ P2C system approaches the top-10 accuracy of Google IME. On DC corpus, our Gated C+ P2C model with the best setting achieves 90.14% accu- racy, surpassing all the baselines. The comparison shows our gated-attention system outperforms all state-of-the-art baselines with better user experi- ence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>For the first time, this work makes an attempt to introduce additional context in neural pinyin-to- character converter for pinyin-based Chinese IME as to our best knowledge. We propose a gated- attention enhanced model for digging significant context information to improve conversion qual- ity. More importantly, the resulting IME supports incomplete user pinyin input but returns complete, extra and even corrected character outputs, which brings about a story-telling mode change for all existing IMEs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of the candidates list given by the proposed IMEs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention visualization. Deeper color mean larger value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Data statistics for the sentence number and sen- tence length (L) of two corpora. Our model is evaluated on two datasets, namely the People's Daily (PD) corpus and Douban con- versation (DC) corpus. The former is extracted from the People's Daily from 1992 to 1998 that has word segmentation annotations by Peking University. The DC corpus is created by (Wu et al., 2017) from Chinese open domain conversa- tions. One sentence of the DC corpus contains one complete utterance in a continuous dialogue situ- ation. The statistics of two datasets is shown in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>CoCat 49.72 0.7393 48.78 0.7868 Basic P2C 52.31 0.7305 47.95 0.7879 Simple C+ P2C 23.83 0.5431 43.95 0.7495 Gated C+ P2C 53.76 0.8896 48.31 0.7931 CoCat 59.15 0.7651 61.42 0.7933 Basic P2C 71.31 0.8845 70.5 0.8301 Simple C+ P2C 61.28 0.7963 60.87 0.7883 Gated C+ P2C 73.89 0.8935 70.98 0.8407</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The effect of P2C modules with Different Input Forms. means that the input is incomplete.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 shows the evaluation results of P2C modules with different input forms. It should DC PD Top-1 Top-5 Top-10 KySS Top-1 Top-5 Top-10</head><label>2</label><figDesc></figDesc><table>KySS 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison with previous state-of-the-art models. 

</table></figure>

			<note place="foot" n="2"> https://github.com/pytorch/pytorch</note>

			<note place="foot" n="3"> Note as we evaluate our model only on two available corpora, but not the real world case from true user inputting history, which makes the instance situation limit to the domain feature of the given corpora.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Architecture of the Evaluation Models</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep enhanced representation for implicit discourse relation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="571" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hybrid model for Chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yuzhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Zhongye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Asian LowResource Language Information Process</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and accurate neural word segmentation for Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="608" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A full end-to-end semantic role labeler, syntaxagnostic or syntax-aware?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2753" to="2765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network language model for Chinese pinyin input method engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC-29</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="455" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional and joint models for grapheme-to-phoneme conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurospeech 2003-INTERSPEECH 2003</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2033" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new statistical approach to Chinese pinyin input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Fu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="241" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1832" to="1846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Syntax for semantic role labeling, to be, or not to be</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Shexia He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongshen</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2061" to="2071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new input method for human translators: integrating machine translation effectively and imperceptibly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1163" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moon ime: Neural-based Chinese pinyin aided input method with customizable association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="140" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph model for Chinese spell checking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGHAN-7</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="88" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kyss 1.0: a framework for automatic evaluation of Chinese input method engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1195" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A joint graph model for pinyin-to-Chinese conversion with typo correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pinyin to character conversion model based on support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">Long</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><forename type="middle">Quan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Chinese Information Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="100" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seq2seq dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shexia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3203" to="3214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel statistical Chinese language model and its application in pinyin-tocharacter conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1433" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2016. I can guess what you mean: A monolingual query enhancement for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCL</title>
		<imprint>
			<biblScope unit="volume">10035</biblScope>
			<biblScope unit="page" from="50" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic article commenting: the task and dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural machine translation with extended context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Finding better subword segmentation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lsequential matching network: A new architecture for multi-turn response selection in retrieval-based chatbots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="496" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A machine translation approach for Chinese wholesentence pinyin-to-character conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Asian Pacific conference on language and information and computation</title>
		<meeting>the 26th Asian Pacific conference on language and information and computation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural network language model for Chinese pinyin input method engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACLIC-29</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="455" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Tracing a loose wordhood for Chinese input method engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04158</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rulebased post-processing of pin to Chinese characters conversion system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Chinese Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Subword-augmented embedding for cloze reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3740" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective characteraugmentedword embedding for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling multi-turn conversation with deep utterance aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1802" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One-shot learning for question-answering in gaokao history challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="449" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lingke: A fine-grained multi-turn chatbot for customer service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuosheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoLING: System Demonstrations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="108" to="112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
