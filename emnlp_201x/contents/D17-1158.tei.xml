<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hieber</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Amazon Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1500" to="1505"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language model-ing and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of mono-lingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, neural encoder-decoder models <ref type="bibr" target="#b9">(Kalchbrenner and Blunsom, 2013;</ref>) have significantly advanced the state of the art in NMT, and now con- sistently outperform Statistical Machine Transla- tion (SMT) ( <ref type="bibr">Bojar et al., 2016</ref>). However, their suc- cess hinges on the availability of sufficient amounts of parallel data, and contrary to the long line of research in SMT, there has only been a limited amount of work on how to effectively and effi- ciently make use of monolingual data which is typically amply available. We propose a modi- fied neural sequence-to-sequence model with atten- tion ( <ref type="bibr" target="#b13">Luong et al., 2015b</ref>) that uses multi-task learning on the decoder side to jointly learn two strongly related tasks: target-side language modeling and translation. Our approach does not require any pre-translation or pre-training to learn from monolingual data and thus provides a principled way to integrate monolingual data re- sources into NMT training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Gülçehre et al. (2015) investigate two ways of inte- grating a pre-trained neural Language Model (LM) into a pre-trained NMT system: shallow fusion, where the LM is used at test time to rescore beam search hypothesis, requiring no additional fine- tuning and deep fusion, where hidden states of NMT decoder and LM are concatenated before making a prediction for the next word. Both com- ponents are pre-trained separately and fine-tuned together.</p><p>More recently, <ref type="bibr" target="#b17">Sennrich et al. (2016)</ref> have shown significant improvements by back-translating target-side monolingual data and using such syn- thetic data as additional parallel training data. One downside of this approach is the significantly in- creased training time, due to training of a model in the reverse direction and translation of monolingual data. In contrast, we propose to train NMT mod- els from scratch on both bilingual and target-side monolingual data in a multi-task setting.</p><p>Our approach aims to exploit the signals from target-side monolingual data to learn a strong lan- guage model that supports the decoder in making translation decisions for the next word. Our ap- proach further relates to <ref type="bibr" target="#b21">Zhang and Zong (2016)</ref>, who investigate multi-task learning for sequence- to-sequence models by strengthening the encoder using source-side monolingual data. A shared en- coder architecture is used to predict both, transla-tions of parallel source sentences and permutations of monolingual source sentences. In this paper we focus on target-side monolingual data and only up- date encoder parameters based on existing parallel data.</p><p>In a broader context, multi-task learning has shown to be effective in the context of sequence- to-sequence models ( <ref type="bibr" target="#b12">Luong et al., 2015a)</ref>, where different parts of the network can be shared across multiple tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural Machine Translation</head><p>We briefly recap the baseline NMT model <ref type="bibr" target="#b13">Luong et al., 2015b</ref>) and high- light architectural differences of our implementa- tion where necessary.</p><p>Given source sentence x = x 1 , ..., x n and target sentence y = y 1 , ..., y m , NMT models p(y|x) as a target language sequence model, conditioning the probability of the target word y t on the target his- tory y 1:t−1 and source sentence x. Each x i and y t are integer ids given by source and target vocabu- lary mappings, V src , V trg , built from the training data tokens. The target sequence is factorized as:</p><formula xml:id="formula_0">p(y|x; θ) = m t=1</formula><p>p(y t |y 1:t−1 , x; θ).</p><p>(1)</p><p>The model, parameterized by θ, consists of an en- coder and a decoder part ( ). For training set P consisting of parallel sentence pairs (x, y), we minimize the cross-entropy loss w.r.t θ:</p><formula xml:id="formula_1">L θ = (x,y)∈P − log p(y|x; θ).<label>(2)</label></formula><p>Encoder Given source sentence x = x 1 , ..., x n , the encoder produces a sequence of hidden states h 1 . . . h n through an Recurrent Neural Network (RNN), such that:</p><formula xml:id="formula_2">− → h i = f enc (E S x i , − → h i−1 ),<label>(3)</label></formula><p>where h 0 = 0, x i ∈ {0, 1} |Vsrc| is the one-hot encoding of x i , E S ∈ R e×|Vsrc| is a source embed- ding matrix with embedding size e, and f enc some non-linear function, such as the Gated Rectified Unit (GRU) ( ) or a Long Short- Term Memory (LSTM) (Hochreiter and Schmidhu- ber, 1997) network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentional Decoder</head><p>The decoder also consists of an RNN to predict one target word at a time through a state vector s:</p><formula xml:id="formula_3">s t = f dec ([E T y t−1 ; ¯ s t−1 ], s t−1 ),<label>(4)</label></formula><p>where y t−1 ∈ {0, 1} |Vtrg| is the one-hot encod- ing of the previous target word, E T ∈ R e×|Vtrg| the target word embedding matrix, f dec an RNN, s t−1 the previous state vector, and ¯ s t−1 the source- dependent attentional vector. The initial decoder hidden state is a non-linear transformation of the last encoder hidden state:</p><formula xml:id="formula_4">s 0 = tanh(W init h n + b init ).</formula><p>The attentional vector ¯ s t combines the de- coder state with a context vector c t :</p><formula xml:id="formula_5">¯ s t = tanh(W ¯ s [s t ; c t ]),<label>(5)</label></formula><p>where c t is a weighted sum of encoder hidden states: c t = n i=1 α ti h i and brackets denote vec- tor concatenation.</p><p>The attention vector α t is computed by an atten- tion network ( <ref type="bibr" target="#b13">Luong et al., 2015b)</ref>:</p><formula xml:id="formula_6">α ti = sof tmax(score(s t , h i )) score(s, h) = v a tanh(W u s + W v h).<label>(6)</label></formula><p>The next target word y t is predicted through a soft- max layer over the attentional vector ¯ s t :</p><formula xml:id="formula_7">p(y t |y 1:t−1 , x; θ) = sof tmax(W o ¯ s t + b o ) (7)</formula><p>where W o maps ¯ s t to the dimension of the target vocabulary. <ref type="figure" target="#fig_0">Figure 1a</ref> depicts this decoder archi- tecture. Note that source information from c indi- rectly influences the states s of the decoder RNN as it takes ¯ s as one of its inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incorporating Monolingual Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Separate Decoder LM layer</head><p>The decoder RNN <ref type="figure" target="#fig_0">(Figure 1a</ref>) is essentially a target- side language model, additionally conditioned on source-side sequences. Such sequences are not available for monolingual corpora and previous work has tried to overcome this problem by either using synthetically generated source sequences or using a NULL token as the source sequence <ref type="bibr" target="#b17">(Sennrich et al., 2016)</ref>. As previously shown empiri- cally, the model tends to "forget" source-side infor- mation if trained on much more monolingual than parallel data.  In our approach we explicitly define a source- independent network that only learns from target- side sequences (a language model), and a source- dependent network on top, that takes information from the source sequence into account (a transla- tion model) through the attentional vector ¯ s. For- mally, we modify the decoder RNN of Equation 4 to operate on the outputs an LM layer, which is independent of any source-side information:</p><note type="other">s t-1 c t softmax y t RNN LM r t r t-1 RNN TM y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN LM r t r t-1 softmax y t (a) baseline RNN y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN TM y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN LM r t r t-1 RNN TM y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN LM r t r t-1 softmax y t (b) +LML RNN y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN TM y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN LM r t r t-1 RNN TM y t-1 E T s t s̄ t s̄ t-1 s t-1 c t softmax y t RNN LM r t r t</note><formula xml:id="formula_8">s t = f dec ([r t ; ¯ s t−1 ], s t−1 )<label>(8)</label></formula><formula xml:id="formula_9">r t = f lm (E T y t−1 , r t−1 )<label>(9)</label></formula><p>Figure 1b illustrates this separation graphically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-task Learning</head><p>The separation from above allows us to train the target embeddings E T and f lm parameters from monolingual data, concurrent to training the rest of the network on bilingual data. Let us denote the source-independent parameters by σ. We connect a second loss to f lm to predict the next target word also conditioned only on target history informa- tion ( <ref type="figure" target="#fig_0">Figure 1c</ref>). Parameters for softmax layers are shared such that predictions of the LM layer are given by:</p><formula xml:id="formula_10">p(y t |y 1:t−1 , σ) = sof tmax(W o r t + b o ).<label>(10)</label></formula><p>Formally, for a heterogeneous data set Z = {P, M}, consisting of parallel and monolingual sentences (x, y), (y), we optimize the following joint loss:</p><formula xml:id="formula_11">L θ,σ = 1 |P| (x,y)∈P − log p(y|x; θ) +γ 1 |M| y∈M − log p(y; σ),<label>(11)</label></formula><p>where the source-independent parameters σ ⊂ θ are updated by gradients from both mono-and par- allel data examples, and source-dependent param- eters θ are updated only through gradients from parallel data examples. γ ≥ 0 is a scalar to influ- ence the importance of the monolingual loss. In practice, we construct mini-batches of training ex- amples, where 50% of the data is parallel, and 50% of the data is monolingual and set γ = 1. Since parts of the decoder are shared among both tasks and we optimize both loss terms concurrently, we view this approach as an instance of multi-task learning rather than transfer learning, where opti- mization is typically carried out sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conduct experiments for three different lan- guage pairs in the news domain: FR→EN, EN→DE, and CS→EN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>For EN→DE and CS→EN we use news- commentary-v11 as bilingual training data, <ref type="bibr">NewsCrawl 2015</ref>   <ref type="bibr" target="#b6">Glorot and Bengio, 2010)</ref> method. We use early stopping with respect to perplexity on the de- velopment set. We train each model configuration three times with different seeds and report average metrics across the three runs.</p><p>Further, we train models with synthetic parallel data generated through back-translation ( <ref type="bibr" target="#b17">Sennrich et al., 2016)</ref>. For this, we first train a baseline model in the reverse direction and then translate a random sample of 200k sentences from the mono- lingual target data. On the combined parallel and synthetic training data we train a new model with the same training hyper-parameters as the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model Layer</head><p>The architecture with an additional source-independent LM layer (+LML) is trained with the same hyper-parameters and data as the baseline model. The LM RNN uses a hidden size of 1024. The multi-task sys- tem (+LML + MTL) is trained on both parallel and monolingual data. In practice, all +LML +MTL models converge before seeing the entire mono- lingual corpus and at about the same number of updates as the baseline. <ref type="table">Table 1</ref> shows results on the held-out test sets. We observe that a separate LM layer does not sig- nificantly impact performance across all metrics. Adding monolingual data in the described multi- task setting improves translation performance by a small but consistent margin across all metrics. Interestingly, the improvements from monolingual data are additive to the gains from ensembling of 3 models with different random seeds. However, the use of synthetic parallel data still outperforms our approach both in single and ensemble systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>While separating out a language model allowed us to carry out multi-task training on mixed data types, it constrains gradients from monolingual data examples to a subset of source-independent network parameters (σ). In contrast, synthetic data always affects all network parameters (θ) and has a positive effect despite source sequences being noisy. We speculate that training from synthetic source data may also act as a model regularizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We proposed a way to directly integrate target-side monolingual data into NMT through multi-task learning. Our approach avoids costly pre-training processes and jointly trains on bilingual and mono- lingual data from scratch. While initial results show only moderate improvements over the baseline and fall short against using synthetic parallel data, we believe there is value in pursuing this line of re- search further to simplify training procedures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the proposed decoder architecture. (a) Baseline model with a single-layer decoder RNN and attention (b) Addition of a source-independent LM layer that feeds into the source-dependent decoder (c) Multi-task setting next-word prediction from both layers; green softmax layers are shared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>as monolingual data, and news development and test sets from</figDesc><table>System 

Data 
EN→DE 
FR→EN 
CS→EN 

baseline 
20.3 39.9 63.0 21.7 27.5 59.1 17.0 24.4 65.2 
+ LML 
20.4 39.8 63.1 21.3 27.2 59.8 16.9 24.4 65.4 
+ LML + MTL 
+ mono 
21.4 40.8 61.4 22.3 27.7 58.3 17.2 24.7 64.3 

Sennrich et al. (2016) 
+ synthetic 24.4 43.4 56.4 27.4 31.5 52.1 21.2 27.5 59.4 

ensemble baseline 
22.2 41.6 60.6 23.9 29.1 56.4 18.3 25.5 63.0 
+ LML 
22.4 41.8 60.9 23.5 28.7 57.2 18.3 25.6 63.4 
+ LML + MTL 
+ mono 
23.6 42.8 58.9 24.2 29.2 55.9 18.8 25.9 62.2 

ensemble Sennrich et al. (2016) + synthetic 25.7 44.6 55.0 29.1 32.6 50.3 22.5 28.4 57.8 

Table 1: BLEU/METEOR/TER scores on test sets for different language pairs. For BLEU and METEOR 
higher is better. For TER lower is better. 

WMT2016 (Bojar et al., 2016). For FR→EN 
we use newscommentary-v9 as bilingual data, 
NewsCrawl 2009-13 as monolingual data, and 
news development and test sets from WMT 
2014 (Bojar et al., 2014). The number of sentences 
for these corpora is shown below: 

Data Set bilingual monolingual 

EN→DE 242,770 
51,315,088 
FR→EN 183,251 
51,995,709 
CS→EN 191,432 
27,236,445 

5.2 Experimental Setup 

We tokenize all data and apply Byte Pair Encod-
ing (BPE) (Sennrich et al., 2015) with 30k merge 
operations learned on the joined bilingual data. 
Models are evaluated in terms of BLEU (Papineni 
et al., 2002), METEOR (Lavie and Denkowski, 
2009) and TER (Snover et al., 2006) on tokenized, 
cased test data. Decoding is performed using beam 
search with a beam of size 5. We implement all 
models using MXNet (Chen et al., 2015) 1 . 

Baselines Our baseline model consists of a 1-
layer bi-directional LSTM encoder with an embed-
ding size of 512 and a hidden size of 1024. The 
1-layer LSTM decoder with 1024 hidden units uses 
an attention network with 256 hidden units. The 
model is optimized using Adam (Kingma and Ba, 
2014) with a learning rate of 0.0003, no weight 
decay and gradient clipping if the norm exceeds 
1.0. The batch size is set to 64 and the maximum 
sequence length to 100. Dropout (Srivastava et al., 
2014) of 0.3 is applied to source word embed-
dings and outputs of RNN cells. We initialize all 

1 Baseline systems are equivalent to an earlier version of 
Sockeye: https://github.com/awslabs/sockeye 

RNN parameters with orthogonal matrices (Saxe 
et al., 2013) and the remaining parameters with the 
Xavier (</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Findings of the 2014 workshop on statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Saint-Amand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation (WMT&apos;14)</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation (WMT&apos;14)<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="12" to="58" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Antonio Jimeno Yepes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<imprint>
			<pubPlace>Philipp Koehn, Varvara Logacheva</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Christof Monz</orgName>
		</respStmt>
	</monogr>
	<note>et al. 2016. Findings of the 2016 conference on machine translation (WMT16)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Proceedings of the First Conference on Machine Translation (WMT&apos;16)</title>
		<meeting>the First Conference on Machine Translation (WMT&apos;16)</meeting>
		<imprint>
			<biblScope unit="page" from="131" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AIStats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On using monolingual corpora in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huei-Chi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1503.03535</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The METEOR metric for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Denkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multitask sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>abs/1511.06114</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;15)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;15)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BLEU: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL&apos;02)</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics (ACL&apos;02)<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>abs/1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>abs/1508.07909</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL&apos;16</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL&apos;16<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Study of Translation Edit Rate with Targeted Human Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting source-side monolingual data in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
