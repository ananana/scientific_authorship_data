<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Global Visual Features into Attention-Based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
							<email>iacer.calixto@adaptcentre.ie</email>
							<affiliation key="aff0">
								<orgName type="institution">ADAPT Centre Dublin City University School of Computing Glasnevin</orgName>
								<address>
									<settlement>Dublin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@adaptcentre.ie</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ADAPT Centre Dublin City University School of Computing Glasnevin</orgName>
								<address>
									<settlement>Dublin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Global Visual Features into Attention-Based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="992" to="1003"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce multi-modal, attention-based Neural Machine Translation (NMT) models which incorporate visual features into different parts of both the encoder and the decoder. Global image features are extracted using a pre-trained convo-lutional neural network and are incorporated (i) as words in the source sentence, (ii) to initialise the encoder hidden state, and (iii) as additional data to initialise the decoder hidden state. In our experiments , we evaluate translations into En-glish and German, how different strategies to incorporate global image features compare and which ones perform best. We also study the impact that adding synthetic multi-modal, multilingual data brings and find that the additional data have a positive impact on multi-modal models. We report new state-of-the-art results and our best models also significantly improve on a comparable Phrase-Based Statistical MT (PBSMT) model trained on the Multi30k data set according to all metrics evaluated. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model on all met-rics evaluated on this data set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural Machine Translation (NMT) has recently been proposed as an instantiation of the sequence to sequence (seq2seq) learning problem <ref type="bibr" target="#b22">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b7">Cho et al., 2014b;</ref><ref type="bibr" target="#b43">Sutskever et al., 2014)</ref>. In this problem, each train- ing example consists of one source and one tar- get variable-length sequence, with no prior infor- mation regarding the alignments between the two. A model is trained to translate sequences in the source language into corresponding sequences in the target. This framework has been successfully used in many different tasks, such as handwritten text generation <ref type="bibr" target="#b18">(Graves, 2013)</ref>, image description generation ( <ref type="bibr" target="#b20">Hodosh et al., 2013;</ref><ref type="bibr" target="#b25">Kiros et al., 2014;</ref><ref type="bibr" target="#b30">Mao et al., 2014;</ref><ref type="bibr" target="#b13">Elliott et al., 2015;</ref><ref type="bibr" target="#b47">Vinyals et al., 2015)</ref>, machine trans- lation ( <ref type="bibr" target="#b7">Cho et al., 2014b;</ref><ref type="bibr" target="#b43">Sutskever et al., 2014</ref>) and video description generation ( .</p><p>Recently, there has been an increase in the num- ber of natural language generation models that explicitly use attention-based decoders, i.e. de- coders that model an intra-sequential mapping be- tween source and target representations. For in- stance, <ref type="bibr" target="#b48">Xu et al. (2015)</ref> proposed an attention- based model for the task of Image Description Generation (IDG) where the model learns to at- tend to specific parts of an image (the source) as it generates its description (the target). In MT, one can intuitively interpret this attention mechanism as inducing an alignment between source and tar- get sentences, as first proposed by <ref type="bibr">Bahdanau et al. (2015)</ref>. The common idea is to explicitly frame a learning task in which the decoder learns to attend to the relevant parts of the source sequence when generating each part of the target sequence.</p><p>We are inspired by recent successes in using attention-based models in both IDG and NMT. Our main goal in this work is to propose end-to- end multi-modal NMT models which effectively incorporate visual features in different parts of the attention-based NMT framework. The main con- tributions of our work are:</p><p>• We propose novel attention-based multi- modal NMT models which incorporate visual features into the encoder and the decoder.</p><p>• We discuss the impact that adding synthetic multi-modal and multilingual data brings to multi-modal NMT.</p><p>• We show that images bring useful informa- tion to an NMT model and report state-of- the-art results.</p><p>One additional contribution of our work is that we corroborate previous findings by <ref type="bibr" target="#b47">Vinyals et al. (2015)</ref> that suggested that using image features di- rectly as additional context to update the hidden state of the decoder (at each time step) prevents learning.</p><p>The remainder of this paper is structured as fol- lows. In §1.1 we briefly discuss relevant previous related work. We then revise the attention-based NMT framework and further expand it into differ- ent multi-modal NMT models ( §2). In §3 we intro- duce the data sets we use in our experiments. In §4 we detail the hyperparameters, parameter initiali- sation and other relevant details of our models. Fi- nally, in §6 we draw conclusions and provide some avenues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>Attention-based encoder-decoder models for MT have been actively investigated in recent years. Some researchers have studied how to improve at- tention mechanisms ( <ref type="bibr" target="#b29">Luong et al., 2015;</ref><ref type="bibr" target="#b44">Tu et al., 2016</ref>) and how to train attention-based models to translate between many languages ( <ref type="bibr" target="#b11">Dong et al., 2015;</ref><ref type="bibr" target="#b15">Firat et al., 2016)</ref>.</p><p>However, multi-modal MT has only recently been addressed by the MT community in the form of a shared task ( . We note that in the official results of this first shared task no submissions based on a purely neural architecture could improve on the Phrase-Based SMT (PB- SMT) baseline. Nevertheless, researchers have proposed to include global visual features in re- ranking n-best lists generated by a PBSMT sys- tem or directly in a purely NMT framework with some success <ref type="bibr" target="#b1">(Caglayan et al., 2016;</ref><ref type="bibr" target="#b4">Calixto et al., 2016;</ref><ref type="bibr" target="#b28">Libovick´yLibovick´y et al., 2016;</ref><ref type="bibr" target="#b38">Shah et al., 2016)</ref>. The best results achieved by a purely NMT model in this shared task are those of <ref type="bibr" target="#b21">Huang et al. (2016)</ref>, who proposed to use global and regional image features extracted with the VGG19 <ref type="bibr" target="#b39">(Simonyan and Zisserman, 2014</ref>) and the RCNN ( <ref type="bibr" target="#b17">Girshick et al., 2014</ref>) convolutional neural networks (CNNs).</p><p>Similarly to one of the three models we pro- pose, 1 <ref type="bibr" target="#b21">Huang et al. (2016)</ref>  <ref type="table">extract global features  for an image, project these features into the vector  space of the source words and then add it as a word  in the input sequence. Their best model improves  over a strong NMT baseline and is comparable</ref> to results obtained with a PBSMT model trained on the same data, although not significantly bet- ter. For that reason, their models are used as base- lines in our experiments. Next, we point out some key differences between the work of <ref type="bibr" target="#b21">Huang et al. (2016)</ref> and ours.</p><p>Architecture Their implementation is based on the attention-based model of <ref type="bibr" target="#b29">Luong et al. (2015)</ref>, which has some differences to that of <ref type="bibr">Bahdanau et al. (2015)</ref>, used in our work ( §2.1). Their en- coder is a single-layer unidirectional LSTM and they use the last hidden state of the encoder to ini- tialise the decoder's hidden state, therefore indi- rectly using the image features to do so. We use a bi-directional recurrent neural network (RNN) with GRU ( <ref type="bibr" target="#b6">Cho et al., 2014a</ref>) as our encoder, bet- ter encoding the semantics of the source sentence.</p><p>Image features We include image features sep- arately either as a word in the source sen- tence ( §2.2.1) or directly for encoder ( §2.2.2) or decoder initialisation ( §2.2.3), whereas <ref type="bibr" target="#b21">Huang et al. (2016)</ref> only use it as a word. We also show it is better to include an image exclusively for the en- coder or the decoder initialisation <ref type="table" target="#tab_3">(Tables 1 and 2)</ref>.</p><p>Data <ref type="bibr" target="#b21">Huang et al. (2016)</ref> use object detections obtained with the RCNN of <ref type="bibr" target="#b17">Girshick et al. (2014)</ref> as additional data, whereas we study the impact that additional back-translated data brings.</p><p>Performance All our models outperform <ref type="bibr" target="#b21">Huang et al. (2016)</ref>'s according to all metrics evaluated, even when they use additional object detections.</p><p>If we use additional back-translated data, the dif- ference becomes even larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention-based NMT</head><p>In this section, we briefly revise the attention- based NMT framework ( §2.1) and expand it into a multi-modal NMT framework ( §2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-only attention-based NMT</head><p>We follow the notation of <ref type="bibr">Bahdanau et al. (2015)</ref> and <ref type="bibr" target="#b15">Firat et al. (2016)</ref> throughout this section.</p><p>Given a source sequence X = (x 1 , x 2 , · · · , x N ) and its translation Y = (y 1 , y 2 , · · · , y M ), an NMT model aims at building a single neural network that translates X into Y by directly learning to model p(Y |X). Each x i is a row index in a source lookup matrix W x ∈ R |Vx|×dx (the source word embeddings matrix) and each y j is an index in a target lookup matrix W y ∈ R |Vy|×dy (the target word embeddings matrix). V x and V y are source and target vocabularies and d x and d y are source and target word embeddings dimensionalities, re- spectively.</p><p>A bidirectional RNN with GRU is used as the encoder. A forward RNN − → Φ enc reads X word by word, from left to right, and gener- ates a sequence of forward annotation vectors</p><formula xml:id="formula_0">( − → h 1 , − → h 2 , · · · , − → h N ) at each encoder time step i ∈ [1, N ].</formula><p>Similarly, a backward RNN ← − Φ enc reads X from right to left, word by word, and gener- ates a sequence of backward annotation vectors</p><formula xml:id="formula_1">( ← − h 1 , ← − h 2 , · · · , ← − h N ), as in (1): − → h i = − → Φ enc W x [x i ], − → h i−1 , ← − h i = ← − Φ enc W x [x i ], ← − h i+1 .</formula><p>(1) The final annotation vector for a given time step i is the concatenation of forward and backward vec- tors</p><formula xml:id="formula_2">h i = − → h i ; ← − h i .</formula><p>In other words, each source sequence X is encoded into a sequence of annotation vectors h = (h 1 , h 2 , · · · , h N ), which are in turn used by the decoder: essentially a neural language model (LM) ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>) conditioned on the pre- viously emitted words and the source sentence via an attention mechanism.</p><p>At each time step t of the decoder, we compute a time-dependent context vector c t based on the annotation vectors h, the decoder's previous hid- den state s t−1 and the target word˜yword˜ word˜y t−1 emitted by the decoder in the previous time step. <ref type="bibr">2</ref> We follow <ref type="bibr">Bahdanau et al. (2015)</ref> and use a single-layer feed-forward network to compute an expected alignment e t,i between each source anno- tation vector h i and the target word to be emitted at the current time step t, as in <ref type="formula" target="#formula_3">(2)</ref>:</p><formula xml:id="formula_3">e t,i = v a T tanh(U a s t−1 + W a h i ).<label>(2)</label></formula><p>In Equation (3), these expected alignments are <ref type="bibr">2</ref> At training time, the correct previous target word yt−1 is known and therefore used instead of˜ytof˜ of˜yt−1. At test or in- ference time, yt−1 is not known and˜ytand˜ and˜yt−1 is used instead.  discussed problems that may arise from this difference between training and inference distributions. normalised and converted into probabilities:</p><formula xml:id="formula_4">α t,i = exp (e t,i ) N j=1 exp (e t,j ) ,<label>(3)</label></formula><p>where α t,i are called the model's attention weights, which are in turn used in computing the time-dependent context vector c t = N i=1 α t,i h i . Finally, the context vector c t is used in computing the decoder's hidden state s t for the current time step t, as shown in Equation <ref type="formula" target="#formula_5">(4)</ref>:</p><formula xml:id="formula_5">s t = Φ dec (s t−1 , W y [˜ y t−1 ], c t ),<label>(4)</label></formula><p>where s t−1 is the decoder's previous hidden state, W y [˜ y t−1 ] is the embedding of the word emitted in the previous time step, and c t is the updated time- dependent context vector.</p><p>We use a single-layer feed-forward neural net- work to initialise the decoder's hidden state s 0 at time step t = 0 and feed it the concatenation of the last hidden states of the encoder's forward RNN ( − → Φ enc ) and backward RNN ( ← − Φ enc ), as in <ref type="formula" target="#formula_6">(5)</ref>:</p><formula xml:id="formula_6">s 0 = tanh W di [ ← − h 1 ; − → h N ] + b di ,<label>(5)</label></formula><p>where W di and b di are model parameters. Since RNNs normally better store information about recent inputs in comparison to more distant ones <ref type="bibr" target="#b19">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr">Bahdanau et al., 2015)</ref>, we expect to initialise the de- coder's hidden state with a strong source sentence representation, i.e. a representation with a strong focus on both the first and the last tokens in the source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multi-modal NMT (MNMT)</head><p>Our models can be seen as expansions of the attention-based NMT framework described in §2 with the addition of a visual component to incor- porate image features.</p><p>Simonyan and Zisserman (2014) trained and evaluated an extensive set of deep Convolutional Neural Networks (CNNs) for classifying images into one out of the 1000 classes in ImageNet ( <ref type="bibr" target="#b35">Russakovsky et al., 2015</ref>). We use their 19-layer VGG network (VGG19) to extract image feature vec- tors for all images in our dataset. We feed an image to the pre-trained VGG19 network and use the 4096D activations of the penultimate fully- connected layer FC7 3 as our image feature vector, henceforth referred to as q.</p><p>We propose three different methods to incor- porate images into the attentive NMT framework:  using an image as words in the source sentence ( §2.2.1), using an image to initialise the source language encoder ( §2.2.2) and the target language decoder ( §2.2.3).</p><p>We also evaluated a fourth mechanism to incor- porate images into NMT, namely to use an image as one of the different contexts available to the de- coder at each time step of the decoding process. We added the image features directly as an addi- tional context, in addition to W y [˜ y t−1 ], s t−1 and c t , to compute the hidden state s t of the decoder at a given time step t. We corroborate previous findings by <ref type="bibr" target="#b47">Vinyals et al. (2015)</ref> in that adding the image features as such prevents the model from learning. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Images as source words: IMG W</head><p>One way we propose to incorporate images into the encoder is to project an image feature vector into the space of the words of the source sentence. We use the projected image as the first and/or last word of the source sentence and let the attention model learn when to attend to the image represen- tation. Specifically, given the global image feature vector q ∈ R 4096 , we compute (6): ).</p><formula xml:id="formula_7">d = W 2 I · (W 1 I · q + b 1 I ) + b 2 I ,<label>(6)</label></formula><p>An illustration of this idea is given in <ref type="figure" target="#fig_1">Figure 1a</ref>, where a source sentence that originally contained N tokens, after including the image as source words will contain N + 1 tokens (model IMG 1W ) or N + 2 tokens (model IMG 2W ). In model IMG 1W , the image is projected as the first source word only (solid line in <ref type="figure" target="#fig_1">Figure 1a)</ref>; in model IMG 2W , it is projected into the source words space as both first and last words (both solid and dashed lines in <ref type="figure" target="#fig_1">Figure 1a)</ref>.</p><p>Given a sequence X = (x 1 , x 2 , · · · , x N ) in the source language, we concatenate the transformed image vector d to W x [X] and apply the forward and backward encoder RNN passes, generating hidden vectors as in <ref type="figure" target="#fig_1">Figure 1a</ref>. When comput- ing the context vector c t (Equations <ref type="formula" target="#formula_3">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref> respectively, as illustrated in <ref type="figure" target="#fig_1">Figure 1b</ref>. Similarly to §2.2.1, given a global image feature vector q ∈ R 4096 , we compute a vector d using Equation <ref type="formula" target="#formula_7">(6)</ref>, only this time the parameters W 2 I and b 2 I project the image features into the same di- mensionality as the textual encoder hidden states.</p><p>The feed-forward networks used to initialise the encoder hidden state are computed as in <ref type="formula" target="#formula_8">(7)</ref>:</p><formula xml:id="formula_8">← − h init = tanh W f d + b f , − → h init = tanh W b d + b b ,<label>(7)</label></formula><note type="other">where W f and W b are multi-modal projection matrices that project the image features d into the encoder forward and backward hidden states di- mensionality, respectively, and b f and b b are bias vectors.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Images for decoder initialisation:</head><p>IMG D To incorporate an image into the decoder, we in- troduce a new single-layer feed-forward neural network to be used instead of the one described in Equation 5. Originally, the decoder's initial hid- den state was computed using the concatenation of the last hidden states of the encoder forward</p><formula xml:id="formula_9">RNN ( − → Φ enc ) and backward RNN ( ← − Φ enc ), respec- tively − → h N and ← − h 1 .</formula><p>Our proposal is that we include the image fea- tures as additional input to initialise the decoder hidden state at time step t = 0, as in <ref type="formula" target="#formula_10">(8)</ref>:</p><formula xml:id="formula_10">s 0 = tanh W di [ ← − h 1 ; − → h N ] + W m d + b di ,<label>(8)</label></formula><p>where W m is a multi-modal projection matrix that projects the image features d into the decoder hid- den state dimensionality and W di and b di are the same as in Equation <ref type="formula" target="#formula_6">(5)</ref>. Once again we compute d by applying Equa- tion (6) onto a global image feature vector q ∈ R 4096 , only this time the parameters W 2 I and b 2 I project the image features into the same dimen- sionality as the decoder hidden states. We illus- trate this idea in <ref type="figure" target="#fig_1">Figure 1c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data set</head><p>Our multi-modal NMT models need bilingual sen- tences accompanied by one or more images as training data. The original Flickr30k data set con- tains 30K images and 5 English sentence descrip- tions for each image ( <ref type="bibr" target="#b49">Young et al., 2014</ref>). We use the translated and the comparable Multi30k datasets ( , henceforth referred to as M30k T and M30k C , respectively, which are multilingual expansions of the original Flickr30k. </p><note type="other">of the 30K images in the Flickr30k, the M30k T has one of its English descriptions manually translated into German by a professional translator. Training, validation and test sets con- tain 29K, 1014 and 1K images, respectively, each accompanied by one sentence pair (the original English sentence and its German translation). For each of the 30K images in the Flickr30k, the M30k C has five descriptions in German collected independently of the English descriptions. Train- ing, validation and test sets contain 29K, 1014 and 1K images, respectively, each accompanied by 5 English and 5 German sentences. We use the scripts in Moses (Koehn et al., 2007) to normalise, truecase and tokenize English and German descriptions and we also convert space- separated tokens into subwords (Sennrich et al., 2016b). All models use a common vocabulary of ∼83K English and ∼91K German subword to- kens. If sentences in English or German are longer than 80 tokens, they are discarded.</note><p>We use the entire M30k T training set for train- ing, its validation set for model selection with BLEU, and its test set to evaluate our models. In order to study the impact that additional train- ing data brings to the models, we use the base- line model described in §2 trained on the textual part of the M30k T data set (German→English and English→German) without the images to build back-translation models ( <ref type="bibr" target="#b36">Sennrich et al., 2016a</ref>). We back-translate the 145K German (English) de- scriptions in the M30k C into English (German) and include the triples (synthetic English descrip- tion, German description, image) as additional training data when translating into German, and (synthetic German description, English descrip- tion, image) when translating into English.</p><p>We train models to translate from English into German and from German into English, and re- port evaluation of cased, tokenized sentences with punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>Our encoder is a bidirectional RNN with GRU (one 1024D single-layer forward RNN and one 1024D single-layer backward RNN). Source and target word embeddings are 620D each and both are trained jointly with our model. All non- recurrent matrices are initialised by sampling from a Gaussian (µ = 0, σ = 0.01), recurrent matrices are orthogonal and bias vectors are all initialised to zero. Our decoder RNN also uses GRU and is a neural LM ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>) conditioned on its previous emissions and the source sentence by means of the source attention mechanism. Image features are obtained by feeding im- ages to the pre-trained VGG19 network of Si- monyan and Zisserman (2014) and using the ac- tivations of the penultimate fully-connected layer FC7. We apply dropout with a probability of 0.2 in both source and target word embeddings and with a probability of 0.5 in the image features (in all MNMT models), in the encoder and decoder RNNs inputs and recurrent connections, and be- fore the readout operation in the decoder RNN. We follow <ref type="bibr" target="#b16">Gal and Ghahramani (2016)</ref> and apply dropout to the encoder bidirectional RNN and de- coder RNN using the same mask in all time steps.</p><p>Our models are trained using stochastic gra- dient descent with Adadelta (Zeiler, 2012) and minibatches of size 40 for improved generalisa- tion ( <ref type="bibr" target="#b24">Keskar et al., 2017)</ref>, where each training in- stance consists of one English sentence, one Ger- man sentence and one image. We apply early stop- ping for model selection based on BLEU scores, so that if a model does not improve on the valida- tion set for more than 20 epochs, training is halted.</p><p>We evaluate our models' translation qual- ity quantitatively in terms of BLEU4 ( <ref type="bibr" target="#b33">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b9">(Denkowski and Lavie, 2014</ref>), TER ( <ref type="bibr" target="#b40">Snover et al., 2006</ref>), and chrF3 scores 5 (Popovi´cPopovi´c, 2015) and we report statisti- cal significance for the three first metrics us- ing approximate randomisation computed with <ref type="bibr">MultEval (Clark et al., 2011)</ref>.</p><p>As our main baseline we train an attention- based NMT model ( §2) in which only the tex- tual part of M30k T is used for training. We also train a PBSMT model built with Moses on the same English→German (German→English) data, where the LM is a 5-gram LM with modified Kneser-Ney smoothing <ref type="bibr" target="#b26">(Kneser and Ney, 1995)</ref> trained on the German (English) of the M30k T dataset. We use minimum error rate training <ref type="bibr" target="#b32">(Och, 2003</ref>) for tuning the model parameters for BLEU scores. Our third baseline (English→German), is the best comparable multi-modal model by <ref type="bibr" target="#b21">Huang et al. (2016)</ref> and also their best model with addi- tional object detections: respectively models m1 (image at head) and m3 in the authors' paper. Fi- nally, our fourth baseline (German→English) is <ref type="bibr">5</ref> We specifically compute character 6-gram F3 scores. <ref type="bibr">BLEU4↑</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METEOR↑ TER↓ chrF3↑</head><p>English→German   <ref type="bibr" target="#b31">Miller, 1995)</ref> to re- rank n-best lists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results</head><p>The Multi30K dataset contains images and bilin- gual descriptions. Overall, it is a small dataset with a small vocabulary whose sentences have simple syntactic structures and not much ambigu- ity ( . This is reflected in the fact that even the simplest baselines perform fairly well on it, i.e. the smallest BLEU scores of 32.9 for translating into German, which are still reason- ably good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BLEU4↑</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METEOR↑ TER↓ chrF3↑</head><p>English→German original training data IMG <ref type="bibr">2W</ref> 36 </p><formula xml:id="formula_11">IMG 2W 42.4 † ‡ (↓ 0.2) 39.0 † ‡ (↑ 0.1) 34.7 † ‡ (↓ 1.4) 67.6 (↑ 0.0) IMG E 43.9 † ‡ (↑ 1.3) 39.7 † ‡ (↑ 0.8) 34.8 † ‡ (↓ 1.3) 68.7 (↑ 1.1) IMG D 43.4 † ‡ (↑ 0.8) 39.3 † ‡ (↑ 0.4) 35.2 † ‡ (↓ 0.9) 67.8 (↑ 0.2)</formula><p>Improvements (original vs. + back-translated) <ref type="table">Table 2</ref>: BLEU4, METEOR, TER and chrF3 scores on the M30k T test set for models trained on original and additional back-translated data. Best text-only baselines are underscored and best over- all results in bold. We highlight in parentheses the improvements brought by our models com- pared to the best baseline score. Results differ significantly from PBSMT baseline ( †) or NMT baseline ( ‡) with p = 0.05. We also show the improvements each model yields in each metric when only trained on the original M30k T training set vs. also including additional back-translated data. PBSMT + is the best model in the multi- modal MT shared task ( ).</p><formula xml:id="formula_12">English→German / German→English IMG 2W ↓ 0.2 / ↑ 2.9 ↑ 0.1 / ↑ 1.9 ↑ 0.1 / ↓ 2.4 ↑ 0.0 / ↑ 3.8 IMG E ↑ 1.4 / ↑ 2.8 ↑ 0.7 / ↑ 2.0 ↓ 1.8 / ↓ 3.1 ↑ 0.7 / ↑ 2.9 IMG D ↑ 1.2 / ↑ 2.1 ↑ 0.8 / ↑ 1.5 ↓ 1.2 / ↓ 2.7 ↑ 0.7 / ↑ 2.1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi30k</head><p>In <ref type="table" target="#tab_3">Table 1</ref>, we show results for translating from English→German and German→English. When translating into Ger- man, our multi-modal models perform well, with models IMG E and IMG D improving on both baselines according to all metrics analysed. We also note that all models but IMG 2W+D perform consistently better than the strong multi-modal NMT baseline of <ref type="bibr" target="#b21">Huang et al. (2016)</ref>, even when this model has access to more data (+RCNN features). <ref type="bibr">6</ref> Combining image features in the encoder and the decoder at the same time does not seem to improve results compared to using the image features in only the encoder or the decoder when translating into German. To the best of our knowledge, it is the first time a purely neural model significantly improves over a PBSMT model in all metrics on this data set. When translating into English, all multi-modal models significantly improve over the NMT baseline, with the only exception being model IMG 2W 's BLEU scores. In this scenario, model IMG E+D is the best performing one according to all but one metric. However, differences between multi-modal models are not statistically signifi- cant, i.e. all multi-modal models but IMG <ref type="bibr">2W</ref> and IMG 2W+D perform comparably.</p><p>Additional back-translated data Arguably, the main downside of applying multi-modal NMT in a real-world scenario is the small amount of pub- licly available training data (∼30K entries). For that reason, we back-translated the German and English sentences in the M30k C and created two sets of 145K synthetic triples, one for each trans- lation direction, as described in §3.</p><p>In <ref type="table">Table 2</ref>, we present results for some of the models evaluated in <ref type="table" target="#tab_3">Table 1</ref> but when also trained on the additional data. In order to add more data to our PBSMT baseline, we simply added the German sentences in the M30k C to train the LM. <ref type="bibr">7</ref> We also include results for PBSMT + , which uses image features as well as additional features extracted using WordNet ( <ref type="bibr" target="#b38">Shah et al., 2016)</ref>. When translating into German, both our models IMG E and IMG D that use global im- age features to initialise the encoder and the de- coder, respectively, significantly improve accord- ing to BLEU, METEOR and TER with the ad- ditional back-translated data, and also achieved better chrF3 scores. When translating into En- glish, IMG E is the only model to significantly im- prove over both baselines according to all met- rics with the additional back-translated data, also improving chrF3 scores. We highlight that our best-performing model IMG E significantly outper- forms PBSMT + according to BLEU and TER, and all our other multi-modal models perform com- parably to it. This is a noteworthy finding, since same data. <ref type="bibr">7</ref> Adding the synthetic sentence pairs to train the baseline PBSMT model, as we did with all neural MT models, deteri- orated the results.   PBSMT + uses image features as well as additional data from WordNet and, to the best of our knowl- edge, is the best published model in this language pair and data set to date.</p><p>Ensemble decoding We now report on how can ensemble decoding be used to improve transla- tions obtained with our multi-modal NMT mod- els. In order to do that, we use different combi- nations of models trained on the original M30k T training set to translate from English into German. We built ensembles of different models by start- ing with our best performing multi-modal model on this language pair and data set, IMG D , and by adding new models to the ensemble one by one, until we reach a maximum of four independent models, all of which are trained separately and on the original M30k T training data only. In <ref type="table" target="#tab_6">Table 3</ref>, we show results when translating the M30k T 's test set. These models were also evaluated in our re- cent participation in the WMT 2017 multi-modal MT shared task ( <ref type="bibr">Calixto et al., 2017a</ref>). We first note that to add more models to the en- semble seems to always improve translations, and by a considerable margin (∼ 3 BLEU/METEOR points). Adding model IMG 2W to the ensemble already consisting of models IMG E and IMG D im- proves translations according to all metrics evalu- ated. This is an interesting result, since compared to these other two multi-modal models, model IMG 2W performs poorly according to BLEU, ME- TEOR and chrF3. Regardless of that fact, our best results are obtained with an ensemble of four different multi-modal models, including model IMG <ref type="bibr">2W</ref> .</p><p>By using an ensemble of four different multi- modal NMT models trained on the translated Multi30k training data, we were able to obtain translations comparable to or even better than those obtained with the strong multi-modal NMT model of <ref type="bibr" target="#b5">Calixto et al. (2017b)</ref>, which is pre- trained on large amounts of English-German data and uses local image features. Finally, we have recently participated in the WMT 2017 multi- modal MT shared task, and our system submis- sions ranked among the best performing systems under the constrained data regime <ref type="bibr">(Calixto et al., 2017a</ref>). We note that our models performed par- ticularly well on the ambiguous MSCOCO test set ( <ref type="bibr" target="#b12">Elliott et al., 2017)</ref>, which indicate its abil- ity to use the image information in disambiguating difficult source sentences into their correct transla- tions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Error Analysis</head><p>In <ref type="table" target="#tab_8">Table 4</ref> we show translations into German gen- erated by different models for one entry of the M30k test set. In this example, the last three multi- modal models extrapolate the reference+image and describe "ceremony" as a "wedding cere- mony" (IMG 2W ) and as an "Olympics ceremony" (IMG E and IMG D ). This could be due to the fact that the training set is small, depicts a small variation of different scenes and contains different forms of biasses <ref type="bibr" target="#b45">(van Miltenburg, 2015)</ref>.</p><p>In <ref type="table" target="#tab_9">Table 5</ref>, we draw attention to an example where some models generate what seems to be novel visual terms. Neither the source German sentence nor the English reference translation con- tained the translated units "having fun" or "Mex- ican restaurant", although both could have been inferred at least partially from the image. In this example, the visual term "having fun" is also gen- erated by the baseline NMT model, making it clear that at times what seems like a translation ex- tracted exclusively from the image may have been learnt from the training text data. However, none of the two baselines translated "Mexikanischen Setting" as "Mexican restaurant", but four out of the five multi-modal models did. The multi-modal models also had problems translating the German "trinkt Shots" (drinking shots). We observe trans- lations such as "having drinks" (IMG 2W ), which src.</p><p>a woman with long hair is at a graduation ceremony . ref.</p><p>eine Frau mit langen Haaren bei einer Abschluss Feier.   although not a novel translation is still correct, but also "drinking apples" (IMG E ), "drinking food" (IMG D ), and "drinking dishes" (IMG E+D ), which are clearly incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future work</head><p>In this work, we introduced models that incor- porate images into state-of-the-art attention-based NMT, by using images as words in the source sen- tence, to initialise the encoder's hidden state and as additional data in the initialisation of the de- coder's hidden state. The intuition behind our ef- fort is to use images to visually ground transla- tions, and consequently increase translation qual- ity. We demonstrate with extensive experiments that adding global image features into NMT sig- nificantly improves the translations of image de- scriptions compared to text-only NMT and PB- SMT. It also improves significantly on the previ- ous state-of-the-art model of <ref type="bibr">Huang et al. (2016) (English→German)</ref>, and performs comparably to the best published results of <ref type="bibr">Shah et al. (2016) (German→English)</ref>. Overall, we note that using images as words in the source sequence (IMG 1W , IMG 2W ), an idea similarly entertained by <ref type="bibr" target="#b21">Huang et al. (2016)</ref>, does not fare as well as to directly incorporate the image either in the encoder or the decoder (IMG E and IMG D ), independently of the target language. The fact that multi-modal NMT models can benefit from back-translated data is also an interesting finding.</p><p>In future work, we will conduct a more sys- tematic study on the impact that synthetic back- translated data brings to multi-modal NMT, and run an error analysis to identify what particular types of errors our models make (and prevent).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>a) An encoder bidirectional RNN that uses image features as words in the source sequence. (b) Using an image to initialise the en- coder hidden states. (c) Image as additional data to initialise the decoder hidden state s0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-modal neural machine translation models IMG W , IMG E , and IMG D .</figDesc><graphic url="image-1.png" coords="4,72.00,62.81,140.59,98.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>For each</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>BLEU4, METEOR, chrF3 (higher is bet-
ter) and TER scores (lower is better) on the M30k T 
test set for the two text-only baselines PBSMT 
and NMT, the two multi-modal NMT models by 
Huang et al. (2016) (English→German only) and 
our MNMT models that: (i) use images as words 
in the source sentence (IMG 1W , IMG 2W ), (ii) use 
images to initialise the encoder (IMG E ), and (iii) 
use images as additional data to initialise the de-
coder (IMG D ). Best text-only baselines are un-
derscored and best overall results appear in bold. 
We highlight in parentheses the improvements 
brought by our models compared to the best cor-
responding text-only baseline score. Results differ 
significantly from PBSMT baseline ( †) or NMT 
baseline ( ‡) with p = 0.05. 

the best-performing model in the WMT'16 multi-
modal MT shared task (Shah et al., 2016), hence-
forth PBSMT + . It uses image features as well as 
additional data from WordNet (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for different combinations of multi-modal models, all trained on the original M30k T 
training data only, evaluated on the M30k T test set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>eine Frau mit langen Haaren ist an einer StaZeremonie. PBSMT eine Frau mit langen Haaren steht an einem Abschluss IMG 1W eine Frau mit langen Haaren ist an einer warmen Zeremonie teil</head><label></label><figDesc></figDesc><table>IMG 2W eine Frau mit langen Haaren steht bei einer Hochzeit Feier. 
IMG E 
eine lang haarige Frau bei einer olympischen Zeremonie. 
IMG D 
eine lang haarige Frau bei einer olympischen Zeremonie. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Translations for the 668th example in the M30k test set. 

src. 
eine Gruppe junger Menschen trinkt Shots in einem Mexikanischen Setting . 
ref. 
a group of young people take shots in a Mexican setting . 

NMT 
a group of young people are having fun in an auditorium . 
PBSMT 
a group of young people drinking at a Shots Mexikanischen Setting . 
IMG 2W 
a group of young people having drinks in a Mexican restaurant . 
IMG E 
a group of young people drinking apples in a Mexican restaurant . 
IMG D 
a group of young people drinking food in a Mexican restaurant . 
IMG 2W+D a group of young people having fun in a Mexican room . 
IMG E+D 
a group of young people drinking dishes in a Mexican restaurant . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Translations for 300th example in the M30k test set.</figDesc><table></table></figure>

			<note place="foot" n="1"> This idea has been developed independently by both research groups.</note>

			<note place="foot" n="3"> We use the activations of the FC7 layer, which encode information about the entire image, of the VGG19 network (configuration E) in Simonyan and Zisserman (2014)&apos;s paper.</note>

			<note place="foot" n="4"> Outputs would typically consist of sets of 2-5 words repeated many times, usually without any syntax. For comparison, translations for the translated Multi30k test set (described in §3) achieve just 3.8 BLEU (Papineni et al., 2002), 15.5 METEOR (Denkowski and Lavie, 2014) and 93.0 TER (Snover et al., 2006).</note>

			<note place="foot" n="6"> In fact, model IMG2W+D still improves on the multimodal baseline of Huang et al. (2016) when trained on the</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This project has received funding from Science Foundation Ireland in the ADAPT Centre for Dig-ital Content Technology (www.adaptcentre. ie) at Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Re-gional Development Fund and the European Union Horizon 2020 research and innovation pro-gramme under grant agreement 645452 (QT21). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Does Multimodality Help Human and Machine for Translation and Image Captioning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Koel Dutta Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>2017a. DCU System Report on the WMT</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-modal Machine Translation Task</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DCU-UvA Multimodal MT System Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2359" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="634" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Conference of the Association for Computational Linguistics</title>
		<meeting>the 55th Conference of the Association for Computational Linguistics<address><addrLine>Long Papers. Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Paper Accepted</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation page 103</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Longterm Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, US</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-Task Learning for Multiple Language Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P15-1166" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-language image description with neural sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>CoRR abs/1510.04709</idno>
		<ptr target="http://arxiv.org/abs/1510.04709" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual English-German Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W/W16/W16-3210.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language, VL@ACL 2016. Berlin, Germany</title>
		<meeting>the 5th Workshop on Vision and Language, VL@ACL 2016. Berlin, Germany</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N16-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="866" to="875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Theoretically Grounded Application of Dropout in Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2014.81</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.81" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA, CVPR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generating Sequences With Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno>CoRR abs/1308.0850</idno>
		<ptr target="http://arxiv.org/abs/1308.0850" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="doi">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Framing Image Description As a Ranking Task: Data, Models and Evaluation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-based Multimodal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015<address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>CoRR abs/1411.2539</idno>
		<ptr target="http://arxiv.org/abs/1411.2539" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing<address><addrLine>Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Detroit</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moses: Open Source Toolkit for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1557769.1557821" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions<address><addrLine>Prague, Czech Republic, ACL &apos;07</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Tlust´ytlust´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2361" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1410.1090" />
		<title level="m">Explain Images with Multimodal Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="doi">10.1145/219717.219748</idno>
		<ptr target="https://doi.org/10.1145/219717.219748" />
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<idno type="doi">10.3115/1075096.1075117</idno>
		<ptr target="https://doi.org/10.3115/1075096.1075117" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Sapporo, Japan, ACL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, Pennsylvania, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics. Philadelphia, Pennsylvania, ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">chrf: character n-gram fscore for automatic mt evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovi´cpopovi´c</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W15-3049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="doi">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SHEF-Multimodal: Grounding Machine Translation on Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josiah</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Shared Task on</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multimodal Machine Translation and Crosslingual Image Description</title>
		<ptr target="http://aclweb.org/anthology/W/W16/W16-2346.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation, WMT 2016</title>
		<meeting>the First Conference on Machine Translation, WMT 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling Coverage for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1008" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Stereotyping and bias in the flickr30k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emiel Van Miltenburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Multimodal Corpora, MMC-2016. Portorož</title>
		<meeting>the Workshop on Multimodal Corpora, MMC-2016. Portorož<address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sequence to Sequence-Video to Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, Massachusetts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://jmlr.org/proceedings/papers/v37/xuc15.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
