<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Evaluation of Multimodal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
							<email>de@di.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Evaluation of Multimodal Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2974" to="2978"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2974</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The promise of combining vision and language in multimodal machine translation is that systems will produce better translations by leveraging the image data. However, inconsistent results have lead to uncertainty about whether the images actually improve translation quality. We present an adversarial evaluation method to directly examine the utility of the image data in this task. Our evaluation measures whether multimodal translation systems perform better given either the congruent image or a random incongruent image, in addition to the correct source language sentence. We find that two out of three publicly available systems are sensitive to this perturbation of the data, and recommend that all systems pass this evaluation in the future.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multimodal machine translation is the task of translating sentences situated in a visual context, such as captioned images on social media. The core argument of this area of research is that we can produce better translations by exploiting both the source language sentence and the visual con- text ( <ref type="bibr" target="#b4">Elliott et al., 2015;</ref><ref type="bibr" target="#b8">Hitschler et al., 2016)</ref>. There is some evidence to support this argument for human translation: <ref type="bibr" target="#b6">Frank et al. (2018)</ref> found that 13% of the German evaluation data in the Multi30K dataset ) needed at least one post-edit to reflect the joint meaning of the visual and linguistic context. However, the evidence that visual context helps computational models is less clear. Consider the three teams that submitted contrastive multimodal and text-only variants of their systems to the 2017 Multimodal Translation Shared Task ( <ref type="bibr" target="#b3">Elliott et al., 2017)</ref>: the University of Le Mans' multimodal system out- performed their text-only variant (Caglayan et al., * Work carried out at the University of Edinburgh.</p><p>Two dogs play with an orange toy in tall grass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Zwei Hunde spielen im hohen Gras mit einem orangen Spielzeug.  <ref type="bibr" target="#b10">Libovick´y and Helcl, 2017)</ref>. In light of these re- sults, we need a better understanding of the role of visual context in multimodal translation systems.</p><p>We propose an adversarial evaluation Method to determine whether multimodal translation sys- tems are aware of the visual context. We introduce a measure of image awareness to quantify the dif- ference in performance in two settings: (i) when a system is presented with congruent visual data; (ii) when it is presented with incongruent visual data. In both settings, a system is presented with the correct source language sentence. See <ref type="figure" target="#fig_0">Figure  1</ref> for an illustration of our evaluation. We hypoth- esise that if a system is aware of the visual con- text, i.e. it is actually using the image for trans- lation, then the system will perform better when it is presented with the congruent visual data than incongruent visual data. Our evaluation is related to the foiled image captions evaluation, in which the performance of an image captioning system is measured when a single word is replaced with an incorrect, but similar word ( <ref type="bibr" target="#b12">Shekhar et al., 2017)</ref>; the main difference is that we replace the visual data instead of manipulating the text. Our work is also related to a study of question-answering systems, in which additional text was appended to the end of a document <ref type="bibr" target="#b9">(Jia and Liang, 2017)</ref>. They found that these additional text segments dis- tracted QA systems from producing the correct an- swer. In contrast, our evaluation does not manipu- late the textual data, instead we replace the origi- nal visual input with a random distractor.</p><p>We evaluate three publicly available multimodal translation systems with our adversarial evalua- tion. The main finding of this paper is that one publicly available multimodal translation system is not aware of the congruent image data. This finding raises doubts about whether state-of-the- art multimodal translation systems actually use the visual context to produce better translations. We conclude this paper by discussing whether this is likely to be due to problems with the data or with the model architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adversarial Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Awareness</head><p>We propose an adversarial evaluation method for multimodal machine translation. This method measures how a system performs when it is pre- sented with the correct text data and either the con- gruent image or with an incongruent image. In this section we define two image awareness functions to measure whether a multimodal translation sys- tem is aware of the congruent visual data.</p><p>Let x be a source language sentence, y be a tar- get language sentence, v be the congruent image, and ¯ v be an incongruent image. Image awareness is calculated using an evaluable performance mea- sure E. The overall image awareness of a model M on an evaluation dataset D is:</p><formula xml:id="formula_0">∆-Awareness = 1 |D| |D| i a M (x i , y i , v i , ¯ v i ) (1)</formula><p>The image awareness of a model M for a single instance a M (x i , y i , v i , ¯ v i ) is given by:</p><formula xml:id="formula_1">a M (x i , y i , v i , ¯ v i ) = E(x i , y i , v i ) − (2) E(x i , y i , ¯ v i )</formula><p>Under this definition, the output of the evalu- able performance measure should be higher in the presence of the congruent data than the incongru- ent data, i.e. E(x i , y i , v i ) &gt; E(x i , y i , ¯ v i ). 1 If this is the case, on average, then the overall image aware- ness of a model ∆-Awareness is positive. This can only happen when model outputs are evaluated more favourably in the presence of the the congru- ent image data than the incongruent image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model-internal awareness ∆ I</head><p>A model-internal image measure of awareness is the difference in the probability assigned to the target language sentence y in the congruent and incongruent conditions. This is model-internal because it has the same form as the maximum- likelihood objective used to train the translation model. In this case, E = p(y|x, ·), and the dif- ference in performance for a single instance is:</p><formula xml:id="formula_2">a M = ∆ I = p(y i |x i , v i ) − p(y i |x i , ¯ v i )<label>(3)</label></formula><p>2.3 Model-external awareness ∆ E A model-external awareness measure could be a text-similarity evaluation or human judgement. In this paper, we use the Meteor text-similarity score <ref type="bibr" target="#b2">(Denkowski and Lavie, 2014</ref>) because it naturally decomposes to the sentence level, and it is already the de-facto evaluation metric for multimodal ma- chine translation ( . Let E be any text-similarity scoring function T that decom- poses to the sentence level. The difference in per- formance for a single instance is defined as:</p><formula xml:id="formula_3">a M = ∆ E = T (x i , y i , v i ) − T (x i , y i , ¯ v i ) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Systems Evaluation</head><p>We evaluate the image awareness of three pre- trained multimodal translations systems that we received by direct correspondence:</p><formula xml:id="formula_4">decinit:</formula><p>The initial state of the decoder net- work is set with a learned transformation of the visual data ( <ref type="bibr" target="#b1">Caglayan et al., 2017)</ref>.</p><p>trgmul: The target language word embed- dings are modulated by an element-wise mul- tiplication with a learned transformation of the visual data ( <ref type="bibr" target="#b1">Caglayan et al., 2017)</ref>.</p><formula xml:id="formula_5">C I ∆ E -Awareness</formula><p>trgmul 57.3 57.3 ± 0.2 -0.001 ± 0.002 decinit 57.0 56.8 ± 0.1 0.003 ± 0.001 hierattn 55.0 53.3 ± 0.3 0.019 ± 0.003 hierattn: The decoder network learns to se- lectively attend to a combination of the source language and the visual data (Li- bovick´ybovick´y and Helcl, 2017).</p><p>Each system was trained on the 29,000 English- German-image tuples in the Multi30K dataset . We evaluate the image aware- ness of these systems using the 1,014 tuples in the validation data, which is typically used for model selection. We select the incongruent images ¯ v by randomly shuffling the order in which the images v are associated with the source language text x. In our evaluation, we report the mean and stan- dard deviation of randomly shuffling the image data five times. The code to evaluate your own system is publicly available. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Statistical test</head><p>To determine if a model passes the proposed eval- uation, we conduct a non-parametric Wilcoxon signed-rank test of the following hypothesis: H 1 : Congruent images improve the quality of multimodal translation compared to incongruent images. H 0 : Congruent images make no difference to the quality of multimodal translation compared to incongruent images.</p><p>We conduct this statistical test using the pairs of values that are calculated in the process of com- puting the the image awareness scores (Eq. 2), i.e. E(x i , y i , v i ) and E(x i , y i , ¯ v i ). We combine the k=5 separate p values from each test using Fisher's method and reject the null hypothesis H 0 if the result of the χ 2 test with 2k degrees of freedom is p ≤ 0.005. 3 <ref type="table" target="#tab_0">Table 1</ref> shows the corpus-level results of a Meteor- based evaluation and the Meteor-awareness evalu- ation. We find that images improve the quality of the hierattn system (χ 2 = 136.74, p &lt; 0.0001), and images also improve the quality of the decinit system (χ 2 = 32.79, p = 0.0003). Images make no difference to the quality of the translations gener- ated by the trgmul system (χ 2 = 8.98, p = 0.533).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>To complement these tests, <ref type="figure" target="#fig_1">Figure 2</ref> shows vio- lin plots of the Meteor-awareness scores. These show that the translations generated by the trgmul and decinit systems are most likely to result in no difference in Meteor score between the congruent and incongruent conditions. We now turn our attention to the results of the probability-awareness evaluation. Images im- prove the quality of the trgmul system (χ 2 = 52.55, p &lt; 0.0001), and images also improve the quality of the hierattn system (χ 2 = 622.03, p &lt; 0.0001). Images make no difference to the quality of decinit system (χ 2 = 6.49, p = 0.772). <ref type="figure" target="#fig_2">Figure 3</ref> shows examples of translations pro- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meteor</head><p>Mann mit Mardi-Gras-Perlen um den Hals trägt Stange mit Banner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="45.9">40.9 (a) Congruent is better than Incongruent</head><p>Two cyclists cross the street on a very breezy California day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Zwei Radfahrerüberqueren Radfahrer¨Radfahrerüberqueren auf einer stark befahrenen Straße am Abend die Straße.</p><p>Zwei Radfahrerüberqueren Radfahrer¨Radfahrerüberqueren auf einer stark befahrenen Straße die Straße.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meteor</head><p>Zwei RadfahrerüberquerenRadfahrerüberqueren die Straße an einem sehr windigen Tag in Kalifornien.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="35.3">35.6</head><p>(b) Incongruent is better than Congruent shows an example with a negative difference in Meteor score. The congruent image results in a long trans- lation with poor coverage of the reference, which Meteor punishes more severely than the shorter translation arising from the incongruent image. In neither setting does the model translate the prepo- sitional phrase "on a very breezy California day".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data problems</head><p>We posit that the current Multi30K training data does not necessarily require systems to use the vi- sual context to solve the translation task.  note that the German translation data was produced without showing the translators the images, and <ref type="bibr" target="#b6">Frank et al. (2018)</ref> found that 13% of the Multi30K test data needed to be post-edited to reflect the joint semantics of both modalities. We recommend that entirety of the German Multi30K training data should be post-edited so that future systems are more likely to require a joint under- standing of the visual and linguistic context. <ref type="bibr">4</ref> We note that a similar issue was found in a visual ques- tion answering dataset, resulting in the creation of a new "balanced" dataset ( <ref type="bibr" target="#b7">Goyal et al., 2017</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The role of model architectures</head><p>The key difference between the systems evaluated in this paper is how they use the visual context. The hierattn system learns a timestep-dependent context vector over a location-preserving 3D vol- ume of image features, whereas the trgmul and decinit systems use an average-pool of the 3D location-preserving features. In our evaluation, the only system that is aware of the congruent image data for both types of image-awareness is the hi- erattn system that learns a spatial context over the image. Learning to attend to specific regions of the image may prove to be crucial to improving translations with visual context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed an adversarial evaluation method to determine whether multimodal translation systems are aware of the visual context. This evalua- tion method measures the difference in the perfor-mance of a system given the congruent or an in- congruent image as additional context. We found that two out of three publicly available multimodal translation systems were improved by the congru- ent visual context, when compared to the incon- gruent visual context. We encourage researchers to use this method to evaluate their own sys- tems. Future work includes augment existing mul- timodal translation models with an additional ad- versarial objective that forces the model to per- form better in the presence of the congruent im- age than a random incongruent image. We will also apply this evaluation method to other tasks that use additional context, e.g. images in visual- question answering, or part-of-speech tags in neu- ral machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An adversarial evaluation for multimodal translation. We measure the difference in performance when a model sees a congruent image (left) or an incongruent image (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Violin plots of the Meteor-awareness scores for evaluated models. The white dot marks the median value, the thick gray bar shows the interquartile range, and the thin gray bar is the 95% confidence interval. The width of the plots show the kernel density estimate of the distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of the difference in Meteor awareness for the hierattn system. In each example, the source sentence is shown at the top and the reference sentence is shown at the bottom, both in Typewriter font. The congruent image is on the left, and the incongruent image is on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Corpus-level Meteor scores in the 
Congruent and Incongruent settings, along with 
the Meteor-awareness results. Incongruent and 
∆ E -Awareness scores are the mean and standard 
deviation of five permutations of the visual data. 

</table></figure>

			<note place="foot" n="1"> This assumes that a higher score means better performance for the performance measure E. Swap the order of the operands if lower performance means better.</note>

			<note place="foot" n="3"> We use a stricter threshold to reduce the chance of false positive findings (Benjamin et al., 2018).</note>

			<note place="foot" n="4"> We planned to repeat the adversarial evaluation with the Multi30K French data, which was created by showing the annotators the images. However, we did not receive pre-trained models for all the systems for English-French translation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Barry Haddow asked if we knew whether the ad-ditional image data actually improved the quality of multimodal machine translation. I claimed that there was indisputable evidence, given the results of the human evaluation at the 2017 Multimodal Translation Shared Task. I am not sure if I agree with myself any more.</p><p>Desmond Elliott is supported by an Amazon Research Award. Stella Frank, ´ Akos Kádár, Emiel van Miltenburg, and the Edinburgh NLP group provided valuable feedback on this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Redefine statistical significance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">O</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannesson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E-J</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wagenmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Kenneth A Bollen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Brembs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Camerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LIUM-CVC Submissions for WMT17 Multimodal Translation Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bardet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="215" to="233" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Multi-Language Image Description with Neural Sequence Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<idno>abs/1510.04709</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi30K: Multilingual EnglishGerman Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Vision and Language</title>
		<meeting>the 5th Workshop on Vision and Language</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="413" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal Pivots for Image Caption Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigehiko</forename><surname>Schamoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2399" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial Examples for Evaluating Reading Comprehension Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention Strategies for Multi-Source Sequence-to-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovick´ylibovick´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">OSU Multimodal Machine Translation System Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="465" to="469" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">FOIL it! Find One mismatch between Image and Language caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Klimovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurélie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Shared Task on Multimodal Machine Translation and Crosslingual Image Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
