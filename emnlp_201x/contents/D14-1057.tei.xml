<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparison of Selectional Preference Models for Automatic Verb Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 25-29, 2014. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Anglistik und Amerikanistik</orgName>
								<orgName type="institution">Humboldt University</orgName>
								<address>
									<postCode>10099</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Egg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut für Anglistik und Amerikanistik</orgName>
								<orgName type="institution">Humboldt University</orgName>
								<address>
									<postCode>10099</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparison of Selectional Preference Models for Automatic Verb Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="511" to="522"/>
							<date type="published">October 25-29, 2014. 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a comparison of different selec-tional preference models and evaluate them on an automatic verb classification task in German. We find that all the models we compare are effective for verb clustering; the best-performing model uses syntactic information to induce nouns classes from unlabelled data in an unsupervised manner. A very simple model based on lexical preferences is also found to perform well.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Selectional preferences <ref type="bibr" target="#b16">(Katz and Fodor, 1963;</ref><ref type="bibr" target="#b50">Wilks, 1975;</ref><ref type="bibr" target="#b31">Resnik, 1993)</ref> are the tendency for a word to semantically select or constrain which other words may appear in a direct syntactic re- lation with it. Selectional preferences (SPs) have been a perennial knowledge source for NLP tasks such as word sense disambiguation <ref type="bibr" target="#b32">(Resnik, 1997;</ref><ref type="bibr" target="#b44">Stevenson and Wilks, 2001;</ref><ref type="bibr" target="#b24">McCarthy and Carroll, 2003</ref>) and semantic role labelling <ref type="bibr" target="#b9">(Erk, 2007)</ref>; and recognising selectional violations is thought to play a role in identifying and interpreting meta- phor <ref type="bibr" target="#b51">(Wilks, 1978;</ref><ref type="bibr" target="#b43">Shutova et al., 2013)</ref>. We focus on the SPs of verbs, since determining which argu- ments are typical of a given verb sheds light on the semantics of that verb.</p><p>In this study, we present the first empirical com- parison of different SP models from the perspective of automatic verb classification (Schulte im <ref type="bibr" target="#b41">Walde, 2009;</ref><ref type="bibr" target="#b47">Sun, 2012)</ref>, the task of grouping verbs to- gether based on shared syntactic and semantic prop- erties.</p><p>We cluster German verbs using features captur- ing their valency or subcategorisation, following prior work <ref type="bibr" target="#b38">(Schulte im Walde, 2000;</ref><ref type="bibr" target="#b10">Esteve Ferrer, 2004;</ref><ref type="bibr" target="#b40">Schulte im Walde, 2006;</ref><ref type="bibr" target="#b46">Sun et al., 2008;</ref><ref type="bibr" target="#b21">Li and Brew, 2008)</ref>, and investigate the effect of adding information about verb argument preferences. SPs are represented by features capturing lexical information about the heads of arguments to the verbs; we restrict our focus here to nouns.</p><p>We operationalise a selectional preference model as a function which maps such an argument head to a concept label. We submit that the primary characteristic of such a model is its granularity. In our baseline condition, all nouns are mapped to the same label; this effectively captures no information about a verb's SPs (i.e., we cluster verbs using sub- categorisation information only). On the other ex- treme, each noun is its own concept label; we term this condition lexical preferences (LP). Between the baseline and LP lie a spectrum of models, in which multiple concepts are distinguished, and each concept label can represent multiple nouns. Our main hypothesis is that verb clustering will work best using a model of such intermediate gran- ularity. This follows the intuition that verbs would seem to select for classes of nouns; for instance, we suppose that essen 'eat' would tend to prefer as a direct object a noun from the abstract concept Es- sen ('food'). We assume that these concepts can be expressed independently of particular predicates; that is, there exist selectional preference models that will work for all verbs (and all grammatical relations). Further benefits of grouping nouns into classes include combating data sparsity, as well as deriving models which can generalise to nouns unseen in training data.</p><p>Another parameter of a selectional preference model is the methodology used to induce the con- ceptual classes; put another way, the success of an SP model hinges on how it represents concepts. In this paper, we investigate the choice of noun categorisation method through an empirical com- parison of selectional preference models previously used in the literature.</p><p>We set out to investigate the following questions:</p><p>1. What classes of nouns are effective descriptors of selectional preference concepts? For ex- ample, do they correspond to features such as ANIMATE?</p><p>2. What is the appropriate granularity of selec- tional preference concepts?</p><p>3. Which methods of classifying nouns into con- cepts are most effective at capturing selec- tional preferences for verb clustering?</p><p>This paper is structured as follows: In Section 2, we introduce our baseline method of clustering verbs using subcategorisation information and de- scribe evaluation; Section 3 lists the models of se- lectional preferences that we compare in this work; Section 4 presents results and discussion; Section 5 summarises related work; and Section 6 concludes with directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Automatic verb classification</head><p>Verb classifications such as VerbNet <ref type="bibr">(KipperSchuler, 2005</ref>) allow generalisations about the syn- tax and semantics of verbs and have proven useful for a range of NLP tasks; however, creation of these resources is expensive and time-consuming. Auto- matic verb classification seeks to learn verb classes automatically from corpus data in a cheaper and faster way. This endeavour is possible due to the link between a verb's semantics and its syntactic be- haviour <ref type="bibr" target="#b19">(Levin, 1993)</ref>. Recent research has found that even automatically-acquired classifications can be useful for NLP applications <ref type="bibr" target="#b42">(Shutova et al., 2010;</ref><ref type="bibr" target="#b12">Guo et al., 2011)</ref>. In this section, we introduce the verb classification method used by our baseline model, which clusters verbs based on subcategor- isation information. Following this, Section 2.2 ex- plains the gold standard verb clustering and cluster purity metric which we use for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Baseline model</head><p>In this work, we take subcategorisation to mean the requirement of a verb for particular types of argument or concomitant. For example, the English verb put subcategorises for subject, direct object, and a prepositional phrase (PP) like on the shelf : A subcategorisation frame (SCF) describes a combination of arguments required by a specific verb; a description of the set of SCFs which a verb may take is called its subcategorisation preference.</p><p>We acquire descriptions of verbal SCF preferences on the basis of unannotated corpus data.</p><p>Our experiments use the SdeWaC corpus <ref type="bibr" target="#b11">(Faaß and Eckart, 2013)</ref>, containing 880 million words in 45 million sentences; this is a subset of deWaC ( <ref type="bibr" target="#b0">Baroni et al., 2009</ref>), a corpus of 10 9 words extrac- ted from Web search results. SdeWaC is filtered to include only those sentences which are max- imally parsable <ref type="bibr">1</ref> . We parsed SdeWaC with the mate-tools dependency parser <ref type="bibr" target="#b3">(Bohnet et al., 2013</ref>) 2 , which performs joint POS and morpholo- gical tagging, as well as lemmatisation. Our sub- categorisation analyses are delivered by the rule- based SCF tagger described by <ref type="bibr" target="#b34">Roberts et al. (2014)</ref>, which operates using the dependency parses and as- signs each finite verb an SCF type. The SCF tags are taken from the SCF inventory proposed by <ref type="bibr" target="#b39">Schulte im Walde (2002)</ref>, which indicates combinations of nominal and verbal complement types, such as nap:für.Acc (transitive verb, with a PP headed by für 'for'). Examples of complements are n for nominative subject, and a for accusative direct ob- ject; in SCFs which include PPs (p), the SCF tag specifies the head of the PP and the case of the pre- positional argument (Acc in our example indicates the accusative case of the prepositional argument). The SCF tagger undoes passivisation and analyses verbs embedded in modal and tense constructions. We record 673 SCF types in SdeWaC.</p><p>From SdeWaC, we extracted the first 3,000,000 verb instances assigned an SCF tag by the SCF tag- ger, where the verb lemma is one of the 168 listed in our gold standard clustering (this requires ap- proximately 270 million words of parsed text, or 25% of SdeWaC). We refer to this as our test set. In this set, each verb is seen on average 17,857 times; the most common is geben <ref type="bibr">('give', 328,952 instances)</ref>, and the least is grinsen ('grin', 50).</p><p>We represent verbs as vectors, where each di- mension represents a different SCF type. Vector entries are initialised with SCF code counts over the test set, and each vector is then normalised to sum to 1, so that a vector represents a discrete prob- ability distribution over the SCF inventory. We use the Jensen-Shannon divergence as a dissimilarity measure between pairs of verb vectors. The Jensen- Shannon divergence <ref type="bibr" target="#b22">(Lin, 1991)</ref> is an information- theoretic, symmetric measure (Equation (2)) re-lated to the Kullback-Leibler divergence <ref type="bibr">(Equation (3)</ref>).</p><formula xml:id="formula_0">JS(p, q) = D(p|| p + q 2 ) + D(q|| p + q 2 ) (2) D(p||q) = i p i log p i q i<label>(3)</label></formula><p>With this dissimilarity measure, we use hier- archical clustering with Ward's criterion <ref type="bibr" target="#b49">(Ward, Jr, 1963)</ref> to partition the verbs into K disjoint sets (i.e., hard clustering), where we match K to the number of classes in our gold standard (described below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation paradigm</head><p>We evaluate the automatically induced verb cluster- ings against a manually-constructed gold standard, published by Schulte im <ref type="bibr">Walde (2006, page 162ff.)</ref>. This Levin-style classification groups 168 high- and low-frequency verbs into 43 semantic classes; examples include Aspect (e.g., anfangen 'begin'), Propositional Attitude (e.g., denken 'think'), and Weather (e.g., regnen 'rain'). Some of the classes are further sub-classified; for the purposes of our evaluation, we ignore the hierarchical structure of the classification and consider each class or sub- class to be a separate entity. In this way, we obtain classes of fairly comparable size and sufficient se- mantic consistency. <ref type="bibr">3</ref> We evaluate a given verb clustering against the gold standard using the pairwise F -score ( <ref type="bibr" target="#b15">Hatzivassiloglou and McKeown, 1993)</ref>. To calcu- late this statistic, we construct a contingency table over the n 2 pairs of verbs, the idea being that the gold standard provides binary judgements about whether two verbs should be clustered together or not. If a clustering agrees with the gold standard as to whether a pair of verbs belong together or not, this is a "correct" answer. Using the contingency table, the standard information retrieval measures of precision (P ) and recall (R) can be computed; the F -score is then the harmonic mean of these: F = 2P R/(P + R). The random baseline is 2.08 (calculated as the average score of 50 random parti- tions), and the optimal score is 95.81, calculated by evaluating the gold standard against itself. As the gold standard includes polysemous verbs, which belong to more than one cluster, the optimal score is calculated by randomly picking one of their senses; the average is then taken over 50 such trials. The pairwise F -score is known to be somewhat nonlinear <ref type="bibr" target="#b40">(Schulte im Walde, 2006</ref>), penalising early clustering "mistakes" more than later ones, but it has the advantage that we can easily determ- ine statistical significance using the contingency table and McNemar's test.</p><p>We use only one clustering algorithm and one purity metric, because our prior work shows that the most important choices for verb clustering are the distance measure used, and how verbs are rep- resented. These factors set, we expect similar per- formance trends from different algorithms, with predictable variation (e.g., spectral tends to outper- form hierarchical clustering, which in turn outper- forms k-means). Combining Ward's criterion and F -score is a trade-off at this point; the criterion is deterministic, giving reproducible results without computational complexity, but disallows estimates of density over our evaluation metric and is greedy (see discussion in Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Selectional preference models</head><p>In this section, we introduce the various SP models that we compare in this paper. In all cases, we hold the verb clustering procedure described in the previous section unchanged, with the exception that SCF tags for verbs are parameterised for selectional preferences. As an example, a verb instance observed in a simple transitive frame with a nominal subject and accusative object would receive the SCF tag na. Assuming that a given SP model places the subject noun in the SP concept animate and the object noun in the concept concrete, the parameterised SCF tag would be na * subj-{animate} * obj-{concrete}. This process captures argument co-occurrence information about verb instances, and has the effect of multiplying the SCF inventory size, making the verb vectors described in Section 2.1 both longer and sparser.</p><p>We evaluate various types of SP models: the simple lexical preferences model; three models which perform automatic unsupervised induction of noun concepts from unlabelled data; and one which uses a manually-built lexical resource. As far as we are aware, two of these, the word space and LDA models, have never been applied to verb classification before. N Coverage of test set 100 12.08% 200 17.18% 500 26.11% 1,000 32.70% 5,000 45.31% 10,000 49.09% 50,000 55.69% 100,000 57.67% <ref type="table">Table 1</ref>: Fraction of verb instances in the test set parameterised by LP as a function of the number of nouns N included in the LP model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lexical preferences</head><p>The LP model is the simplest in our study after the baseline condition; it simply maps a noun to its own lemma. We include as a parameter of the LP model a maximum number of nouns N to admit as LP tags. In this way, the LP model parameterises SCFs using only the N most frequent nouns in SdeWaC; nouns beyond rank N are treated as if they were unseen. <ref type="table">Table 1</ref> indicates what fraction of the 3 million verb instances receive SCF tags specifying one or more LPs as a function of this parameter. Note that the coverage approaches an asymptote of around 60%. This is due to the fact that noun arguments are not observed for every verb instance; many verbs' arguments are pronominal or verbal and are not treated by our SP models. Setting N allows a simple way of tuning the LP model: With increasing N , the LP model should capture more data about verb instances, but after a point this benefit should be cancelled out by the increasing sparsity in the verb vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sun and Korhonen model</head><p>The SP model described in this section (SUN) was first used by <ref type="bibr" target="#b45">Sun and Korhonen (2009)</ref> to de- liver state-of-the-art verb classification perform- ance for English; more recently, the technique was applied to successfully identify metaphor in free text ( <ref type="bibr" target="#b42">Shutova et al., 2010;</ref><ref type="bibr" target="#b43">Shutova et al., 2013)</ref>. It uses co-occurrence counts that describe which nouns are found with which verbs in which gram- matical relations; this information is used to sort the nouns into classes in a procedure almost identical to our verb clustering method described in Sec- tion 2.1.</p><p>We extract all verb instances in SdeWaC which are analysed by the SCF tagger, and count all (verb, grammatical relation, nominal argument head) triples, where the grammatical relation is subject, direct (accusative) object, indirect (dative) object, or prepositional object 4 , and is listed in the verb instance's SCF tag; we undo passivisation, re- move instances of auxiliary and modal verbs, and filter out those triples seen less than 10 times in the corpus. These observations cover 60,870 noun types and 33,748,390 tokens, co-occurring with 6,705 verb types (11,426 verb-grammatical-relation types); an example is (sprechen, obj, Wort) ('speak' with dir- ect object 'word', occurring 1,585 times) <ref type="bibr">5</ref> . We rep- resent each noun by a vector whose 11,426 dimen- sions are the different verb-grammatical-relation pairs; coordinates in the vector indicate the ob- served corpus counts. The vectors are then norm- alised to sum to 1, such that each represents some particular noun's discrete probability distribution over the set of verb-grammatical-relation pairs. The distance between two noun vectors is defined to be the Jensen-Shannon divergence between their probability distributions, and we partition the set of nouns into M groups using hierarchical Ward's clustering.</p><p>The SP model then maps a noun to an arbitrary label indicating which of the M disjoint sets that noun is to be found in (i.e., all nouns in the first noun class map to the concept label concept1); we employ the parameter M to model SP concept granularity. As with the LP model, we use the parameter N to indicate how many nouns are in- cluded in the SUN model; we search the parameter values N = {300, 500, 1000, 5000, 10000} and N M = {5, 10, 15, 20, 30, 50}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word space model</head><p>Word space models <ref type="bibr">(WSMs, (Sahlgren, 2006;</ref><ref type="bibr" target="#b48">Turney and Pantel, 2010)</ref>) use word co-occurrence counts to represent the distributional semantics of a word. This strategy makes possible a clustering of nouns that does not depend on verbal dependencies in the first place. <ref type="bibr" target="#b8">Dagan et al. (1999)</ref> address the problem of data sparseness for the automatic determination of word co-occurrence probabilities, which includes selec- tional preferences. They introduce the idea of es- timating the probability of hitherto unseen word combinations using available information on words that are closest w.r.t. distributional word similar- ity. Following this idea, <ref type="bibr" target="#b9">Erk (2007)</ref> and <ref type="bibr" target="#b28">Padó et al. (2007)</ref> describe a memory-based SP model, using a WSM similarity measure to generalise the model to unseen data.</p><p>We build a WSM of German nouns and use it to partition nouns into disjoint sets, which we then employ as with the SUN model. We compute word co-occurrence counts across the whole SdeWaC corpus, using as features the 50,000 most common words in SdeWaC, skipping the first 50 most com- mon words (i.e., we use words 50 through 50,050), with sentences as windows. We lemmatise the cor- pus and remove all punctuation; no other normalisa- tion is performed. Co-occurrence counts between a word w i and a feature c j are weighted using the t-test scheme:</p><formula xml:id="formula_1">ttest(w i , c j ) = p(w i , c j ) − p(w i )p(c j ) p(w i )p(c j )</formula><p>We use a recent technique called context selec- tion ( <ref type="bibr" target="#b30">Polajnar and Clark, 2014</ref>) to improve the word space model, whereby only the C most highly weighted features are kept for each word vector. We set C by optimising the correlation between the word space model's cosine similarity and a data set of human semantic relatedness judgements for 65 word pairs <ref type="bibr" target="#b13">(Gurevych and Niederlich, 2005</ref>); at C = 380, we obtain Spearman ρ = 0.813 and Pear- son r = 0.707 (human inter-annotator agreement for this data set is given as r = 0.810).</p><p>After this, we build a similarity matrix between all pairs of nouns using the cosine similarity, and then partition the set of N nouns into M disjoint classes using spectral clustering with the MNCut algorithm <ref type="bibr" target="#b25">(Meil˘ a and Shi, 2001</ref>). As with the SUN model, this SP model assigns labels to nouns indic- ating which noun class they belong to. We search the same parameter space for N and M as for the SUN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GermaNet</head><p>Statistical models of SPs have often used WordNet as a convenient and well-motivated inventory of concepts (e.g., <ref type="bibr" target="#b32">Resnik (1997)</ref>, <ref type="bibr" target="#b20">Li and Abe (1998)</ref>, <ref type="bibr" target="#b7">Clark and Weir (2002)</ref>). Typically, such models make use of probabilistic treatments to determine an appropriate concept granularity separately for each predicate; we opt here for a simple model that allows more direct control over concept granularity. We take the set of concepts relevant to describing selectional preferences to be a target set of synsets in GermaNet ( <ref type="bibr" target="#b14">Hamp and Feldweg, 1997</ref></p><note type="other">), and rep- resent the target set as the set of synsets which are at some depth d or less in the GermaNet noun hier- archy: {s | depth(s) ≤ d} where depth(s) counts the number of hypernym links separating s from the root of the hierarchy. We model concept gran- ularity by varying d = 1 . . . 6; at d = 1, the target set is of size 5, and at d = 6, it is of size 17,125. Nouns are attributed to concepts as follows: Given a noun belonging to a synset s, either s is in the target set, or we take s's lowest hypernym in the target set. For polysemous nouns, each synset list- ing a sense of the noun votes for a member of the target set; the noun observation is then spread over the target set using the votes as weights.</note><p>This procedure makes our GermaNet SP model a soft clustering over nouns (i.e., a noun can belong to more than one SP concept); a consequence of this is that a single verb occurrence in the corpus can contribute fractional counts to multiple SCF types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">LDA</head><p>Latent Dirichlet allocation ( <ref type="bibr" target="#b2">Blei et al., 2003</ref>) is a generative model that discovers similarities in data using latent variables; it is frequently used for topic modelling. LDA models of SPs have been proposed by Ó Séaghdha (2010) and <ref type="bibr" target="#b33">Ritter et al. (2010)</ref>; previous to this, Rooth et al. (1999) also described a latent variable model of <ref type="bibr">SPs.</ref> We implement the LDA model of selectional pref- erences described by Ó Séaghdha (2010). Gener- atively, the model produces nominal arguments to verbs as follows: For a given (verb, grammatical re- lation) pair (v, r), (1) Sample a noun class z from a from a multinomial distribution Φ v,r with a Dirich- let prior parameterised by α; (2) Sample a noun n from a multinomial distribution Θ z with a Dirichlet prior parameterised by β. Like Ó Séaghdha, we use an asymmetric Dirichlet prior for Φ v,r (i.e., α can differ for each noun class) and a symmetric prior for Θ z (β is the same for each Θ z ). We estimate the LDA model using the MALLET software <ref type="bibr" target="#b23">(McCallum, 2002</ref>) using the same (verb, grammatical relation, argument head) co-occurrence statistics used for the SUN model. We train for 1,000 it- erations using the software's default parameters, allowing the LDA hyperparameters α and β to be re-estimated every 10 iterations. We build mod- els with 50 or 100 topics as a proxy to concept granularity; models include number of nouns N of {500, 1000, 5000, 10000, 50000, 100000}.</p><p>As with the GermaNet-based model, the LDA model creates a soft clustering of nouns; the abil- ity of a noun to have degrees of membership in multiple concepts might be a good way to model polysemy. We also experiment with a hard cluster- ing version of the LDA model; to do this, we assign each noun n its most likely class label z using the model's estimate for P (z|n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We experimented with applying the SP models to different combinations of grammatical relations (e.g., only subject, only object, subject+object, etc.), but generally obtained better results by para- meterising SCF tags for all grammatical relations. <ref type="table" target="#tab_1">Table 2</ref> summarises the evaluation scores and para- meter settings for the best-performing SP models, applied to verb arguments in all four grammatical relations (subject, direct, indirect and prepositional object) <ref type="bibr">6</ref> . The table also indicates the number of SCF types constructed by each SP model (i.e., the number of dimensions of the vectors representing verbs).</p><p>All the SP models we compare help with auto- matic verb clustering. Using McNemar's test on the contingency tables underlying the F -scores, all models score better than the baseline at at least the p &lt; 0.01 level. LDA-hard is better than the Ger- maNet, LDA-soft, WSM and LP models at at least the p &lt; 0.05 level; SUN is better (p ≤ 0.05) than all models except LDA-hard. All other performance differences are not statistically significant <ref type="bibr">7</ref> .</p><p>We can also demonstrate the effectiveness of the SP models with a regression analysis on the models' coverage of the test set. By varying the number of nouns N included in the SP models which use this parameter (LP, SUN, WSM, LDA), or by paramet- erising SCF tags with SP information only for par- <ref type="bibr">6</ref> Due to space constraints, we do not present here a de- tailed per-model study of performance as a function of para- meter settings; we feel a summary to be adequate, since the relative performances of the models reflect trends across a range of parameter settings. <ref type="bibr">7</ref> Using a significance criterion of p &lt; 0.05. ticular combinations of grammatical relations, dif- ferent numbers of the verb instances in the test data will end up with SP information in their SCF tags (this is the "coverage" statistic in <ref type="table">Table 1)</ref>; with the exception of the GermaNet model, all of the SP models we examine here show positive correlation between the number of verb instances tagged for SP information and verb clustering performance. This effect is independent of parameter settings, indicating the performance benefit conferred by the SP models is robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison of SP models</head><p>The GermaNet model is the least successful in our study. It achieves its best performance with a depth of 5; after this, verb clustering performance drops off again. Verb clustering using the GermaNet SP model is only slightly better than the baseline condition.</p><p>Against our expectations, the hard clustering LDA models perform better than the soft cluster- ing ones, achieving the second highest score in our evaluation; also, in contrast to the other SP mod- els studied in this paper, LDA performs best with fewer, coarser-grained topics. We observe that the soft clustering models produce verb vectors more than an order of magnitude longer than the hard clustering models, and suggest that simple soft clus- tering may be causing problems with data sparsity that interfere with verb clustering. We have also observed that the topics found by LDA do not rep- resent polysemy as we had hoped. While some of the topics discovered by the LDA models can be easily assigned labels (e.g., body parts, people, quantities, emotions, places, buildings, tools, etc.), others are less cohesive. We found that frequent words (e.g., time, person) are generated with high probability by multiple topics in ways that do not appear to reflect multiple word senses, and that the 100-topic models exhibit this property to a greater extent. For instance, Zeit 'time' is highly predict- ive of three topics in the 50-topic models, of which only the highest-weighted topic groups time ex- pressions together; in the 100-topic models, Zeit is found in six topics. Again, of these six, only the topic with the highest α consists of time expres- sions. In the 50-topic models, we find 11 topics that we cannot assign a coherent label; in the 100-topic models, there are 38 of these mismatched topics. In our work to date, we have not found that LDA models with greater numbers of topics find more   The LP model is very effective, which is surpris- ing given its simplicity. As expected, with increas- ing N , we do observe sparsity effects which hurt verb clustering performance (see <ref type="figure" target="#fig_0">Figure 1)</ref>. Our best performing model is SUN. Our best res- ult is obtained with 10,000 nouns (the maximum value of N that we tried) in 1,000 classes, giving relatively fine-grained classes (on average 10 nouns per class). <ref type="table">Table 3</ref> shows some example noun classes learned by the SUN model. These include: groups with synonyms or near synonyms, often in- cluding alternate spellings of the same word (such as in the truck grouping); and groups of closely- related co-hyponyms, such as the body part group- ing and the clothing grouping. In the latter, bill, joint responsibility, complicity and inscription are also included as things which can be borne, this is due to the fact that the SUN noun clustering is based on triples of verbs, grammatical relations, and nouns. LKW (truck), Lkw (truck), Lastwagen (truck), Castor (container for highly radioactive mater- ial), Laster (truck), Krankenwagen (ambulance), Transporter (van), Traktor (tractor)</p><formula xml:id="formula_2">Hand (hand), Kopf (head), Fuß (foot), Haar (hair), Bein (leg), Arm (arm), Zahn (tooth), Fell (fur) Leiche (corpse), Leichnam (body), Schädel (skull), Skelett (skeleton), Wrack (wreck), Mu- mie (mummy), Trümmer (debris) Sauna (sauna), Badezimmer (bathroom), Schwimmbad (swimming pool), Nachbildung (replica), Kamin (fireplace), Aufenthaltsraum (common room), Mensa (cafeteria)</formula><p>Rechnung (bill), Kopftuch (headscarf), Uniform (uniform), Anzug (suit), Helm (helmet), Gewand (garment), Handschuh (glove), Mitverantwor- tung (joint responsibility), Bart (beard), Rüs- tung (armour), Mitschuld (complicity), Socke (sock), Jeans (jeans), Sonnenbrille (sunglasses), Aufschrift (inscription), Pullover (sweater),</p><formula xml:id="formula_3">Weste (vest), Handschellen (handcuffs), Hörner (horns), Kennzeichen (marking), Tracht (tradi- tional costume), Korsett (corset), Schuhwerk (footwear), Kopfbedeckung (headgear), Pelz (fur), Maulkorb (muzzle)</formula><p>Missionar (missionary), Weihnachtsmann (Santa Claus), Selbstmordattentäter (sui- cide bomber), Bote (messenger), Nikolaus (Nicholas), Killer (killer), Bomber (bomber), Osterhase (Easter bunny) <ref type="table">Table 3</ref>: Example noun clusters in the SUN SP model. Furthermore, there are thematically related groups (corpse, body, etc., and sauna, bathroom, etc.). All months are placed together in one 12- word group.</p><p>Some classes can be easily subdivided into sep- arate groups, and sometimes the source for this can be guessed: For example, sports (football, golf, ten- nis) are lumped together with musical instruments (guitar, piano, violin) and film roles (starring role, supporting role), these all being things that can be played. Many groups of personal roles (such as various kinds of government ministers) are dis- tinguished, as are diseases and medications; other groupings contain proper names or geographical locations, sometimes of surprising specificity (e.g., authors, Biblical names, philosophers, NGOs, East- ern European countries, foreign currencies, Ger- man male first names, newspapers, television chan- nels). The last group in <ref type="table">Table 3</ref> shows a grouping which appears to combine two of these semantic- ally narrow categories, in which Santa Claus and the Easter bunny are united with killers and suicide bombers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Noun classes as SP concepts</head><p>The WSM SP model is not as successful as SUN, but, due to the methodological similarity between these two (SP concepts modelled as hard partitions of nouns), it affords us an opportunity to investigate the question of what properties might make for an effective noun partition.</p><p>The WSM model partitions nouns based on paradigmatic information (which sentence con- texts a noun appears in), rather than SUN's use of syntagmatic information (which grammatical contexts a noun appears in). Therefore, it is per- haps not surprising that the noun classes derived by the WSM are organised thematically, and the synonym/co-hyponym structure observed in the SUN noun classes is in many cases absent (e.g., {Pferd (horse), Reiter (rider), Stall (stable), Sattel (saddle), Stute (mare)}; these classes can easily conflate semantic roles (e.g., Agent for rider and Location for stable), which is presumably unhelp- ful for representing selectional preferences.</p><p>The distribution of noun classes also differs between SUN and WSM. The largest noun class in the WSM model contains 1,076 high-frequency nouns which are semantically unrelated (day, ques- tion, case, part, reason, kind, form, week, person, month, . . . ). We suppose that these nouns are them- atically "neutral" and are classed together by virtue of their usage in a wide variety of sentences. This one noun class by itself subsumes 13.6% of all noun tokens in SdeWaC. WSM also includes 56 singleton noun classes; the variance in noun class size is 2800. For comparison, in SUN, the largest noun class has 73 words, and the smallest, 2 (there are 12 of these two-word classes); noun class size variance is 37. The 73-word class in SUN does in- deed appear to be a grab bag (including gas, taboo, pioneer, mustard, spy, mafia, and skinhead), but these are uncommon words and account for only 0.1% of noun tokens in SdeWaC. The next two most common classes (with some 40 nouns each) are lists of names (politicians' surnames, and male first names). The noun class in the SUN model con- taining the largest number of high-frequency nouns (28 nouns: human, child, woman, man, people, Mr., mother, father, . . . ) only covers 3.6% of noun us- ages in SdeWaC and is both semantically cohesive and intuitively useful as a SP concept.</p><p>These issues raise the question of why the WSM model is effective at all for verb classification. We think that the larger less-related noun classes neither help nor hurt verb clustering, and we find that some of the thematic classes represent abstrac- tions that should be useful for describing SPs. Ex- amples include lists of body parts, countries (separ- ate classes for Europe, Africa, Asia, etc.), diseases, human names, articles of clothing, and the group {fruit, apple, banana, pear, strawberry}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effects of test set size</head><p>We were curious if the success of the LP model might be due to the size of the test set preventing sparsity from becoming a problem. To pursue this question, we take the four best performing SP mod- els and run the verb clustering evaluation with the number of verb instances in the test set varying between 10,000 and the full SdeWaC corpus (11 million). The results are displayed in <ref type="figure" target="#fig_2">Figure 2</ref>. This graph indicates that below 3 × 10 5 verb instances, sparsity seems to become a problem for all mod- els on this task, and the baseline delivers the best performance. Above this threshold, it seems that sparsity is not a major issue: LP performs fairly con- sistently, and is competitive with the SUN model. We attribute this to our use of the Jensen-Shannon divergence as a verb dissimilarity measure, which seems relatively robust to data sparsity. The LDA- hard model with its fewer topics seems to do quite well with fewer data; as the test set size increases, it drops off in the rankings. At the maximum number of verb instances, the best-performing models are SUN, WSM and the lexical preferences. The figure also shows that our evaluation metric is not smooth (note, e.g., the fluctuations in the baseline score). We believe that this reflects a degree of instability in the Ward's hierarchical clustering algorithm; this clustering method is greedy, and clustering errors can be expected to propagate, which might explain the jaggedness of the plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conclusions</head><p>To conclude, we summarise the results of our ana- lysis, using the questions formulated in the Intro- duction as guidelines.</p><p>First, we wanted to compare the efficiency of different classes of nouns as descriptors of selec- tional preference concepts. Our findings suggest that noun classes are most effective when they are semantically highly consistent, representing groups of strongly related nouns. It seems reasonable that SP concepts representing collections of synonyms would be useful for generalising observations, and should represent arguments better than simple LP. A classification of proper names (e.g., as human, corporation, country, medication) is also useful. This implies that we can expect features such as ANIMATE to be shared by all members of a noun cluster.</p><p>Second, we were interested in the appropriate granularity of selectional preference concepts. In our evaluation, we have observed a tendency for smaller, more specific noun classes to be superior; this holds because data sparsity is not a problem in our experiment. Beyond this finding, we would have liked to present a direct juxtaposition of differ- ent models on "granularity" but this is difficult: We have not yet identified a strong abstraction of gran- ularity from the proxies we use (e.g., GermaNet depth, or SUN's N/M ).</p><p>Finally, which methods of classifying nouns into concepts are most effective at capturing selectional preferences for verb clustering? In our experiments, the SUN and LDA-hard models proved to be more effective than lexical preferences, supporting our primary hypothesis that some level of SP concept granularity above the lexical level is desirable for verb clustering. On the other hand, the LP model is only slightly worse than SUN and LDA-hard, mak- ing it attractive because it is so simple. As we have shown, the potential data sparsity issues with LP can be alleviated by judiciously choosing the value of the N parameter that controls the number of nouns included in the model. In addition, compar- ing the SUN and WSM models, and observing the performance of the LDA-hard method, we conclude that inducing noun classes using syntagmatic in- formation is more effective than using paradigmatic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>In this study, we have looked at the utility of selec- tional preferences for automatic verb classification. Some previous research has followed this line of inquiry, though prior studies have not compared alternative methods of modelling SPs. Schulte im <ref type="bibr" target="#b40">Walde (2006)</ref> presented a detailed examination of parameters for k-means-based verb clustering in German, using the same gold standard that we em- ploy here. She reports on the effects of adding SP information to a SCF-based verb clustering using 15 high-level GermaNet synsets as SP concepts; SP information for some combinations of grammatical relations improves clustering performance slightly, but neither are the effects consistent, nor is the improvement delivered by the SP model over the SCF-based baseline statistically significant. Schulte im <ref type="bibr" target="#b37">Walde et al. (2008)</ref> used expectation maximisa- tion to induce latent verb clusters from the British National Corpus while simultaneously building a tree cut model of SPs on the WordNet hierarchy using a minimum description length method; their evaluation focuses on the induced soft verb clusters, reporting the model's estimated perplexity of (verb, grammatical relation, argument head) triples. The SPs are described qualitatively by presenting two example cases. <ref type="bibr" target="#b45">Sun and Korhonen (2009)</ref> study the effect of adding selectional preferences to a subcategorisation-based verb clustering in Eng- lish using the SUN model (see Section 3.2). They demonstrate that adding SPs to the SCF preference data leads to the best results on their two clustering evaluations; overall, their best results come from using SP information only for the subject gram- matical relation. They employ coarse SP concepts (20 or 30 noun clusters) which capture general se- mantic categories <ref type="bibr">(Human, Building, Idea, etc.)</ref>.</p><p>Selectional preferences are usually evaluated either from a word sense disambiguation stand- point using pseudo-words <ref type="bibr" target="#b6">(Chambers and Jurafsky, 2010)</ref>, or in terms of how acceptable an ar- gument is with a verb, via regression against hu- man plausibility judgements. Several studies have compared SP methodologies from the latter per- spective. These include <ref type="bibr" target="#b4">Brockmann and Lapata (2003)</ref>, who compared three GermaNet-based mod- els of <ref type="bibr">SP,</ref> showing that different models were most effective for describing different grammatical re- lations; Ó Séaghdha (2010), who compared dif- ferent LDA-based models of SP, showing these to be effective for a variety of grammatical relations; and Ó <ref type="bibr" target="#b26">Séaghdha and Korhonen (2012)</ref>, who show that WordNet tree cut models, LDA, and a hybrid LDA-WordNet model are effective for describing verb-object relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future work</head><p>Our GermaNet model delivered disappointing per- formance in this study; we would be interested in seeing whether a more sophisticated implementa- tion such as the tree cut model of <ref type="bibr" target="#b20">Li and Abe (1998)</ref> would be more competitive. We also would like to explore alternative noun clustering methods such as CBC ( <ref type="bibr" target="#b29">Pantel and Lin, 2002</ref>) and Brown clusters <ref type="bibr" target="#b5">(Brown et al., 1992</ref>), which were not covered in this work; these would fit easily into our SP eval- uation paradigm. More challenging would be a verb classification-based evaluation of the SP mod- els of <ref type="bibr" target="#b35">(Rooth et al., 1999</ref>) and (Schulte im <ref type="bibr" target="#b37">Walde et al., 2008)</ref>, which use expectation maximisation to simultaneously cluster verbs into verb classes and nominal arguments into noun classes; these ap- proaches are not compatible with the evaluation framework we have used here. Finally, the SP model of <ref type="bibr" target="#b1">Bergsma et al. (2008)</ref> has also achieved impressive results on a number of tasks, but has not been investigated for use in verb classification.</p><p>Our verb clustering evaluation in this work has matched K, the number of clusters found by Ward's method, to the number of classes in the gold standard. Since the number of clusters has an influence on the quality of the ensuing semantic classification <ref type="bibr" target="#b40">(Schulte im Walde, 2006</ref>, page 180f.), we will also be running our experiments with dif- ferent settings of K to explore whether this also influences the overall results of our evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 )</head><label>1</label><figDesc>[ NP Al] put [ NP the book] [ PP on the shelf].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Verb clustering performance (black) and test set coverage (grey) of the LP model as a function of the number of nouns N included in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Verb clustering performance of SP models as a function of number of verb instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Evaluation of the best SP models. 

</table></figure>

			<note place="foot" n="1"> The filtering used a rule-based dependency parser to estimate a per-token parse error rate for each sentence, and removed those sentences with very high error rates. 2 https://code.google.com/p/mate-tools/</note>

			<note place="foot" n="3"> In contrast, a top-level class like &apos;Transfer of Possession (Obtaining)&apos;, not only covers 25% of the gold standard, it also comprises the semantically very diverse subclasses &apos;Transfer of Possession (Giving)&apos;, &apos;Manner of Motion&apos;, and &apos;Emotion&apos;.</note>

			<note place="foot" n="4"> We have also experimented with adding features for each noun showing nominal modification features (e.g., (schwarz, nmod, Haar), &apos;hair&apos; modified by &apos;black&apos;), but these seem to hurt performance. 5 Triples representing prepositional object relations are distinguished by preposition (e.g., the triple (geben, prep-in, Auftrag), &apos;give&apos; with PP headed by &apos;in&apos; with argument head &apos;contract&apos;, an idiomatic expression meaning &apos;to commission&apos; something).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The WaCky wide web: A collection of very large linguistically processed Webcrawled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Bernardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><surname>Ferraresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eros</forename><surname>Zanchetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Discriminative learning of selectional preference from unlabeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint morphological and syntactic analysis for richly inflected languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Boguslavsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="415" to="428" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating and combining approaches to selectional preference acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Brockmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Tenth Conference on European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving the use of pseudo-words for evaluating selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="445" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class-based probability estimation using a semantic hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="206" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Similarity-based models of word cooccurrence probabilities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="43" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple, similarity-based model for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards a semantic classification of Spanish verbs based on subcategorisation information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><forename type="middle">Esteve</forename><surname>Ferrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop at the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Student Research Workshop at the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SdeWaC-A corpus of parsable sentences from the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gertrud</forename><surname>Faaß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kerstin</forename><surname>Eckart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language processing and knowledge in the Web</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A weakly-supervised approach to argumentative zoning of scientific documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Poibeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness in German with revised information content metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Niederlich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of &quot;OntoLex 2005-Ontologies and Lexical Resources&quot; IJCNLP&apos;05 Workshop</title>
		<meeting>&quot;OntoLex 2005-Ontologies and Lexical Resources&quot; IJCNLP&apos;05 Workshop</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="28" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GermaNet: A lexical-semantic net for German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birgit</forename><surname>Hamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Feldweg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</title>
		<meeting>ACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards the automatic identification of adjectival scales: Clustering adjectives according to meaning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 31st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="172" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The structure of a semantic theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerrold</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><forename type="middle">A</forename><surname>Fodor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="170" to="210" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">VerbNet: A broadcoverage, comprehensive verb lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The choice of features for classification of verbs in biomedical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Krymolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics</title>
		<meeting>the 22nd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">English verb classes and alternations: A preliminary investigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Levin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generalizing case frames using a thesaurus and the MDL principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="244" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Which are the best features for automatic verb classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="434" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Divergence measures based on the Shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Information Theory</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="145" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">MALLET: A machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="654" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A random walks view of spectral segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modelling selectional preferences in a lexical hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the 1st Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Latent variable models of selectional preference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diarmuid Ó Séaghdha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flexible, corpus-based modelling of human plausibility judgements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrike</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="400" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering word senses from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="613" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving distributional semantic vectors through context selection and normalisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Polajnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Selection and information: A class-based approach to lexical relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selectional preference and sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How</title>
		<meeting>the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A latent Dirichlet allocation method for selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="424" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Subcategorisation acquisition from raw text for a free word-order language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Egg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valia</forename><surname>Kordoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inducing a semantically annotated lexicon via EM-based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mats</forename><surname>Rooth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Riezler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detlef</forename><surname>Prescher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
	<note>Glenn Carroll, and Franz Beil</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Sahlgren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Hying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Clustering verbs semantically according to their alternation behaviour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
		<meeting>the 18th Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="747" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A subcategorisation lexicon for German verbs induced from a lexicalised PCFG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 3rd Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1351" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Experiments on the automatic induction of German semantic verb classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="194" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The induction of verb frames and verb classes from corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Corpus linguistics: An international handbook</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="952" to="971" />
		</imprint>
	</monogr>
	<note>Anke Lüdeling and Merja Kytö. Mouton de Gruyter, Berlin</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Metaphor identification using verb and noun clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1002" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical metaphor processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The interaction of knowledge sources in word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="321" to="349" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving verb clustering with automatically acquired selectional preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="638" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Verb class discovery from rich syntactic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Krymolowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<meeting>the Ninth International Conference on Intelligent Text Processing and Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Automatic induction of verb classes using clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Cambridge</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><forename type="middle">H</forename><surname>Ward</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">An intelligent analyzer and understander of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="264" to="274" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Making preferences more active</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="223" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
