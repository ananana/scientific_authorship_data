<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-linear Mapping for Improved Identification of 1300+ Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
							<email>ralf@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University Language Technologies Institute</orgName>
								<address>
									<addrLine>5000 Forbes Avenue</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-linear Mapping for Improved Identification of 1300+ Languages</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="627" to="632"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Non-linear mappings of the form P (ngram) γ and log(1+τ P (ngram)) log(1+τ) are applied to the n-gram probabilities in five trainable open-source language identifiers. The first mapping reduces classification errors by 4.0% to 83.9% over a test set of more than one million 65-character strings in 1366 languages, and by 2.6% to 76.7% over a subset of 781 languages. The second mapping improves four of the five identifiers by 10.6% to 83.8% on the larger corpus and 14.4% to 76.7% on the smaller corpus. The subset corpus and the modified programs are made freely available for download at</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language identification, particularly of short strings, is a task which is becoming quite impor- tant as a preliminary step in much automated pro- cessing of online data streams such as microblogs (e.g. Twitter). In addition, an increasing num- ber of languages are represented online, so it is desireable that performance remain high as more languages are added to the identifier.</p><p>In this paper, we stress-test five open-source n-gram-based language identifiers by presenting them with 65-character strings (about one printed line of text in a book) in up to 1366 languages. We then apply a simple modification to their scoring algorithms which improves the classification ac- curacy of all five of them, three quite dramatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>The selected modification to the scoring algorithm is to apply a non-linear mapping which spreads out the lower probability values while compact- ing the higher ones. This low-end spreading of values is the opposite of what one sees in a Zip- fian distribution <ref type="bibr" target="#b16">(Zipf, 1935)</ref>, where the proba- bilities of the most common items are the most spread out while the less frequent items become ever more crowded as there are increasing num- bers of them in ever-smaller ranges. The hypoth- esis is that regularizing the spacing between val- ues will improve language-identification accuracy by avoiding over-weighting frequent items (from having higher probabilities in the training data and also occurring more frequently in the test string).</p><p>Two functions were selected for experiments:</p><formula xml:id="formula_0">x = P (ngram) gamma: y = x γ loglike: y = log(1 + 10 τ x) log(1 + 10 τ )</formula><p>The first simply raises the n-gram probabil- ity to a non-unity power; this exponent is named "gamma" as in image processing <ref type="bibr" target="#b11">(Poynton, 1998)</ref>. The second mapping function is a normalized vari- ant of the logarithm function; the normalization provides fixed points at 0 and 1, as is the case for gamma. Each of the functions gamma and loglike has one tunable parameter, γ and τ , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Although n-gram statistics as a basis for language identification has been in use for two decades since <ref type="bibr" target="#b4">Cavnar and Trenkle (1994)</ref> and <ref type="bibr" target="#b5">Dunning (1994)</ref>, little work has been done on trying to optimize the values used for those n-gram statistics. Where some form of frequency mapping is used, it is of- ten implicit (as in Cavnar and Trenkle's use of ranks instead of frequencies) and generally goes unremarked as such.</p><p>Vogel and Tresner-Kirsch (2012) use the log- arithm of the frequency for some experimental runs, reporting that it improved accuracy in some cases. <ref type="bibr" target="#b6">Gebre et al (2013)</ref> used logarithmic term- frequency scaling of words in an English-language essay to classify the native language of the writer, reporting an improvement from 82.36% accuracy to 84.55% in conjunction with inverse document frequency (IDF) weighting, and from 79.18% ac- curacy to 80.82% without IDF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Programs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LangDetect</head><p>LangDetect, version 2011-09-13 <ref type="bibr" target="#b13">(Shuyo, 2014)</ref>, uses the Naive Bayes approach. Inputs are split into a bag of character n-grams of length 1 through 3; each randomly-drawn n-gram's prob- ability in each of the trained models is multiplied by the current score for that model. After 1000 n-grams, or when periodic renormalization into a probability distribution detects that one model has accumulated an overwhelming probability mass, the iteration is terminated. After averaging seven randomized iterations, each with a random gaussian offset (mean 5×10 −6 , standard deviation 0.5 × 10 −6 ) that is added to each probability prior to multiplication (to avoid multiplication by zero), the highest-scoring model is declared to be the language of the input.</p><p>The mapping function is applied to the model's probability before adding the randomized off- set. To work around the limitation of one model per language code, disambiguating digits are ap- pended to the language code during training and removed from the output prior to scoring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">libtextcat</head><p>libtextcat, version 2.2-9 <ref type="bibr" target="#b7">(Hugueney, 2011)</ref>, is a C reimplementation of the <ref type="bibr" target="#b4">Cavnar and Trenkle (1994)</ref> method. It compiles "fingerprints" con- taining a ranked list of the 400 (by default) most frequent 1-through 5-grams in the training data. An unknown text is classified by forming its fin- gerprint and comparing that fingerprint against the trained fingerprints. A penalty is assigned based on the number of positions by which each n-gram differs between the input and the trained model; n-grams which appear in only one of the two are assigned the maximum penalty, equal to the size of the fingerprints. The model with the lowest penalty score is selected as the language of the in- put.</p><p>For this work, the libtextcat source code was modified to remove the hard-coded fingerprint size of 400 n-grams. While adding the frequency mapping, the code was discovered to also hard- code the maximum distortion penalty at 400; this was corrected to set the maximum penalty equal to the maximum size of any loaded fingerprint. <ref type="bibr">1</ref> Score mapping was implemented by dividing each penalty value by the maximum penalty to produce a proportion, applying the mapping func- tion, and then multiplying the result by the maxi- mum penalty and rounding to an integer (to avoid other code changes). Because there are only a lim- ited number of possible penalties, a lookup table is pre-computed, eliminating the impact on speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">mguesser</head><p>mguesser, version 0.4 <ref type="bibr">(Barkov, 2008)</ref>, is part of the mnoGoSearch search engine. While its doc- umentation indicates that it implements the Cav- nar and Trenkle approach, its actual similarity computation is very different. Each training and test text is converted into a 4096-element hash ta- ble by extracting byte n-grams of length 6 (trun- cated at control characters and multiple consecu- tive blanks), hashing each n-gram using CRC-32, and incrementing the count for the corresponding hash entry. The hash table entries are then nor- malized to a mean of 0.0 and standard deviation of 1.0, and the similarity is computed as the inner (dot) product of the hash tables treated as vectors. The trained model receiving the highest similarity score against the input is declared the language of the input.</p><p>Nonlinear mapping was added by inserting a step just prior to the normalization of the hash ta- ble. The counts in the table are converted to proba- bilities by dividing by the sum of counts, the map- ping is applied to that probability, and the result is converted back into a count by multiplying by the original sum of counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">whatlang</head><p>whatlang, version 1.24 <ref type="bibr" target="#b2">(Brown, 2014a)</ref>, is the stand-alone identification program from LA- Strings <ref type="bibr" target="#b1">(Brown, 2013)</ref>. It performs identifica- tion by computing the inner product of byte tri- grams through k-grams (k=6 by default and in this work) between the input and the trained mod- els; for speed, the computation is performed in- crementally, adding the length-weighted probabil-ity of each n-gram as it is encountered in the in- put. Models are formed by finding the highest- frequency n-grams of the configured lengths, with some filtering as described in <ref type="bibr" target="#b0">(Brown, 2012</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">YALI</head><p>YALI (Yet Another Language Identifier) <ref type="bibr" target="#b10">(Majlis, 2012</ref>) is an identifier written in Perl. It performs minor text normalization by collapsing multiple blanks into a single blank and removing leading and trailing blanks from lines. Thereafter, it uses a sliding window to generate byte n-grams of a (configurable) fixed length, and sums the proba- bilities for each n-gram in each trained model. As with whatlang, this effectively computes the in- ner products between the input and the models.</p><p>Mapping was added by applying the mapping function to the model probabilities as they are read in from disk. As with LangDetect, disam- biguating digits were used to allow multiple mod- els per language code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data</head><p>The data used for the experiments described in this paper comes predominantly from Bible trans- lations, Wikipedia, and the Europarl corpus of Eu- ropean parliamentary proceedings ( <ref type="bibr" target="#b8">Koehn, 2005</ref> The test data is word-wrapped to 65 characters or less, and wrapped lines shorter than 25 bytes are excluded. Up to the first 1000 lines of wrapped text are used for testing. One language with fewer than 50 test strings is excluded from the test set, as is the constructed language Klingon due to heavy pollution with English. In total, the test files con- tain 1,090,571 lines of text in 1366 languages.</p><p>Wikipedia text and many of the Bible transla- tions are redistributable under Creative Commons licenses, and have been packaged into the LTI LangID Corpus <ref type="bibr" target="#b3">(Brown, 2014b)</ref>. This smaller corpus contains 781 languages, 119 of them with development sets, and a total of 649,589 lines in the test files. The languages are a strict subset of those in the larger corpus, but numerous lan- guages have had Wikipedia text substituted for non-redistributable Bible translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>Using the data sets described in the previous sec- tion, we ran a sweep of different gamma and tau values for each language identifier to determine their optimal values on both development and test strings.</p><p>Step sizes for γ were generally 0.1, while those for τ were 1.0, with smaller steps near the minima. Since it does not provide explicit con- trol over model sizes, LangDetect was trained on a maximum of 1,000,000 bytes per model, as reported optimal in <ref type="bibr" target="#b1">(Brown, 2013)</ref>. The other pro- grams were trained on a maximum of 2,500,000 bytes per model; libtextcat and whatlang used default model sizes of 400 and 3500, respec- tively, while mguesser was set to the previously- reported 1500 n-grams per model. After some ex- perimentation, YALI was set to use 5-grams, with 3500 n-grams per model to match whatlang. <ref type="table">Tables 1 and 2</ref> show the absolute performance and relative percentage change in classification errors for the five programs using the two mapping func- tions, as well as the values of γ and τ at which the fewest errors were made on the development set. Overall, the smaller corpus performed worse due to the greater percentage of Wikipedia texts, which are polluted with words and phrases in other lan- guages. In the test set, this occasionally causes a correct identification as another language to be scored as an error. <ref type="figure" target="#fig_3">Figures 2 and 3</ref> graph the classification error rates (number of incorrectly-labeled strings di- vided by total number of strings in the test set) in percent for different values of γ. A gamma of 1.0 is the baseline condition. The dramatic improve- ments in mguesser, whatlang and YALI are quite evident, while the smaller but non-trivial im-  <ref type="table">Table 2</ref>: Language-identification accuracy on the 781-language corpus. γ and τ were tuned on the 119- language development set. libtextcat did not improve with the loglike mapping (see text).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head><note type="other">gamma mapping loglike mapping Program Error% Error% ∆% γ Error% ∆% τ LangDet. 3.</note><p>provements in libtextcat are difficult to dis- cern at this scale. Since libtextcat uses much smaller models than the others by default, <ref type="figure" target="#fig_1">Figure  1</ref> gives a closer look at its performance for larger model sizes. As the models grow, the absolute baseline performance improves, but the change from gamma-correction decreases and the optimal value of γ also decreases toward 1.0. This hints that the implicit mapping of ranks either becomes closer to optimal, or that gamma becomes less ef- fective at correcting it. At a model size of 3000 n-grams, the baseline error rate is 2.465% while the best performance is 2.457% at γ = 1.10.</p><p>That the best γ for libtextcat is greater than 1.0 was not entirely unexpected. The power- law distribution of n-gram frequencies implies that the conversion from frequencies to ranks is essentially logarithmic, and log n eventually be- comes less than n c for any c &gt; 0. The implication of γ &gt; 1 is simply that the conversion to ranks is too strong a correction, which must be partially undone by the gamma mapping.  Gamma mguesser (n=1500) YALI (5gr, n=3500) libtextcat (n=400) LangDetect whatlang (n=3500) <ref type="figure">Figure 2</ref>: Performance of the identifiers on the 1366-language corpus using the gamma mapping.  identifier using four highly-divergent algorithms for computing model scores from n-gram statis- tics. Improvements range from small -2.6% re- duction in classification errors -to dramatic for the three programs with the worst baselines -65.2 to 76.7% reduction in errors on the smaller cor- pus and 72.4 to 83.9% on the larger. While both mappings have similar performance for four of the programs, libtextcat only benefits from the gamma mapping, as it can also reduce n-gram scores, unlike the loglike mapping. Tau mguesser (n=1500) YALI (5gr, n=3500) libtextcat (n=400) LangDetect whatlang (n=3500) <ref type="figure">Figure 4</ref>: Performance of the identifiers on the 1366-language corpus using the loglike mapping. Training data, source code, and supple- mentary information may be downloaded from http://www.cs.cmu.edu/∼ralf/langid.html.</p><p>Future work includes modifying additional lan- guage identifiers such as langid.py <ref type="bibr">(Lui and Baldwin, 2012)</ref> and <ref type="bibr">VarClass (Zampieri and Gebre, 2014)</ref>, experimenting with other mapping functions, and investigating the method's efficacy on pluricentric languages like those VarClass is designed to identify.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figures</head><label></label><figDesc>Figures 4 and 5 graph the error rates for different values of τ. On the graph, zero is the baseline condition without mapping for comparison purposes; the mapping function is not the identity for τ = 0. It can clearly be seen that libtextcat is hurt by the loglike mapping, which never reduces values, even with negative τ. Using the inverse of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: libtextcat performance at different fingerprint sizes. γ = 1 is the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of the identifiers on the 781-language corpus using the gamma mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of the identifiers on the 781-language corpus using the loglike mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). The 1459 files of the training corpus generate 1483 models in 1368 languages. A number of train- ing files generate models in both UTF-8 and ISO 8859-1, numerous languages have multiple train- ing files in different writing systems, and several have multiple files for different regional variants (e.g. European and Brazilian Portugese). The text for a language is split into training, test, and possibly a disjoint development set. The amount of text per language varies, with quartiles of 1.19/1.47/2.22 million bytes. In general, ev- ery thirtieth line of text is reserved for the test set; some smaller languages reserve a higher propor- tion. If more than 3.2 million bytes remain af- ter reserving the test set, every thirtieth line of the remaining text is reserved as a development set. There are development sets for 220 languages. The unreserved test is used for model training.</figDesc><table></table></figure>

			<note place="foot" n="1"> The behavior observed by (Brown, 2013) of performance rapidly degrading for fingerprints larger than 500 disappears with this correction. It was an artifact of an increasing proportion of n-grams present in the model receiving penalties greater than n-grams absent from the model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions and Future Work</head><p>Non-linear mapping is shown to be effective at improving the accuracy of five different language</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ralf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
		<title level="m">Finding and Identifying Text in 900+ Languages. Digital Investigation</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selecting and Weighting NGrams to Identify 1100 Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ralf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Text, Speech, and Discourse</title>
		<meeting>Text, Speech, and Discourse</meeting>
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language-Aware String Extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Brown</surname></persName>
		</author>
		<idno>2014-08-19</idno>
		<ptr target="https://sourceforge.net/projects/la-strings/" />
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/∼ralf/langid.html" />
	</analytic>
	<monogr>
		<title level="j">LTI LangID Corpus, Release</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NGram-Based Text Categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Cavnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trenkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</title>
		<meeting>SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval</meeting>
		<imprint>
			<publisher>UNLV Publications/Reprographics</publisher>
			<date type="published" when="1994-04" />
			<biblScope unit="page" from="161" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical Identification of Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Dunning</surname></persName>
		</author>
		<idno>MCCS 94-273</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>New Mexico State University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving Native Language Identification with TF-IDF Weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Binyam Gebrekidan Gebre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Wittenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA8)</title>
		<meeting>the 8th NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA8)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">libtextcat 2.29: Faster Unicode-focused C++ reimplementation of libtextcat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Hugueney</surname></persName>
		</author>
		<ptr target="https://github.com/scientific-coder/-libtextcat" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>accessed 2014-08-19</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Europarl: A Parallel Corpus for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Machine Translation Summit (MT Summix X)</title>
		<meeting>the Tenth Machine Translation Summit (MT Summix X)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">2012. langid.py: An Off-the-shelf Language Identification Tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012)</meeting>
		<imprint>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yet Another Language Identifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Majlis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-04" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Rehabilitation of</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Poynton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gamma</surname></persName>
		</author>
		<ptr target="http://www.poynton.com/PDFs/-Rehabilitationofgamma.pdf" />
		<title level="m">Human Vision and Electronic Imaging III, Proceedings of SPIE/IS&amp;T Conference</title>
		<imprint>
			<biblScope unit="volume">3299</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language Detection Library for Java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nakatani</forename><surname>Shuyo</surname></persName>
		</author>
		<idno>2014-08-19</idno>
		<ptr target="http://code.google.com/p/-language-detection/" />
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust Language Identification in Short, Noisy Texts: Improvements to LIGA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tresner-Kirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Mining Ubiquitous and Social Environments</title>
		<meeting>the Third International Workshop on Mining Ubiquitous and Social Environments</meeting>
		<imprint>
			<date type="published" when="2012-09" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VarClass: An Open Source Language Identification Tool for Language Varieties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binyam Gebrekidan Gebre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC 2014)</title>
		<meeting>the Ninth International Language Resources and Evaluation Conference (LREC 2014)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The Psycho-biology of Language: An Introduction to Dynamic Philology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kingsley Zipf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935" />
			<publisher>Houghton-Mifflin Co</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
