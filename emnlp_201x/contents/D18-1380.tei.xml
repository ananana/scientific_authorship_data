<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-grained Attention Network for Aspect-Level Sentiment Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifan</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-grained Attention Network for Aspect-Level Sentiment Classification</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3433" to="3442"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3433</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a novel multi-grained attention network (MGAN) model for aspect level sentiment classification. Existing approaches mostly adopt coarse-grained attention mechanism , which may bring information loss if the aspect has multiple words or larger context. We propose a fine-grained attention mechanism , which can capture the word-level interaction between aspect and context. And then we leverage the fine-grained and coarse-grained attention mechanisms to compose the MGAN framework. Moreover, unlike previous works which train each aspect with its context separately, we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same context. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twit-ter dataset. Experimental results show that the multi-grained attention network consistently outperforms the state-of-the-art methods on all three datasets. We also conduct experiments to evaluate the effectiveness of aspect alignment loss, which indicates the aspect-level interactions can bring extra useful information and further improve the performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect level sentiment classification is a funda- mental task in sentiment analysis ( <ref type="bibr" target="#b12">Pang et al., 2008;</ref><ref type="bibr" target="#b7">Liu, 2012)</ref>, which aims to infer the senti- ment polarity (e.g. positive, neutral, negative) of sentence with respect to the aspects. For exam- ple, in sentence "I like coming back to Mac OS but this laptop is lacking in speaker quality com- pared to my $400 old HP laptop", the polarity of the sentence towards the aspect "Mac OS" is pos- itive while the polarity is negative in terms of as- pect "speaker quality". * corresponding author.</p><p>Many statistical methods, such as support vec- tor machine (SVM) ( <ref type="bibr" target="#b20">Wagner et al., 2014;</ref><ref type="bibr" target="#b4">Kiritchenko et al., 2014</ref>), are employed with well- designed handcrafted features. In recent years, neural network models <ref type="bibr" target="#b16">(Socher et al., 2011;</ref><ref type="bibr" target="#b1">Dong et al., 2014;</ref><ref type="bibr">Nguyen and Shirai, 2015)</ref> are stud- ied to automatically learn low-dimensional repre- sentations for aspects and their context. Attention mechanism ( <ref type="bibr" target="#b21">Wang et al., 2016;</ref><ref type="bibr" target="#b9">Ma et al., 2017</ref>) is also be studied to character- ize the effect of aspect on enforcing the model to pay more attention on the important words of the context. Previous works ( <ref type="bibr" target="#b18">Tang et al., 2016b;</ref><ref type="bibr" target="#b0">Chen et al., 2017</ref>) mainly employed the simple av- eraged aspect vector to learn the attention weights on the context words. Ma et al. <ref type="bibr">[2017]</ref> further proposed the bidirectional attention mechanism, which interactively learns the attention weights on context/aspect words, with respect to the averaged vector of aspect/context, respectively.</p><p>These above attention methods are all at the coarse-grained level, which simply averages the aspect/context vector to guide learning the atten- tion weights on the context/aspect words. The simple average pooling mechanism might cause information loss, especially for the aspect with multiple words or larger context. For example, in sentence "I like coming back to Mac OS but this laptop is lacking in speaker quality compared to my $400 old HP laptop", the simple averaged vec- tor of long context might lose information when steering the attention weights on aspect words. Similarly, the simple averaged vector of aspect (i.e. "speaker quality") may deviate from the intu- itive core meaning (i.e. "quality") when enforcing the model to pay varying attentions on the context words. From another perspective, previous works all regard the aspect and its context words as one instance, and train each instance separately. How- ever, they do not consider the relationship among the instances that have the same context words. The aspect-level interactions among the instances with same context might bring extra useful in- formation. Considering the above example, in- tuitively, the aspect "speaker quality" should pay more attention on "lacking" and less attention on "like", compared with aspect "Mac OS", since they have different sentiment polarities.</p><p>In this paper, we propose a multi-grained at- tention network to address the above two is- sues in aspect level sentiment classification. Specifically, we propose a fine-grained atten- tion mechanism (i.e. F-Aspect2Context and F- Context2Aspect), which is employed to character- ize the word-level interactions between aspect and context words, and relieve the information loss occurred in coarse-grained attention mechanism. In addition, we utilize the bidirectional coarse- grained attention (i.e. C-Aspect2Context and C- Context2Aspect) and combine them with fine- grained attention vectors to compose the multi- grained attention network for the final sentiment polarity prediction, which can leverage the advan- tages of them. More importantly, in order to make use of the valuable aspect-level interaction infor- mation, we design an aspect alignment loss in the objective function to enhance the difference of the attention weights towards the aspects which have the same context and different sentiment polari- ties. As far as we know, we are the first to explore the interactions among the aspects with the same context.</p><p>To evaluate the proposed approach, we conduct experiments on three datasets: laptop and restau- rant are from the SemEval 2014 Task 4 and the third one is a tweet collection. Experimental re- sults show that our method achieves the best per- formance on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Aspect-level sentiment analysis is a branch of sen- timent classification, which requires considering both the sentence and aspect information.</p><p>Traditional approaches <ref type="bibr" target="#b3">(Jiang et al., 2011;</ref><ref type="bibr" target="#b4">Kiritchenko et al., 2014;</ref><ref type="bibr" target="#b19">Vo and Zhang, 2015)</ref> regard this task as the text classification problem and de- sign effective features, which are utilized in sta- tistical learning algorithms for training a classi- fier. <ref type="bibr" target="#b4">Kiritchenko et al. [2014]</ref> proposed to use SVM based on n-gram features, parse features and lexicon features, which achieved the best perfor- mance in <ref type="bibr">SemEval 2014. Vo and</ref><ref type="bibr" target="#b19">Zhang [2015]</ref> designed sentiment-specific word embedding and sentiment lexicons as rich features for prediction. These methods highly depend on the effectiveness of the laborious feature engineering work and eas- ily reach the performance bottleneck.</p><p>In recent works, there are growing studies on neural network based methods due to their ca- pability of encoding original features as continu- ous and low-dimensional vectors without feature engineering. Recursive Neural Network <ref type="bibr" target="#b16">(Socher et al., 2011;</ref><ref type="bibr" target="#b1">Dong et al., 2014;</ref><ref type="bibr">Nguyen and Shirai, 2015)</ref> are studied to conduct semantic composi- tions on tree structures, and generate representa- tions for prediction. Methods on LSTM (Hochre- iter and <ref type="bibr" target="#b2">Schmidhuber, 1997</ref>) were proposed to model the context information and use an ag- gregated vector for prediction. TD-LSTM ( <ref type="bibr" target="#b17">Tang et al., 2016a</ref>) adopted LSTM to model the left context and right context of the aspect, and con- catenate them as the representation for prediction. However, these works only focused on model- ing the contexts without considering the aspects, which performed an important role in estimate the sentiment polarity.</p><p>Attention mechanisms ( <ref type="bibr" target="#b21">Wang et al., 2016;</ref><ref type="bibr" target="#b5">Lei et al., 2016;</ref> are studied to enhance the influence of aspects on the final representa- tion for prediction. Many approaches ( <ref type="bibr" target="#b18">Tang et al., 2016b;</ref><ref type="bibr" target="#b0">Chen et al., 2017</ref>) adopted the averaged as- pect vector to learn the attention weights on the hidden vectors of context words. Ma et al. <ref type="bibr">[2017]</ref> further proposed bidirectional attention mecha- nism, which also learns the attention weights on aspect words towards the averaged vector of con- text words. These attention methods only con- sider the coarse-grained level attention, through using the simple averaged aspect/context vector to steer the attention weights learning on the con- text/aspect words, which might cause some infor- mation loss on the long aspect or context case.</p><p>In contrast, motivated by the bidirectional atten- tion flow approaches ( <ref type="bibr" target="#b15">Seo et al., 2017;</ref><ref type="bibr" target="#b11">Pan et al., 2017</ref>) in machine comprehension, we propose a fine-grained attention mechanism which is re- sponsible for linking and fusing information from the aspect and the context words. Furthermore, we leverage both the coarse-grained and fine- grained attentions to compose the multi-grained attention network (MGAN). In addition, existing works train each instance separately. However, we observe that the interactions among the aspects, which have the same context words, could bring extra useful information. Thus we design the as- pect alignment loss in the objective function to de- pict such kind of relationship, which is the first work to explore the aspect-level interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Definition</head><p>Given a sentence s = {w 1 , w 2 , · · · , w N } con- sisting of N words, and an aspect list A = {a 1 , · · · , a k }, where the aspect list size is k and each aspect a i = {w i 1 , · · · , w i M } is a subse- quence of sentence s, which contains M ∈ [1, N ) words. Aspect-level sentiment classification eval- uates sentiment polarity of the sentence s with re- spect to each aspect a i .</p><p>We present the overall architecture of the pro- posed Multi-grained Attention Network (MGAN) model in <ref type="figure" target="#fig_0">Figure 1</ref>. It consists of the Input Em- bedding layer, the Contextual Layer, the Multi- grained Attention Layer and the Output Layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Input Embedding Layer</head><p>Input Embedding Layer maps each word to a high dimensional vector space. We employ the pre- trained word vector, <ref type="bibr">GloVe (Pennington et al., 2014</ref>), to obtain the fixed word embedding of each word. Specifically, we denote the embedding lookup matrix as L ∈ R dv×|V | , where d v is the word vector dimension and |V | is the vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contextual Layer</head><p>We employ a bidirectional Long Short-Term Memory Network (BiLSTM) on top of the em- bedding layer to capture the temporal interactions among words. Specifically, at time step t, given the input word embedding x, the update process of forward LSTM network can be formalized as follows:</p><formula xml:id="formula_0">i t = σ( − → W i · [ − → h t−1 , − → x t ] + − → b i ) (1) f t = σ( − → W f · [ − → h t−1 , − → x t ] + − → b f ) (2) o t = σ( − → W o · [ − → h t−1 , − → x t ] + − → b o ) (3) g t = tanh( − → W g · [ − → h t−1 , − → x t ] + − → b g ) (4) − → c t = f t * − → c t−1 + i t * g t (5) − → h t = o t * tanh( − → c t ) (6)</formula><p>Where σ is the sigmoid activation function, i t , f t , o t are the input gate, forget gate and output gate, respectively.</p><formula xml:id="formula_1">− → W i , − → W f , − → W o , − → W g ∈ R d * (d+dv) , − → b i , − → b f , − → b o , − → b g ∈ R d</formula><p>, and d is the hidden di- mension size. The backward LSTM does the simi- lar process and we can get the concatenated output</p><formula xml:id="formula_2">h t = [ − → h t , ← − h t ] ∈ R 2d</formula><p>. Given the word embed- dings of a context sentence s and a corresponding aspect a j , we will employ the BiLSTM separately and get the sentence contextual output H ∈ R 2d * N and aspect contextual output Q ∈ R 2d * M .</p><p>In addition, considering that the context words with closer distance to an aspect may have higher influence to the aspect, we utilize the position encoding mechanism to simulate the observation. Formally, the weight for a context word w j , which has l word-level distance from the aspect (here we treat the aspect phrase as a single unit), is defined as follows:</p><formula xml:id="formula_3">w t = 1 − l N − M + 1 (7)</formula><p>Specifically, we treat the weights of words within the aspect as 0 in order to focus on the context words in the sentence. Then we can obtain the final contextual outputs of context words</p><formula xml:id="formula_4">H = [H 1 * w 1 , · · · , H N * w N ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-grained Attention Layer</head><p>Attention mechanism is a common way to capture the interactions between the aspect and context words. Previous methods ( <ref type="bibr" target="#b18">Tang et al., 2016b;</ref><ref type="bibr" target="#b9">Ma et al., 2017;</ref><ref type="bibr" target="#b0">Chen et al., 2017</ref>) only adopt coarse- grained attentions, which simply use the aver- aged aspect/context vector as the guide to learn the attention weights on context/aspect. How- ever, the simple average pooling in generating the guide vector might bring some information loss, especially for the aspect with multiple words or larger context. We propose the fine-grained at- tention mechanism, which is responsible for link- ing and fusing information from the aspect and context words. This mechanism is designed to capture the word-level interactions which esti- mate how each aspect/context word affect each context/aspect word. In addition, we concate- nate both the fine-grained and coarse-grained at- tention vectors to obtain the final representation. From other perspective, we observe the relation- ship among aspects can introduce extra valuable information. Hence, we propose an aspect align- ment loss, which is designed to strengthen the at- Aspect Alignment Loss tention difference among aspects with same con- text and different sentiment polarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-grained Attention</head><p>Coarse-grained attention is a widely used mech- anism to capture the interactions between aspect and context, which utilizes an averaged aspect vector to steer the attention weights on the con- text words. Follow the work in ( <ref type="bibr" target="#b9">Ma et al., 2017)</ref>, we employ the bidirectional attention mechanism, namely C-Aspect2Context and C-Context2Aspect.</p><p>(1) C-Aspect2Context learns to assign atten- tion scores to the context words with respect to the averaged aspect vector. Here we employ an aver- age pooling layer above aspect contextual output Q to generate the averaged aspect vector Q avg ∈ R 2d . For each word vector H i in context, we can compute the attention score a ca i as follows:</p><formula xml:id="formula_5">s ca (Q avg , H i ) = Q avg * W ca * H i<label>(8)</label></formula><formula xml:id="formula_6">a ca i = exp(s ca (Q avg , H i )) N k=1 exp(s ca (Q avg , H k ))<label>(9)</label></formula><p>Where the score function s ca computes the weight which indicates the importance of a context word towards aspect sentiment. W ca ∈ R 2d * 2d is the attention weight matrix. Then the weighted com- bination of the context output m ca ∈ R 2d is calcu- lated as follows:</p><formula xml:id="formula_7">m ca = N i=1 a ca i · H i<label>(10)</label></formula><p>(2) C-Context2Aspect learns to assign atten- tion weights on aspects words, which follows the similar learning process with C-Aspect2Context. We utilize the average pooling mechanism to ob- tain the averaged context vector H avg , and com- pute the weights for each word w i in the aspect phrase. We compute the final weighted combina- tion of aspect vector m cc ∈ R 2d as follows:</p><formula xml:id="formula_8">s cc (H avg , Q i ) = H avg * W cc * Q i<label>(11)</label></formula><formula xml:id="formula_9">a cc i = exp(s cc (H avg , Q i )) M k=1 exp(s cc (H avg , Q k ))<label>(12)</label></formula><formula xml:id="formula_10">m cc = M i=1 a cc i · Q i<label>(13)</label></formula><p>where W cc ∈ R 2d * 2d is the attention weight ma- trix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-grained Attention</head><p>As introduced above, we propose a fine-grained at- tention mechanism to characterize the word-level interactions and evaluate how each aspect/context word affect each context/aspect word. Consider- ing the previous example "I like coming back to Mac OS but this laptop is lacking in speaker qual- ity compared to my $400 old HP laptop", the word "quality" in aspect "speaker quality" should have more effect on the context words compared with word "speaker". Accordingly, the context words should pay more attention on "quality" instead of "speaker". Formally, we define an alignment matrix U ∈ R N * M , between the contextual output of and the context H and the aspect Q, where U ij indicates the similarity between i-th context word and j-th aspect word. The similarity matrix U is computed by</p><formula xml:id="formula_11">U ij = W u ([H i ; Q j ; H i * Q j ])<label>(14)</label></formula><p>Where W u ∈ R 1 * 6d is the weight matrix, <ref type="bibr">[; ]</ref> is the vector concatenation across row, * is the elemen- twise multiplication. Then we use U to calculate the attention vectors in both directions.</p><p>(1) F-Aspect2Context estimates which context word has the closest similarity to one of the aspect word and are hence critical for determining the sentiment. We can compute the attention weights a f a on context words by</p><formula xml:id="formula_12">s f a i = max(U i,: )<label>(15)</label></formula><formula xml:id="formula_13">a f a i = exp(s f a i ) N k=1 exp(s f a k )<label>(16)</label></formula><p>where s f a i obtains the maximum similarity across column. And then we can get the attended vector m f a ∈ R 2d as follows:</p><formula xml:id="formula_14">m f a = N i=1 a f a i · H i<label>(17)</label></formula><p>(2) F-Context2Aspect measures which aspect words are most relevant to each context word. Let a f c i ∈ R M be the attention weights on aspect contextual output Q with respect to the i-th con- text word vector H i . The attended aspect vector q f c ∈ R 2d * N is defined as follows:</p><formula xml:id="formula_15">a f c ij = exp(U ij ) M k=1 exp(U ik )<label>(18)</label></formula><formula xml:id="formula_16">q f c i = M j=1 a f c ij · Q j<label>(19)</label></formula><p>Then we use an average pooling layer on q f c to get the attended vector m f c ∈ R 2d :</p><formula xml:id="formula_17">m f c = P ooling([q f c 1 , · · · , q f c N ])<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Output Layer</head><p>At last, we concatenate both the coarse-grained and fine-grained attention vectors as the final rep- resentation m ∈ R 8d , which will be fed to a soft- max layer for determining the aspect sentiment po- larity. m = [m ca ; m cc ; m f a ; m f c ]</p><formula xml:id="formula_18">p = sof tmax(W p * m + b p )<label>(21)</label></formula><p>where p ∈ R C is the probability distribution for the polarity of aspect sentiment, W p ∈ R C * 8d and b p ∈ R C are the weight matrix and bias, respec- tively. Here we set C = 3, which is the number of aspect sentiment classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model Training</head><p>Aspect Alignment Loss Existing approaches train each aspect with its con- text separately, without considering the relation- ship among the aspects. However, we observe the aspect-level interactions can bring extra valuable information. In order to enhance the attention dif- ferences of aspects, which have the same context and different sentiment polarities, we design the aspect alignment loss on the C-Aspect2Context at- tention weights. C-Aspect2Context is employed to find the important context words in terms of a spe- cific aspect. With the constraint of aspect align- ment loss, each aspect will pay more attention on the important words through the comparisons with other related aspects. In terms of the previous ex- ample, the aspect "speaker quality" should pay more attention on "lacking" and less attention on "like", compared with aspect "Mac OS" due to their different sentiment polarities. Specifically, for each aspect pair a i and a j in aspect list A, we compute the square loss on the coarse-grained attention vector a ca i and a ca j , and also estimate the distance d ij ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref> between a i and a j as the loss weight.</p><formula xml:id="formula_20">d ij = σ(W d ([Q i ; Q j ; Q i * Q j ])<label>(23)</label></formula><formula xml:id="formula_21">L align = − M −1 i=1 M j=i+1,y i =y j N k=1 d ij · (a ca ik − a ca jk ) 2</formula><p>(24) Where σ is the sigmoid function, W d ∈ R 1 * 6d is weight matrix for computing the distance, y i and y j are the true labels of the aspect a i and a j , a ca ik and a ca jk are the attention weights on k-th context word towards aspect a i and a j , respectively.</p><p>For training the multi-grained attention network (MGAN), we should optimize all the parameters Θ from the LSTM networks:</p><formula xml:id="formula_22">[W i , W o , W f , W g , b i , b o , b f , b g ]</formula><p>, the attention and alignment loss parameters:</p><formula xml:id="formula_23">[W ca , W cc , W u , W d ] and softmax parameters: [W p , b p ].</formula><p>The final loss function is consisting of the cross-entropy loss, aspect alignment loss and regularization item as follows:</p><formula xml:id="formula_24">L = − C i=1 y i log(p i ) + βL align + λ Θ 2 (25)</formula><p>Where β ≥ 0 and λ ≥ 0 controls the influence of the aspect alignment loss and the L 2 regulariza- tion item, respectively. We employ the stochastic gradient descent (SGD) optimizer to compute and update the training parameters. In addition, we uti- lize dropout strategy to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct experiments to eval- uate our two hypotheses: (1) whether the word- level interaction between aspect and context can help relieve the information loss and improve the performance. <ref type="formula">(2)</ref> whether the relationship among the aspects, which have the same context and dif- ferent sentiment polarities, can bring extra useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setting</head><p>We conduct experiments on three datasets, as shown in <ref type="table">Table 1</ref>. The first two are from the Se- mEval 2014 Task 4 1 ( <ref type="bibr" target="#b14">Pontiki et al., 2014</ref>), which contains the reviews in laptop and restaurants, re- spectively. The third one is a tweet collection, which are gathered by <ref type="bibr" target="#b1">(Dong et al., 2014</ref>). Each aspect with the context is labeled by three senti- ment polarities, namely Positive, Neutral and Neg- ative. In addition, we adopt Accuracy and Macro- F1 as the metrics to evaluate the performance of aspect-level sentiment classification, which is widely used in previous works ( <ref type="bibr" target="#b18">Tang et al., 2016b;</ref><ref type="bibr" target="#b9">Ma et al., 2017;</ref><ref type="bibr" target="#b0">Chen et al., 2017;</ref><ref type="bibr" target="#b21">Wang et al., 2016)</ref>.</p><p>In our experiments, word embeddings for both context and aspect words are initialized by <ref type="bibr">Glove (Pennington et al., 2014</ref>). The dimension of word embedding d v and hidden state d are <ref type="bibr">1</ref> The detailed task introduction can be found in http://alt.qcri.org/semeval2014/task4/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Positive <ref type="table" target="#tab_1">Neutral  Negative  Train Test Train Test Train Test  Laptop  994  341  870  128  464  169  Restaurant  2164 728  807  196  637  196  Twitter  1561 173 3127 346 1560 173   Table 1</ref>: The statistics of the datasets.</p><p>set to 300. The weight matrix and bias are ini- tialized by sampling from a uniform distribution U (0.01, 0.01). The coefficient λ of L 2 regulariza- tion item is 10 −5 , the parameter β of aspect align- ment loss and drop out rate are set to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Compared Methods</head><p>To evaluate the performance of proposed ap- proach, we compared with the following methods: Majority is the basic baseline, which chooses the largest sentiment polarity in the training set to each instance in the test set. ) learns multi-hop atten- tions on the hidden states of bidirectional LSTM networks for context words, and proposes to use GRU network to get the aggregated vector from the attentions. Similar with MemNet, the atten-tion weights on context words are steered by the simple averaged aspect vector. We also list the variants of MGAN model, which are used to analyze the effects of coarse- grained attention, fine-grained attention and aspect alignment loss, respectively. MGAN-C only employs the coarse-grained atten- tions for prediction, which is similar with IAN. MGAN-F only utilizes the proposed fine-grained attentions for prediction. MGAN-CF adopts both the coarse-grained and fine-grained attentions, while without applying the aspect alignment loss. MGAN is the complete multi-grained attention network model. <ref type="table" target="#tab_2">Table 2</ref> shows the performance comparison results of MGAN with other baseline methods. We can have the following observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Performance Comparison</head><p>(1) Majority performs worst since it only uti- lizes the data distribution information. Fea- ture+SVM can achieve much better performance on all the datasets, with the well-designed feature engineering. Our method MGAN outperforms Majority and Feature+SVM since MGAN could learn the high quality representation for predic- tion.</p><p>(2) ATAE-LSTM is better than LSTM since it employs attention mechanism on the hidden states and combines with attention embedding to gener- ate the final representation. TD-LSTM performs slightly better than ATAE-LSTM, and it employs two LSTM networks to capture the left and right context of the aspect. TD-LSTM performs worse than our method MGAN since it could not prop- erly pay more attentions on the important parts of the context.</p><p>(3) IAN achieves slightly better results with the previous LSTM-based methods, which interac- tively learns the attended aspect and context vector as final representation. Our method consistently performs better than IAN since we utilize the fine- grained attention vectors to relieve the informa- tion loss in IAN. MemNet continuously learns the attended vector on the context word embed- ding memory, and updates the query vector at each hop. BILSTM-ATT-G models left context and right context using attention-based LSTMs, which achieves better performance than MemNet. RAM performs better than other baselines. It employs bidirectional LSTM network to generate contex- tual memory, and learns the multiple attended vec- tor on the memory. Similar with MemNet, it uti- lizes the averaged aspect vector to learn the atten- tion weights on context words.</p><p>Our proposed MGAN consistently performs better than MemNet, BILSTM-ATT-G and RAM on all three datasets. On one hand, they only consider to learn the attention weights on context towards the aspect, and do not consider to learn the weights on aspect words towards the context. On the other hand, they just use the averaged as- pect vector to guide the attention, which will lose some information, especially on the aspects with multiple words. From another perspective, our method employs the aspect alignment loss, which can bring extra useful information from the aspect- level interactions. <ref type="table" target="#tab_3">Table 3</ref> shows the performance comparison among the variants of MGAN model. We can have the following observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of MGAN model</head><p>(1) the proposed fine-grained attention mech- anism MGAN-F, which is responsible for link- ing and fusing the information between the con- text and aspect word, achieves competitive per- formance compared with MGAN-C, especially on laptop dataset. To investigate this case, we col- lect the percentage of aspects with different word lengths in <ref type="table" target="#tab_4">Table 4</ref>. We can find that laptop dataset has the highest percentage on the aspects with more than two words, and the second-highest per- centage on two words. It demonstrates MGAN- F has better performance on aspects with more words, and make use of the word-level interactions to relieve the information loss occurred in coarse- grained attention mechanism.</p><p>(2) MGAN-CF is better than both MGAN-C and MGAN-F, which demonstrates the coarse- grained attentions and fine-grained attentions could improve the performance from different per- spectives. Compared with MGAN-CF, the com- plete MGAN model gains further improvement by bringing the aspect alignment loss, which is designed to capture the aspect level interactions. Specifically, we collect the statistics of sentence- level with different aspect amounts, which is shown in     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study</head><p>In order to demonstrate the effect of aspect align- ment loss, we visualize the attention weights of the C-Aspect2Context mechanism. <ref type="figure">Figure 2</ref> shows the attention weights of two aspects "res- olution" and "fonts", whose sentiment polarities are positive and negative, respectively. From the above two bars, we can observe that the C- Aspect2Context can enforce the model to pay more attentions on the important words with re- spect to the aspect. For example, in terms of the aspect"resolution", the words "has", "higher" and "but" have higher attention weights compared with other words. In contrast, aspect "fonts" pays more attentions on words "but", "fonts" and "small". In addition, we evaluate the effect of as- pect alignment loss, which enhances the attention difference between the aspect "resolution" and "fonts". For the two bars at bottom, we can find that aspect "fonts" has more attention on "small" and less attention on "higher", compared with the aspect "resolution". This phenomenon shows that with the constraint of aspect alignment loss, C- Aspect2Context can not only learn the important context words for each aspect, but also can make the attention gaps on the important words be as large as possible for aspects with different polari- ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a multi-grained atten- tion network (MGAN) for aspect-level sentiment classification. Specifically, we propose a fine- grained attention mechanism, which is responsible for linking and fusing the words from the aspect and context, to capture the word-level interaction.</p><p>And we combine it with the coarse-grained atten-  <ref type="figure">Figure 2</ref>: The attention visualizations on aspect "resolution" and "fonts". The above two bars are from the C- Aspect2Context attention mechanism, and the two bars at bottom are from the C-Aspect2Context attention mech- anism with the constraint of aspect alignment loss.</p><p>tion mechanism to compose the MGAN model. In addition, we design an aspect alignment loss to characterize the aspect-level interactions among aspects, which have the same context and dif- ferent sentiment polarities, to explore extra valu- able information. Experimental results demon- strate the effectiveness of our approach on three public datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of the proposed multi-grained attention network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Feature+SVM (Kiritchenko et al., 2014) uses n- gram features, parse features and lexicon features based on SVM, which achieves the state-of-the-art performance in SemEval 2014. LSTM (Wang et al., 2016) utilizes one LSTM net- work to learn the hidden states and obtain the av- eraged vector to predict the sentiment polarity. ATAE-LSTM (Wang et al., 2016) learns attention embeddings and combine them with the LSTM hidden states to predict the polarity. TD-LSTM (Tang et al., 2016a) employs two di- rectional LSTM networks, which estimate the left context and right context of the target aspect, re- spectively. Finally it takes the last hidden states of LSTM networks for prediction. MemNet (Tang et al., 2016b) applys multi-hop at- tentions on the word embeddings, learns the atten- tion weights on context word vectors with respect to the averaged query vector. IAN (Ma et al., 2017) interactively learns the coarse-grained attentions between the context and aspect, and concatenate the vectors for prediction. BILSTM-ATT-G (Liu and Zhang, 2017) mod- els left and right context with two attention-based LSTMs and utilizes gates to control the impor- tance of left context, right context and the entire sentence for prediction. RAM(Chen et al., 2017</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 5 .</head><label>5</label><figDesc>We can observe that both laptop and restaurant datasets have relatively high per- centage on the sentences with multiple aspects.</figDesc><table>Method 
Laptop 
Restaurant 
Twitter 
Acc 
Macro-F1 
Acc 
Macro-F1 
Acc 
Macro-F1 
Majority 
0.5350 
0.3333 
0.6500 
0.3333 
0.5000 
0.3333 
Feature-SVM 
0.7049 
-
0.8016 
-
0.6340 
0.6330 
ATAE-LSTM 
0.6870 
-
0.7720 
-
-
-
TD-LSTM 
0.7183 
0.6843 
0.7800 
0.6673 
0.6662 
0.6401 
IAN 
0.7210 
-
0.7860 
-
-
-
MemNet 
0.7237 
-
0.8032 
-
0.6850 
0.6691 
BILSTM-ATT-G 
0.7312 
0.6980 
0.7973 
0.6925 
0.7038 
0.6837 
RAM 
0.7449 
0.7135 
0.8023 
0.7080 
0.6936 
0.6730 
MGAN 
0.7539 
0.7247 
0.8125 
0.7194 
0.7254 
0.7081 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The performance comparisons of different methods on the three datasets, where the results of baseline 
methods are retrieved from published papers. The best performances are marked in bold. 

Method 
Laptop 
Restaurant 
Twitter 
Acc 
Macro-F1 
Acc 
Macro-F1 
Acc 
Macro-F1 
MGAN-C 
0.7273 
0.6933 
0.8054 
0.7099 
0.7153 
0.6952 
MGAN-F 
0.7398 
0.7082 
0.8000 
0.7092 
0.7110 
0.6918 
MGAN-CF 
0.7445 
0.7121 
0.8089 
0.7135 
0.7254  *  
0.7081  *  
MGAN 
0.7539 
0.7247 
0.8125 
0.7194 
0.7254 
0.7081 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The performance comparisons of MGAN variants.  *  means MGAN-CF and MGAN can be regarded as 
the same method on twitter dataset. 

Dataset 
#words=1 #words=2 #words&gt;2 
Laptop 
61.60% 
29.16% 
9.24% 
Restaurant 
74.47% 
17.32% 
8.21% 
Twitter 
29.99% 
69.91% 
0.10% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>The percentage of aspects with different word 
length on three datasets. Here we give the overall statis-
tic of each dataset. 

The improved performance on the two datasets 
shows the importance of capturing the aspect-level 
interactions. In terms of twitter dataset, almost all 
of the sentences only has one aspect. In this case, 
the method MGAN can be regarded as MGAN-
CF. 

Dataset 
#aspects=1 #aspects=2 #aspects&gt;2 
Laptop 
63.94% 
23.32% 
12.74% 
Restaurant 
50.89% 
28.60% 
20.51% 
Twitter 
99.91% 
0.09% 
0.00% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The percentage of sentences with different as-
pect numbers on three datasets. Aspects with the same 
context are regarded as the same sentence. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the National Nat-ural Science Foundation of China (61672057, 61672058); KLSTSPI Key Lab. of Intelligent Press Media Technology. For any correspondence, please contact Yansong Feng.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nrc-canada-2014: Detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep memory networks for attitude identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="671" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention modeling for targeted sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="572" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interactive attention networks for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Phrasernn: Phrase recursive neural network for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>Thien Hai Nguyen and Kiyoaki Shirai</editor>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Memen: Multi-layer embedding with memory networks for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Boyuan Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Bin Cao, Deng Cai, and Xiaofei He</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Opinion mining and sentiment analysis. Foundations and Trends R in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
	<note>Ion Androutsopoulos, and Suresh Manandhar</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing</title>
		<meeting>the conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective lstms for target-dependent sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Target-dependent twitter sentiment classification with rich automatic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duy-Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1347" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dcu: Aspect-based polarity classification for semeval task 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utsab</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamia</forename><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention-based lstm for aspect-level sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
