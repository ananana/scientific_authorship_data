<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Universal Sentiment Classifier in Multiple languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Peking University</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Universal Sentiment Classifier in Multiple languages</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="511" to="520"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing sentiment classifiers usually work for only one specific language, and different classification models are used in different languages. In this paper we aim to build a universal sentiment classifier with a single classification model in multiple different languages. In order to achieve this goal, we propose to learn multilingual sentiment-aware word embeddings simultaneously based only on the labeled reviews in English and unlabeled parallel data available in a few language pairs. It is not required that the parallel data exist between English and any other language , because the sentiment information can be transferred into any language via pivot languages. We present the evaluation results of our universal sentiment classifier in five languages, and the results are very promising even when the parallel data between English and the target languages are not used. Furthermore, the universal single classifier is compared with a few cross-language sentiment classifiers relying on direct parallel data between the source and target languages, and the results show that the performance of our universal sentiment classifier is very promising compared to that of different cross-language classifiers in multiple target languages .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, a large amount of user-generated con- tent (UGC) appears online everyday, such as tweets, comments and product reviews. Sentiment classification on these data has become a popular research topic over the past few years ( <ref type="bibr" target="#b17">Pang et al., 2002;</ref><ref type="bibr" target="#b2">Blitzer et al., 2007;</ref><ref type="bibr" target="#b0">Agarwal et al., 2011;</ref><ref type="bibr" target="#b13">Liu, 2012</ref>). Distributed representations of words or word embeddings have been widely explored, and have proved its great usability for the sentimen- t classification task ( <ref type="bibr" target="#b29">Xu et al., 2015;</ref><ref type="bibr" target="#b3">Bollegala et al., 2016;</ref><ref type="bibr" target="#b8">Ferreira et al., 2016)</ref>.</p><p>Most existing sentiment classifiers rely on la- beled training data and the data are usually language-dependent. In other words, a sentiment classifier is learned from a labeled dataset in a spe- cific language and this sentiment classifier can be used for sentiment classification in this language. However, labeled training data for sentiment clas- sification are not available or not easy to obtain in many languages in the world (e.g., Malaysian, Mongolian, Uighur). Without reliable labeled da- ta, it is hard to build a sentiment classifier in these resource-poor languages.</p><p>Fortunately, there are a few studies investigating the task of cross-language sentiment classification ( <ref type="bibr" target="#b1">Banea et al., 2008;</ref><ref type="bibr" target="#b26">Wan, 2009;</ref><ref type="bibr" target="#b16">Meng et al., 2012;</ref><ref type="bibr" target="#b28">Xiao and Guo, 2013;</ref><ref type="bibr" target="#b9">Gao et al., 2015;</ref><ref type="bibr" target="#b12">Li et al., 2017;</ref><ref type="bibr">Zhou et al., 2016a,b)</ref>, which aims to make use of the labeled data in a source language (English in most cas- es) to build a sentiment classifier in a target lan- guage. However, cross-language sentiment classi- fication methods rely on parallel data between the source and target languages <ref type="bibr">1</ref> In a resource-poor language, the parallel data between this language and the source language may not be available or is not easy to obtain. In this circumstance, previ- ous cross-language sentiment classification meth-ods will fail to work.</p><p>Another shortcoming of previous cross- language sentiment classification researches is that we have to build an individual cross-language sentiment classifier for each target language, even when we want to perform sentiment classification in a couple of languages at the same time.</p><p>In this study, instead of building a sentiment classifier for each target language, we aim to build a universal sentiment classifier in multiple lan- guages and this universal sentiment classifier on- ly learns one single sentiment classification model and it can be applied for sentiment classification in many languages.</p><p>In order to achieve this goal, we propose an ap- proach to learn multilingual sentiment-aware word embeddings simultaneously based only on the la- beled reviews in English and unlabeled parallel da- ta available in a few language pairs. As mentioned earlier, in some resource-poor languages, there do not exist direct parallel data between these lan- guages and the source English language. In order to address this problem, we propose a pivot-based model to transfer the sentiment information from the source language to any resource-poor language via pivot languages. Finally, a universal sentimen- t classifier can be built because the multilingual word embeddings are in the same semantic space.</p><p>We build three different models (Bilingual Model, Pivot-Driven Bilingual Model and Univer- sal Multilingual Model) and compare them em- pirically in order to answer two questions in this paper: 1) Can pivot-based models learn bilingual sentiment-aware word embeddings effectively? 2) Can an effective universal sentiment classifier be built for multiple languages?</p><p>Without loss of generality, we present and com- pare the evaluation results of the models in five languages. Evaluation results show that pivot- driven bilingual models perform as well as the bilingual model using direct parallel data, which lays the solid foundation of our universal mod- el. Moreover, it is very promising that our univer- sal sentiment classifier can work well in five lan- guages, and it can achieve very promising classifi- cation results as compared to several typical cross- language sentiment classification models.</p><p>The main contributions of our study in this pa- per are summarized as follows:</p><p>• We are the first to build a universal sentimen- t classifier in multiple languages by learning multilingual sentiment-aware word embed- dings, which can not be addressed by previ- ous researches on cross-language sentiment classification.</p><p>• We propose pivot-based models to bridge two languages in which there do not exist parallel data, and thus the sentiment information can be transferred to any target language.</p><p>• Evaluation results on five languages demon- strate the efficacy of our proposed pivot- based models and the universal sentimen- t classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>In order to build a universal sentiment classifi- er, we propose an approach to learn multilingual sentiment-aware word embeddings simultaneous- ly, and then train a universal sentiment classifica- tion model in the embedding space by averaging the word embeddings in a document as the doc- ument representation. Note that in this study, we focus on only using the labeled data in English and do not make use of any labeled data in other lan- guages, which makes the task more challenging 2 . Formally, we aims to build a single sentiment clas- sifier which can perform sentiment classification in many languages {S, T 1 , T 2 , ..., T N }, where S refers to English language, and T 1 to T N refer to other N languages. In our approach, the multilingual sentiment- aware word embeddings play the key role in building the universal sentiment classifier, and now the question is how to learn the multilingual sentiment-aware word embeddings? Inspired by previous studies on cross-lingual sentiment clas- sification and bilingual word embedding learn- ing, we can leverage the labeled data in S (i.e., English) and unlabeled parallel data between S and language T to learn bilingual sentiment-aware word embeddings in both English and T languages with a bilingual model. However, such unlabeled parallel data are not always easy to obtain for all other languages. For a specific language T , if the unlabeled parallel data between T and S do not exist, the bilingual model cannot be applied. In order to address this problem, we propose a pivot- driven bilingual model to leverage pivot languages to bridge T and S. We choose a pivot language P where the parallel data between P and S, and the parallel data between P and T are easy to ob- tain, and then leverage them to learn the multi- lingual sentiment-aware word embeddings in the three languages P , T and S. Furthermore, we can leverage more parallel data between multiple lan- guages, some of which are parallel data between S and other languages, and some of which are par- allel data within other languages, to build an uni- versal multilingual model. The sentiment informa- tion will be directly or indirectly transfered to each language as well, and thus we obtain multilingual sentiment-aware word embeddings in many lan- guages.</p><p>The bilingual model, pivot-driven bilingual model and universal multilingual model will be described in next sections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bilingual Model</head><p>The bilingual model tries to induce bilingual word embeddings from a parallel corpus, and in the meantime make similar words from the two lan- guages share adjacent vector representations in the same vector space.</p><p>Formally, we assume a source language S with |S| words and a target language T with |T | words. We use s and t to represent a word from S and T , respectively. Given the bilingual parallel corpus C between language S and T , it can be divided into a corpus C S in language S and a corpus C T in lan- guage T . And we use a notation S − T to indicate a parallel corpus between languages S and T .</p><p>Previous studies have proposed some bilingual models for learning bilingual word embeddings, so we extend the well-behaved BiSkip model (  to Bilingual Model (BM). This model requires word alignment information, and in this study word alignment is automatically ob- tained from parallel sentences by using a word alignment tool.</p><p>In our bilingual model, every word s in lan- guage S is required to predict the adjacent words of itself and the aligned word t in the target lan- guage T . For corpus C S , the monolingual con- straint on itself (C S → C S ) is:</p><formula xml:id="formula_0">Obj(C S |C S ) = s∈C S w∈adj(s)</formula><p>log p(w|s), <ref type="formula">(1)</ref> and the cross-lingual constraint on</p><formula xml:id="formula_1">C T (C S → C T )</formula><p>is:</p><formula xml:id="formula_2">Obj(C T |C S ) = s∈C S w∈adj(t),s↔t log p(w|s) (2)</formula><p>where s↔t means word s(∈ C S ) is aligned to word t(∈ C T ) and adj(s) or adj(t) mean the adja- cent words of word s or t.</p><p>Similarly, for corpus C T we can obtain:</p><formula xml:id="formula_3">Obj(C T |C T ) = t∈C T w∈adj(t)</formula><p>log p(w|t), <ref type="formula">(3)</ref> and</p><formula xml:id="formula_4">Obj(C S |C T ) = t∈C T w∈adj(s),t↔s log p(w|t) (4)</formula><p>Combining equations 1, 2, 3 and 4, we get the objective for obtaining bilingual word embeddings from parallel corpus:</p><formula xml:id="formula_5">Obj(C) = α 1 Obj(C S |C S ) + α 2 Obj(C T |C S ) +α 3 Obj(C T |C T ) + α 4 Obj(C S |C T )</formula><p>where α 1 , α 2 , α 3 and α 4 are scalar parameters.</p><p>We still have to incorporate the sentiment infor- mation into the bilingual word embeddings. Sim- ilar to previous studies (Zhou et al., 2015), we make use of the sentiment polarity of texts as su- pervision in the learning process. Given a labeled sentimental corpus C L 3 , we use S * to represent a sentence in C L and w as a word in S * . And x T is a sum of word embeddings in S * . We simply adopt the logistic regression classifier to enforce the sentiment constraint, and thus make the bilin- gual word embeddings absorb the corresponding sentiment information. The objective function is:</p><formula xml:id="formula_6">L(C L ) = S * ∈C L y log σ(W x T + b) + (1 − y) log σ(1 − (W x T + b)) (5)</formula><p>where y is the label of the sentence S * , W is a weight vector and b is a bias. The overall objective function for inducing bilingual sentiment-aware word embeddings is to maximize:</p><formula xml:id="formula_7">Obj(C) + L(C L )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pivot-Driven Bilingual Model</head><p>For some resource-poor target language T , it is quite expensive to get direct parallel corpus be- tween T and the source language S. Without such parallel corpus, it is not possible to apply the above bilingual model to learn bilingual sentiment-aware word embeddings. In order to address this prob- lem we propose our Pivot-Driven Bilingual Mod- el (PDBM) by using a pivot language to bridge T and S. The model is inspired by <ref type="bibr" target="#b27">(Wu and Wang, 2007)</ref>, in which pivot languages are used for phrase-based SMT. A pivot language P is cho- sen if the parallel corpus between P and S, and the parallel corpus between P and T are easy to obtain. Given two parallel corpora: S-P and P -T , our PDBM model tries to get the trilingual word embeddings by putting constrains on the two cor- pora. Under the well-designed constraint, the piv- ot language P can pass the sentiment information from the source language S to the target language T . Similarly, we further assume the pivot language P with |P | words, and use C P to denote the corpus in language P .</p><p>We design constraints on the two parallel corpo- ra S-P and P -T , instead of direct constraints on S and T . Derived from the BM model, we can get three monolingual constraints C S → C S , C T →C T , C P →C P and four bilingual constraints C S →C P , C T →C P , C P →C S and C P →C T . The final objec- tive function for learning the trilingual word em- beddings can be summarized as:</p><formula xml:id="formula_8">Obj p (C) = β 1 Obj(C S |C S ) + β 2 Obj(C S |C P ) +β 3 Obj(C T |C T ) + β 4 Obj(C T |C P ) +β 5 Obj(C P |C S ) + β 6 Obj(C P |C T ) +β 7 Obj(C P |C P )</formula><p>where β 1 , β 2 , β 3 , β 4 , β 5 , β 6 , β 7 are scalar parame- ters. Similarly, the objective for enforcing the sen- timent constraint is the same as equation 5, so we combine them together to get the overall objective function:</p><formula xml:id="formula_9">Obj p (C) + L(C L )</formula><p>Through the pivot language, the sentiment in- formation can be passed from a source language to a target language by maximizing the above ob- jective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Universal Multilingual Model</head><p>The bilingual model and the pivot-driven bilin- gual model lay the foundations of build a univer- sal multilingual model for sentiment classification in many languages. Given a source language S and a few other languages {T 1 , T 2 , ..., T N }. If there exist parallel data between a language T i and S, then the bilingual sentiment-aware word em- beddings can be learned by the bilingual model. If the parallel data between languages T i and S are not available, a pivot language can be select- ed and the pivot-driven model can be applied to learn the trilingual sentiment-aware word embed- dings. Even when a single pivot language cannot be found for languages T i and S, we still can find two or more pivot languages {P 1 , P 2 , ..., P M } to form a pivot chain and the sentiment information in the source language can be passed through the pivot chain (S − P 1 − ... − P M − T i ) to the target language.</p><p>Therefore, in this model, we will make use of all parallel corpora between any pair of lan- guages (including parallel corpora between the source language and any other language, and par- allel corpora between other languages) and learn the sentiment-aware word embeddings in all the languages simultaneously. The monolingual ob- jective in each language and the cross-lingual ob- jective for any available parallel corpus are de- fined in the same way as in the above models, and we sum all the objectives and denote it as Obj universal (C), and this objective is then com- bined with the sentiment constraint as follows:</p><formula xml:id="formula_10">Obj universal (C) + L(C L )</formula><p>By maximizing the above objective function, the sentiment-aware word embeddings in all the languages will be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>Without loss of generality, we evaluate our mod- els in five languages (including three western lan- guages and two Asian languages): English (en), German (de), French (fr), Japanese (jp) and Chi- nese (en/zh). Among these languages, the English language is the source language with labeled train- ing data, and we do no use any labeled data in the other languages.</p><p>Particularly, we use the multilingual multi- domain Amazon review dataset 4 provided by <ref type="bibr" target="#b19">(Prettenhofer and Stein, 2010)</ref> and the NLPC- C2013 dataset <ref type="bibr">5</ref> . The review dataset provided by <ref type="bibr" target="#b19">(Prettenhofer and Stein, 2010)</ref> </p><note type="other">contains labeled da- ta in four languages: English, German, French and Japanese, and the NLPCC2013 dataset further provides labeled data in Chinese. The reviews in each language are divided into three domains: dvd, music and books. Each domain of product reviews contains a balanced training set and test set, each of which consists of 1000 positive and 1000 nega- tive reviews for each language except for Chinese. While for Chinese language, the test set consists of 2000 positive and 2000 negative reviews. We only use English training data as the labeled data in the experiments.</note><p>We further obtain unlabeled parallel data from Europarl v7 6 <ref type="bibr" target="#b11">(Koehn, 2004)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Eu v7) and The United Nations Parallel Corpus v1.0 7 (Ziems- ki et al., 2016) (UN v1.0). The Europarl cor- pus contains bilingual parallel corpus between En- glish and other 20 Europe languages. The Unit- ed Nations Parallel Corpus is composed of offi- cial records and other parliamentary documents of the United Nations that are in the public domain.</head><p>These documents are mostly available in the six official languages of the United Nations. Besides, we use the cldc-2009-004 8 Chinese-English (CN- EN) news parallel corpus and Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles Version 2.01 9 (JP-EN), which is created manually by translating Japanese Wikipedia articles (related to Kyoto) into English. In addition, CJWikiCor- pus (CN-JP) is a Chinese-Japanese Parallel Cor- pus Constructed from Wikipedia <ref type="bibr">10</ref> For the the BM model, we use en-de (∈ Eu v7) and en-fr (∈ Eu v7), en-zh (∈ CN-EN), and en-jp (∈ JP-EN).</p><p>For the PDBM model, we use en-fr (∈ UN v1.0) with fr-de (∈ Eu v7) to get the case en-fr-de (fr act- s as a pivot), en-zh (∈ CN-EN) with zh-jp (∈ CN- JP) to build the case en-zh-jp (zh acts as a pivot), en-zh (∈ CN-EN) with zh-fr (∈ UN v1.0) to build en-zh-fr (zh acts as a pivot), and en-fr (∈ Eu v7) with zh-fr (∈ UN v1.0) to build en-fr-zh (fr acts as a pivot). Note that any pivot language can be selected if the parallel corpora between the piv- ot language and other languages can be obtained, but in our experiments, we only use one pivot lan- guage in each test case to validate the feasibility of our proposed model. In practice, a popular lan- guage (such as English, Chinese) can be used as the pivot because it can act as a link between two unpopular languages.</p><p>While for the UMM model, we use all the cor- pora used in PDBM to build a universal model. All the details can be found in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Methods</head><p>In addition to the comparison between our mod- els, we further compare them with popular cross- lingual (CL) sentiment classification methods.</p><p>For comparison in German, French and Japanese, we adopt a few typical CL classifica- tion methods, and the results are directly borrowed from the corresponding published papers:</p><p>MT-BOW: It is a simple model to train a lin- ear classifier based on the bag-of-words features, and it uses a machine translator to translate the test data into the source language <ref type="bibr" target="#b19">(Prettenhofer and Stein, 2010)</ref> .</p><p>CL-SCL: It is the cross-lingual structural corre- spondence learning algorithm proposed by <ref type="bibr" target="#b19">(Prettenhofer and Stein, 2010</ref>) and the features in the two languages are mapped to a unified space. BSE: It is introduced in <ref type="bibr" target="#b23">(Tang and Wan, 2014</ref>) by forcing the representations of words from both the source and target languages to share the same feature space. In this way, bilingual word embed- dings are learned for cross-lingual sentiment clas- sification.</p><p>CR-RL: It is the bilingual word representation learning method of <ref type="bibr" target="#b28">(Xiao and Guo, 2013)</ref>. It learns different representations for words in different lan- guages. Part of the word vector is shared among different languages and the rest is language depen- dent. The document representation is calculated by taking average over all words in the document.</p><p>Bi-PV: It extends the paragraph vector model into bilingual setting by sharing the document rep- resentation of a pair of parallel documents .</p><p>For comparison in Chinese, we adopt several typical CL classification methods:</p><p>MT-LR and MT-SVM: We use logistic regres-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Parallel corpora with size Test case BM en-de (∈ Eu v7, 1.92M) en-de en-fr (∈ Eu v7, 2.0M) en-fr en-zh (∈ CN-EN, 1.0M) en-zh en-jp (∈ JP-EN, 0.5M) en-jp PDBM en-fr (∈ UN v1.0, 2.0M) + fr-de (∈ Eu v7, 1.5M) en-fr-de en-zh (∈ CN-EN, 1.0M) + zh-jp (∈ CN-JP, 0.12M) en-zh-jp en-zh (∈ CN-EN, 1.0M) + zh-fr (∈ UN v1.0, 2.0M) en-zh-fr en-fr (∈ Eu v7, 2.0M) + zh-fr (∈ UN v1.0, 2.0M) en-fr-zh UMM all the corpora used in PDBM en,de,fr,zh,jp Bi-PV: The same as that described above.</p><p>BSWE: It uses the bilingual sentiment word embedding algorithm based on denoising autoen- coders ( ) to learns word represen- tations. Each document is then represented by the sentiment words and the corresponding negation words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Settings and Preprocessing</head><p>We utilize cdec ( <ref type="bibr" target="#b7">Dyer et al., 2010)</ref> as an alignment tool to get word-level alignment, and we also use it to lowercase the characters in western languages. We use the stanford-segmenter 11 to segment Chi- nese words, and use Mecab 12 to segment Japanese words. The SnowNLP 13 is used to convert tradi- tional words to simplified ones. Besides, we re- move all the irregular characters (e.g., c</p><p>, £, ♥) in the texts.</p><p>For all the three models, we use stochastic gra- dient descent (SGD) for learning, with a default learning rate of 0.025, negative sampling with 30 samples, skip-gram with context window of size 5, and a subsampling rate of value 1e-4. The em- bedding size is set to 400. The training epochs are all set to 10. All the parameters of α and β used in the three models are simply set to 1. The word embeddings in a document are averaged to get the document representation, and then the lo- gistic regression classier is adopted for sentiment classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results</head><p>The sentiment classification results of our three models and the CL classification methods in the three domains and in the German, French and Japanese languages are presented in <ref type="table" target="#tab_2">Table 2</ref>. The results in the Chinese language are presented in <ref type="table" target="#tab_3">Table 3</ref>. Note that the results of the CL methods are not reported on English test sets, and we only compare our three models on English test sets in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>First and most importantly, we compare our three models. The BM model relies on the di- rect parallel data between the source and target languages, and it generally works slightly better than the other models, including the PMDB mod- el and the UMM model. The reason is that di- rect parallel data can be used for transferring the sentiment information from the source language to the target language directly. However, the perfor- mance achieved by the PDBM model is very close to the BM model in most test cases. In some cas- es (DE-DVD, JP-book and EN-music), the PDBM model can even outperform the BM model. Note that the PDBM model does not leverage the di- rect parallel data between the source and target languages, but uses a pivot language as a bridge. The results demonstrate that the pivot-driven mod- el is very effective for learning bilingual / trilin- gual sentiment-aware word embeddings. The re- sults also verify the feasibility of using pivot lan- guages to address the problem of sentiment clas- sification in resource-poor languages, which lays a good foundation for building a universal senti- ment classifier in multiple languages. When com- paring the UMM model with BM and PDBM, the results of UMM are very close to that of BM and PDBM in most cases, Note that the UMM model does not use the direct parallel corpora of en-de   and en-jp, but relies on pivot-based methods for bridging language gaps. We also find that the d- ifferent parallel corpora used by the UMM model are of different quality and genres, and if they are used at the same time, they may have some neg- ative influence on each other and thus the learned word embeddings are not always better than the BM and PDBM models using only one or two par- allel corpora. What's more, the available parallel data in different language pairs are of various sizes (0.12M ∼ 2.0M). Considering all these issues, the results of UMM are promising because the learned single sentiment classifier can work generally well in multiple languages. We believe that if more high-quality and balanced parallel data are used, the performance of the universal sentiment classi- fier will be improved.</p><p>Second, we compare our models with typical CL classification methods. In <ref type="table" target="#tab_2">Table 2</ref>, we can see our models can outperform MT-BOW, CL-SCL, and CR-RL in most test cases, and outperform BSE in the German language. Our models can achieve very close results with the other sophisti- cated CL methods, including Bi-PV. In <ref type="table" target="#tab_3">Table 3</ref>, we can see our models can generally outperform MT- LR and MT-SVM, and achieve very competitive results with other strong CL methods, including Bi-PV and BSWE. Most CL classification meth- ods rely on commercial machine translation sys- tems (e.g. Google Translate) for translating the reviews (including the training reviews, the test reviews and additional unlabeled reviews) to get parallel data. Compared with the large amount of parallel data used by commercial machine transla- tion systems, the parallel data used by our models are of a very small size. Though our models are simply based on word embeddings, and the paral- lel data used by our models are in a small scale, the performance achieved by our models are very competitive.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the visualization of word embeddings learned by the UMM model for some example words. We can see that similar sentiment words in different languages appear nearby with each other. The figure demonstrate that the UMM model are successful in learning sentiment-aware word embeddings in multiple languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The most closely related work is cross-lingual sen- timent classification, which aims to leverage the labeled sentiment data from a language with rich sentiment resources (e.g., English) to perform sen- timent classification in a target language lacking sentiment resources (e.g., Japanese). Some stud- ies tried to transfer labeled data from the source language to the target language ( <ref type="bibr" target="#b1">Banea et al., 2008;</ref><ref type="bibr" target="#b26">Wan, 2009;</ref><ref type="bibr" target="#b9">Gao et al., 2015;</ref>, and some other studies tried to build a unified feature/semantic space in both two lan- guages <ref type="bibr" target="#b19">(Prettenhofer and Stein, 2010;</ref><ref type="bibr" target="#b28">Xiao and Guo, 2013;</ref><ref type="bibr" target="#b33">Zhou et al., , 2016b</ref><ref type="bibr" target="#b12">Li et al., 2017</ref>). In the latter case, the sentiment classifier learned in the source language can be used for sen- timent classification in both languages. Particular- ly, <ref type="bibr" target="#b26">Wan (2009)</ref> used machine translation to trans- late the source language to the target language to bridge the gap and applied the co-training ap- proach. <ref type="bibr" target="#b19">Prettenhofer and Stein (2010)</ref> provided a CL-SCL model based on structural correspon- dence learning (SCL) for sentiment classification. <ref type="bibr" target="#b14">Lu et al. (2011)</ref> explored to increase the labeled data in both the source and target languages by applying an extra unlabeled parallel data. <ref type="bibr" target="#b28">Xiao and Guo (2013)</ref> expected to get cross-lingual discriminative word embeddings to perform the multiple document classification tasks. Their in- tuitive thought is based on a delicate log-losses function, which aims to increase the probabili- ty of the documents with their labels. Like Lu et al. (2011), <ref type="bibr" target="#b16">Meng et al. (2012)</ref> also proposed their cross-lingual mixture model to leverage an unlabeled parallel dataset. They intended to learn the previously unseen sentimental words from the big parallel corpus. Some studies have attempt- ed to address multi-lingual sentiment classification ( <ref type="bibr" target="#b5">Deriu et al., 2017)</ref>, but different from our study, they directly leverage training data in multiple lan- guages, by assuming the training data can be ob- tained directly or in a distant supervision way in each language, and they did not consider the re- source or data transfer problem at all.</p><p>Word embeddings have shown its great practi- cable usability in plenty of natural language pro- cessing tasks, such as information retrieval <ref type="bibr" target="#b6">(Diaz et al., 2016;</ref><ref type="bibr" target="#b36">Zuccon et al., 2015)</ref>, machine trans- lation ( <ref type="bibr" target="#b21">Shi et al., 2016;</ref><ref type="bibr" target="#b30">Zhang et al., 2014)</ref>, sen- timent analysis <ref type="bibr" target="#b29">Xu et al., 2015;</ref>) and so on. Bilingual word em- beddings have been induced for cross-lingual NLP tasks <ref type="bibr" target="#b25">(Vuli´cVuli´c and Moens, 2015;</ref><ref type="bibr" target="#b10">Guo et al., 2014;</ref><ref type="bibr" target="#b35">Zou et al., 2013;</ref>). In particular,  proposed the BiSkip model to induce bilin- gual word embeddings, which is extended from the monolingual skip-gram model in word2vec to a bilingual model. They added constraint mu- tually on both the source language and the tar- get language, while the monolingual model only has constraint on a single language.  proposed an approach to learning bilin- gual sentiment word embeddings by using sen- timent information of text as supervision, based on labeled corpora and their translations. <ref type="bibr" target="#b8">Ferreira et al. (2016)</ref> used a single optimization problem by combining a co-regularizer for the bilingual em- beddings with a task-specific loss. However, these methods for inducing bilingual word embeddings usually rely on directly parallel corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we proposed an approach to build a universal sentiment classifier in multiple lan- guages. Particularly we proposed a pivot-based model to transfer the sentiment information from the source language to any resource-poor lan- guage via pivot languages. Evaluation results show that the pivot-based model can learn bilin- gual sentiment-aware word embeddings as well as the bilingual model using direct parallel data. Moreover, the universal sentiment classifier built in the five languages can achieve promising result- s.</p><p>In future work, we will investigate using more advanced document embedding techniques (e.g., CNN, RNN) to directly model document-level sentiment information. We will also extend our model to other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>518</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of word embeddings in UMM (Chinese, Japanese, English, French, German). The similar words are marked in the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Parallel corpora used in our models. sion and SVM to learn different classifiers based on the translated Chinese training data. Bag of words features are used for classification.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results (accuracy) on DE (German), FR (French) and JP (Japanese). 

TL 
Domain 
BM 
PDBM UMM 
MT-LR MT-SVM Bi-PV BSWE 

CN 
book 
79.7 77.8 
78.4 
76.5 
77.9 
78.5 81.1 
DVD 
81.7 80.9 
79.8 
79.6 
81.4 
82.0 81.6 
music 
79.2 77.3 
75.8 
74.1 
70.7 
75.3 79.4 

EN 
book 
81.6 80.5 
80.2 
-
-
-
-
DVD 
81.7 80.8 
79.5 
-
-
-
-
music 
76.8 78.8 
77.9 
-
-
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison results (accuracy) on CN (Chinese) and EN (English). 

</table></figure>

			<note place="foot" n="1"> Note that a few methods rely on a machine translation system to produce parallel data between the two languages, while the machine translation system is built on a large amount of parallel data between the two languages. In this sense, the methods rely on both the parallel data for machine translation and the pseudo parallel data produced by machine translation systems.</note>

			<note place="foot" n="2"> Note that the labeled data in other languages can be easily used by our approach in the same way as the English labeled data, and we believe more labeled data will eventually improve the performance of the sentiment classifier.</note>

			<note place="foot" n="3"> Note that the labeled corpus is usually provided in the source language S, which means L is S. but the labeled corpus usually does not overlap with the parallel corpus.</note>

			<note place="foot" n="4"> https://www.uni-weimar.de/medien/webis/corpora/corpuswebis-cls-10/ 5 http://tcci.ccf.org.cn/conference/2013/pages/page04 evares.html 6 http://www.statmt.org/europarl/v7/ 7 https://conferences.unite.un.org/UNCorpus/ 8 http://www.chineseldc.org/resource info.php?rid=141 9 http://alaginrc.nict.go.jp/WikiCorpus/index E.html 10 http://lotus.kuee.kyoto-u.ac.jp/ chu/resource/wiki zh ja.tgz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This work was supported by NSFC (61331011), 863 Program of China (2015AA015403) and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent</head><p>Press Media Technology). We thank the anony-mous reviewers for helpful comments. Xiaojun Wan is the corresponding author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on languages in social media</title>
		<meeting>the workshop on languages in social media</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity analysis using machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification using sentiment sensitive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danushka</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Y</forename><surname>Goulermas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="398" to="410" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to adapt credible knowledge in cross-lingual sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xule</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="419" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Leveraging large amounts of weakly supervised data for multilanguage sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Deriu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valeria</forename><forename type="middle">De</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Cieliebak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
		<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1045" to="1052" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Query expansion with locally-trained word embeddings. Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="367" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">cdec: a decoder, alignment, and learning framework for finite-state and context-free translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Weese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendra</forename><surname>Setiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferhan</forename><surname>Ture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Eidelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
	<note>System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly learning to embed and predict with multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><forename type="middle">S C</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment lexicon learning with bilingual word graph label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning sense-specific word embeddings by exploiting bilingual resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational LinguisticsCOLING</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="497" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Structural correspondence learning for cross-lingual sentiment classification with oneto-many mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nana</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boying</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3490" to="3496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on human language technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint bilingual sentiment classification with unlabeled parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Vector Space Modeling for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-lingual mixture model for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinfan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics: Long Papers</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="572" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning distributed representations for multilingual text sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Workshop on Vector Space Modeling for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crosslanguage text classification using structural correspondence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2010, Proceedings of the Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-07-11" />
			<biblScope unit="page" from="1118" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A topic-enhanced word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">369</biblScope>
			<biblScope unit="page" from="188" to="198" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Knowledge-based semantic embedding for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2245" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning sentiment-specific word embedding for twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1555" to="1565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning bilingual embedding model for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ieee/Wic/</forename><surname>Acm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="134" to="141" />
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Co-training for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2009, Proceedings of the Meeting of the Association for Computational Linguistics and the International Joint Conference on Natural Language Processing of the Afnlp</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02-07" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pivot language approach for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="165" to="181" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised representation learning for cross-lingual text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1465" to="1475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word embedding composition for data imbalances in sentiment and emotion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="226" to="240" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bilinguallyconstrained phrase embeddings for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning bilingual sentiment word embeddings for cross-language sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Degen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention-based lstm network for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-lingual sentiment classification with bilingual document representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1403" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Ziemski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Pouliquen</surname></persName>
		</author>
		<title level="m">The united nations parallel corpus v1.0. In Lrec</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Integrating and evaluating neural word embeddings in information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Zuccon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Azzopardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
