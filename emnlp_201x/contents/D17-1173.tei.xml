<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Graph-based Neural Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
							<email>zhengxq@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Shanghai Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental Graph-based Neural Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1655" to="1665"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Very recently, some studies on neural dependency parsers have shown advantage over the traditional ones on a wide variety of languages. However, for graph-based neural dependency parsing systems, they either count on the long-term memory and attention mechanism to implicitly capture the high-order features or give up the global exhaustive inference algorithms in order to harness the features over a rich history of parsing decisions. The former might miss out the important features for specific headword predictions without the help of the explicit structural information, and the latter may suffer from the error propagation as false early structural constraints are used to create features when making future predictions. We explore the feasibility of explicitly taking high-order features into account while remaining the main advantage of global inference and learning for graph-based parsing. The proposed parser first forms an initial parse tree by head-modifier predictions based on the first-order factorization. High-order features (such as grandparent, sibling, and uncle) then can be defined over the initial tree, and used to refine the parse tree in an iterative fashion. Experimental results showed that our model (called INDP) archived competitive performance to existing benchmark parsers on both English and Chinese datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Motivation</head><p>The rise of machine learning methods in natural language processing (NLP) coupled with the avail- ability of treebanks ( <ref type="bibr" target="#b2">Buchholz and Marsi, 2006)</ref> for a wide variety of languages has led to a rapid increase in research on data-driven dependency parsing. Two predominant paradigms for the data- driven dependency parsing are often called graph- based and transition-based dependency parsing <ref type="bibr">Nivre, 2007, 2011</ref>). The first cat- egory learns the parameters to score correct de- pendency subgraphs over incorrect ones, typically by factoring the graphs into their component di- rected arcs, and performs parsing by searching the highest-scoring graph for a given sentence. The second category of parsing systems instead learns to predict one transition from one parse state to the next given a parse history, and performs parsing by taking the predicted transitions at each parse state until a complete dependency graph is derived.</p><p>Empirical studies show that the graph-based and transition-based models exhibit no statistical- ly significant difference in accuracy on a variety of languages, although they are very different the- oretically ( <ref type="bibr" target="#b25">McDonald and Nivre, 2011</ref>). Graph- based models are usually trained by maximizing the difference in score between the entire correct dependency graph and all incorrect ones for ev- ery training sentence. However, exhaustive infer- ence is generally NP-hard when the score is fac- tored over any extended scope of the dependency subgraph beyond a single arc <ref type="bibr" target="#b27">(McDonald and Satta, 2007)</ref>, which is the primary shortcoming of the graph-based systems. In transition-based parsing, the feature representations are not restricted to a small number of arcs in the graph but can be de- rived from all the dependency subgraphs built so far, while the main disadvantage of these models is that the local greedy parsing strategy may lead to the error propagation because false early predic- tions can eliminate valid parse trees.</p><p>With a few exceptions ( <ref type="bibr">Zeman and Ë˜ Zabokrtsk`Zabokrtsk`y, 2005;</ref><ref type="bibr" target="#b41">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b40">Zhang et al., 2014</ref>), the graph-based parsers usually require global learning and inference, but define features over a limited scope of the dependency graph, while the transition-based ones typically use local, greedy training and inference, but introduce a rich feature space based on the history of parsing decisions.</p><p>Many approaches have been proposed to over- come the weaknesses of traditional graph-based or transition-based models. There are at least three ways for potential improvement: ensemble- weighting the predictions of multiple parsing sys- tems ( <ref type="bibr" target="#b33">Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b13">Hall et al., 2007)</ref>, feature integration-combining the two models by allowing the output of one model to define fea- tures for the other <ref type="bibr" target="#b21">(Martins et al., 2008;</ref><ref type="bibr" target="#b30">Nivre and McDonald, 2008;</ref><ref type="bibr" target="#b25">McDonald and Nivre, 2011)</ref>, and novel approaches-changing the underlying model structure directly by constructing globally trained transition-based parsers <ref type="bibr" target="#b41">(Zhang and Clark, 2008;</ref><ref type="bibr" target="#b15">Huang and Sagae, 2010)</ref> or graph-based parsers with rich features ( <ref type="bibr" target="#b31">Riedel and Clarke, 2006;</ref><ref type="bibr" target="#b29">Nakagawa, 2007;</ref><ref type="bibr" target="#b34">Smith and Eisner, 2008;</ref><ref type="bibr" target="#b22">Martins et al., 2009</ref>).</p><p>Very recently, some studies on the deep archi- tectures have shown advantage over the shallow ones on a wide variety of dependency parsing benchmarks. Deep neural networks were used to replace the classifiers for predicting optimal tran- sitions in transition-based parers <ref type="bibr" target="#b3">(Chen and Manning, 2014</ref>) or the scoring functions for ranking the subgraphs in graph-based rivals <ref type="bibr">(Kiperwasser and Goldberg, 2016a,b)</ref>. There are several re- cent developments in neural dependency parsing <ref type="bibr" target="#b37">(Weiss et al., 2015;</ref><ref type="bibr" target="#b43">Zhou et al., 2015;</ref><ref type="bibr" target="#b8">Dyer et al., 2015)</ref>, which can be viewed as targeting the weak- nesses of locally greedy algorithms in transition- based models by using the beam search and con- ditional random field loss objective, although us- ing the beam search instead of strictly determin- istic parsing can to some extent alleviate the error propagation problem but does not eliminate it.</p><p>For graph-based neural dependency parsing systems, they either count on the long-term mem- ory and neural attention to implicitly capture the high-order features <ref type="bibr" target="#b19">(Kiperwasser and Goldberg, 2016b;</ref><ref type="bibr" target="#b4">Cheng et al., 2016;</ref><ref type="bibr" target="#b7">Dozat and Manning, 2017)</ref> or give up the global inference algorithm- s in order to introduce features over a rich his- tory of parsing decisions by a greedy, bottom-up method <ref type="bibr" target="#b18">(Kiperwasser and Goldberg, 2016a</ref>). The former might miss out the important information for specific headword predictions without the help of the structural features derived from the entire parse tree, while the latter may suffer from the er- ror propagation as false structural constraints are used to create features when making future pre- dictions. In this study, we explore the feasibility of explicitly taking advantage of high-order features while remaining the strength of global exhaustive inference and learning as a graph-based parser.</p><p>The proposed parser first encodes each word in a sentence by distributed embeddings using a con- volutional neural network and constructs an initial parse graph by head-modifier predictions with a maximum directed spanning tree algorithm based on the first-order features (i.e. the score is fac- tored over the arcs in a graph). Once an initial parse graph is built, the high-order features (such as grandparent, sibling, and uncle) can be defined, and used to refine the structure of the parse tree in an iterative way. Theoretically, the refinement will continue until no change is made in the iteration. But experimental results demonstrated that pretty good performance can be achieved with no more than twice updates because many dependencies are determined by independent arc prediction and a few head-modifier pairs need to be re-estimated after one update (i.e. only a few changes above and beyond the dominant first-order scores). We call this proposed model an incremental neural de- pendency parsing (INDP) 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Incremental Neural Dependency Parser</head><p>Given an input sentence x, we denote the set of all valid dependency parse trees that can be con- structed from x as Y(x). Assuming there exists a graph scoring function s, the dependency parsing problem can be formulated as finding the highest scoring directed spanning tree for the sentence x.</p><formula xml:id="formula_0">y * (x) = argmaxË†yâˆˆY argmaxË† argmaxË†yâˆˆY(x) s(x, Ë† y; Î¸)<label>(1)</label></formula><p>where y * (x) is the parse tree with the highest s- core, and Î¸ is a set of the parameters used to com- pute the scores. To make the search tractable, the score of a graph is usually factorized into the sum of its arc (head-modifier) scores <ref type="bibr" target="#b23">(McDonald et al., 2005a</ref>).</p><formula xml:id="formula_1">s(x, Ë† y; Î¸) = (h,m)âˆˆA(Ë† y) s(h, m; Î¸)<label>(2)</label></formula><p>where A(Ë† y) represents a set of directed arcs in the parse treÃª y. The score of an arc (h, m) represents the likelihood of creating a dependency from head h to modifier (or dependent) m in a dependency tree. If each arc score is estimated independent- ly, we call it a first-order factorization. When the scoring is based on two or more arcs, second-or high-order factorizations are applied.</p><p>In traditional approaches, this score is common- ly defined to be the product of a high dimension- al feature representation of the arc and a learned weighting parameter vector. The performance of those systems is heavily dependent on the choice of features. For that reason, much effort in design- ing such systems goes into the feature engineer- ing, which is important but labor-intensive, mainly first based on human ingenuity and linguistic intu- ition, and then confirmed or refined by empirical analyses. In this study, a neural network is de- signed instead to estimate the arc scores using the high-order features. In the following, we first de- scribe how the word representations are produced. Then, the key components of the INDP, direction- specific scoring with special normalization and in- cremental refinement with high-order features, are discussed in detail. Finally, we present the entire parsing algorithm of the INDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Word Feature Representations</head><p>In graph-based neural dependency parsing work, such as <ref type="bibr">(Kiperwasser and Goldberg, 2016a,b;</ref><ref type="bibr" target="#b7">Dozat and Manning, 2017)</ref>, recurrent neural net- work (RNN) is a popular statistical learner used to produce the continuous vector representation- s for each word in a sentence due to its ability to bridge long time lags between relevant inputs. We chose to use one-dimensional convolution instead as a building block because it is good enough to capture the interactions of word feature represen- tations in a context window with less computa- tional cost. Such a design makes the parameters of our first-order parser to be optimized efficient- ly, which will be augmented with the high-order features (i.e. long distance dependencies) at incre- mental refinement stages.</p><p>The words are fed into the network as indices that are used by a lookup operation to transform words into their feature vectors. We consider a fixed-sized word dictionary D 2 . The vector repre-sentations are stored in a word embedding matrix E word âˆˆ R dÃ—|D| , where d is the dimensionality of the vector space (a hyper-parameter to be chosen) and |D| is the size of the dictionary. Like <ref type="bibr" target="#b3">(Chen and Manning, 2014;</ref><ref type="bibr" target="#b8">Dyer et al., 2015;</ref><ref type="bibr" target="#b37">Weiss et al., 2015;</ref><ref type="bibr" target="#b4">Cheng et al., 2016)</ref>, we also map part-of- speech (POS) tags to another q-dimensional vector space, and provide POS type features for words. Formally, assume we are given a sentence x <ref type="bibr">[1:n]</ref> that is a sequence of n words x i , 1 â‰¤ i â‰¤ n. For each word x i âˆˆ D that has an associated index k i into the column of the matrix E word , and is labeled as a POS tag of type l i , its feature representation is obtained by concatenating both word and POS tag embeddings as:</p><formula xml:id="formula_2">E(xi) = E word e k i âŠ• E pos e l i (3)</formula><p>where E pos âˆˆ R qÃ—|P| is a POS tag embedding matrix and |P| is the size of POS tag set P (fine- grained POS tags are used if available). Binary e k i and e l i are one-hot encoding vectors for the ith word in the sentence. The lookup table layer extracts features for each single word, but the meaning of a word is strong- ly related to its surrounding words. Given a word, we consider a fixed size window w (another hyper- parameter) of words around it. More precisely, given an input sentence x <ref type="bibr">[1:n]</ref> , the feature window produced by the first lookup table layer at position x i can be written as:</p><formula xml:id="formula_3">f win x i = (E(x iâˆ’w/2 ) Â· Â· Â· E(xi) Â· Â· Â· E(x i+w/2 )) (4)</formula><p>where the word feature window is a matrix f win âˆˆ R (d+q)Ã—w , and each column of the matrix is the word feature vector in the context window. A one- dimensional convolution is used to yield another feature vector by taking the dot product of filter vectors with the rows of the matrix f win at the same dimension. After each row of f win is con- volved with the corresponding column of a filter matrix W 1 , some non-linear function Ï†(Â·) will be applied as:</p><formula xml:id="formula_4">f con = Ï†(f win W 1 ) (5)</formula><p>where the weights in the matrix W 1 âˆˆ R wÃ—(d+q) are the parameters to be trained, and the output f con âˆˆ R (d+q) is a vector. We choose a hyper- bolic tangent as the non-linear function Ï†. The word feature vectors from a window of text can be computed efficiently thanks to the speed advan- tage of the one-dimensional convolution <ref type="bibr" target="#b16">(Kalchbrenner et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Direction-Specific Scoring</head><p>For the same head-modifier arc (h, m), the head word h may occur on the left size of m (i.e. left- arc) in some sentences while it also can appear on the right size of m (i.e. right-arc) in other ones. Considering two English sentences excerpt- ed from the Penn Treebank ( <ref type="bibr" target="#b2">Buchholz and Marsi, 2006</ref>): "A group of workers exposed to it.", and "Mr. Vinken is chairman of Elsevier, the Dutch publishing group.", they have the same (group, of) head-modifier arc, but those two words occur in different orders. This would not be problem in the traditional models, such as ( <ref type="bibr" target="#b23">McDonald et al., 2005a;</ref><ref type="bibr" target="#b30">Nivre and McDonald, 2008)</ref>, in which the arc directions are directly used as features by their structured learning algorithms. However, it is hard to train a single neural network that gives a higher score to the left-arc case than the right-arc one in some situations while reverses in others because of the symmetries in weight space (Note that we can- not tell which case is correct in advance, and both cases need to be scored). It would be more serious when the first-order factorization is applied due to the lack of context information. Based on the above observations, we use a multi-layer perceptron (MLP) to score the left- arc cases, and another MLP to score the right-arc ones. Those two MLPs share the word and POS tag embeddings, and can update them when nec- essary during the training process. Formally, if a MLP with one hidden layer is used, the score of each possible head-modifier arc is computed as:</p><formula xml:id="formula_5">s(h, m; Î¸) = W 3 (Ï†(W 2 (f con h âŠ• f con m âŠ• f dis h,m ) + b 2 )) (6)</formula><p>where the convolutional outputs of the head and dependent words are concatenated with a bucket- ed distance between the head and modifier, denot- ed by f dis h,m , in buckets of 0 (root), 1, 2, 3-5, and 6+, and feed into the MLP for scoring. The weights in the hidden and output layers are denoted by W 2 and W 3 respectively, and the corresponding bias by b 2 . Once every possible arc is scored, we ob- tain a matrix like <ref type="figure">Figure 1</ref>, in which the element at the row i and column j is the score for (x i , x j ) arc, denoted by s(i, j). An artificial word, x 0 , has been inserted at the beginning of a sentence that will always serve as the single root of the graph and is primarily a means to simplify computation. The scores at the lower (or upper) triangular are computed by the left-arc (or right-arc) MLP, and the shaded elements do not need to be calculated. We can treat s(i, j) as a score of the corresponding arc and then search for the highest scoring directed spanning tree to form a dependency parse tree as proposed in <ref type="bibr" target="#b26">(McDonald et al., 2005b</ref>). This prob- lem can be solved using the Chu-Liu-Edmonds algorithm ( <ref type="bibr" target="#b5">Chu and Liu, 1965;</ref><ref type="bibr" target="#b9">Edmonds, 1967)</ref>, which can be implemented in O(n 2 ).</p><formula xml:id="formula_6">â€¢ â€¢ â€¢ x 0 x n x i â€¢ â€¢ â€¢ x 0 â€¢ â€¢ â€¢ x i â€¢ â€¢ â€¢ x n</formula><p>The score of (xiâˆ’1, xi ) arc</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left-arc scores</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right-arc scores s 0, i s iâˆ’1, i s i+1, i s n, i</head><p>Figure 1: Scoring matrix for possible head and modifier arcs, in which the element at the row i and column j is the score for (xi, xj) arc, denoted by s(i, j). A dependency tree can be formed by finding the highest scoring directed span- ning tree over the scoring matrix.</p><p>The left-arc and right-arc MLPs should care- fully collaborate with each other; otherwise, one MLP would be overwhelmed by another (i.e. the maximum score produced by one MLP is less than the minimum by another). To overcome this bias problem, we use the partition function by sum- ming over the elements in each row of the scoring matrix, namely the scores/probabilities are nor- malized across the two MLPs. The conditional probability of arc (x i , x j ) given a sentence x <ref type="bibr">[1:n]</ref> is defined as:</p><formula xml:id="formula_7">p((xi, xj)|x [1:n] ; Î¸) = exp s(xi, xj; Î¸) Zi(x [1:n] ; Î¸)</formula><p>where Zi(x <ref type="bibr">[1:n]</ref> ; Î¸) = Î£ iâˆˆ{0...n},i =j exp s(xi, xj; Î¸)</p><p>Each Z i (x <ref type="bibr">[1:n]</ref> ; Î¸) is a normalization term used to predict x i 's head word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Incremental Refinement with High-order Features</head><p>Given an input sentence, once the initial depen- dency tree is built using the first-order factoriza- tion, we can define the high-order features over the resulting tree. For each head-modifier arc, the modifier's left sibling, right sibling, leftmost child, and rightmost child vector representations are con- catenated with the inputs of Equation <ref type="formula">(6)</ref>, which are then feed into two new left-arc and right-arc MLPs to update the scoring matrix. Like the head and modifier, those additional feature representa- tions are added as the results produced by the con- volution layer. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, commonly- used high-order features have been take into ac- count, such as consecutive sibling (H, B, S), tri- siblings (B, M, S), and grandparent (H, M, R). The missing feature vectors are replaced by one of four special vectors, namely "left-sibling", "right- sibling", "leftmost-child", and "rightmost-child" according to their relations to the modifier word. Although high-order features are used, the high- est scoring parse tree still can be founded efficient- ly in O(n 2 ) by the Chu-Liu-Edmonds algorithm ( <ref type="bibr" target="#b5">Chu and Liu, 1965;</ref><ref type="bibr" target="#b9">Edmonds, 1967)</ref>. The main rationale is that, even in the presence of high-order features, the resulting scores remain based on s- ingle head-modifier arcs. The higher-order fea- tures are derived from the parse tree obtained with first-order inference, and because that tree is al- ready pretty good, these higher-order features end up being a good approximation, and such approx- imation can be further improved by incremental refinements upon the parse tree. Thus, the high- order features used by the scoring MLPs can offer deliberate refinement above and beyond the first- order results. Theoretically, the refinement can be made until there is no update in the scoring matrix. However, experimental results show that compa- rable performance can be achieved with no more than twice high-order refinements (see Section 3).</p><p>We add a softmax layer to the network (after re- moving the last scoring layer) to predict syntactic labels for each arc. Labeling is trained by mini- mizing the cross-entropy error of the softmax layer using backpropagation. The network performs the structure prediction and labeling jointly. The two tasks shared the several layers (from the input to convolutional layers) of the network. When mini-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs:</head><p>Î¸: neural network parameters.</p><p>x: an input sentence.</p><p>T : maximum number of iterations. Output: optimal dependency tree y * . Algorithm: 1: form an initial tree using the first-order features; 2: t = 0; 3: repeat 4: update the scoring matrix using the high-order features; 5: find the highest scoring tree y by Chu-Liu-Edmonds algorithm; 6: t = t + 1; 7: until no change in this iteration or t â‰¥ T ; 8: predict syntactic labels based on the parse tree y; 9: return y * = y; mizing the cross-entropy error of the softmax lay- er, the error will also backpropagate and influence both the network parameters and the embeddings. We list our incremental neural dependency pars- ing algorithm in <ref type="figure" target="#fig_1">Figure 3</ref>. Staring with an initial tree formed using the first-order features, the al- gorithm makes changes to the parse tree with the high-order refinements in an attempt to climb the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training</head><p>Given a training example (x, y), we defined a structured margin âˆ†(x, y, Ë† y) loss for proposing a parsÃª y for sentence x when y is the true parse. This penalty is proportional to the number of un- labeled arcs on which the two parse trees do not agree. In general, âˆ†(x, y, Ë† y) is equal to 0 if y = Ë† y. The loss function is defined as a penalization of incorrect arcs:</p><formula xml:id="formula_9">âˆ†(x, y, Ë† y) = (h,m)âˆˆA(Ë† y) Îº1{(h, m) / âˆˆ A(y)}<label>(8)</label></formula><p>where Îº is a penalization term to each incorrect arc, and A(y) is a set of arcs in the true parse y.</p><p>For a training set, we seek a function with small expected loss on unseen sentences. The function we consider take the following form as Equation (1). The score of a treÃª y is higher if the algorithm is more confident that the structure of the tree is correct. In the max-margin estimation framework, we want to ensure that the highest scoring tree is the true parse for all training instances (x i , y i ), i = 1, Â· Â· Â· , h, and it's score to be larger up to a margin defined by the loss. For all i in the training data:</p><formula xml:id="formula_10">s(Î¸, x i , y i ) â‰¥ s(Î¸, x i , Ë† y) + âˆ†(x i , y i , Ë† y)<label>(9)</label></formula><p>These lead us to minimize the following regu- larized objective for h training instances:</p><formula xml:id="formula_11">J(Î¸) = 1 h h i=1 E i (Î¸) + Î» 2 ||Î¸|| 2 , where</formula><formula xml:id="formula_12">Ei(Î¸) = maxË†yâˆˆY maxË† maxË†yâˆˆY(x i ) (0, (s(Î¸, x i , Ë† y) + âˆ†(x i , y i , Ë† y)) âˆ’ s(Î¸, x i , y i ))<label>(10)</label></formula><p>where the coefficient Î» governs the relative impor- tance of the regularization term compared with the error. The trees are penalized more by the loss when they deviate from the correct one. Minimiz- ing this objective maximizes the score of the cor- rect tree, and minimizes that of the highest scoring but incorrect parse tree. The objective is not differ- entiable due to the hinge loss. We use the subgra- dient method to compute a gradient-like direction for minimizing the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conducted three sets of experiments. The first one is to test several variants of the INDP on the development set, to gain some understanding of how the choice of hyper-parameters impacts up- on the performance. The goal of the second one is to see how well the incremental approach en- hanced with the high-order features to improve the first-order results by analysing parsing errors rela- tive to sentence length. In the third set, we com- pared the performance of the INDP with existing state-of-the-art models on both English and Chi- nese datasets. We report unlabeled attachment s- cores (UAS) and labeled attachment scores (LAS) with punctuations being omitted from evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We show test results for the proposed model on the English Penn Treebank (PTB), converted into Stanford dependencies using version 3.3.0 of the Stanford dependency converter, and the Chinese Penn Treebank (CTB). We follow the standard s- plits of PTB, using section 2-21 for training, sec- tion 22 as development set and 23 as test set. We use POS tags generated from the Stanford POS tagger ( <ref type="bibr" target="#b36">Toutanova et al., 2003)</ref>; for the Chinese PTB dataset, we use gold word segmentation and POS tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Strategy</head><p>Previous work demonstrated that the performance can be improved by using word embeddings learned from large-scale unlabeled data in many NLP tasks both in <ref type="bibr">English (Collobert et al., 2011;</ref><ref type="bibr" target="#b35">Socher et al., 2011</ref>) and Chinese ( <ref type="bibr" target="#b42">Zheng et al., 2013)</ref>. Unsupervised pretraining guides the learn- ing towards basins of attraction of minima that support better generalization ( <ref type="bibr" target="#b11">Erhan et al., 2010)</ref>. We leveraged large unlabeled corpus to learn word embeddings, and then used these improved em- beddings to initialize the word embedding ma- trices of the neural networks. English and Chi- nese Wikipedia documents were used to train the word embeddings by Word2Vec tool 3 proposed in ( <ref type="bibr" target="#b28">Mikolov et al., 2013</ref>). Previous studies show that a joint solution (i.e., performing several tasks at the same time) usu- ally leads to the improvement in accuracy over pipelined systems because the error propagation is avoided and the various information normally used in the different steps of pipelined systems can be integrated. The INDP networks are also trained in a joint way, but adopting three-step strat- egy. The parameters of the parsing neural network using the first-order factorization are first learned, and when its unlabeled parsing accuracy exceeds a given threshold (e.g. 85%), we start to train the high-order parsing network. The weights already trained in the first step will remain unchanged for the first several epochs, and they are in fact used to generate the high-order features. After the parsing accuracy reaches another threshold (e.g. 90%), all the parameters for the first-order, and high-order predictions as well as labeling are trained jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyper-parameter Choices</head><p>Hyper-parameters was tuned with the PTB 3.3.0 development set by trying only a few different net- works. Generally, the dimensionality of the em- beddings, and the numbers of hidden units, pro- vided they are large enough, have a limited impact on the generalization performance. In the follow- ing experiments, the window size was set to 5, the learning rate to 0.02, and the number of hidden layer to 300. The embedding size of words was set to 50, and that of tags to 30, which achieved a good trade-off between speed and performance. All ex- periments were run on a computer equipped with an Intel Xeon processor working at 2.2GHz, with 16GB RAM and a NVIDIA Titan GPU. The pars- ing speed of the INDP is around 250-300 sents/sec in average on the PTB dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sentence Length Factors</head><p>It is well known that dependency parsers tend to have lower accuracies for longer sentences be- cause the increased presence of complex syntactic structures. In order to get a better understanding of how well the incremental strategy and high-order features benefit the models, <ref type="figure" target="#fig_2">Figure 4</ref> shows the ac- curacy of our neural dependency parser using the first-order features only (indicated with "NDP + First-order") and INDP with at most twice high- order refinements (indicated with "INDP + High- order + M2") on the English PTB develop set. For simplicity, the experiments report unlabeled pars- ing accuracy, and identical experiments using la- beled parsing accuracy did not reveal any addition- al information. The INDP with the high-order refinements is more precise than the parser using only the first- order features. Due to the fact that longer de- pendencies are typically harder to parse, there is still a degradation in performance for our INDP. However, the accuracy curve for INDP is slightly flatter than its reduced version in which the high- order features and incremental recipe are not ap- plied when the sentence length is within 11-50. This behavior can be explained by the reasons that the feature representations are not restricted to a limited number of graph arcs, but can take into ac- count with the (almost) entire dependency graph built so far at the refinement stages of the INDP, and it do offer substantial refinements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Results</head><p>We report the experimental results on the English PTB and Chinese CTB datasets in <ref type="table" target="#tab_0">Table 1 and 2</ref> respectively, in which our networks are denoted by "INDP". The "M1" indicates that the results are obtained by the INDP with just one refinemen- t over the parse graphs built using the first-order features, and similarly, the "M2" indicates the re- sults are achieved by the INDP with at most twice high-order refinements, while the "UNC" in the last row indicates that the refinements will contin- ue until no change is made in the structure pre- dictions (see the algorithm listed in <ref type="figure" target="#fig_1">Figure 3</ref>). All compared transition-based parsing systems are in- dicated by a " â€¡", and graph-based ones by " Â§". From these numbers, a handful of trends are readily apparent. Firstly, we note that the "full- fledged" INDP (indicated with "UNC") is superi- or to that without the high-order refinements by a fairly significant margin (5.01% for English and 6.55% for Chinese in LAS). Another striking re- sult of these experiments is that comparable per- formance can be obtained by no more than twice refinements with high-order features, and "INDP + High-order + M2" achieves a good trade-off be- tween the performance and parsing complexity. Our INDP gets nearly the same performance on the English PTB as the current models of <ref type="bibr" target="#b20">(Kuncoro et al., 2016)</ref> and <ref type="bibr" target="#b7">(Dozat and Manning, 2017)</ref> in spite of its simpler architectures, and gets state- of-the-art UAS accuracy on the Chinese CTB. The INDP lags behind in LAS, indicating one of a few possibilities. Firstly, we tried only a few differ- ent network configurations, and there are many ways (such as using deeper architectures, and re- cruiting bi-directional recurrent neural networks to produce word feature representations) that we could improve it further. Secondly, the model of ( <ref type="bibr" target="#b20">Kuncoro et al., 2016</ref>) is particularly designed to capture phrase compositionality, and thus, another possible improvement is to capture such composi- tionality by optimizing the network architectures, which may also lead to a better label score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Dependency-based syntactic representations of sentences have been found to be useful for vari- ous NLP tasks, especially for those involving nat- ural language understanding in some way. We briefly review prior work both on graph-based and transition-based neural dependency parsers.</p><p>In transition-based parsing, we learn a model for scoring transitions from one state to the next, conditioned on the parse history, and parse a sen- tence by taking the highest-scoring transition out of every state until a complete dependency graph has been derived. <ref type="bibr" target="#b3">Chen and Manning (2014)</ref> made the first successful attempt at introducing deep learning into a transition-based dependency pars- er. At each step, the feed-forward neural network assigns a probability to every action the parse can take from certain state (words on the stack and buffer). Some researchers have attempted to ad- dress the limitations of <ref type="bibr" target="#b3">(Chen and Manning, 2014)</ref> by augmenting it with additional complexity.</p><p>A beam search and a conditional random field loss function were incorporated into the transition- based neural network models ( <ref type="bibr" target="#b37">Weiss et al., 2015;</ref><ref type="bibr" target="#b43">Zhou et al., 2015;</ref><ref type="bibr" target="#b0">Andor et al., 2016)</ref>, which allow the parsers to keep the top-k partial parse trees and revoke previous actions once it finds evidence that they may have been incorrect by locally greedy choices. <ref type="bibr" target="#b8">Dyer et al (2015)</ref> used three LSTMs to represent the buffer, stack, and parsing history, getting state-of-the-art results on Chinese and En- glish dependency parsing tasks.</p><p>Graph-based parsers use machine learning for scoring each possible edge for a given sentence, typically by factoring the graphs into their compo- nent arcs, and constructing the parse tree with the highest score from these weighted edges. <ref type="bibr" target="#b19">Kiperwasser and Goldberg (2016b)</ref> presented a neural graph-based parser in which the bi-directional L- STM's recurrent output vector for each word is concatenated with each possible head's vector (al- so produced by the same biLSTM), and the result is used as input to a multi-layer perceptron (MLP) for scoring this modifier-head pair. Given the s- cores of the arcs, the highest scoring tree is con- structed using Eisner's decoding algorithm <ref type="bibr" target="#b10">(Eisner, 1996)</ref>. Labels are predicted similarly, with each word's recurrent output vector and its head's vector being used in a multi-class MLP.</p><p>Kiperwasser and Goldberg (2016a) also pro- posed a hierarchical tree LSTM to model the de- pendency tree structures in which each word is represented by the concatenation of its left and right modifier (child) vectors, and the modifier vectors are generated by two (leftward or right- ward) recurrent neural networks. The tree repre- sentations were produced in a bottom-up recursive way with the (greedy) easy-first parsing algorithm <ref type="bibr" target="#b12">(Goldberg and Elhadad, 2010)</ref>. Similarly, Cheng et al <ref type="formula" target="#formula_0">(2016)</ref> proposed a graph-based neural depen- dency parser that is able to predict the scores for the next arc, conditioning on previous parsing de- cisions. In addition to using one bi-directional re- current network that produces a recurrent vector for each word, they also have uni-directional re- current neural networks (left-to-right and right-to- left) that keep track of the probabilities of each previous parsing actions. In their many-task neural model, Hashimoto et al (2016) included a graph-based dependency parse in which the traditional MLP-based method that <ref type="bibr" target="#b19">Kiperwasser and Goldberg (2016b)</ref> used was replaced with a bilinear one. <ref type="bibr" target="#b7">Dozat and Manning (2017)</ref> modified the neural graph-based approach of <ref type="bibr" target="#b19">(Kiperwasser and Goldberg, 2016b</ref>) in a few ways to improve the performance. In addition to building a network that is larger and uses more regularization, they replace the traditional MLP- based attention mechanism and affine label classi- fier with biaffine ones. This work is most closely related to the graph- based parsing approaches with multiple high-order refinements <ref type="bibr" target="#b32">(Rush and Petrov, 2012;</ref><ref type="bibr" target="#b40">Zhang et al., 2014</ref>), although the neural networks were not used in their parsers. <ref type="bibr" target="#b32">Rush and Petrov (2012)</ref> proposed a multi-pass coarse-to-fine approach in which a coarse model was used to prune the search space in order to make the inference with up to third- order features practical. They start with a linear- time vine pruning pass and build up to high-order models. <ref type="bibr" target="#b40">Zhang et al (2014)</ref> introduced a random- ized greedy algorithm for dependency parsing in which they begin with a tree drawn from the u- niform distribution and use hill-climbing strategy to find the optimal parse tree. Although they re- ported that drawing the initial tree randomly re- sults in the same performance as when initialized from a trained first-order distribution, but multi- ple random restarts are required to avoid getting stuck in a locally optimal solution. Their greedy algorithm breaks the parsing into a sequence of lo- cal steps, which correspond to choosing the head for each modifier word (one arc at a time) in the bottom-up order relative to the current tree. In contrast, we employed the global inference algo- rithm to change the entire tree (all at a time) in each refinement step, which makes the improve- ment more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Graph-based parsers cannot easily condition on any extended scope of the dependency parse tree beyond a single arc, which is their primary short- coming relative to transition-based competitors. We have shown that a simple, generally applica- ble incremental neural dependency parsing algo- rithm can deliver close to state-of-the-art parsing performance, which allows the high-order features to be taken into account without hurting the advan- tage of global exhaustive inference and learning as a member of graph-based parsing systems. Fu- ture work will involve exploring ways of augment- ing the parser with a more innovative architecture than the relatively simple one used in current neu- ral graph-based parsers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-order features. (a) An example dependency parse tree. (b) The subgraph used to define high-order features for the head-modifier arc (H, M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Incremental neural dependency parsing (INDP) algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy relative to sentence length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Results on the English PTB dataset.</head><label>1</label><figDesc></figDesc><table>Model 
UAS 
LAS 
Zhou et al (2015) â€¡ 
93.28 92.35 
Weiss et al (2015) â€¡ 
94.26 92.41 
Ballesteros et al (2016) â€¡ 
93.56 91.42 
Kiperwasser and Goldberg (2016b) â€¡ 
93.90 91.90 
Andor et al (2016) â€¡ 
94.61 92.79 
Kuncoro et al (2016) â€¡ 
95.80 94.60 
Kiperwasser and Goldberg (2016a) Â§ 
93.00 90.90 
Cheng et al (2016) Â§ 
94.10 91.49 
Hashimoto et al (2016) Â§ 
94.67 92.90 
Dozat and Manning (2017) Â§ 
95.74 94.08 
NDP + First-order 
90.88 88.93 
INDP + High-order + M1 
93.31 91.51 
INDP + High-order + M2 
94.76 93.12 
INDP + High-order + UNC 
95.53 93.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Results on the Chinese CTB dataset.</head><label>2</label><figDesc></figDesc><table>Model 
UAS 
LAS 
Ballesteros et al (2016) â€¡ 
87.65 86.21 
Kiperwasser and Goldberg (2016b) â€¡ 
87.60 86.10 
Kiperwasser and Goldberg (2016a) Â§ 
87.10 85.50 
Cheng et al (2016) Â§ 
88.10 85.70 
Dozat and Manning (2017) Â§ 
89.30 88.23 
NDP + First-order 
82.97 81.39 
INDP + High-order + M1 
87.35 85.82 
INDP + High-order + M2 
88.78 87.28 
INDP + High-order + UNC 
89.42 87.94 

</table></figure>

			<note place="foot" n="1"> The source code is available at http://homepage.fudan. edu.cn/zhengxq/deeplearning/</note>

			<note place="foot" n="2"> Unless otherwise specified, the word dictionary is extracted from the training set. Unknown words are mapped to a special symbol that is not used elsewhere.</note>

			<note place="foot" n="3"> Available at http://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their valuable comments. This work was partly supported by a grant from Shang-hai Municipal Natural Science Foundation (No. 15511104303).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL&apos;16)</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics (ACL&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Training with exploration improves a greedy stack-LSTM parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;16)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;16)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoNLLX shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computational Natural Language Learning (CoNLL&apos;06)</title>
		<meeting>the International Conference on Computational Natural Language Learning (CoNLL&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bi-directional attention with agreement for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<idno>arXiv:</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the shortest arborescence of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoeng-Jin</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientia Sinica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1396" to="1400" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR&apos;17)</title>
		<meeting>the International Conference on Learning Representations (ICLR&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transitionbased dependency paring with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;15)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimum branchings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Computational Linguistics (COLING&apos;96)</title>
		<meeting>the 16th International Conference on Computational Linguistics (COLING&apos;96)</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Why does unsupervised pre-training help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL&apos;10)</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single Malt or blended? a study in multilingual parser optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>GÃ¼lsen EryiË˜ git, BeÃ¡ta Megyesi, Mattias Nilsson, and Markus Saers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka Caiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01587</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenjie</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL&apos;10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL&apos;14)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Easyfirst dependency parsing with hierarchical tree LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="445" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations. Transactions of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="313" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What do recurrent neural network grammars learn about syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer Lingpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Neibig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacking dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>AndrÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanian</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Concise integer liear programming formulations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>AndrÃ©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACLIJCNLP&apos;09)</title>
		<meeting>the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing (ACLIJCNLP&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online large-margin training of dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characterizing the errors of data-driven dependency parsing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analyzing and integrating dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Lingustics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="197" to="230" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>HanjiË˜ C</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conferece and the International Conference on Empirical Methods in Natural Language Processing (HLTEMNLP&apos;05)</title>
		<meeting>the Human Language Technology Conferece and the International Conference on Empirical Methods in Natural Language Processing (HLTEMNLP&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the complexity of non-projective data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Parsing Technologies (IWPT&apos;07)</title>
		<meeting>the International Workshop on Parsing Technologies (IWPT&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multilingual depdendency parsing using global features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tetsuji</forename><surname>Nakagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</title>
		<meeting>the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL&apos;08:HLT)</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL&apos;08:HLT)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incremental integer linear programming for non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;06)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vine pruning for efficient multi-pass dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno>NAA- CL&apos;12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parser combination by reparsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL&apos;06)</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics (NAACL&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C-Y.</forename><surname>Cliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine learning (ICML&apos;11)</title>
		<meeting>the International Conference on Machine learning (ICML&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature-rich partof-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL&apos;03)</title>
		<meeting>the Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stuctured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd</title>
		<meeting>the 53rd</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL&apos;15)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving parsing accuracy by combining diverse dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">ZdenË˜</forename><surname>Zabokrtsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Parsing Technologies (IWPT&apos;05)</title>
		<meeting>the International Workshop on Parsing Technologies (IWPT&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Zabokrtsk`y</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Greed is good if randomized: New inference for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A tale of two parses: Investigating and bombining graph-based and transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning for Chinese word segmentation and pos tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)</title>
		<meeting>the International Conference on Empirical Methods in Natural Language Processing (EMNLP&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A neural probabilistic structuredprediction model for transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;15)</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
