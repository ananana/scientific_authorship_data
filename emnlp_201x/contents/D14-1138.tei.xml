<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moontae</forename><surname>Lee</surname></persName>
							<email>moontae@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Information Science</orgName>
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
							<email>mimno@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Information Science</orgName>
								<orgName type="institution" key="instit1">Cornell University Ithaca</orgName>
								<orgName type="institution" key="instit2">Cornell University Ithaca</orgName>
								<address>
									<postCode>14853, 14853</postCode>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1319" to="1328"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The anchor words algorithm performs provably efficient topic model inference by finding an approximate convex hull in a high-dimensional word co-occurrence space. However, the existing greedy algorithm often selects poor anchor words, reducing topic quality and interpretability. Rather than finding an approximate convex hull in a high-dimensional space, we propose to find an exact convex hull in a visualizable 2-or 3-dimensional space. Such low-dimensional embeddings both improve topics and clearly show users why the algorithm selects certain words.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Statistical topic modeling is useful in exploratory data analysis <ref type="bibr" target="#b4">(Blei et al., 2003</ref>), but model infer- ence is known to be NP-hard even for the sim- plest models with only two topics ( <ref type="bibr" target="#b16">Sontag and Roy, 2011)</ref>, and training often remains a black box to users. Likelihood-based training requires expensive approximate inference such as varia- tional methods ( <ref type="bibr" target="#b4">Blei et al., 2003)</ref>, which are deter- ministic but sensitive to initialization, or Markov chain Monte Carlo (MCMC) methods ( <ref type="bibr" target="#b9">Griffiths and Steyvers, 2004</ref>), which have no finite conver- gence guarantees. Recently Arora et al. proposed the Anchor Words algorithm ( <ref type="bibr" target="#b3">Arora et al., 2013)</ref>, which casts topic inference as statistical recovery using a separability assumption: each topic has a specific anchor word that appears only in the context of that single topic. Each anchor word can be used as a unique pivot to disambiguate the corresponding topic distribution. We then recon- struct the word co-occurrence pattern of each non- anchor words as a convex combination of the co- occurrence patterns of the anchor words. This algorithm is fast, requiring only one pass through the training documents, and provides provable guarantees, but results depend entirely on selecting good anchor words. ( <ref type="bibr" target="#b3">Arora et al., 2013)</ref> propose a greedy method that finds an approxi- mate convex hull around a set of vectors corre- sponding to the word co-occurrence patterns for each vocabulary word. Although this method is an improvement over previous work that used im- practical linear programming methods ( <ref type="bibr" target="#b2">Arora et al., 2012</ref>), serious problems remain. The method greedily chooses the farthest point from the cur- rent subspace until the given number of anchors have been found. Particularly at the early stages of the algorithm, the words associated with the farthest points are likely to be infrequent and id- iosyncratic, and thus form poor bases for human interpretation and topic recovery. This poor choice of anchors noticeably affects topic quality: the an- chor words algorithm tends to produce large num- bers of nearly identical topics.</p><p>Besides providing a separability criterion, an- chor words also have the potential to improve topic interpretability. After learning topics for given text collections, users often request a label that sum- marizes each topic. Manually labeling topics is ar- duous, and labels often do not carry over between random initializations and models with differing numbers of topics. Moreover, it is hard to con- trol the subjectivity in labelings between annota- tors, which is open to interpretive errors. There has been considerable interest in automating the labeling process ( <ref type="bibr" target="#b14">Mei et al., 2007;</ref><ref type="bibr" target="#b13">Lau et al., 2011;</ref><ref type="bibr" target="#b6">Chuang et al., 2012</ref><ref type="bibr" target="#b6">). (Chuang et al., 2012</ref>) pro- pose a measure of saliency: a good summary term should be both distinctive specifically to one topic and probable in that topic. Anchor words are by definition optimally distinct, and therefore may seem to be good candidates for topic labels, but greedily selecting extreme words often results in anchor words that have low probability.</p><p>In this work we explore the opposite of Arora et al.'s method: rather than finding an approximate convex hull for an exact set of vectors, we find an exact convex hull for an approximate set of vec- tors. We project the V × V word co-occurrence matrix to visualizable 2-and 3-dimensional spaces using methods such as t-SNE (van der <ref type="bibr" target="#b17">Maaten and Hinton, 2008)</ref>, resulting in an input matrix up to 3600 times narrower than the original input for our training corpora. Despite this radically low- dimensional projection, the method not only finds topics that are as good or better than the greedy anchor method, it also finds highly salient, in- terpretable anchor words and provides users with a clear visual explanation for why the algorithm chooses particular words, all while maintaining the original algorithm's computational benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b4">Blei et al., 2003</ref>) models D documents with a vocabulary V using a predefined number of topics by K. LDA views both {A k } K k=1 , a set of K topic-word distri- butions for each topic k, and</p><formula xml:id="formula_0">{W d } D d=1 , a set of D document-topic distributions for each document d, and {z d } D d=1</formula><p>, a set of topic-assignment vectors for word tokens in the document d, as randomly gen- erated from known stochastic processes. Merging {A k } as k-th column vector of V × K matrix A, {W d } as d-th column vector of K × D matrix W , the learning task is to estimate the posterior dis- tribution of latent variables A, W , and {z d } given V × D word-document matrixˆMmatrixˆ matrixˆM , which is the only observed variable where d-th column corre- sponds to the empirical word frequencies in the training documents d.</p><p>( <ref type="bibr" target="#b3">Arora et al., 2013</ref>) recover word-topic matrix A and topic-topic matrix R = E[W W T ] instead of W in the spirit of nonnegative matrix factoriza- tion. Though the true underlying word distribu- tion for each document is unknown and could be far from the sample observationˆMobservationˆ observationˆM , the empirical word-word matrixˆQmatrixˆ matrixˆQ converges to its expectation AE[W W T ]A T = ARA T as the number of docu- ments increases. Thus the learning task is to ap- proximately recover A and R pretending that the empiricaî Q is close to the true second-order mo- ment matrix Q.</p><p>The critical assumption for this method is to suppose that every topic k has a specific anchor word s k that occurs with non-negligible probabil- ity (&gt; 0) only in that topic. The anchor word s k need not always appear in every document about the topic k, but we can be confident that the doc- ument is at least to some degree about the topic k if it contains s k . This assumption drastically im- proves inference by guaranteeing the presence of a diagonal sub-matrix inside the word-topic ma- trix A. After constructing an estimatê Q, the al- gorithm in ( <ref type="bibr" target="#b3">Arora et al., 2013</ref>) first finds a set S = {s 1 , ..., s K } of K anchor words (K is user- specified), and recovers A and R subsequently based on S. Due to this structure, overall perfor- mance depends heavily on the quality of anchor words.</p><p>In the matrix algebra literature this greedy anchor finding method is called QR with row- pivoting. Previous work classifies a matrix into two sets of row (or column) vectors where the vec- tors in one set can effectively reconstruct the vec- tors in another set, called subset-selection algo- rithms. ( <ref type="bibr" target="#b10">Gu and Eisenstat, 1996</ref>) suggest one im- portant deterministic algorithm. A randomized al- gorithm provided by <ref type="bibr" target="#b5">(Boutsidis et al., 2009</ref>) is the state-of-the art using a pre-stage that selects the candidates in addition to ( <ref type="bibr" target="#b10">Gu and Eisenstat, 1996)</ref>. We found no change in anchor selection using these algorithms, verifying the difficulty of the an- chor finding process. This difficulty is mostly be- cause anchors must be nonnegative convex bases, whereas the classified vectors from the subset se- lection algorithms yield unconstrained bases.</p><p>The t-SNE model has previously been used to display high-dimensional embeddings of words in 2D space by Turian. 1 Low-dimensional embed- dings of topic spaces have also been used to sup- port user interaction with models: <ref type="bibr" target="#b8">(Eisenstein et al., 2011</ref>) use a visual display of a topic embed- ding to create a navigator interface. Although t-SNE has been used to visualize the results of topic models, for example by <ref type="bibr" target="#b12">(Lacoste-Julien et al., 2008)</ref> and ( <ref type="bibr" target="#b19">Zhu et al., 2009</ref>), we are not aware of any use of the method as a fundamental compo- nent of topic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Low-dimensional Embeddings</head><p>Real text corpora typically involve vocabularies in the tens of thousands of distinct words. As the input matrixˆQmatrixˆ matrixˆQ scales quadratically with V , the Anchor Words algorithm must depend on a low- dimensional projection ofˆQofˆ ofˆQ in order to be practi- cal. Previous work ( <ref type="bibr" target="#b3">Arora et al., 2013</ref>) uses ran- dom projections via either Gaussian random ma- trices <ref type="bibr" target="#b11">(Johnson and Lindenstrauss, 1984)</ref> or sparse random matrices <ref type="bibr" target="#b0">(Achlioptas, 2001)</ref>, reducing the representation of each word to around 1,000 di- mensions. Since the dimensionality of the com- pressed word co-occurrence space is an order of magnitude larger than K, we must still approxi- mate the convex hull by choosing extreme points as before.</p><p>In this work we explore two projection meth- ods: PCA and t-SNE (van der <ref type="bibr" target="#b17">Maaten and Hinton, 2008)</ref>. Principle Component Analysis (PCA) is a commonly-used dimensionality reduction scheme that linearly transforms the data to new coordi- nates where the largest variances are orthogonally captured for each dimension. By choosing only a few such principle axes, we can represent the data in a lower dimensional space. In contrast, t-SNE embedding performs a non-linear dimensionality reduction preserving the local structures. Given a set of points {x i } in a high-dimensional space X, t-SNE allocates probability mass for each pair of points so that pairs of similar (closer) points be- come more likely to co-occur than dissimilar (dis- tant) points.</p><formula xml:id="formula_1">p j|i = exp(−d(x i , x j ) 2 /2σ 2 i ) k =i exp(−d(x i , x k ) 2 /2σ 2 i )<label>(1)</label></formula><formula xml:id="formula_2">p ij = p j|i + p i|j 2N (symmetrized)<label>(2)</label></formula><p>Then it generates a set of new points {y i } in low-dimensional space Y so that probability dis- tribution over points in Y behaves similarly to the distribution over points in X by minimizing KL- divergence between two distributions:</p><formula xml:id="formula_3">q ij = (1 + y i − y j 2 ) −1 k =l (1 + y k − y l 2 ) −1 (3) min KL(P ||Q) = i =j p ij log p ij q ij (4)</formula><p>Instead of approximating a convex hull in such a high-dimensional space, we select the exact vertices of the convex hull formed in a low- dimensional projected space, which can be calcu- lated efficiently. <ref type="figure" target="#fig_0">Figures 1 and 2</ref> show the con- vex hulls for 2D projections ofˆQofˆ ofˆQ using t-SNE and PCA for a corpus of Yelp reviews. <ref type="figure" target="#fig_2">Figure 3</ref> il- lustrates the convex hulls for 3D t-SNE projection for the same corpus. Anchor words correspond to the vertices of these convex hulls. Note that we present the 2D projections as illustrative examples only; we find that three dimensional projections perform better in practice. In addition to the computational advantages, this approach benefits anchor-based topic model- ing in two aspects. First, as we now compute the exact convex hull, the number of topics depends on the dimensionality of the embedding, v. For example in the figures, 2D projection has 21 ver- tices, whereas 3D projection supports 69 vertices. This implies users can easily tune the granularity of topic clusters by varying v = 2, 3, 4, ... with- out increasing the number of topics by one each time. Second, we can effectively visualize the the- matic relationships between topic anchors and the rest of words in the vocabulary, enhancing both interpretability and options for further vocabulary curation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We find that radically low-dimensional t-SNE pro- jections are effective at finding anchor words that are much more salient than the greedy method, and topics that are more distinctive, while maintain- ing comparable held-out likelihood and semantic coherence. As noted in Section 1, the previous greedy anchor words algorithm tends to produce many nearly identical topics. For example, 37 out of 100 topics trained on a 2008 political blog cor- pus have obama, mccain, bush, iraq or palin as their most probable word, including 17 just for obama. Only 66% of topics have a unique top word. In contrast, the t-SNE model on the same dataset has only one topic whose most probable word is obama, and 86% of topics have a unique top word (mccain is the most frequent top word, with five topics).</p><p>We use three real datasets: business reviews from the Yelp Academic Dataset, 2 political blogs from the 2008 US presidential election <ref type="bibr" target="#b7">(Eisenstein and Xing, 2010)</ref>, and New York Times ar- ticles from 2007. 3 Details are shown in <ref type="table">Table  1</ref>. Documents with fewer than 10 word tokens are discarded due to possible instability in con- structingˆQstructingˆ structingˆQ. We perform minimal vocabulary cu- ration, eliminating a standard list of English stop- words <ref type="bibr">4</ref> and terms that occur below frequency cut- offs: 100 times (Yelp, Blog) and 150 times (NYT). We further restrict possible anchor words to words that occur in more than three documents. As our datasets are not artificially synthesized, we reserve 5% from each set of documents for held-out like- lihood computation.  <ref type="table">Table 1</ref>: Statistics for datasets used in experiments Unlike ( <ref type="bibr" target="#b3">Arora et al., 2013)</ref>, which presents results on synthetic datasets to compare perfor- mance across different recovery methods given in- creasing numbers of documents, we are are inter- ested in comparing anchor finding methods, and are mainly concerned with semantic quality. As a result, although we have conducted experiments on synthetic document collections, <ref type="bibr">5</ref> we focus on real datasets for this work. We also choose to com- pare only anchor finding algorithms, so we do not report comparisons to likelihood-based methods, which can be found in ( <ref type="bibr" target="#b3">Arora et al., 2013</ref>).</p><p>For both PCA and t-SNE, we use three- dimensional embeddings across all experiments. This projection results in matrices that are 0.03% as wide as the original V × V matrix for the NYT dataset. Without low-dimensional embed- ding, each word is represented by a V-dimensional vector where only several terms are non-zero due to the sparse co-occurrence patterns. Thus a ver-tex captured by the greedy anchor-finding method is likely to be one of many eccentric vertices in very high-dimensional space. In contrast, t-SNE creates an effective dense representation where a small number of pivotal vertices are more clearly visible, improving both performance and inter- pretability.</p><p>Note that since we can find an exact convex hull in these spaces, <ref type="bibr">6</ref> there is an upper bound to the number of topics that can be found given a partic- ular projection. If more topics are desired, one can simply increase the dimensionality of the projec- tion. For the greedy algorithm we use sparse ran- dom projections with 1,000 dimensions with 5% negative entries and 5% positive entries. PCA and t-SNE choose (49, 32, 47) and <ref type="bibr">(69,</ref><ref type="bibr">77,</ref><ref type="bibr">107</ref>) an- chors, respectively for each of three Yelp, Blog, and NYTimes datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Anchor-word Selection</head><p>We begin by comparing the behavior of low- dimensional embeddings to the behavior of the standard greedy algorithm. <ref type="table" target="#tab_2">Table 2</ref> shows ordered lists of the first 12 anchor words selected by three algorithms: t-SNE embedding, PCA embedding, and the original greedy algorithm. Anchor words selected by t-SNE (police, business, court) are more general than anchor words selected by the greedy algorithm (cavalry, al-sadr, yiddish). Ad- ditional examples of anchor words and their asso- ciated topics are shown in <ref type="table" target="#tab_4">Table 3</ref>    <ref type="table" target="#tab_4">t restaurant fast salsa  Greedy  34 35 good great food place service restaurant it's mexican  t-SNE  6  0  beer selection good pizza great wings tap nice  PCA  39 6  wine beer selection nice list glass wines bar  Greedy  99 11 beer selection great happy place wine good bar  t-SNE  3  0</ref> prices great good service selection price nice quality PCA 12 0 atmosphere prices drinks friendly selection nice beer ambiance Greedy 34 35 good great food place service restaurant it's mexican t-SNE 10 0 chicken salad good lunch sauce ordered fried soup PCA 10 0 chicken salad lunch fried pita time back sauce Greedy 69 12 chicken rice sauce fried ordered i'm spicy soup  dimensions, finding the word that has the next most distinctive co-occurrence pattern tends to prefer overly eccentric words with only short, in- tense bursts of co-occurring words. While the bases corresponding to these anchor words could be theoretically relevant for the original space in high-dimension, they are less likely to be equally important in low-dimensional space. Thus project- ing down to low-dimensional space can rearrange the points emphasizing not only uniqueness, but also longevity, achieving the ability to form mea- surably more specific topics.</p><note type="other">HR Top Words (Yelp) t-SNE 16 0 mexican good service great eat restaurant authentic delicious PCA 15 0 mexican authentic eat chinese don'</note><p>Concretely, neither cavalry, al-sadr, yiddish nor police, business, court are full representations of New York Times articles, but the latter is a much better basis than the former due to its greater gen- erality. We see the effect of this difference in the specificity of the resulting topics (for example in 17 obama topics). Most words in the vocabulary have little connection to the word cavalry, so the probability p(z|w) does not change much across different w. When we convert these distributions into P (w|z) using the Bayes' rule, the resulting topics are very close to the corpus distribution, a unigram distribution p(w).</p><formula xml:id="formula_4">p(w|z = k cavalry ) ∝ p(z = k cavalry |w)p(w) ≈ p(w)</formula><p>This lack of specificity results in the observed sim- ilarity of topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>In this section we compare PCA and t-SNE pro- jections to the greedy algorithm along several quantitative metrics. To show the effect of dif- ferent values of K, we report results for varying numbers of topics. As the anchor finding algo- rithms are deterministic, the anchor words in a K- dimensional model are identical to the first K an- chor words in a (K + 1)-dimensional model. For the greedy algorithm we select anchor words in the order they are chosen. For the PCA and t- SNE methods, which find anchors jointly, we sort words in descending order by their distance from their centroid.</p><p>Recovery Error. Each non-anchor word is ap- proximated by a convex combination of the K anchor words. The projected gradient algorithm ( <ref type="bibr" target="#b3">Arora et al., 2013</ref>) determines these convex coef- ficients so that the gap between the original word vector and the approximation becomes minimized. As choosing a good basis of K anchor words de- creases this gap, Recovery Error (RE) is defined by the average 2 -residuals across all words.</p><formula xml:id="formula_5">RE = 1 V V i=1 ¯ Q i − K k=1 p(z 1 = k|w 1 = i) ¯ Q S k 2</formula><p>(5) Recovery error decreases with the number of top- ics, and improves substantially after the first 10-15 anchor words for all methods. The t-SNE method has slightly better performance than the greedy al- gorithm, but they are similar. Results for recovery with the original, unprojected matrix (not shown) are much worse than the other algorithms, sug- gesting that the initial anchor words chosen are es- pecially likely to be uninformative. Normalized Entropy. As shown previously, if the probability of topics given a word is close to uniform, the probability of that word in topics will be close to the corpus distribution. Normalized Entropy (NE) measures the entropy of this distri- bution relative to the entropy of a K-dimensional uniform distribution:</p><formula xml:id="formula_6">N E = 1 V V i=1 H(z|w = i) log K .<label>(6)</label></formula><p>The normalized entropy of topics given word dis- tributions usually decreases as we add more top- ics, although both t-SNE and PCA show a dip in entropy for low numbers of topics. This result in- dicates that words become more closely associated with particular topics as we increase the number of topics. The low-dimensional embedding methods (t-SNE and PCA) have consistently lower entropy. Topic Specificity and Topic Dissimilarity. We want topics to be both specific (that is, not overly general) and different from each other. When there are insufficient number of topics, p(w|z) often re- sembles the corpus distribution p(w), where high frequency terms become the top words contribut- ing to most topics. Topic Specificity (TS) is de- fined by the average KL divergence from each topic's conditional distribution to the corpus dis- tribution. 7</p><formula xml:id="formula_7">T S = 1 K K k=1 KL p(w|z = k) || p(w)<label>(7)</label></formula><p>One way to define the distance between multiple points is the minimum radius of a ball that cov- ers every point. Whereas this is simply the dis- tance form the centroid to the farthest point in the Euclidean space, it is an itself difficult opti- mization problem to find such centroid of distri- butions under metrics such as KL-divergence and Jensen-Shannon divergence. To avoid this prob- lem, we measure Topic Dissimilarity (TD) view- ing each conditional distribution p(w|z) as a sim- number of topics, suggesting that with few anchor words, the topic distributions are close to the over- all corpus distribution and very similar to one an- other. The t-SNE and PCA algorithms produce consistently better specificity and dissimilarity, in- dicating that they produce more useful topics early with small numbers of topics. The greedy algo- rithm produces topics that are closer to the corpus distribution and less distinct from each other (17 obama topics). Topic Coherence is known to correlate with the semantic quality of topic judged by human anno- tators ( <ref type="bibr" target="#b15">Mimno et al., 2011</ref>). Let W (T ) k be T most probable words (i.e., top words) for the topic k.</p><formula xml:id="formula_8">ple V -dimensional vector in R V . Recall a ik = p(w = i|z = k), T D = max 1≤k≤K 1 K K k =1 a * k − a * k 2 .<label>(</label></formula><formula xml:id="formula_9">T C = w 1 =w 2 ∈W (T ) k log D(w 1 , w 2 ) + D(w 1 )<label>(9)</label></formula><p>Here D(w 1 , w 2 ) is the co-document frequency, which is the number of documents in D consisting of two words w 1 and w 2 simultaneously. D(w) is the simple document frequency with the word w. The numerator contains smoothing count in order to avoid taking the logarithm of zero. Coherence scores for t-SNE and PCA are worse The greedy algorithm creates more co- herent topics (higher is better), but at the cost of many overly general or repetitive topics.</p><p>than those for the greedy method, but this result must be understood in combination with the Speci- ficity and Dissimilarity metrics. The most frequent terms in the overall corpus distribution p(w) often appear together in documents. Thus a model creat- ing many topics similar to the corpus distribution is likely to achieve high Coherence, but low Speci- ficity by definition. Saliency. (Chuang et al., 2012) define saliency for topic words as a combination of distinctive- ness and probability within a topic. Anchor words are distinctive by construction, so we can increase saliency by selecting more probable anchor words. We measure the probability of anchor words in two ways. First, we report the zero-based rank of anchor words within their topics. Examples of this metric, which we call "hard" rank are shown in Ta- ble 3. The hard rank of the anchors in the PCA and t-SNE models are close to zero, while the anchor words for the greedy algorithm are much lower ranked, well below the range usually displayed to users. Second, while hard rank measures the per- ceived difference in rank of contributing words, position may not fully capture the relative impor- tance of the anchor word. "Soft" rank quantifies the average log ratio between probabilities of the prominent word w * k and the anchor word s k . Lower values of soft rank <ref type="figure" target="#fig_7">(Fig. 8</ref> indicate that the anchor word has greater relative probability to occur within a topic. As we increase the num- ber of topics, anchor words become more promi- nent in topics learned by the greedy method, but t-SNE anchor words remain relatively more prob- able within their topics as measured by soft rank.</p><formula xml:id="formula_10">SR = 1 K K k=1 log p(w = w * k |z = k) p(w = s k |z = k)<label>(</label></formula><p>Held-out Probability. Given an estimate of the topic-word matrix A, we can compute the marginal probability of held-out documents under that model. We use the left-to-right estimator in- troduced by <ref type="bibr" target="#b18">(Wallach et al., 2009)</ref>, which uses a sequential algorithm similar to a Gibbs sampler. This method requires a smoothing parameter for document-topic Dirichlet distributions, which we set to α k = 0.1. We note that the greedy algo- rithm run on the original, unprojected matrix has better held-out probability values than t-SNE for the Yelp corpus, but as this method does not scale to realistic vocabularies we compare here to the sparse random projection method used in <ref type="bibr" target="#b3">(Arora et al., 2013</ref>). The t-SNE method appears to do best, particularly in the NYT corpus, which has a larger vocabulary and longer training documents. The length of individual held-out documents has no correlation with held-out probability. We emphasize that Held-out Probability is sen- sitive to smoothing parameters and should only be used in combination with a range of other topic- quality metrics. In initial experiments, we ob- served significantly worse held-out performance for the t-SNE algorithm. This phenomenon was because setting the probability of anchor words to zero for all but their own topics led to large neg- ative values in held-out log probability for those words. As t-SNE tends to choose more frequent terms as anchor words, these "spikes" significantly affected overall probability estimates. To make the calculation more fair, we added 10 −5 to any zero entries for anchor words in the topic-word matrix A across all models and renormalized.</p><p>Because t-SNE is a stochastic model, different initializations can result in different embeddings. To evaluate how steady anchor word selection is, we ran five random initializations for each dataset. For the Yelp dataset, the number of anchor words varies from 59 to 69, and 43 out of those are shared across at least four trials. For the Blog dataset, the number of anchor words varies from 80 to 95, with 56 shared across at least four trials. For the NYT dataset, this number varies between 83 and 107, with 51 shared across at least four models. <ref type="table" target="#tab_4">Table 3</ref> shows topics trained by three methods (t- SNE, PCA, and greedy) for all three datasets. For each model, we select five topics at random from the t-SNE model, and then find the closest topic from each of the other models. If anchor words present in the top eight words, they are shown in boldface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>A fundamental difference between anchor- based inference and traditional likelihood-based inference is that we can give an order to top- ics according to their contribution to word co- occurrence convex hull. This order is intrinsic to the original algorithm, and we heuristically give orders to t-SNE and PCA based on their contri- butions. This order is listed as # in the previous table. For all but one topic, the closest topic from the greedy model has a higher order number than the associated t-SNE topic. As shown above, the standard algorithm tends to pick less useful anchor words at the initial stage; only the later, higher or- dered topics are specific.</p><p>The most clear distinction between models is the rank of anchor words represented by Hard Rank for each topic. Only one topic correspond- ing to (initial) has the anchor word which does not coincide with the top-ranked word. For the greedy algorithm, anchor words are often tens of words down the list in rank, indicating that they are unlikely to find a connection to the topic's se- mantic core. In cases where the anchor word is highly ranked (unbelievers, parenthood) the word is a good indicator of the topic, but still less deci- sive.</p><p>t-SNE and PCA are often consistent in their se- lection of anchor words, which provides useful validation that low-dimensional embeddings dis- cern more relevant anchor words regardless of lin- ear vs non-linear projections. Note that we are only varying the anchor selection part of the An- chor Words algorithm in these experiments, recov- ering topic-word distributions in the same manner given anchor words. As a result, any differences between topics with the same anchor word (for ex- ample chicken) are due to the difference in either the number of topics or the rest of anchor words. Since PCA suffers from a crowding problem in lower-dimensional projection (see <ref type="figure" target="#fig_1">Figure 2</ref>) and the problem could be severe in a dataset with a large vocabulary, t-SNE is more likely to find the proper number of anchors given a specified granu- larity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>One of the main advantages of the anchor words algorithm is that the running time is largely inde- pendent of corpus size. Adding more documents would not affect the size of the co-occurrence ma- trix, requiring more times to construct the co- occurrence matrix at the beginning. While the inference is scalable depending only on the size of the vocabulary, finding quality anchor words is crucial for the performance of the inference.</p><p>( <ref type="bibr" target="#b3">Arora et al., 2013</ref>) presents a greedy anchor finding algorithm that improves over previous lin- ear programming methods, but finding quality an- chor words remains an open problem in spec- tral topic inference. We have shown that previ- ous approaches have several limitations. Exhaus- tively finding anchor words by eliminating words that are reproducible by other words <ref type="bibr" target="#b2">(Arora et al., 2012</ref>) is impractical. The anchor words se- lected by the greedy algorithm are overly eccen- tric, particularly at the early stages of the algo- rithm, causing topics to be poorly differentiated. We find that using low-dimensional embeddings of word co-occurrence statistics allows us to ap- proximate a better convex hull. The resulting anchor words are highly salient, being both dis- tinctive and probable. The models trained with these words have better quantitative and qualita- tive properties along various metrics. Most im- portantly, using radically low-dimensional projec- tions allows us to provide users with clear visual explanations for the model's anchor word selec- tions.</p><p>An interesting property of using low- dimensional embeddings is that the number of topics depends only on the projecting dimen- sion. Since we can efficiently find an exact convex hull in low-dimensional space, users can achieve topics with their preferred level of granularities by changing the projection dimension. We do not insist this is the "correct" number of topics for a corpus, but this method, along with the range of metrics described in this paper, provides users with additional perspective when choosing a dimensionality that is appropriate for their needs.</p><p>We find that the t-SNE method, besides its well-known ability to produce high quality lay- outs, provides the best overall anchor selection performance. This method consistently selects higher-frequency terms as anchor words, resulting in greater clarity and interpretability. Embeddings with PCA are also effective, but they result in less well-formed spaces, being less effective in held- out probability for sufficiently large corpora.</p><p>Anchor word finding methods based on low- dimensional projections offer several important advantages for topic model users. In addition to producing more salient anchor words that can be used effectively as topic labels, the relationship of anchor words to a visualizable word co-occurrence space offers significant potential. Users who can see why the algorithm chose a particular model will have greater confidence in the model and in any findings that result from topic-based analy- sis. Finally, visualizable spaces offer the poten- tial to produce interactive environments for semi- supervised topic reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: 2D t-SNE projection of a Yelp review corpus and its convex hull. The words corresponding to vertices are anchor words for topics, whereas non-anchor words correspond to the interior points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 2D PCA projections of a Yelp review corpus and its convex hulls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 3D t-SNE projection of a Yelp review corpus and its convex hull. Vertices on the convex hull correspond to anchor words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Recovery error is similar across algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Words have higher topic entropy in the greedy model, especially in NYT, resulting in less specific topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Greedy topics look more like the corpus distribution and more like each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The greedy algorithm creates more coherent topics (higher is better), but at the cost of many overly general or repetitive topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Anchor words have higher probability, and therefore greater salience, in t-SNE and PCA models (1 ≈ one third the probability of the top ranked word).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: t-SNE topics have better held-out probability than greedy topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The first 12 anchor words selected by 
three algorithms for the NYT corpus. 

The Gram-Schimdt process used by Arora et 
al. greedily selects anchors in high-dimensional 
space. As each word is represented within V -

6 In order to efficiently find an exact convex hull, we use 
the Quickhull algorithm. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Example t-SNE topics and their most similar topics across algorithms. The Greedy algo- rithm can find similar topics, but the anchor words are much less salient.</figDesc><table></table></figure>

			<note place="foot" n="2"> https://www.yelp.com/academic dataset 3 http://catalog.ldc.upenn.edu/LDC2008T19 4 We used the list of 524 stop words included in the Mallet library. 5 None of the algorithms are particularly effective at finding synthetically introduced anchor words possibly because there are other candidates around anchor vertices that approximate the convex hull to almost the same degree.</note>

			<note place="foot" n="7"> We prefer specificity to (AlSumait et al., 2009)&apos;s term vacuousness because the metric increases as we move away from the corpus distribution.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank David Bindel and the anonymous re-viewers for their valuable comments and sugges-tions, and Laurens van der Maaten for providing his t-SNE implementation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database-friendly random projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="274" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topic significance ranking of lda generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loulwah</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning topic models-going beyond svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Preliminary version in NIPS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An improved approximation algorithm for the column subset selection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Termite: Visualization techniques for assessing textual topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Working Conference on Advanced Visual Interfaces (AVI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="74" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">The CMU 2008 political blog corpus</title>
		<imprint>
			<date type="published" when="2010-03" />
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Topicviz: Semantic navigation of document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Kittur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
		<idno>abs/1110.6200</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient algorithms for computing a strong rank-revealing qr factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">C</forename><surname>Eisenstat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In SIAM J. Sci Comput</title>
		<imprint>
			<biblScope unit="page" from="848" to="869" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DiscLDA: Discriminative learning for dimensionality reduction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic labelling of topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Jey Han Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1536" to="1545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic labeling of multinomial topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="490" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Complexity of inference in latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1008" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Visualizing high-dimensional data using t-SNE. JMLR</title>
		<imprint>
			<date type="published" when="2008-11" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MedLDA: Maximum margin supervised topic models for regression and classication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
