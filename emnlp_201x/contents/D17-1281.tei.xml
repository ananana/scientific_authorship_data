<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sharma</surname></persName>
							<email>adisharma075@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
							<email>zaranaparekh17@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">DA-IICT Gandhinagar</orgName>
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country>India, India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Science Bangalore</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Speeding up Reinforcement Learning-based Information Extraction Training using Asynchronous Methods</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2658" to="2663"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>RLIE-DQN is a recently proposed Reinforcement Learning-based Information Extraction (IE) technique which is able to incorporate external evidence during the extraction process. RLIE-DQN trains a single agent sequentially, training on one instance at a time. This results in significant training slowdown which is undesirable. We leverage recent advances in parallel RL training using asynchronous methods and propose RLIE-A3C. RLIE-A3C trains multiple agents in parallel and is able to achieve upto 6x training speedup over RLIE-DQN, while suffering no loss in average accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extracting information about an event (or entity) involves multiple decisions, as one first needs to identify documents relevant to the event, extract relevant information from those documents, and finally reconcile various values obtained for the same relation of the event from different sources <ref type="bibr" target="#b0">(Ahn, 2006</ref>). Search based methods for Infor- mation Extraction have been increasingly investi- gated ( <ref type="bibr" target="#b10">West et al., 2014)</ref>; <ref type="bibr" target="#b4">(Hegde and Talukdar, 2015)</ref>; <ref type="bibr" target="#b11">(Zhang et al., 2016)</ref>; <ref type="bibr" target="#b2">(Bing et al., 2017)</ref>.</p><p>( <ref type="bibr" target="#b5">Kanani and McCallum, 2012</ref>) combine search and Information Extraction (IE) using Reinforce- ment Learning (RL), with the goal of selecting good actions while staying within resource con- straints, but don't optimze for extraction accuracy. More recently, ( <ref type="bibr" target="#b6">Narasimhan et al., 2016)</ref> proposed a RL-based approach to model the IE process out- lined above. We shall refer to this approach as RLIE-DQN in this paper. RLIE-DQN trains an RL * Research carried out during an internship at the Indian Institute of Science, Bangalore. agent using Deep Q-Network (DQN) ( <ref type="bibr">Mnih et al., 2015)</ref> to select optimal actions to query for docu- ments and also reconcile extracted values.</p><p>DQN trains a single agent sequentially, up- dating parameters based on one instance at a time. Each such instance is sampled from the en- tire training data, also called the experience re- play. Such sequential experience replay-based training results in slow learning, while requiring high memory and computation resources. In order to overcome this challenge, A3C (Asynchronous Advantage Actor-Critic), an asynchronous deep RL training algorithm, has been proposed recently <ref type="bibr">(Mnih et al., 2016</ref>). A3C trains multiple RL agents in parallel, each of which estimates gradients lo- cally, and asynchronously updates globally shared parameters. Recent work has explored applica- tions for A3C in varied domains ( <ref type="bibr" target="#b3">Fernando et al., 2017)</ref>, <ref type="bibr">(Mirowski et al., 2016)</ref>.</p><p>In this paper, we propose RLIE-A3C which uses A3C-based parallel asynchronous agents for train- ing. This is in contrast to the sequential DQN training in RLIE-DQN. Differences between the training regimes of the two methods are shown in <ref type="figure">Figure 1</ref>. Through experiments on multiple real- world datasets, we find that RLIE-A3C achieves upto 6x training speedup compared to RLIE-DQN, while suffering no loss in accuracy. To the best of our knowledge, this is the first application of asynchronous deep RL methods in IE (and also in NLP), and we hope this paper will foster fur- ther adoption and research into such methods in the NLP community. RLIE-A3C code is available at https://github.com/adi-sharma/RLIE A3C <ref type="figure">Figure 1</ref>: Left: DQN-based sequential learning framework used in RLIE-DQN ( <ref type="bibr" target="#b6">Narasimhan et al., 2016)</ref>, as discussed in Section 2. At each time step, the agent looks at a specific instance from the training data. Right: A3C-based parallel learning framework in RLIE-A3C (proposed approach). The parallel agents look at different parts of the training data, estimate parameter update statistics locally, and then perform asynchronous updates on the globally shared parameters (θ t ) at time step t. See Section 3 for details. Due to the asynchronous parallel updates, RLIE-A3C achieves significant training speedup without loss in accuracy, as we shall see in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RLIE-DQN: Information Extraction using Reinforcement Learning</head><p>We first present a brief overview of RLIE-DQN ( <ref type="bibr" target="#b6">Narasimhan et al., 2016)</ref>. Given a document to extract information about an event, RLIE-DQN is- sues a search query to retrieve other documents related to the event, extracts event information from those documents, and finally reconciles val- ues extracted from the documents. If confidence in the extracted values are low, then RLIE-DQN repeats this process with additional queries. This way, RLIE-DQN incorporates evidences from ex- ternal sources to improve information extraction (IE) from a given source document.</p><p>RLIE-DQN models the task as a Markov De- cision Process (MDP) in order to reconcile newly extracted information selectively and dynamically. The MDP describes the environment in which the RL agent learns to make decisions. The MDP is represented using a tuple S, A, R, T , where S is the set of states, A = {a d , a q } is the set of actions, R(s, a) is the reward for taking action a ∈ A from state s ∈ S, and T (s |s, (a d , a q )) is the state tran- sition function. Here, a d is the reconciliation ac- tion, while a q is the query action. Based on a d , the agent may accept extracted values for one or all relations, reject all newly extracted values or stop (episode ends).</p><p>A sample state transition in RLIE-DQN's MDP is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. State representation con- sists of many details such as confidence scores of current and newly extracted relation values, con- text statistics from which the extractions are per- formed, etc. But for better readability, only the set of current and new values are shown for the states in this figure. Each transition consists of two actions: reconcile decision and query. The RL agent uses the reconcile action (a d ) to update value of the ShooterName relation from Paul Kiska to Kevin Wardzala. The agent uses the query ac- tion (a q ) to issue a new query ("Cleveland shoot- ing" + "injured") to retrieve other relevant docu- ments and extract new values 3 and 1 for relations NumKilled and NumWounded, respectively. The transitions stop whenever a d is a stop decision.</p><p>The reward function at each state, is the cum- mulative difference of current and previous ex- tracted accuracies, summed over all relations. Also, a negative reward per step is added to the reward in order to penalize the agent for longer episodes, since issuing queries to a search engine is expensive. Deep Q-Learning (DQN): Let Q(s, a) be the measure of long-term cumulative reward obtained by taking action a from state s. The RL agent makes use of Q(s, a) to select the next action from a from state s. Q-learning ( <ref type="bibr" target="#b9">Watkins and Dayan, 1992</ref>) is a popular technique to estimate this func- tion, in which the function for optimal Q-value is estimated using the Bellman equation <ref type="bibr" target="#b7">(Sutton and Barto, 1998)</ref>  </p><formula xml:id="formula_0">Q i+1 (s, a) = E (s,a) [R(s, a) + γ max a Q i (s , a</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach: RLIE-A3C</head><p>The DQN-based agent used in RLIE-DQN is se- quential, as it moves from one instance to another to update parameters. This can result in signif- icant training time slowdown, especially in large data settings. Instead of using experience replay of the DQN algorithm for stabilizing updates, we consider a framework with multiple asynchronous agents, each of which explore different areas of the environment in parallel.</p><p>Asynchronous Advantage Actor-critic (A3C) ( <ref type="bibr">Mnih et al., 2016</ref>) is a recently proposed deep RL algorithm which makes use of parallel agents for parameter estimation. We replace DQN with A3C in RLIE-DQN and call the resulting method RLIE- A3C -our proposed approach (See <ref type="figure">Figure 1)</ref>.</p><p>At time instant t, RLIE-A3C poses decision policy π d (a d |s t ), and query policy π q (a q |s t ) as probability distributions over candidate actions a d and a q , respectively. RLIE-A3C also calculates the state value function V (s t ), as an estimate of the cumulative long-term reward obtained starting from state s t . Owing to the large continuous state space of the problem, RLIE-A3C approximates each of these three functions using three sepa- rate deep neural networks, which are parametrized by θ d , θ q , and θ v , as π d (a d |s t ) ≈ π d (a d |s t ; θ d ), π q (a q |s t ) ≈ π q (a q |s t ; θ d ) and V (s t ) ≈ V (s t ; θ v ). These parameters are updated by agents working in parallel. Based on the policies, each agent se- lects the query and decision actions to be per- formed and updates it's state accordingly. This is repeated up to t max steps or until a terminal state is reached. Local Gradient Calculation: The agents estimate parameter gradients using a local copy of the net- work parameters, and then perform asynchronous updates on the globally shared parameters θ d , θ q , and θ v . Hence, the policy and value functions are jointly estimated. The policy gradient update equations calculated over the local copy of net- work parameters of each parallel RLIE-A3C agent p are as follows:</p><formula xml:id="formula_1">dθ p x ← dθ p x + θ p x log π d (a x i , s ; θ p x )A(s i , a xt ; θ p v ) + β x θ p x H(π x (s t ; θ p x )) (1)</formula><p>where, x ∈ {d, q} for decision and query. The ad- vantage function A(s t , a t ;</p><formula xml:id="formula_2">θ v ) = k−1 i=0 γ i R t+i + γ k V (s t+k ; θ v ) − V (s t ; θ v )</formula><p>above significantly re- duces the variance of the policy gradient, where γ is the discount factor and k can vary from state to state and is upper-bounded by t max . Since the gradient updates are accumulated, training stabil- ity increases. Further, exploration is encouraged by introducing the entropy regularization term β x θ p x H(π x (s t ; θ p x )) in the equation above. Here, H is the entropy function and β x controls domi- nance of the entropy term.</p><p>The gradient update for the parameters of value function V is calculated locally by every parallel agent p as follows:</p><formula xml:id="formula_3">dθ p v ← dθ p v + ∂(G t − V (s t ; θ p v )) 2 /∂θ p v where G t = E (s) [ ∞ k=0 γ k R t+k+1 ]</formula><p>is the return. Global Parameter Update: The parameters θ d , θ q and θ v are learnt using stochastic gradient descent with <ref type="bibr">RMSprop (Tieleman and Hinton, 2012</ref>). The standard non-centered RMSProp up- date is used by the parallel agents p to update the shared parameters asynchronously using the gra- dients obtained from Equation (1), as follows:</p><formula xml:id="formula_4">g = αg + (1 − α)(dθ p x ) 2 ; θ x ← θ x − η dθ p x √ g +</formula><p>where x ∈ {d, q, v}, α is the decay factor, η is the learning rate, is the smoothing constant and g is the moving average of element-wise squared gradients. The pseudo-code for RLIE-A3C can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>Setup: We compare RLIE-A3C against RLIE- DQN using the same protocol and hyperparame- ters as reported in <ref type="bibr" target="#b6">(Narasimhan et al., 2016</ref>). Also, we experiment with the same two datasets used in that paper: the Gun Violence Archive 1 and the Foodshield EMA database 2 . The train, dev and test datasets contain 372, 146 and 146 source ar- ticles respectively for the Shooting incident cases and 292, 42 and 148 source articles respectively for the food adulteration cases. We used the im- plementation of RLIE-DQN provided by the au- thors of that system. For more details on the dataset and other parameters, we refer the reader to ( <ref type="bibr" target="#b6">Narasimhan et al., 2016)</ref>.</p><p>RLIE-A3C: This is our proposed method which is described in Section 3. For the sake of fair com- parison, the network, base classifier, and evalua- tion metrics are same as that of RLIE-DQN. Each of the three deep networks in RLIE-A3C, one each for π(a d |s), π(a q |s) and V (s), is built using two linear layers with 20 hidden units, followed by Re- LUs. MaxEnt classifier is used as the base extrac- tor, and the model is evaluated on the entire test set for 1.6 million steps. The dev set is used to tune all hyperparameters, which can be found in the Appendix. For RLIE-A3C, the evaluation is carried out 50 times after training and the average accuracy values are taken over the top 5 evalua- tions, as done in ( <ref type="bibr">Mnih et al., 2016</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results and Discussion</head><p>Extraction Accuracy: Experimental results com- paring extraction accuracies of RLIE-DQN and RLIE-A3C are presented in <ref type="figure" target="#fig_2">Figure 3</ref> 3 (left and middle panels). From this figure, we observe that there is no loss in average accuracy in transition- ing from RLIE-DQN to RLIE-A3C (in fact there is a slight gain in case of the Adulteration dataset). Please note that, improvement in accuracy is not our primary goal in the paper -it is the training time speedup, as discussed next. Speedup: Training times of RLIE-DQN and RLIE-A3C over both datasets are compared in the right panel of <ref type="figure" target="#fig_2">Figure 3 4</ref> . From <ref type="figure" target="#fig_2">Figure 3</ref>, we find that RLIE-A3C is able to achieve upto 6x training speedup over RLIE-DQN. In other words, RLIE- A3C is able to achieve significant speedup in train- ing time over RLIE-DQN, without any loss in av- erage accuracy. This is our main result. While RLIE-DQN was implemented in Torch, RLIE-A3C was implemented in Python using Ten- sorFlow framework. We note that Torch is known to be faster than <ref type="bibr">TensorFlow (Bahrampour et al., 2015)</ref>. This makes the speedup gains above even more impressive, and outlines the possibility that further gains may be possible with a Torch-based implementation of RLIE-A3C. <ref type="figure" target="#fig_3">Figure 4</ref> shows evolution of test accuracy of RLIE-A3C for the four relations of the Shooting Incidents dataset. For this dataset, state value function of RLIE-A3C converged at 48 minutes. However, from <ref type="figure" target="#fig_3">Figure 4</ref>, we observe that accura- cies converge to the final values much before that. Why do Asynchronous Methods work? An asynchronous approach fits more naturally with the training data, since different parallel agents look at different events at the same time (see <ref type="figure">Fig- ure 1</ref>), and the model is able to exploit the regu- larities between events in the dataset. The gradi- ent updates to the global network are less biased and the model is not easily distracted by noise in the data. The asynchronous parallel model is able to converge much faster as compared to replay memory based methods like DQN, as also seen in <ref type="bibr">(Mnih et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed RLIE-A3C, an asyn- chronous deep Reinforcement Learning (RL) al- gorithm for Information Extraction (IE). In con- trast to sequential training in previously proposed <ref type="bibr">RLIE-DQN (Narasimhan et al., 2016)</ref>, RLIE-A3C employs asynchronous parallel training. This re- sults in upto 6x training speedup, without suffer- ing any loss in average accuracy. We hope that this first application of asynchronous deep RL al- gorithms will open up more adoption of such tech- niques in the NLP community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample state transition in the MDP of RLIE-DQN (and also RLIE-A3C). In each transition, two actions are carried out: (1) reconcile new values with current values,; and (2) issue query to retrieve other relevant documents and extract values from those documents. Please see Section 2 for details.</figDesc><graphic url="image-2.png" coords="2,72.00,322.50,218.26,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>)|s, a]. Here, R(s, a) is the imme- diate reward and γ is the discount factor for the value of future rewards. For high dimensional state spaces, Deep Q- Network (DQN) (Mnih et al., 2013) approximates Q(s, a) as Q(s, a; θ) using parameters θ of a deep network. RLIE-DQN used such a DQN-based agent to learn optimal policy, as shown in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left, Middle Panels: Extraction accuracy of the baseline (RLIE-DQN) and our system (RLIEA3C) on the Shooting Incidents and Food Adulteration datasets. Right Panel: Training time comparison between RLIE-DQN and RLIE-A3C. Overall, we observe that RLIE-A3C results in upto 6x speedup over RLIE-DQN, without any loss in average extraction accuracy. This is our main result. Please see Section 4.1 for details.</figDesc><graphic url="image-3.png" coords="4,72.00,62.81,162.96,100.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Evolution of accuracy of RLIE-A3C for the four relations on test set of the Shooting Incidents dataset. Please see Section 4.1 for details.</figDesc><graphic url="image-6.png" coords="4,307.28,247.12,218.27,141.73" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.shootingtracker.com/Main_Page 2 http://www.foodshield.org/member/login/ 3 The tolerance for all the relation accuracy values for RLIE-A3C is within ± 1% 4 During test, policies learned by RLIE-DQN and RLIEA3C are executed. Since this is an identical process for both, we don&apos;t compare test runtimes in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank members of the MALL Lab, IISc who read drafts of the paper and gave valuable feed-back. We also thank the anonymous reviewers for their insightful comments. Special thanks to Karthik Narasimhan for all the helpful discus-sions. We also gratefully acknowledge support from a gift from Microsoft Research India.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The stages of event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Annotating and Reasoning about Time and Events</title>
		<meeting>the Workshop on Annotating and Reasoning about Time and Events</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Bahrampour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06435</idno>
		<title level="m">Comparative study of deep learning software frameworks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Towards a languageindependent solution: Knowledge base completion by searching the web and deriving language pattern. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="80" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yori</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08734</idno>
		<title level="m">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An entity-centric approach for overcoming knowledge graph sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="530" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Selecting actions for resource-bounded information extraction using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pallika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew K</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on Web search and data mining</title>
		<meeting>the fifth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving information extraction by acquiring external evidence with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07954</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Qlearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jch</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="515" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A joint model for entity set expansion and attribute extraction from web search queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3101" to="3107" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
