<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertainty-aware generative models for inferring document class prevalence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>O&amp;apos;connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Information and Computer Sciences</orgName>
								<orgName type="institution">University of Massachusetts Amherst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertainty-aware generative models for inferring document class prevalence</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4575" to="4585"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4575</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Prevalence estimation is the task of inferring the relative frequency of classes of unlabeled examples in a group-for example, the proportion of a document collection with positive sentiment. Previous work has focused on aggregating and adjusting discriminative individual classifiers to obtain prevalence point estimates. But imperfect classifier accuracy ought to be reflected in uncertainty over the predicted prevalence for scientifically valid inference. In this work, we present (1) a genera-tive probabilistic modeling approach to prevalence estimation, and (2) the construction and evaluation of prevalence confidence intervals; in particular, we demonstrate that an off-the-shelf discriminative classifier can be given a generative re-interpretation, by backing out an implicit individual-level likelihood function, which can be used to conduct fast and simple group-level Bayesian inference. Empirically, we demonstrate our approach provides better confidence interval coverage than an alternative , and is dramatically more robust to shifts in the class prior between training and testing. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of prevalence estimation is to infer the relative frequency of classes y i associated with un- labeled examples (e.g. documents) from a group, x i ∈ D. For example, one might want to es- timate the proportion of blogs with a positive sentiment towards a political candidate <ref type="bibr" target="#b17">(Hopkins and King, 2010)</ref>, sentiment of responses to nat- ural disasters on social media ( <ref type="bibr" target="#b20">Mandel et al., 2012)</ref>, or prevalence of car types in street pho- tos to infer neighborhood demographics ( <ref type="bibr" target="#b12">Gebru et al., 2017)</ref>. Often, an analyst wants to com- pare prevalence between multiple groups, such as inferring prevalence variation over time (e.g., changes to online abuse content ( <ref type="bibr" target="#b3">Bissias et al., 2016)</ref>), or across other covariates (e.g., changes in police officers' "respect" when speaking to mi- norities ( <ref type="bibr" target="#b33">Voigt et al., 2017)</ref>). This problem has been re-introduced in many different fields: as "quantification" in data mining <ref type="bibr" target="#b9">(Forman, 2005</ref><ref type="bibr" target="#b10">(Forman, , 2008</ref>, "prevalence estimation" in statistics and epidemiology <ref type="bibr" target="#b11">(Gart and Buck, 1966)</ref>, and "class prior estimation" in machine learning <ref type="bibr" target="#b34">(Vucetic and Obradovic, 2001;</ref><ref type="bibr" target="#b28">Saerens et al., 2002</ref>). In NLP, SemEval 2016 and 2017 included Twitter senti- ment class prevalence tasks ( <ref type="bibr" target="#b24">Nakov et al., 2016;</ref><ref type="bibr" target="#b27">Rosenthal et al., 2017)</ref>.</p><p>Prevalence estimation assumes access to a (po- tentially small) set of labeled examples to train a classifier; but unlike the task of individual classi- fication, the goal is to estimate the proportion of a class among examples in a group. If a perfectly accurate classifier is available, it is trivial to con- struct a perfect prevalence estimate by counting the classification decisions ( §3.1). In fact, most application papers in the previous paragraph use this or a similar aggregation rule to conduct their prevalence estimates. However, classifiers often exhibit errors from different sources, including:</p><p>• Shifts in the class distribution from training to testing (P train (y) = P test (y)). A classifier may be biased toward predicting P train (y).</p><p>• Difficult classification tasks (such as predict- ing sentiment or sarcasm) that result in low accuracy classifiers; this can be exacerbated by limited training data, as is common in so- cial science or industry settings that require manual human annotation for labels.</p><p>It is typically assumed (and sometimes confirmed) that when an individual classifier has less than 100% accuracy, it can still give reasonable preva-lence estimates. <ref type="bibr">2</ref> However, there is relatively lit- tle understanding to what extent the quality of the document-level model impacts prevalence es- timates. Imperfect classifier accuracy ought to be reflected in uncertainty over the predicted preva- lence.</p><p>In this work, we tackle both of these challenges simultaneously, using a generative probabilistic modeling approach to prevalence estimation. This model directly parameterizes and conducts infer- ence for the unknown prevalence, naturally ac- commodating shifts between training and testing, and also allows us to infer confidence intervals for the prevalence. We show that our best model can be seen as an implicit likelihood generative re-interpretation of an off-the-shelf discriminative classifier ( §4.2); this unifies it with previous work, and also is easy for a practitioner to apply.</p><p>We additionally review several types of class prevalence estimators from the literature ( §3), and conduct a robust empirical evaluation on senti- ment analysis over hundreds of document groups, illustrating the methods' biases and robustness to class prior shift between training and testing. Our method provides better confidence interval cover- age and is more robust to class prior shift than pre- vious methods, and is substantially more accurate than an algorithm in widespread use in political science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem definition</head><p>We consider two prevalence estimation problems: (1) point prediction and (2) confidence interval prediction. In this work, we are most interested in supervised learning for discrete-valued document labels, with access to a small to moderate number (e.g. around 1000) of labeled documents with text x and label y: (x i , y i ) ∈ D train . We restrict at- tention to binary-valued labels y ∈ {0, 1}. At test time, there are one or more groups of unlabeled test documents, D (1) , · · · , D <ref type="bibr">(G)</ref> ; for example, one group might be a set of tweets sent during a cer- tain month, or a set of online reviews associated with a particular product. For each group D, let θ * ≡ (1/n) n i y i be the true proportion of posi- tive labels (where n = |D|).</p><p>The prevalence point prediction problem is to take an unlabeled document group D as input and 2 For example, Bissias et al. find a relative mean absolute error of less than 0.01 when the individual classifier has ROC AUC of 0.91. θ (solid line) and the true prevalence, θ * (dashed line). A desirable property is that confidence intervals, technically Bayesian credible intervals, (shaded regions) will be wider for more uncertain models. For exam- ple, the wider CI on the right (green) contains θ * whereas the narrower CI interval on the left (red) does not.</p><p>infer an estimatedˆθestimatedˆ estimatedˆθ ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>. Ideally, this point estimate should be close to the true prevalence θ * ; we evaluate this by mean absolute error.</p><p>In this work, we are the first (that we know of) to introduce the question of uncertainty in preva- lence estimation. Since document classifiers are typically far from perfectly accurate, we should expect substantial error in prevalence prediction, and inference methods should quantify such un- certainty. We formalize this as a prevalence con- fidence interval (CI) inference, which takes as in- put a desired nominal coverage level (1 − α), and predicts a real-valued interval</p><formula xml:id="formula_0">[ ˆ θ lo , ˆ θ hi ] ⊆ [0, 1].</formula><p>Ideally, a CI prediction algorithm should have fre- quentist coverage semantics: over a large number of test groups, <ref type="bibr">3</ref> (1−α)% of the predicted intervals ought to contain the true value θ * . If the problem is hard-for example, the relationship between doc- ument features and the label is not captured well by the model-the CI should be wide. We em- pirically evaluate coverage of CI-aware prevalence inference models. See <ref type="figure" target="#fig_0">Fig. 1</ref> for an intuitive exam- ple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Review and baselines: Discriminative individual classification aggregation</head><p>The most straightforward baseline approach to prevalence estimation is to build on discrimina- tive, supervised learning for individual-level la- bels, such as binary logistic regression with bag- of-words features, randomized feature hashing <ref type="bibr" target="#b37">(Weinberger et al., 2009)</ref>, or neural networks <ref type="bibr" target="#b13">(Goldberg, 2016)</ref>. Such a model defines an indi- vidual document's label probability p i ≡ p β (y i = 1 | x i ) where parameters β are fit by maximizing regularized likelihood on the labeled training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Classify and Count (CC)</head><p>For prevalence point estimation, <ref type="bibr" target="#b9">Forman (2005)</ref> defines the "classify and count" (CC) method as simply averaging the most-likely individual label predictions,</p><formula xml:id="formula_1">ˆ θ CC = 1 n i 1{p i &gt; 0.5}.<label>(1)</label></formula><p>This is the most obvious approach for practition- ers, but it has at least two weaknesses, which have been addressed in different groups of prior work. First, the class proportions may change be- tween training and test groups, which the Adjusted CC and ReadMe algorithms attempt to fix ( §3.2- 3.3). Second, it discards probabilistic informa- tion, which is remedied by the Probabilistic CC method, and an extension we propose ( §3.4-3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adjusted Classify and Count (ACC)</head><p>CC may encounter problems if the test class dis- tribution is different than the training's. The "adjusted classify-and-count" method ( <ref type="bibr" target="#b11">Gart and Buck, 1966;</ref><ref type="bibr" target="#b9">Forman, 2005</ref>) treats the classifier output as a proxy variable, and estimates a sep- arate confusion model of classifier outputˆyoutputˆ outputˆy i ≡ 1{p i &gt; 0.5} conditional on the true label, p(ˆ y | y), from cross-validation within the training set. As- suming the confusion model extends to the test data, a moment-matching approach is then used to infer the true label proportions, by first observing p test (ˆ y) = y p(ˆ y | y)p test (y) and solving the linear system for p test (y), the test-time expected class prevalence. Using empirical estimates for the true positive rate TPR = p(ˆ y = 1 | y = 1), and false positive rate FPR = p(ˆ y = 1 | y = 0), andˆθ andˆ andˆθ CC = p(ˆ y = 1), it has the closed formˆθ</p><formula xml:id="formula_2">formˆ formˆθ ACC = ˆ θ CC − FPR TPR − FPR .<label>(2)</label></formula><p>By design, ACC is more robust to a new test-time prevalence, but it relies on the accuracy of its TPR and FPR estimates, and its lack of probabilistic se- mantics makes it unclear how to infer confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ReadMe algorithm</head><p>An interesting extension to ACC is to remove the need for a discriminative classifier, by directly modeling text conditional on the latent document class. The ReadMe algorithm, developed in po- litical science <ref type="bibr" target="#b17">(Hopkins and King, 2010)</ref>, extends ACC's linear system for every term type in a (subsampled and augmented) term vocabulary V, and calculates their class-conditional probabilities from the training data. Assuming these condi- tional models also hold in the test data, that im-</p><formula xml:id="formula_3">plies p test (w) = y ˆ p(w | y)p test (y)</formula><p>; the algo- rithm infers p test (y) by minimizing the squared error of predicted versus empirical term frequen- cies in the test set. The open-source ReadMe soft- ware package 4 has been used in numerous politi- cal science studies, including inferring proportions of types of censored Chinese news ( <ref type="bibr" target="#b19">King et al., 2013)</ref>, credit claiming in Congressional press re- leases ( <ref type="bibr" target="#b16">Grimmer et al., 2012)</ref>, and voter intentions among Twitter messages ( <ref type="bibr" target="#b5">Ceron et al., 2015)</ref>.</p><p>ReadMe is theoretically appealing in that it infers latent class prevalences to explain the test group's textual evidence; but as a non- probabilistic model, it does not directly imply a method for confidence intervals (Hopkins and King use the bootstrap). Furthermore, our experi- ments ( §5), contra the original paper, show its im- plementation exhibits poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Probabilistic Classify and Count (PCC)</head><p>Both the CC and ACC methods discard uncer- tainty information from the classification model. In a difficult classification setting, for example, we might expect many probabilities to be near, say, 0.6, in which case the CC method may undercount the negative class. This suggests an alternative method, "probabilistic classify and count" (PCC):</p><formula xml:id="formula_4">ˆ θ P CC = 1 n i p i (3)</formula><p>which is the expected prevalence, (1/n) i y i , as- suming each y i is distributed according to the orig- inal probabilistic classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">PCC Poisson-Binomial distribution (PB-PCC)</head><p>If we assume each y i is conditionally indepen- dent given text x i and model parameters β, this defines a fully probabilistic model for the class prevalence. Let the latent variable S = i y i ; its distribution is thus Poisson-Binomial <ref type="bibr" target="#b6">(Chen and Liu, 1997</ref>). The modeled prevalence distribution p( S n | D) can be exactly inferred by Monte Carlo inference: each iteration samples every y i and sums for an S sample. The S/n distribution over many iterations can be used to construct a Monte Carlo CDFˆFCDFˆ CDFˆF , from which any [ ˆ F (t), ˆ F (t+1−α)] is an (1 − α)-sized credible interval (where 0 ≤ t ≤ t + 1 − α ≤ 1). This model has prevalence expectation E[ S n ] = ˆ θ P CC , and variance</p><formula xml:id="formula_5">Var S n = 1 n 2 i p i (1 − p i ).<label>(4)</label></formula><p>To a certain degree, this model captures uncer- tainty in the classifier since per-document vari- ance, p i (1 − p i ), is high when p i = 0.5 and low when near 0 or 1. However, it also has a major weakness-the variance concentrates with a large test group size n, which is the wrong behavior when a classifier is truly noisy, for example, when a classifier is genuinely uncertain and predicts the same constant p i = q for each document. In this case, the correct behavior would be to maintain a flat, wide posterior belief about θ, which is better accomplished by the generative model we intro- duce in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our approach: generative probabilistic modeling</head><p>We turn to generative modeling, that seeks to to jointly model the probability of labels and text in both the training and test groups, by assum- ing a document's text is generated conditional on the document label. Language models have widespread use in natural language processing, and class-conditional models have been used for document classification (e.g. multinomial Naive Bayes; <ref type="bibr" target="#b21">McCallum and Nigam (1998)</ref>). We use a similar generative setup to explicitly model a class prevalence for test group g, with a generative story for each (bag-of-words) document i in the group:</p><formula xml:id="formula_6">θ g ∼ Dist(α) (5) y i,g ∼ Bernoulli(θ g ) (6) x i,g ∼ Multinomial(φ y i,g )<label>(7)</label></formula><p>The test group is assumed to have a latent class prior θ g , which itself has a prior distribution (we assume Dist(α) = Unif(0, 1) in this work). For</p><formula xml:id="formula_7">✓ y x g 2 {1..G} i 2 D (g) ✓ y x i 2 D train ↵</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Testing</head><p>Figure 2: Our generative model for prevalence esti- mation. Left: Class-conditional language models (φ) are learned at training time. Right: Test-time inference for multiple groups' latent prevalences (θ).</p><p>each class k, φ k is a class-conditional unigram lan- guage model, which is learned from the training data but fixed at test time. We then perform infer- ence to find θ g that gives a high probability to text data {x i ∈ D (g) }. <ref type="figure">Figure 2</ref> shows the probabilistic graphical model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MNB and Loglin language models</head><p>We experiment with two explicit language mod- els in this generative framework: (1) multinomial Naive Bayes (MNB), using a training-time sym- metric Dirichlet prior φ y ∼ Dir(λ/V ) for vo- cabulary size V and "pseudocount" λ, and <ref type="formula" target="#formula_2">(2)</ref> an additive log linear model (Loglin, a.k.a. SAGE (Eisenstein et al., 2011)). Loglin estimates words' probabilities as deviations from a background log- probability m,</p><formula xml:id="formula_8">η y,w ∼ Laplace(λ)<label>(8)</label></formula><formula xml:id="formula_9">φ y,w = exp(m w + η y,w )/ j exp(m j + η y,j )</formula><p>where m w is the empirical log probability of a word w among all training documents, and η y,w denotes class-specific deviations of the log- probability of a word w, MAP estimated under a sparsity-inducing L1 penalty. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implicit likelihoods from discriminative classifiers (LR-Implicit)</head><p>This generative formulation has a major advan- tage over the discriminative, CC-style aggregation models because it sets up a likelihood and pos- terior distribution over θ. But in terms of docu- ment modeling for classification purposes, the in- dependence assumptions of the generative model are typically too strong, and for document-level classification, discriminative models tend to out- perform similarly parameterized generative ones, especially when the training set is sufficiently large ( <ref type="bibr" target="#b25">Ng and Jordan, 2002</ref>). Thus, discrimina- tive models may have information better suited to class prevalence inference. Also, since the most common practice for document classification is to use discriminative models, it would be helpful to more effectively use discriminative posteriors within our generative context. In Naive Bayes-style generative document clas- sification, the model defines p gen (x | y) and class prior p(y), which are combined to calculate the posterior p gen (y | x) ∝ p gen (x | y)p(y). Dis- criminative models, by contrast, directly define a p disc (y | x). We can, however, expand this quan- tity via Bayes Rule:</p><formula xml:id="formula_10">p disc (y | x) = p implicit (x | y)p train (y)/p(x). (9)</formula><p>The "implicit document likelihood" p implicit (x | y) is a likelihood function that, combined with a particular class prior p(y), would have resulted in the same posterior predicted by the discrim- inative model. Given the discriminative poste- rior predictions and the training-time class prior p train (y) = ˆ θ train , an implicit likelihood function can be backed out for any particular document x; we define the "simple implicit" likelihood for doc- ument x to be:</p><formula xml:id="formula_11">p implicit (x | y) = p disc (y | x)/ ˆ θ train .<label>(10)</label></formula><p>This takes the form of a correction of the discrim- inative posterior, by dividing out the training-time class prevalence. <ref type="bibr">5</ref> Our LR-Implicit generative model uses the same class prevalence and document label genera- tion setup as before, but to calculate the individual documents' p(x | y) probabilities, it uses p implicit based on a logistic regression p disc . 6 <ref type="bibr">5</ref> Technically, p implicit is retrievable only up to a constant, and pimplicit is one particular compatible implicit likelihood, since it can be multiplied by any constant and is still consis- tent with Eq. 9, and would give rise to the same document- and group-level posteriors. <ref type="bibr">6</ref> The implicit likelihood still has the form of a logis- tic regression, adjusting its bias term: if pdisc(y | x) = σ(β x + β0), then pimplicit(x | y) = σ(β x + β0 − log (θtrain/(1 − θtrain))). This model is inspired by <ref type="bibr" target="#b28">Saerens et al. (2002)</ref>'s EM algorithm for adjusting a classifier for a test set's class prior; they derive it differently by ap- plying the assumption p train (x | y) = p test (x | y), expanding each side with Bayes' Rule, solving for p test (y | x), then estimating p test (y) via EM. This in fact optimizes the same marginal likelihood function in the next section under the implicit- discriminative generative model; our formulation broadens it as a fully Bayesian or likelihood-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference</head><p>To estimate class prevalence, we use the marginal log likelihood over θ to obtain a posterior over θ. For each each test group g, we have the marginal log probability of all document texts,</p><formula xml:id="formula_12">MLL g (θ) ≡ log p(D (g) | θ)<label>(11)</label></formula><formula xml:id="formula_13">= i∈D (g) log y∈{0,1} p(x i , y i = y | θ) = i∈D (g) log θL + i + (1 − θ)L − i ,</formula><p>where we denote the class-conditional document text likelihoods</p><formula xml:id="formula_14">L + i ≡ p(x i | y i = 1) and L − i ≡ p(x i | y i = 0). The gradient for an individ- ual document is (L + i − L − i )/(θL + i + (1 − θ)L − i )</formula><p>; intuitively, the sign of the numerator says that documents that are more likely under the positive than negative class encourage higher likelihood for larger values of θ. When the model is uncertain about a document-that is, when L + i ≈ L − i -that document contributes a relatively flat likelihood curve, expressing little preference for likely val- ues of θ. If a model is more heavily regularized- for example, when the log-linear additive model is more dominated by the background language model-this condition tends to hold for the doc- uments, leading to a flat, highly uncertain likeli- hood curve.</p><p>The marginal log likelihood is unimodal over θ ∈ <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, since it is concave, being a sum of con- cave log-linear functions, and having negative cur- vature:</p><formula xml:id="formula_15">∂ 2 MLL g ∂θ 2 = − i∈D (g) L + i − L − i θL + i + (1 − θ)L − i 2 .<label>(12)</label></formula><p>Since it is concave and there is only one param- eter, a very wide variety of techniques could be used to reliably find a mode, including EM or first- or second-order methods. At least two approaches to inferring confidence intervals are possible. One is to use a central limit theorem-style approxima- tion, assuming the sampling distribution is approx- imated by a normal with mean θ MLE and variance −[∂ 2 MLL g /∂θ 2 ] −1 . The second, which we focus on, is Bayesian estimation for log p(θ g | D (g) ) ∝ log p(θ g ) + MLL g (θ g ) by simply using a grid search over values θ ∈ {0.001, 0.002, ...0.999} to infer both the posterior mode θ MAP as well as a 90% highest posterior density interval. <ref type="bibr">7</ref> In small- scale experiments, this model had very similar re- sults to the central limit theorem (with EM for θ MLE ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>In order to compare document class prevalence estimators, we desire datasets that (1) have natu- ral document groups that correspond to realistic, real-world applications, (2) have a large number of test groups (hundreds or more), and <ref type="formula">(3)</ref>   <ref type="bibr" target="#b8">and Sebastiani (2015)</ref> use large, pre-existing la- beled document corpora, but they do not contain natural groups; evaluations utilize randomly sam- pled synthetic groups. To better fulfill these criteria, we select the task of business review sentiment prevalence, where the goal is to estimate the proportion of reviews that are positive for one particular business; specif- ically, we use labeled data from the Yelp Dataset Challenge Round Nine 8 corpus, which consists of 4.1M reviews by 1M users for 144K businesses. We sample 500 businesses with at least 200 re- views each as the test groups. We treat the task as binary classification, and assign y i = 1 to reviews with 3 or more stars. This task seems reason- ably representative of real-world sentiment anal- ysis problems, and this type of dataset can easily be collected and reproduced from Yelp or other widely available review data.</p><p>For training, we simulate a small-scale annota- tion project by sampling 2000 labeled documents from the rest of the corpus. This is a natural prevalence that on average is about the same as the test groups, though individual test groups may have a much different prevalence (ranging from 0.096 to 0.997, mean (stdev) 0.823 (0.136)). We also construct a synthetic training setting with a highly skewed class prior, selecting 2000 docu- ments with a 0.1 class prevalence (i.e. 200 posi- tive documents in the group). In each case, for ev- ery model, we re-run and average results over 10 different samples of the training set. For prepro- cessing, we tokenize with NLTK 9 and lowercase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model training</head><p>We use L1 regularization for logistic regression based on the vector of a documents' word counts, to be most directly comparable to the generative models; for each model, we select its hyperparam- eter (LR and Loglin's λ, or MNB's pseudocount) by minimizing cross-validated cross-entropy of in- dividual document posteriors (within the labeled training set), over a grid search of powers of 2. The log-linear additive model is trained with OWL-QN ( <ref type="bibr" target="#b0">Andrew and Gao, 2007)</ref>  <ref type="bibr">10</ref> and the logistic regres- sion model is trained with the default implementa- tion in scikit-learn <ref type="bibr" target="#b26">(Pedregosa et al., 2011</ref>). <ref type="bibr">11</ref> We used ReadMe with its default parameters. <ref type="bibr">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>For each of the 500 test groups, we calculate a prevalence point estimatê θ with each method, and evaluate by averaging across groups for mean ab- solute error  Gold prevalence θ * (x-axis) versus predicted prevalencê θ (y-axis) for each of the 500 test groups with natural (nat) training prevalence (top row) and synthetic (syn) 0.1 training prevalence (bottom row). A black y = x line is plotted for visualization. For the models that allow for confidence intervals, 90% CIs for each group are given by the faint grey lines. Blue dots indicate the CI does not contain θ * and red dots indicate the CI does contain θ * . For each setting, we show the the model with median MAE across training resamplings. prediction, we infer 90% intervals and calculate coverage, which is best if it is 0.90. We also re- port average CI width; a narrower interval indi- cates more confidence (even if misplaced). Re- sults are in <ref type="table">Table 1</ref>; every result is averaged over 10 resamplings of the training set.</p><formula xml:id="formula_16">g | ˆ θ g − θ * g | and bias g ( ˆ θ g − θ * g ). 13</formula><note type="other">For the models that allow for confidence interval Natural training prevalence ≈ 0.8 Synthetic training prevalence = 0.</note><p>The ReadMe software did not have competitive performance; we hope in follow-up work to under- stand why Hopkins and King found it had consid- erably stronger performance than SVM-based CC.</p><p>For the natural training class prevalence setting (first column, Table 1), the discriminative-based models (CC, PCC and the adjusted variants ACC and LR-Implicit) all have very similar point esti- mate performance, outperforming the purely gen- erative models (MNB and Loglin). For CI cov- erage, the log-linear and LR-Implicit generative models have significantly better coverage than the discriminative model (PB-PCC) or MNB. Future work is required to improve coverage to be closer to the nominal ideal of 90%.</p><p>By contrast, when the class prevalences are mismatched (second column, <ref type="table">Table 1</ref>), the non- adjusted CC and PCC methods give extremely poor and biased point estimates, and PB-PCC has incredibly poor CI coverage. ACC and the gener- ative models do much better, presumably because their models directly allow for variability in the test class prior. While Loglin has somewhat higher coverage in this setting, overall, LR-Implicit has consistently strong performance in both training settings, and for both point estimation and (rela- tively, at leas) confidence intervals. <ref type="figure" target="#fig_2">Figure 3</ref> shows θ * versusˆθversusˆ versusˆθ for each of the 500 test groups for each of the models, including pre- dicted CIs. CC's and PCC's erroneous assump- tions are directly viewable: in the natural preva- lence setting, the slope shallower than 1, indicat- ing a persistent under-sensitivity to the true class prevalence-unlike ACC and the generative mod- els. In the synthetic training case, CC and PCC wildly underpredict, presumably because they are biased by the low training-time prevalence θ train = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison of PB-PCC and LR-Implicit</head><p>Since PB-PCC and LR-Implicit represent the strongest members of non-adjusted classification aggregation and generative modeling, respec- tively, we further compare their results. When varying synthetic training prevalence across 0.1 to 0.9 <ref type="figure">(Figure 5a</ref>), LR-Implicit has much better MAE in all settings except near the natural prevalence (the test groups have, on average, 0.82 positive prevalence), and consistently stronger CI cover- age. <ref type="figure">Figure 5b</ref> shows results for natural class preva- lence when varying the training set size. Unfortu- nately, LR-Implicit is disadvantaged at very small test sizes-its MAE is higher when there are only a few hundred training documents (≤ 2 8 = 256), though performance converges after that. We sus- pect this may occur because, when textual evi- dence is weak, the classifier learns to more heavily rely on its bias term, which can be a useful form of bias when the training class prevalence matches the test groups (on average). However, at all lev- els, LR-Implicit's coverage is better.</p><p>Since we hypothesized that PB-PCC may be overconfident for large test groups ( §3.5), we test this by binning test groups by the number of doc- uments per group. <ref type="figure" target="#fig_3">Figure 4</ref> confirms that PB-PCC exhibits overconfidence for larger groups (smaller CI width alongside lower CI coverage), but LR- Implicit suffers from the same problem as well. <ref type="bibr" target="#b14">González et al. (2017a)</ref> reviews the class preva- lence estimation literature, and we note a few threads of work here. <ref type="bibr" target="#b2">Bella et al. (2010)</ref> propose a probabilistic variant of ACC, and Esuli and Se- bastiani (2015) compare many methods on news article topics (RCV1) and medical record subject heading (OHSUMED-S) class prevalence tasks, finding varying results among CC, ACC, and PCC. A number of other empirical evaluations were con- ducted in two SemEval Twitter sentiment preva- lence shared tasks, with varying results among these and other methods with a range of classifiers ( <ref type="bibr" target="#b24">Nakov et al., 2016;</ref><ref type="bibr" target="#b27">Rosenthal et al., 2017</ref>); Nakov et al. note that CC was often one of the strongest methods. Esuli and Sebastiani as well as <ref type="bibr" target="#b38">Xue and Weiss (2009)</ref> present semi-supervised loss- augmented classifier training methods to improve prevalence estimation. <ref type="bibr" target="#b32">Tasche (2017)</ref> presents the- oretical results for ACC and Saerens et al.'s EM method (what we call the LR-Implicit MLE), ar- guing they correctly predict θ * under class prior shift; we confirm that those two methods are in- deed better than many alternatives in our empir- ical evaluation. While we focus on inference of the test-time class prior as a class prevalence esti- mate, <ref type="bibr" target="#b28">Saerens et al. (2002)</ref> also show their method can improve individual-level classification accu- racy, which <ref type="bibr" target="#b30">Sulc and Matas (2018)</ref> use for im- age classification. (From the viewpoint of indi- vidual classification, this phenomenon is known as prior probability shift <ref type="bibr" target="#b23">(Moreno-Torres et al., 2012)</ref>.) <ref type="bibr" target="#b15">González et al. (2017b)</ref> and <ref type="bibr" target="#b4">Card and Smith (2018)</ref>, similarly to our results, find that CC is much poorer than ACC under class shift. Card and Smith also show that PCC can be sensi- tive to properties of the classifier, finding that well- calibrated classifiers can give strong performance. They argue that discriminative aggregation models are appropriate for tasks where humans respond to text. <ref type="bibr" target="#b18">Jerzak et al. (2018)</ref> analyze issues in class prevalence estimation and propose the ReadMe2 algorithm, which adds external word embeddings, optimization-based dimension reduction, and sim- ilarity matching to ReadMe's moment-matching framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Additional Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Document class prevalence estimation is a widespread and much understudied task. We show that simple and obvious classifier aggrega- tion methods display consistent biases, especially under class prior shift. Given how widely some of the less effective methods are used, machine learning and natural language processing research could have real impact in this space.</p><p>We also call attention to the need for uncer- tainty aware inference-methods that give con- fidence intervals to summarize their uncertainty. While our method is a first step, future work is necessary to better understand the problem and develop methods with improved coverage. Also, our framework can accommodate a wide array of document and language models-while we fo- cus on bag-of-words models, recent advances in sequence, neural, and attention-based document models could be added directly to our generative model, or used as a discriminative-implicit com- ponent. The overall framework could also be ex- tended to multiclass, and potentially, structured prediction settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example posterior distributions with MAP prevalence estimates, ˆ θ (solid line) and the true prevalence, θ * (dashed line). A desirable property is that confidence intervals, technically Bayesian credible intervals, (shaded regions) will be wider for more uncertain models. For example, the wider CI on the right (green) contains θ * whereas the narrower CI interval on the left (red) does not.</figDesc><graphic url="image-1.png" coords="2,331.83,75.45,169.15,81.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Such sparse ad- ditive models have been used in both supervised and unsupervised document modeling; for exam- ple, as a document-level posterior classifier it out- performs MNB (Eisenstein et al., 2011), or even discriminative models (Taddy, 2013), and its spar- sity helps interpretability for analyzing political, literary, and legal texts (Monroe et al., 2008; Sim et al., 2013; Bamman et al., 2014; Wang et al., 2012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Gold prevalence θ * (x-axis) versus predicted prevalencê θ (y-axis) for each of the 500 test groups with natural (nat) training prevalence (top row) and synthetic (syn) 0.1 training prevalence (bottom row). A black y = x line is plotted for visualization. For the models that allow for confidence intervals, 90% CIs for each group are given by the faint grey lines. Blue dots indicate the CI does not contain θ * and red dots indicate the CI does contain θ * . For each setting, we show the the model with median MAE across training resamplings.</figDesc><graphic url="image-2.png" coords="7,72.00,308.06,453.53,131.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CI coverage rate (left two graphs) and average CI width (right two graphs) for three bins of the test groups, binned by number of documents.</figDesc><graphic url="image-3.png" coords="8,72.00,62.81,453.52,113.30" type="bitmap" /></figure>

			<note place="foot" n="1"> Code available at http://slanglab.cs.umass. edu/doc_prevalence and https://github.com/ slanglab/doc_prevalence.</note>

			<note place="foot" n="3"> Or in fact, across many experiments in which the model or algorithm is applied (Wasserman, 2011).</note>

			<note place="foot" n="4"> https://gking.harvard.edu/readme</note>

			<note place="foot" n="7"> Since we use a uniform prior, this is just the MLE. Technically, we used a prior of Beta(1.0001, 1.0001) to avoid certain issues with tie-breaking, but it was not necessary. 8 Downloaded June 2017 from https://www.yelp. com/dataset_challenge.</note>

			<note place="foot" n="9"> http://www.nltk.org/ 10 Via github.com/larsmans/pylbfgs 11 Version 0.18.2 12 Version 0.99837 from https://gking.harvard. edu/readme, with default parameters features=15, n.subset=300, prob.wt=1. We bypass the ReadMe software&apos;s text preprocessing pipeline, and instead have it use nearly the same document-term matrices as the other models. Since it only handles binary document-term matrices, we transformed counts to indicators; with other models this change only made a minor difference in results. 13 For the generative (MLL) models, ˆ θ is the MAP estimate; the posterior mean gives similar results.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable training of L1-regularized log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Bayesian mixed effects model of literary character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quantification via probability estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Orallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria Jose Ramirez-Quintana</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Characterization of contact offenders and child exploitation material trafficking on five peer-to-peer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Bissias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Liberatore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juston</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Wolak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Abuse &amp; Neglect</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="185" to="199" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The importance of calibration for estimating proportions from annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using sentiment analysis to monitor electoral campaigns: Method matters-evidence from the United States and Italy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Ceron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Curini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><forename type="middle">M</forename><surname>Iacus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="20" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical applications of the Poisson-binomial and conditional Bernoulli distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="875" to="892" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse additive generative models of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing text quantifiers for multivariate loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Counting positives accurately despite inaccurate classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Quantifying counts and costs via classification. Data Mining and Knowledge Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Forman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="164" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Comparison of a screening test and a reference test in epidemiologic studies. II. A probabilistic model for the comparison of diagnostic tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">A</forename><surname>Gart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Buck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Epidemiology</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="593" to="602" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Lieberman Aiden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A primer on neural network models for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="345" to="420" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A review on quantification learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Castaño</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan José Del</forename><surname>Coz</surname></persName>
		</author>
		<idno>74:1-74:40</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Why is quantification an interesting learning problem?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Díez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan José Del</forename><surname>Coz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How words and money cultivate a personal vote: The effect of legislator credit claiming on constituent credit allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Messing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">J</forename><surname>Westwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="703" to="719" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method of automated nonparametric content analysis for social science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Political Science</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="247" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An improved method of automated nonparametric content analysis for social science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><forename type="middle">T</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Strezhnev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Working paper</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How censorship in china allows government criticism but silences collective expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="343" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A demographic analysis of online sentiment during Hurricane Irene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Boulahanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Rodrigue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Language in Social Media</title>
		<meeting>the Second Workshop on Language in Social Media</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparison of event models for naive Bayes text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-98 Workshop on Learning for Text Categorization</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fightin&apos; Words: Lexical feature selection and evaluation for identifying the content of political conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burt</forename><forename type="middle">L</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Colaresi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Quinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unifying view on dataset shift in classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Moreno-Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocío</forename><surname>Raeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Alaizrodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="521" to="530" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semeval2016 Task 4: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation</title>
		<meeting>the 10th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SemEval-2016</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semeval-2017 Task 4: Sentiment analysis in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Saerens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Latinne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Decaestecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="41" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring ideological proportions in political speeches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchuan</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brice</forename><surname>Acree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving cnn classifiers by estimating test-time priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Sulc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08235</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multinomial inverse regression for text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">503</biblScope>
			<biblScope unit="page" from="755" to="770" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fisher consistency for prior probability shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Tasche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">95</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Language from police body camera footage shows racial disparities in officer respect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinodkumar</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">C</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camilla</forename><forename type="middle">M</forename><surname>Hetey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Classification on data with biased class distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Obradovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="527" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Historical analysis of legal opinions with a sparse mixed-effects latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Mayfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Dittmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">All of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th Annual International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Quantification and semi-supervised classification methods for handling changes in class distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary M</forename><surname>Jack Chongjie Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
