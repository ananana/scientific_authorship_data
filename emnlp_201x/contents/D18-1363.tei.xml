<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
							<email>kann@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CIS LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨</forename><surname>Schütze</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">CIS LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3254" to="3264"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3254</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural state-of-the-art sequence-to-sequence (seq2seq) models often do not perform well for small training sets. We address paradigm completion, the morphological task of, given a partial paradigm, generating all missing forms. We propose two new methods for the minimal-resource setting: (i) Paradigm transduction: Since we assume only few paradigms available for training, neural seq2seq models are able to capture relationships between paradigm cells, but are tied to the idiosyncracies of the training set. Paradigm transduction mitigates this problem by exploiting the input subset of inflected forms at test time. (ii) Source selection with high precision (SHIP): Multi-source models which learn to automatically select one or multiple sources to predict a target inflection do not perform well in the minimal-resource setting. SHIP is an alternative to identify a reliable source if training data is limited. On a 52-language benchmark dataset, we outperform the previous state of the art by up to 9.71% absolute accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Morphological generation of previously unen- countered word forms is a crucial problem in many areas of natural language processing (NLP). High performance can lead to better systems for downstream tasks, e.g., machine translation ( <ref type="bibr">Tamchyna et al., 2017)</ref>. Since existing lexicons have limited coverage, learning morphological inflection patterns from labeled data is an important mission and has recently been the subject of multiple shared tasks ( <ref type="bibr">Cotterell et al., 2016</ref><ref type="bibr" target="#b15">Cotterell et al., , 2017a</ref>).</p><p>In morphologically rich languages, words inflect, i.e., they change their surface form in oder to ex- press certain properties, e.g., number or tense. A word's canonical form, which can be found in a dictionary, is called the lemma, and the set of all in- flected forms is referred to as the lemma's paradigm. ("snowman"). In this running example, the input subset is bold, the output subset italic.</p><p>In this work, we address paradigm completion (PC), the morphological task of, given a partial paradigm of a lemma, generating all of its missing forms. For the partial paradigm represented by the input subset {("Schneemannes", GEN;SG), ("Schneemännern", DAT;PL)} of the German noun "Schneemann" shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the goal of PC is to generate the output subset consisting of the six remaining forms.</p><p>Neural seq2seq models define the state of the art for morphological generation if training sets are large; however, they have been less successful in the low-resource setting ( <ref type="bibr" target="#b15">Cotterell et al., 2017a)</ref>. In this paper, we address an even more extreme minimal- resource setting: for some of our experiments, our training sets only contain k ⇡ 10 paradigms. Each paradigm has multiple cells, so the number of forms (as opposed to the number of paradigms) is not necessarily minimal. However, we will see that generalizing from paradigm to paradigm is a key challenge, making the number of paradigms a good measure of the effective training set size.</p><p>We propose two PC methods for the minimal- resource setting: paradigm transduction and source selection with high precision (SHIP). We define a learning algorithm as transductive 1 if its goal is to generalize from specific training examples to spe- cific test examples <ref type="bibr">(Vapnik, 1998)</ref>. In contrast, in-ductive inference learns a general model that is inde- pendent of any test set. Predictions of transductive inference for the same item are different for different test sets. There is no such dependence in inductive inference. Our motivation for transduction is that, in the minimal-resource setting, neural seq2seq mod- els capture relationships between paradigm cells like affix substitution and umlauting, but are tied to the idiosyncracies of the k training paradigms. For example, if all source forms in the training set start with "b" or "d", a purely inductive model may then be unable to generate targets with different initials. By transductive inference on the information avail- able in the input subset at test time, i.e., the given par- tial paradigm, our model can learn idiosyncracies. For example, if the input subset sources start with "p", we can learn to generate output subset targets that start with "p". Thus, we exploit the input subset for learning idiosyncracies at test time and then gen- erate the output subset using a modified model. This setup employs standard inductive training (on the training set) for learning general rules of inflectional morphology and transductive inference (on the test set) for learning idiosyncracies. Our use of trans- duction is innovative in that most previous work has addressed unstructured problems whereas our prob- lem is structured: we complete a paradigm, a com- plex structure of forms, each of them labeled with a morphological tag. Thus, the test set contains labels, whereas, in transduction for unstructured problems, the test set is a flat set of unlabeled instances. We view our work as an extension of transduction to the structured case, even though not all elements of the theory developed by Vapnik (1998) carry over.</p><p>The motivation for our second PC method for lim- ited training data, SHIP, is as follows. Multi-source models can learn which combination of sources most reliably predicts the target in the high-resource, but less well in the minimal-resource setting. SHIP models the relationship between paradigm slots using edit trees ( <ref type="bibr" target="#b5">Chrupała et al., 2008)</ref>, in order to measure how deterministic each transformation is. Then, it identifies the most deterministic source slot for the generation of each target inflection.</p><p>Paradigm transduction and SHIP can be employed separately or in combination. Our exper- iments show that, in an extreme minimal-resource setting, a combination of SHIP and a non-neural approach is most effective; for slightly more data, a combination of a neural model, paradigm transduction and SHIP obtains the best results.</p><p>Contributions. (i) We introduce neural paradigm transduction, which exploits the structure of the PC task to mitigate the negative effect of limited training data. (ii) We propose SHIP, a new algorithm for picking a single reliable source for PC in the minimal-resource setting. (iii) On average over all languages of a 52-language benchmark dataset, our approaches outperform state-of-the-art baselines by up to 9.71% absolute accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Paradigm Completion</head><p>In this section, we formally define our task, developing the notation for the rest of the paper.</p><p>Given the set of morphological tags T (w) of a lemma w, we define the paradigm of w as the set of tuples of inflected form f k and tag t k :</p><formula xml:id="formula_0">⇡(w) = f k [w],t k t k 2T (w)<label>(1)</label></formula><p>The example in <ref type="figure" target="#fig_0">Figure 1</ref> thus corresponds to:</p><formula xml:id="formula_1">⇡(Schneemann) = "Schneemann", NOM;SG . . . "Schneemänner", ACC;PL .</formula><p>A training set in our setup consists of complete paradigms, i.e., all inflected forms of each lemma are available. This simulates a setting in which a lin- guist annotates complete paradigms, as done, e.g., in <ref type="bibr">Sylak-Glassman et al. (2016)</ref>. In contrast, each el- ement of the test set is a partial paradigm, which we refer to as the input subset. This simulates a setting in which we collect all forms of a lemma occurring in a (manually or automatically) annotated input corpus; this set will generally not be complete. The PC task consists of generating the output subset of the paradigm, i.e., the forms belonging to form-tag pairs which are missing from the collected subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our approach for PC is based on MED (Morpholog- ical Encoder-Decoder), a state-of-the-art model for morphological generation in the high-resource case, which was developed by <ref type="bibr" target="#b18">Kann and Schütze (2016b)</ref>. In this section, we first cover required background on MED and then introduce our new approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MED</head><p>Input and output format. MED converts one inflected form of a paradigm into another, given the two respective tags. Thus, the input of MED is a sequence of subtags of the source and the target form (e.g., NOM and SG are subtags of NOM;SG), as well as the characters of the source form. All elements are represented by embeddings, which are trained together with the model. The output of MED is the character sequence of the target inflected form.</p><p>An example from the paradigm in <ref type="figure" target="#fig_0">Figure 1</ref> is:</p><p>INPUT: DAT S PL S GEN T SG T S c h n e e m ¨ a n n e r n OUTPUT: S c h n e e m a n n e s Encoder. The model's encoder consists of a bidirectional gated recurrent neural network (GRU) with a single hidden layer. It reads an input vector sequence x = (x 1 ,...,x Xt ) and encodes it from two opposite directions into two hidden representations ! h t and h t as</p><formula xml:id="formula_2">! h t = GRU(x t , ! h t1 )<label>(2)</label></formula><formula xml:id="formula_3">h t = GRU(x t , h t+1 )<label>(3)</label></formula><p>which are concatenated to</p><formula xml:id="formula_4">h t = h ! h t ; h t i (4)</formula><p>Decoder. The decoder, another GRU with a single hidden layer, defines a probability distribution over the output vocabulary, which, for paradigm comple- tion, consists of the characters in the language, as</p><formula xml:id="formula_5">p(y) = Ty Y t=1 GRU(y t1 ,s t1 ,c t )<label>(5)</label></formula><p>s t denotes the state of the decoder at step t, and c t is the sum of the hidden representations of the encoder, weighted by an attention mechanism. Additional background on the general model ar- chitecture is given in <ref type="bibr" target="#b1">Bahdanau et al. (2015)</ref>; details on MED can be found in <ref type="bibr" target="#b18">Kann and Schütze (2016b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Semi-supervised MED</head><p>In order to make use of unlabeled data with MED,  defined an auxiliary autoencoding task and proposed a multi-task learning approach.</p><p>For this extension, an additional symbol is added to the input vocabulary. Each input is then of the form (A | M + ) ⌃ + , with A being a novel tag for autoencoding, ⌃ being the alphabet of the language, and M being the set of morphological subtags of the source and the target. As for the basic MED model, all parts of the input are represented by embeddings.</p><p>The training objective is to maximize the joint likelihood for the tasks of paradigm completion and autoencoding:</p><formula xml:id="formula_6">L(✓) = P (s,t S ,t T ,w)2D logp ✓ (w | e ✓ (t S ,t T ,s)) + P a2A logp ✓ (a | e ✓ (a))</formula><p>where A is a set of autoencoding examples, e ✓ is the encoder, and D is a labeled training set of tuples of source s, morphological source tag t S , morphological target tag t T , and target w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MED for Paradigm Completion</head><p>MED was originally developed for morphological reinflection. Thus, it operates on pairs consisting of a single source and a single target form. In order to use it for paradigm completion, where multiple source forms are given, and multiple target forms are expected, we convert the given data into a suitable format in the way described in the following. For a lemma w, let J(w) be the set of tags in the input subset. Recall that J(w) is a subset of T (w), the set of all tags, at test time, but that training paradigms are complete, i.e., J(w) = T (w) for the training set.</p><p>For both training of the inductive model and paradigm transduction, we generate |J(w)|(|J(w)|1) training examples</p><formula xml:id="formula_7">(t i ,t j ,f i [w]) 7 ! f j [w]</formula><p>one for each pair of different tags in J(w). We also generate autoencoding training examples for all tags in J(w) (removing duplicates):</p><formula xml:id="formula_8">A,f i [w] 7 ! f i [w]</formula><p>For the German lemma "Schneemann", assume: J(Schneemann) = {GEN;SG,DAT;PL} at test time. We then produce the following training examples for paradigm transduction:</p><formula xml:id="formula_9">(DAT S PL S GEN T SG T Schneemännern) 7 ! Schneemannes (GEN S SG S DAT T PL T Schneemannes) 7 ! Schneemännern (A Schneemannes) 7 ! Schneemannes (A Schneemännern) 7 ! Schneemännern</formula><p>For completing a partial paradigm, we then select one source form per target slot (the lemma, unless stated otherwise) and create all forms corresponding to the tags in J(w)\T (w) one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Paradigm Transduction</head><p>Motivation. In the minimal-resource setting, parameter estimates are tied to the idiosyncracies of the lemmas seen in training, due to overfitting. Our example in §1 is that the model has difficulties producing initial letters not seen during training. However, within each paradigm, forms are gen- erally similar; thus, input subset sources contain valuable information about how to generate output subset targets. Based on this observation, we solve the problem of overfitting by transduction: we teach the model test idiosyncracies by training it on the input subset before generating the output subset.</p><p>Method description. We first train a general model on the training set in the standard supervised learning setup, i.e., the setup which is called inductive inference by <ref type="bibr">Vapnik (1998)</ref>. At test time, we take the general model as initialization and continue training on examples generated from the input subset as described in §3.3. We do this separately for each lemma, satisfying the defining criterion of transductive inference that predictions depend on the test data. Also, different input subsets (i.e., different subsets of the same paradigm) can in general make different predictions on an output subset target.</p><p>Paradigm transduction is expected to perform best in a setting in which many forms of each paradigm are given as input, i.e., when |J(w)| is big. In <ref type="figure" target="#fig_1">Figure 2</ref> we show the average sizes of the input subsets for all languages in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Source Selection with High Precision</head><p>During PC, some sources contain more information relevant to generating certain targets than others. For instance, the nominative singular and accusative singular in German are generally identical (cf. <ref type="figure" target="#fig_0">Fig- ure 1)</ref>; thus, for generating the accusative singular, we should use the nominative singular as source if it is available-rather than, say, the dative plural. parts before/after LCS, e.g., the root has LCS "Schneem", before part ✏ and after part "ann", thus the lengths are "(0,3)". "sub" = "substitution".</p><p>In fact, for many languages, the entire paradigm of most lemmas is deterministic if the right source forms are known and used for the right targets. A set of forms that determines all other inflected forms is called principal parts ( <ref type="bibr" target="#b12">Finkel and Stump, 2007</ref>  <ref type="bibr">[w]</ref> can be obtained, and they train on hundreds of paradigms per part of speech (POS) and language, which are not available in our setup.</p><p>We propose an alternative for the minimal- resource setting: SHIP, which selects a single best source for each target and is based on edit trees.  We construct edit trees for each pair</p><formula xml:id="formula_10">(f i [w],f j [w]</formula><p>) in the training set, count the number n ij of different edit trees for t i 7 ! t j , and construct a fully connected graph. The tags are nodes of the graph, and the counts n ij are weights. Edges are undirected, since edit trees are bijections (cf. <ref type="figure" target="#fig_4">Figure 4)</ref>. We then interpret the weight of an edge as a measure of the (un)reliability of the corresponding two source-target relationships. Our intuition is that the fewer different edit trees relate source and target, the more reliable the source is for generating the target.</p><p>At test time, we find for each target t j a source t k such that n kj  n ij 8i 2 J(w). We then use f k [w] to generate f j <ref type="bibr">[w]</ref>. Again, <ref type="figure" target="#fig_4">Figure 4</ref> shows examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We run experiments on the datasets from task 2 of the CoNLL-SIGMORPHON 2017 shared task, which have been created using UniMorph ( <ref type="bibr" target="#b21">Kirov et al., 2018</ref>). We give a short overview here; see (Cotterell et al., 2017a) for details. The dataset contains, for each of 52 languages, a development set of 50 partial paradigms, a test set of 50 partial paradigms, and three training sets of complete paradigms. Training set sizes are 10 (SET1), 50 (SET2), and 200 (SET3). Recall that we view the number of paradigms (not the number of forms) as the best measure of the amount of training data available. Even for SET3, there are only 200 lemmas per language in the training set, which are additionally distributed over multiple POS tags, compared to &gt;600 lemmas per POS used by . We, thus, want to emphasize that all settings-SET1, SET2, and SET3-can be considered low-resource.</p><p>We produce training sets for our encoder-decoder as described in §3.3, but limit the total number of training examples to 200,000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters</head><p>With our hyperparameters, we follow <ref type="bibr" target="#b17">Kann and Schütze (2016a)</ref>. In particular, our encoder and decoder GRUs have 100-dimensional hidden states. Our embeddings are 300-dimensional. For training, we use stochastic gradient descent, ADADELTA <ref type="bibr">(Zeiler, 2012)</ref>, and minibatches of size 20. After experiments on the development set, we decide on training SET1, SET2, and SET3 models for 50, 30, and 20 epochs, respectively. For paradigm transduc- tion, we train all models for 25 additional epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>In the following, we describe our baselines. COPY, MED, and PT are used for ablation and SIG17 for comparison with the state of the art.</p><p>COPY. As targets in many paradigm cells in many languages are identical to the lemma, we consider a copy baseline that simply copies the lemma.</p><p>MED. This is the model by <ref type="bibr" target="#b18">Kann and Schütze (2016b)</ref>, which performed best at SIGMORPHON 2016. For decoding, the lemma is used. Since MED is designed for the high-resource setting, we do not expect good performance for our minimal-resource scenario, but the comparison shows how much our enhancements improve performance.</p><p>Pure paradigm transduction (PT). PT is a seq2seq model exclusively trained on the input sub- set. Its performance sheds light on the importance of the initial inductive training.  <ref type="table">Table 1</ref>: Accuracy on PC for SIG17+SHIP (the shared task baseline SIG17 with SHIP), MED+PT (MED with paradigm transduction), MED+PT+SHIP (MED with paradigm trans- duction and SHIP), as well as all baselines (BL). Results are averaged over all languages, and best results are in bold; de- tailed accuracies for all languages can be found in <ref type="bibr">Appendix A.</ref> training data. Its design follows <ref type="bibr" target="#b22">Liu and Mao (2016)</ref>: SIG17 first aligns each input lemma and output inflected form. Afterwards, it assumes that each aligned pair can be split into a prefix, a stem, and a suffix. Based on this alignment, the system extracts prefix (resp. suffix) rules from the prefix (resp. suffix) pairings. At test time, suitable rules are applied to the input string to generate the target; more details can be found in <ref type="bibr" target="#b15">Cotterell et al. (2017a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIG17. SIG17 is the official baseline of the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>Our results are shown in <ref type="table">Table 1</ref>. For SET1, SIG17+SHIP obtains the highest accuracy, while, for SET2 and SET3, MED+PT+SHIP performs best. This difference can be easily explained by the fact that the performance of neural networks decreases rapidly for smaller training sets, and, while paradigm transduction strongly mitigates this problem, it cannot completely eliminate it. Overall, however, SIG17+SHIP, MED+PT, and MED+PT+SHIP all outperform the baselines by a wide margin for all settings.</p><p>Effect of paradigm transduction. On average, MED+PT clearly outperforms SIG17, the strongest baseline: by .0796 (.5808-.5012) on SET1, .0910 (.7486-.6576) on SET2, and .0747 (.8454-.7707) on SET3.</p><p>However, looking at each language individually (refer to Appendix A for those results), we find that MED+PT performs poorly for a few languages, namely Danish, English, and Norwegian <ref type="bibr">(Bokmål &amp; Nynorsk)</ref>. We hypothesize that this can most likely be explained by the size of the input subset of those languages being small (cf. <ref type="figure" target="#fig_1">Figure 2</ref> for average input subset sizes per language). Recall that the input subset is explored by the model during transduction. Most poorly performing languages have input subsets containing only the lemma; in this case paradigm transduction reduces to autoencoding the lemma. Thus, we conclude that paradigm transduction can only improve over MED if two or more sources are given.</p><p>Conversely, if we consider only the languages with an average input subset size of more than 15 (Basque, Haida, Hindi, Khaling, Persian, and Quechua), the average accuracy of MED+PT for SET1 is 0.9564, compared to an overall average of 0.5808. This observation shows clearly that paradigm transduction obtains strong results if many forms per paradigm are given.</p><p>Effect of SHIP. Further, <ref type="table">Table 1</ref> shows that SIG17+SHIP is better than SIG17 by .0959 (.5971-.5012) on SET1, .0779 (.7355-.6576) on SET2, and .0301 (.8008-.7707) on SET3. Stronger effects for smaller amounts of training data indicate that SHIP's strategy of selecting a single reliable source is more important for weaker final models; in these cases, selecting the most deterministic source reduces errors due to noise.</p><p>In contrast, the performance of MED, the neural model, is relatively independent of the choice of source; this is in line with earlier findings <ref type="bibr">(Cotterell et al., 2016</ref>). However, even for MED+PT, adding SHIP (i.e., MED+PT+SHIP) slightly increases accuracy by .0061 (.7547-.7486) on SET2, and .0029 (.8483-.8454) on SET3 (L53).</p><p>Ablation. MED does not perform well for either SET1 or SET2. In contrast, on SET3 it even outper- forms SIG17 for a few languages. However, MED loses against MED+PT in all cases, highlighting the positive effect of paradigm transduction.</p><p>Looking at PT next, even though PT does not have a zero accuracy for any setting or language, it performs consistently worse than MED+PT. For SET3, PT is even lower than MED on average, by .3436 (.4211-.0775). Note that, in contrast to the other methods, PT's performance is not dependent on the size of the training set. The main determinant for PT's performance is the size of the input subset during transductive inference. If the input subset is large, PT can perform better than MED, e.g., for Hindi and Urdu. For Khaling SET1, PT even outperforms both MED and SIG17. However, in most cases, PT does not perform well on its own.</p><p>MED+PT outperforms both MED and PT. This confirms our initial intuition: MED and PT learn complementary information for paradigm input output MED PT MED+PT Schneemann N;GEN;PL GetGächen Scnneeeeennnnnnnnnnnnnnnnnnnnn Schneemänner dish V;V.PTCP;PRS dising dish dishing creer V;SBJV;PRS;1;PL crezcamos creyemos creamos  completion. The base model learns the general structure of the language (i.e., correspondences between tags and inflections) while paradigm transduction teaches the model which character sequences are common in a specific test paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">On the Size of the Input Subset</head><p>We expect paradigm transduction to become more effective as the size of the input subset increases. <ref type="figure" target="#fig_5">Figure 5</ref> shows the accuracy of MED+PT as a function of the average input subset size for SET1, SET2, and SET3. Accuracy for languages with input set sizes above 15 is higher than .8 in all settings. In general, languages with larger input set sizes perform better. The correlation is not perfect because languages have different degrees of morphological regularity. However, the overall trend is clearly recognizable.</p><p>The organizers of CoNLL-SIGMORPHON provided large input subsets in the development and test sets of languages with large paradigms. Thus, PT performs better for languages with many inflected forms per paradigm, i.e., large |T (w)|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">On the Effect of Paradigm Transduction</head><p>We further analyze why paradigm transduction improves the performance of the base model MED, using the German, English, and Spanish SET2 exam- ples for MED, PT, and MED+PT given in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>German. MED generates an almost random sequence. However, it learns that the umlaut "¨ a" must appear in the target. PT only produces correct characters, but it produces far too many. The reason may be that the model is trained on both a double "e" and a double "n", learning that "e" and "n" are likely to appear repeatedly. MED+PT generates the correct target.</p><p>English. MED fails to generate "h" because the bigram "sh" did not occur in training, and so the probability of "h" following "s" is estimated to be low. PT fails to produce the suffix "ing", since it does not occur in the input subset, and, thus, PT has no way of learning it. Again, MED+PT generates the correct target.</p><p>Spanish. MED produces "crezcamos", a form that has the correct tag V;SBJV;PRS;1;PL, but is a form of "crecer" (which appears in the training set), not of "creer" (which does not). This demonstrates the problems resulting from a lack of lemma diversity during training. PT produces a combination of several of the forms in the input subset: subjunctive forms beginning with "crey" and "creemos" V;IND;PRS;1;PL. Again, MED+PT generates the correct target.</p><p>Overall, this analysis confirms that MED learns relationships between paradigm cells, while paradigm transduction adds knowledge about the idiosyncracies of a partial test paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to Multi-Source Models</head><p>In this section, we explicitly compare our approach to neural multi-source models for morphological generation.</p><p>Following <ref type="bibr" target="#b15">Kann et al. (2017a)</ref>, we employ attention-based RNN encoder-decoder networks with two or four input sources. The input to a  multi-source model is the concatenation of all sources and corresponding tags. During training, we randomly sample (with repetition) one or three additional forms from the paradigm of each example. At test time, we sample the additional forms from the given partial paradigm; without repetition first, but repeating if not enough inflected forms are available. For autoencoding examples in the training data, we simply concatenate two or four copies of the source and the autoencoding tag. We randomly select five languages for this experiment. <ref type="table" target="#tab_4">Table 3</ref> shows that, for SET3, four sources (column header "4") are generally better than two sources ("2"), which in turn are better than one source ("1"); thus, as expected, making additional sources available in training improves results. We attribute one exception (German accuracy is .4391 for "1" and .4179 for "2") to the noisiness of the problem-training sets in terms of number of paradigms are relatively small, even for SET3.</p><p>The improvements we see for SET3 are large. This suggests that using more than four sources would further improve results and perhaps reach the level of performance of MED+PT, at the cost of a long training time. However, for SET1 and SET2, there is no consistent improvement from 1 to 2 to 4 sources. While it is possible that further opti- mization could improve the best multi-source result given in <ref type="table" target="#tab_4">Table 3</ref>, the gap to MED+PT is very large, and the improvement from 2 to 4 is small. This indicates that multi-source methods cannot compete with transductive learning for SET1 and SET2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis of SHIP</head><p>For a qualitative analysis of SHIP, we look at the sources it selects for French verbs on the develop- ment set; the complete diagram is shown in <ref type="figure" target="#fig_6">Figure  6</ref>. For most verbs, future and conditional can be predicted from COND;1;PL (e.g., "finirions"), and indicative present, indicative imparfait and subjunc- tive present from IND;PRS;3;PL (e.g., "finissent"). In case of ties, SHIP selects the alphabetically first tag; this explains why COND;1;PL gets preference over IND;PRS;3;PL for indicative present singular. These two forms represent two of the principal parts of French conjugation, the infinitive (almost always derivable from COND;1;PL) and the stem that is used for plural indicative, imparfait, and other paradigm cells-which is sometimes not derivable from the infinitive as is the case for "finir". In comparison, IND;PST;3;SG;IPFV and SBJV;PST;2;PL are less reliable sources. But they are still reasonably accurate if no better alternative is available; consider the following SBJV;PST;2;PL ! IND;PST;1;SG;PFV generations: "parlassiez" 7 ! "parlai", "finissiez" 7 ! "finis", "missiez" 7 ! "mis", "prissiez" 7 ! "pris".</p><p>We thus conclude that SHIP indeed learns to select appropriate source forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Morphological generation. In the last two years, most work on paradigm completion has been done in the context of the SIGMORPHON 2016 and the CoNLL-SIGMORPHON 2017 shared tasks ( <ref type="bibr">Cotterell et al., 2016</ref><ref type="bibr" target="#b15">Cotterell et al., , 2017a</ref>). Due to the success of neural seq2seq models in 2016 ( <ref type="bibr" target="#b18">Kann and Schütze, 2016b;</ref><ref type="bibr" target="#b0">Aharoni et al., 2016)</ref>, systems developed for the 2017 edition were mostly neural ( <ref type="bibr" target="#b24">Makarov et al., 2017;</ref><ref type="bibr" target="#b2">Bergmanis et al., 2017;</ref><ref type="bibr">Zhou and Neubig, 2017</ref>). Besides the shared task systems,  presented a paradigm completion model for a multi-source setting that made use of an attention mechanism to decide which input form to attend to at each time step. They used randomly chosen, independent pairs of source and target forms for training. This differs crucially from the setting we consider in that no complete paradigms were available in their training sets. Only Cotterell et al. (2017b) addressed essentially the same task we do, but they only considered the high-resource setting: their models were trained on hundreds of complete paradigms. The experiments reported in §5.3 empirically confirm that inductive-only models perform poorly in our setting.</p><p>Several ways to employ neural models for morphological generation with limited data have been proposed, e.g., semi-supervised training ( <ref type="bibr">Zhou and Neubig, 2017;</ref> or simultaneous training on multiple languages ( <ref type="bibr" target="#b16">Kann et al., 2017b</ref>). The total number of sources in the training set in some of our settings may be comparable to this earlier work, but our training sets are less diverse since many forms come from the same paradigm. We argue in §1 that the number of paradigms (not the number of sources) measures the effective size of the training set.</p><p>Other important work on morphological generation-neural and non-neural-includes <ref type="bibr" target="#b9">Dreyer et al. (2008)</ref>; <ref type="bibr" target="#b10">Durrett and DeNero (2013)</ref>; <ref type="bibr" target="#b14">Hulden et al. (2014)</ref>; <ref type="bibr" target="#b25">Nicolai et al. (2015);</ref><ref type="bibr" target="#b11">Faruqui et al. (2016)</ref>; <ref type="bibr">Yin et al. (2016)</ref>.</p><p>Seq2seq models in NLP. Even though neural seq2seq models were originally designed for ma- chine translation <ref type="bibr" target="#b27">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, their application has not stayed limited to this area. Similar architectures have been successfully applied to many seq2seq tasks in NLP, e.g., syntactic parsing ( <ref type="bibr">Vinyals et al., 2015)</ref>, language correction ( <ref type="bibr">Xie et al., 2016)</ref>, normalization of historical texts ( <ref type="bibr" target="#b3">Bollmann et al., 2017)</ref>, or text simplification ( <ref type="bibr" target="#b26">Nisioi et al., 2017)</ref>. Transductive inference is similar to domain adaptation, e.g., in machine translation ( <ref type="bibr" target="#b23">Luong and Manning, 2015)</ref>. One difference is that training set and test set can hardly be called different domains in paradigm completion. Another difference is that explicit structured labels (the morphological tags of the forms in the input subset) are available at test time in paradigm completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented two new methods for minimal- resource paradigm completion: paradigm transduc- tion and SHIP. Paradigm transduction learns general inflection rules through standard inductive training and idiosyncracies of a test paradigm through trans- duction. We showed that paradigm transduction effectively mitigates the problem of overfitting due to a lack of diversity in the training data. SHIP is a robust non-neural method that identifies a single reliable source for generating a target. In the minimal-resource setting, this is an effective alter- native to learning how to combine evidence from multiple sources. Considering the average over all languages of a 52-language benchmark dataset, we outperform the previous state of the art by at least 7.07%, and up to 9.71% absolute accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The paradigm of the German noun "Schneemann"</figDesc><graphic url="image-1.png" coords="1,315.87,222.60,200.81,86.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Average amount of sources in the input subset for paradigm transduction, per language.</figDesc><graphic url="image-2.png" coords="4,71.86,62.86,453.54,241.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Edit tree example. Each node gives lengths of the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>An edit tree e(f i [w],f j [w]) is a transformation from a source f i [w] to a target f j [w] (Chrupała et al., 2008); see Figure 3. It is constructed by first determining</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SHIP example for German plural forms (SET1). For the graph constructed in training (see §3.5), subgraphs are extracted in testing for input subset sizes two (left) and three (right). Input subset: yellow and green. Output subset: white and red. For generation of the target shown in red, SHIP selects the source shown in green. the longest common substring (LCS) (Gusfield, 1997) of f i [w] and f j [w] and then modeling the prefix and suffix pairs of the LCS recursively. In the case of an empty LCS, e(f i [w],f j [w]) is the substitution operation that replaces f i [w] with f j [w]. We construct edit trees for each pair (f i [w],f j [w]) in the training set, count the number n ij of different edit trees for t i 7 ! t j , and construct a fully connected graph. The tags are nodes of the graph, and the counts n ij are weights. Edges are undirected, since edit trees are bijections (cf. Figure 4). We then interpret the weight of an edge as a measure of the (un)reliability of the corresponding two source-target relationships. Our intuition is that the fewer different edit trees relate source and target, the more reliable the source is for generating the target. At test time, we find for each target t j a source t k such that n kj  n ij 8i 2 J(w). We then use f k [w] to generate f j [w]. Again, Figure 4 shows examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy of MED+PT as a function of the average input subset size. Red/diamonds: SET1; blue/circles: SET2; green/triangles: SET3.</figDesc><graphic url="image-5.png" coords="7,71.86,172.84,218.28,172.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Right: output set target to be generated. Left: input set source selected by SHIP. Arrows for the two most frequently selected sources are solid, arrows for the two least frequently selected sources are dashed.</figDesc><graphic url="image-6.png" coords="8,307.14,202.71,219.98,439.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>CoNLL-SIGMORPHON 2017 shared task, which was developed to perform well with very little</figDesc><table>SET1 SET2 SET3 
BL: COPY 
.0810 .0810 .0810 
BL: MED 
.0004 .0432 .4211 
BL: PT 
.0833 .0833 .0775 
BL: SIG17 
.5012 .6576 .7707 
SIG17+SHIP 
.5971 .7355 .8008 
MED+PT 
.5808 .7486 .8454 
MED+PT+SHIP .5793 .7547 .8483 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Analysis of the outputs of MED, PT, and MED+PT for SET2. Top to bottom: German, English, Spanish. MED and</head><label>2</label><figDesc></figDesc><table>PT produce incorrect, MED+PT correct inflections. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>MED accuracy on five randomly selected languages with 1, 2, and 4 sources and combined with paradigm transduction 

("+PT"). Best results in bold. 

</table></figure>

			<note place="foot" n="1"> In order to avoid ambiguity, &quot;transduction&quot; is never used in the sense of string-to-string transduction in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Samuel Bowman, Ryan Cotterell, Nikita Nangia, and Alex Warstadt for their feedback on this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Improving sequence to sequence learning for morphological inflection generation: The biu-mit systems for the sigmorphon 2016 shared task for morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMORPHON</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Training data augmentation for low-resource morphological inflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toms</forename><surname>Bergmanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<editor>CoNLL-SIGMORPHON</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning attention for historical text normalization by learning to pronounce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Bollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSST</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning morphology with morfette</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Jason Eisner, and Mans Hulden. 2017a. The CoNLL-SIGMORPHON 2017 shared task: Universal morphological reinflection in 52 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL-SIGMORPHON</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jason Eisner, and Mans Hulden. 2016. The SIGMORPHON 2016 shared taskmorphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMORPHON</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural graphical models over strings for principal parts morphological paradigm completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent-variable modeling of string transductions with finite-state methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised learning of complete morphological paradigms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principal parts and degrees of paradigmatic transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Stump</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Lexington, KY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Kentucky</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Algorithms on strings, trees and sequences: computer science and computational biology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gusfield</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of morphological paradigms and lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Mans Hulden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malin</forename><surname>Forsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural multi-source morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">One-shot neural cross-lingual transfer for paradigm completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMORPHON</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Singlemodel encoder-decoder with explicit morphological representation for reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The LMU system for the CoNLL-SIGMORPHON 2017 shared task on universal morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<editor>CoNLL-SIGMORPHON</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unlabeled data for morphological generation with characterbased sequence-to-sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCLeM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unimorph 2.0: Universal morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christo</forename><surname>Kirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sylak-Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Géraldine</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Vylomova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arya</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kübler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Morphological reinflection with conditional random fields and unsupervised features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingshuang Jack</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMORPHON</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stanford neural machine translation systems for spoken language domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWSLT</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Align and copy: UZH at SIGMORPHON 2017 shared task for morphological reinflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Ruzsics</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</author>
		<editor>CoNLL-SIGMORPHON</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inflection generation as discriminative string transduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><surname>Nicolai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kondrak</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring neural text simplification models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Nisioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Stajner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><forename type="middle">Paolo</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In NIPS</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
