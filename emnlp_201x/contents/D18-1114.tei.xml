<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural-Davidsonian Semantic Proto-role Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins Univeristy</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit5">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Teichert</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins Univeristy</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit5">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Culkin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins Univeristy</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit5">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins Univeristy</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit5">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins Univeristy</orgName>
								<orgName type="institution" key="instit3">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit4">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit5">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural-Davidsonian Semantic Proto-role Labeling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="944" to="955"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>944</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a model for semantic proto-role labeling (SPRL) using an adapted bidirectional LSTM encoding strategy that we call Neural-Davidsonian: predicate-argument structure is represented as pairs of hidden states corresponding to predicate and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art results in SPRL, and (2) that our network naturally shares parameters between attributes, allowing for learning new attribute types with limited added supervision.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Universal Decompositional Semantics (UDS) ( <ref type="bibr">White et al., 2016</ref>) is a contemporary seman- tic representation of text ( <ref type="bibr">Abend and Rappoport, 2017</ref>) that forgoes traditional inventories of se- mantic categories in favor of bundles of simple, interpretable properties. In particular, UDS in- cludes a practical implementation of Dowty's the- ory of thematic proto-roles <ref type="bibr">(Dowty, 1991)</ref>: ar- guments are labeled with properties typical of Dowty's proto-agent (AWARENESS, VOLITION ...) and proto-patient (CHANGED STATE ...).</p><p>Annotated corpora have allowed the exploration of Semantic Proto-role Labeling (SPRL) 1 as a natural language processing task <ref type="bibr">(Reisinger et al., 2015;</ref><ref type="bibr">White et al., 2016;</ref><ref type="bibr">Teichert et al., 2017)</ref>. For example, consider the following sentence, in which a particular pair of predicate and argument heads have been emphasized: "The cat ate the rat." An SPRL system must infer from the con- text of the sentence whether the rat had VOLITION, CHANGED-STATE, and EXISTED-AFTER the eat- ing event (see <ref type="table" target="#tab_1">Table 2</ref> for more properties).</p><p>We present an intuitive neural model that <ref type="figure">Figure 1</ref>: BiLSTM sentence encoder with SPR de- coder. Semantic proto-role labeling is with respect to a specific predicate and argument within a sen- tence, so the decoder receives the two correspond- ing hidden states.</p><p>achieves state-of-the-art performance for SPRL. <ref type="bibr">2</ref> As depicted in <ref type="figure">Figure 1</ref>, our model's architecture is an extension of the bidirectional LSTM, cap- turing a Neo-Davidsonian like intuition, wherein select pairs of hidden states are concatenated to yield a dense representation of predicate-argument structure and fed to a prediction layer for end- to-end training. We include a thorough quanti- tative analysis highlighting the contrasting errors between the proposed model and previous (non- neural) state-of-the-art. In addition, our network naturally shares a sub- set of parameters between attributes. We demon- strate how this allows learning to predict new at-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPR Property</head><p>Explanation of Property</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INSTIGATION</head><p>Arg caused the Pred to happen? ✗ VOLITIONAL Arg chose to be involved in the Pred? ✗</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWARE</head><p>Arg was/were aware of being involved in the Pred?</p><p>PHYSICALLY EXISTED Arg existed as a physical object?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXISTED AFTER</head><p>Arg existed after the Pred stopped? ✗</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHANGED STATE</head><p>The Arg was/were altered or somehow changed during or by the end of the Pred? <ref type="table">Table 1</ref>: Example SPR annotations for the toy example "The cat ate the rat," where the Predicate in question is "ate" and the Argument in question is either "cat" or "rat." Note that not all SPR properties are listed, and the binary labels (, ✗) are coarsened from a 5-point Likert scale.</p><p>tributes with limited supervision: a key finding that could support efficient expansion of new SPR attribute types in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Davidson (1967) is credited for representations of meaning involving propositions composed of a fixed arity predicate, all of its core argu- ments arising from the natural language syn- tax, and a distinguished event variable. The earlier example could thus be denoted (modulo tense) as (∃e)eat[(e, CAT, RAT)], where the vari- able e is a reification of the eating event. The order of the arguments in the predication im- plies their role, where leaving arguments unspec- ified (as in "The cat eats") can be handled ei- ther by introducing variables for unstated argu- ments, e.g., (∃e)(∃x)[eat(e, CAT, x)], or by cre- ating new predicates that correspond to differ- ent arities, e.g., (∃e)eat intransitive[(e, CAT)]. <ref type="bibr">3</ref> The Neo-Davidsonian approach <ref type="bibr">(Castañeda, 1967;</ref><ref type="bibr">Parsons, 1995)</ref>, which we follow in this work, al- lows for variable arity by mapping the argument positions of individual predicates to generalized semantic roles, shared across predicates, 4 e.g., AGENT, PATIENT and THEME, in:</p><formula xml:id="formula_0">(∃e)[eat(e) ∧ Agent(e, CAT) ∧ Patient(e, RAT)].</formula><p>Dowty <ref type="bibr">(1991)</ref> conjectured that the distinction between the role of a prototypical Agent and prototypical Patient could be decomposed into a number of semantic properties such as "Did the argument change state?". Here we formulate this <ref type="bibr">3</ref> This formalism aligns with that used in PropBank ( <ref type="bibr">Palmer et al., 2005</ref>), which associated numbered, core ar- guments with each sense of a verb in their corpus annotation. <ref type="bibr">4</ref> For example, as seen in FrameNet ( <ref type="bibr">Baker et al., 1998</ref>).</p><p>as a Neo-Davidsonian representation employing semantic proto-role (SPR) attributes:</p><formula xml:id="formula_1">(∃e) [ eat(e) ∧ volition(e, CAT) ∧ instigation(e, CAT)... ∧ ¬volition(e, RAT) ∧ destroyed(e, RAT)... ]</formula><p>Dowty's theory was empirically verified by Kako (2006), followed by pilot ( <ref type="bibr">Madnani et al., 2010</ref>) and large-scale ( <ref type="bibr">Reisinger et al., 2015</ref>) cor- pus annotation efforts, the latter introducing a lo- gistic regression baseline for SPRL. <ref type="bibr">Teichert et al. (2017)</ref> refined the evaluation protocol, <ref type="bibr">5</ref> and devel- oped a CRF ( <ref type="bibr">Lafferty et al., 2001</ref>) for the task, rep- resenting existing state-of-the-art.</p><p>Full details about the SPR datasets introduced by <ref type="bibr">Reisinger et al. (2015)</ref> and <ref type="bibr">White et al. (2016)</ref>, which we use in this work, are provided in Ap- pendix B. For clarity, <ref type="table">Table 1</ref> shows a toy SPRL example, including a few sample SPR properties and explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">"Neural-Davidsonian" Model</head><p>Our proposed SPRL model <ref type="figure">(Fig. 1</ref>) determines the value of each attribute (e.g., <ref type="bibr">VOLITION)</ref> on an ar- gument (a) with respect to a particular predication (e) as a function on the latent states associated with the pair, (e, a), in the context of a full sen- tence. Our architecture encodes the sentence using a shared, one-layer, bidirectional LSTM (Hochre- iter and Schmidhuber, 1997; <ref type="bibr">Graves et al., 2013)</ref>. We then obtain a continuous, vector representa- tion h ea = [h e ; h a ], for each predicate-argument pair as the concatenation of the hidden BiLSTM states h e and h a corresponding to the syntactic head of the predicate of e and argument a respec- tively. These heads are obtained over gold syntac- tic parses using the predicate-argument detection tool, <ref type="bibr">PredPatt (White et al., 2016)</ref>. <ref type="bibr">6</ref> For each SPR attribute, a score is predicted by passing h ea through a separate two-layer percep- tron, with the weights of the first layer shared across all attributes:</p><formula xml:id="formula_2">Score(attr, h ea ) = W attr [g (W shared [h ea ])]</formula><p>This architecture accomodates the definition of SPRL as multi-label binary classification given by <ref type="bibr">Teichert et al. (2017)</ref> by treating the score as the log-odds of the attribute being present (i.e. P(attr|h ea ) = 1 1+exp <ref type="bibr">[−Score(attr,hea)]</ref> ). This architecture also supports SPRL as a scalar re- gression task where the parameters of the network are tuned to directly minimize the discrepancy between the predicted score and a reference scalar label. The loss for the binary and scalar models are negative log-probability and squared error, respectively; the losses are summed over all SPR attributes.</p><p>Training with Auxiliary Tasks A benefit of the shared neural-Davidsonian representation is that it offers many levels at which multi-task learning may be leveraged to improve parameter estima- tion so as to produce semantically rich represen- tations h ea , h e , and h a . For example, the sen- tence encoder might be pre-trained as an encoder for machine translation, the argument represen- tation h a can be jointly trained to predict word- sense, the predicate representation, h e , could be jointly trained to predict factuality <ref type="bibr">(Saurí and Pustejovsky, 2009;</ref><ref type="bibr">Rudinger et al., 2018)</ref>, and the predicate-argument representation, h ea , could be jointly trained to predict other semantic role formalisms (e.g. PropBank SRL-suggesting a neural-Davidsonian SRL model in contrast to re- cent BIO-style neural models of SRL ( <ref type="bibr">He et al., 2017)</ref>).</p><p>To evaluate this idea empirically, we exper- imented with a number of multi-task training strategies for SPRL. While all settings outper- formed prior work in aggregate, simply initial- izing the BiLSTM parameters with a pretrained English-to-French machine translation encoder 7 produced the best results, 8 so we simplify discus- sion by focusing on that model. The efficacy of MT pretraining that we observe here comes as no surprise given prior work demonstrating, e.g., the utility of bitext for paraphrase ( <ref type="bibr">Ganitkevitch et al., 2013)</ref>, that NMT pretraining yields improved con- textualized word embeddings 9 ( <ref type="bibr">McCann et al., 2017)</ref>, and that NMT encoders specifically capture useful features for SPRL <ref type="bibr">(Poliak et al., 2018)</ref>.</p><p>Full details about each multi-task experiment, including a full set of ablation results, are reported in Appendix A; details about the corresponding datasets are in Appendix B.</p><p>Except in the ablation experiment of <ref type="figure">Figure  2</ref>, our model was trained on only the SPRL data and splits used by <ref type="bibr">Teichert et al. (2017)</ref> (learning all properties jointly), using GloVe 10 embeddings and with the MT-initialized BiLSTM. Models were implemented in PyTorch and trained end-to-end with Adam optimization <ref type="bibr">(Kingma and Ba, 2014</ref>) and a default learning rate of 10 −3 . Each model was trained for ten epochs, selecting the best-performing epoch on dev.</p><p>Prior Work in SPRL We additionally include results from prior work: "LR" is the logistic- regression model introduced by <ref type="bibr">Reisinger et al. (2015)</ref> and "CRF" is the CRF model (specifically SPRL ⋆ ) from <ref type="bibr">Teichert et al. (2017)</ref>. Although <ref type="bibr">White et al. (2016)</ref> released additional SPR an- notations, we are unaware of any benchmark re- sults on that data; however, our multi-task results in Appendix A do use the data and we find (un- surprisingly) that concurrent training on the two SPR datasets can be helpful. Using only data and splits from <ref type="bibr">White et al. (2016)</ref>, the scalar regres- sion architecture of <ref type="table" target="#tab_9">Table 6</ref> achieves a Pearson's ρ of 0.577 on test.</p><p>There are a few noteworthy differences between our neural model and the CRF of prior work. As an adapted BiLSTM, our model easily ex-      <ref type="formula">(1)</ref> all (sampled) instances; (2) argument is a proper noun; (3) argument is an organization or institu- tion; (4) argument is a pronoun; (5) predicate is phrasal or a particle verb construction; (6) pred- icate is used metaphorically; <ref type="formula">(7)</ref> predicate is a light-verb construction. #DIFFER is the size of the respective subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>original SPR annotations on a 5-point Likert scale, instead of a binary cut-point along that scale (&gt; 3). Manual Analysis We select two properties (VOLITION and MAKES PHYSICAL CONTACT) to perform a manual error analysis with respect to CRF 11 and our binary model from <ref type="table" target="#tab_1">Table 2</ref>. For each property, we sample 40 dev instances with gold labels of "True" (&gt; 3) and 40 instances of "False" (≤ 3), restricted to cases where the two system predictions disagree. <ref type="bibr">12</ref> We manually label each of these instances for the six features shown in <ref type="table" target="#tab_3">Table 3</ref>. For example, given the input "He sits down at the piano and plays," our neural model correctly predicts that He makes physical contact during the sitting, while CRF does not. Since He is a pronoun, and sits down is phrasal, this example contributes −1 to ∆ FALSE-in rows 1, 4 and 5.  Figure 2: Effect of using only a fraction of the training data for a property while either ignoring or co-training with the full training data for the other SPR1 properties. Measurements at 1%, 5%, 10%, 25%, 50%, and 100%.</p><p>For both properties our model appears more likely to correctly classify the argument in cases where the predicate is a phrasal verb. This is likely a result of the fact that the BiLSTM has stronger language-modeling capabilities than the CRF, particularly with MT pretraining. In general, our model increases the false-positive rate for MAKES PHYSICAL CONTACT, but especially when the argument is pronominal.</p><p>Learning New SPR Properties One motiva- tion for the decompositional approach adopted by SPRL is the ability to incrementally build up an in- ventory of annotated properties according to need and budget. Here we investigate (1) the degree to which having less training data for a single prop- erty degrades our F1 for that property on held-out data and (2) the effect on degradation of concur- rent training with the other properties. We focus on two properties only: INSTIGATION, a canonical example of a proto-agent property, and MANIP- ULATED, which is a proto-patient property. For each we consider six training set sizes (1, 5, 10, 25, 50 and 100 percent of the instances). Starting with the same randomly initialized BiLSTM <ref type="bibr">13</ref> , we consider two training scenarios: (1) ignoring the remaining properties or (2) including the model's loss on other properties with a weight of λ = 0.1 in the training objective.</p><p>Results are presented in <ref type="figure">Figure 2</ref>. We see that, in every case, most of the performance is achieved with only 25% of the training data. The curves also suggest that training simultaneously on all SPR properties allows the model to learn the tar-get property more quickly (i.e., with fewer training samples) than if trained on that property in iso- lation. For example, at 5% of the training train- ing data, the "all properties" models are achiev- ing roughly the same F1 on their respective tar- get property as the "target property only" models achieves at 50% of the data. <ref type="bibr">14</ref> As the SPR prop- erties currently annotated are by no means seman- tically exhaustive, 15 this experiment indicates that future annotation efforts may be well served by fa- voring breadth over depth, collecting smaller num- bers of examples for a larger set of attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Inspired by: (1) the SPR decomposition of predicate-argument relations into overlapping fea- ture bundles and (2) the neo-Davidsonian formal- ism for variable-arity predicates, we have pro- posed a straightforward extension to a BiLSTM classification framework in which the states of pre-identified predicate and argument tokens are pairwise concatenated and used as the target for SPR prediction. We have shown that our Neural- Davidsonian model outperforms the prior state of the art in aggregate and showed especially large gains for properties of CHANGED-POSSESSION, STATIONARY, and LOCATION. Our architecture naturally supports discrete or continuous label paradigms, lends itself to multi-task initialization or concurrent training, and allows for parameter sharing across properties. We demonstrated this sharing may be useful when some properties are only sparsely annotated in the training data, which is suggestive of future work in efficiently increas- ing the range of annotated SPR property types.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Mult-Task Investigation</head><p>Multi-task learning has been found to improve performance on many NLP tasks, particularly for neural models, and is rapidly becoming de rigueur in the field. The strategy involves optimizing for multiple training objectives corresponding to dif- ferent (but usually related) tasks. Collobert and Weston (2008) use multi-task learning to train a convolutional neural network to perform multiple core NLP tasks (POS tagging, named entity recog- nition, etc.). Multi-task learning has also been used to improve sentence compression ( <ref type="bibr">Klerke et al., 2016)</ref>, chunking and dependency parsing ( <ref type="bibr">Hashimoto et al., 2017)</ref>. Related work on UDS ( <ref type="bibr">White et al., 2016)</ref> shows improvements on event factuality prediction with multi-task learning on BiLSTM models <ref type="bibr">(Rudinger et al., 2018)</ref>. To com- plete the basic experiments reported in the main text, here we include an investigation of the im- pact of multi-task learning for SPRL. We borrow insights from <ref type="bibr">Mou et al. (2016)</ref> who explore different multi-task strategies for NLP including approach of initializing a network by training it on a related task ("INIT") versus inter- spersing tasks during training ("MULT"). Here we employ both of these strategies, referring to them as pretraining and concurrent training. We also use the terminology target task and auxiliary task to differentiate the primary task(s) we are inter- ested in from those that play only a supporting role in training. In order to tune the impact of aux- iliary tasks on the learned representation, <ref type="bibr">Luong et al. (2016)</ref> use a mixing parameter, α i , for each task i. Each parameter update consists of selecting a task with probability proportional to its α i and then performing one update with respect to that task alone. They show that the choice of α has a large impact on the effect of multi-task training, which influences our experiments here.</p><p>Please refer to Appendix B for details on the datasets used in this section. In particular, with a few exceptions, <ref type="bibr">White et al. (2016)</ref> annotates for the same set of properties as <ref type="bibr">Reisinger et al. (2015)</ref>, but with slightly different protocol and on a different genre. However, in this section we treat the two datasets as if they were separate tasks. To avoid cluttering the results in the main text, we exclusively present results there on what we call SPR1 which consists of the data from <ref type="bibr">Reisinger et al. (2015)</ref> and the train/dev/test splits of <ref type="bibr">Teichert et al. (2017)</ref>. We refer to the analogous tasks built on the data and splits of <ref type="bibr">White et al. (2016)</ref> us- ing the term SPR2. (We are not aware of any prior published results on property prediction for the SPR2.)</p><p>In addition to the binary and scalar SPR archi- tectures outlined in Section 3 of the main paper, we also considered concurrently training the BiL- STM on a fine-grained word-sense disambigua- tion task or on joint SPR1 and SPR2 prediction. We also experimented with using machine trans- lation and PropBank SRL to initialize the parame- ters of the BiLSTM. Preliminary experimentation on dev data with other combinations helped prune down the set of interesting experiments to those listed in <ref type="table" target="#tab_5">Table 4</ref> which assigns names to the mod- els explored here. Our ablation study in Section 4 of the main paper uses the model named SPR1 while the other results in the main paper corre- spond to MT:SPR1 in the case of binary prediction and MT:SPR1S in the case of scalar prediction. Af- ter detailing the additional components used for pretraining or concurrent training, we present ag- gregate results and for the best performing models (according to dev) we present property-level ag- gregate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Auxiliary Tasks</head><p>Each auxiliary task is implemented in the form of a task-specific decoder with access to the hidden states computed by the shared BiLSTM encoder. In this way, the losses from these tasks backpropa- gate through the BiLSTM. Here we describe each task-specific decoder.</p><p>PropBank Decoder The network architecture for the auxiliary task of predicting abstract role types in PropBank is nearly identical to the ar- chitecture for SPRL described in Section 3 of the main paper. The main difference is that the Prop- Bank task is a single-label, categorical classifica- tion task.</p><formula xml:id="formula_3">P(role i |h ea ) = softmax i W propbank [h ea ]</formula><p>The loss from this decoder is the negative log of the probability assigned to the correct label.</p><p>Supersense Decoder The word sense disam- biguation decoder computes a probability distribu- tion over 26 WordNet supersenses with a simple single-layer feedforward network:</p><formula xml:id="formula_4">P(supersense i |h a ) = softmax i (W [h a ])</formula><p>where W ∈ R 1200×26 and h a is the RNN hid- den state corresponding to the argument head to- ken we wish to disambiguate. Since the gold la- bel in the supersense prediction task is a distribu- tion over supersenses, the loss from this decoder is the cross-entropy between its predicted distri- bution and the gold distribution.</p><p>French Translation Decoder Given the en- coder hidden states, the goal of translation is to generate the reference sequence of tokens Y = y 1 , · · · , y n in the target language, i.e., French. We employ the standard decoder architecture for neu- ral machine translation. At each time step i, the probability distribution of the decoded token y i is defined as:</p><formula xml:id="formula_5">P (y i ) = softmax tanh(W fr s i ; c i + b fr )</formula><p>where W fr is a transform matrix, and b fr is a bias. The inputs are the decoder hidden state s i and the context vector c i . The decoder hidden state s i is computed by:</p><formula xml:id="formula_6">s i = RNN(y i−1 , s i−1 )</formula><p>where RNN is a recurrent neural network using L- layer stacked LSTM, y i−1 is the word embedding of token y i−1 , and s 0 is initialized by the last en- coder left-to-right hidden state.  The context vector c i is computed by an at- tention mechanism ( <ref type="bibr">Bahdanau et al., 2014;</ref><ref type="bibr">Luong et al., 2015)</ref>,</p><formula xml:id="formula_7">c i = t α i,t h t , α i,t = exp s ⊤ i (W α h t + b α ) ) k exp s ⊤ i (W α h k + b α ) ,</formula><p>where W α is a transform matrix and b α is a bias. The loss is the negative log-probability of the de- coded sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Results</head><p>In this section, we present a series of experiments using different components of the neural archi- tecture described in Section 3, with various train- ing regimes. Each experimental setting is given a name (in SMALLCAPS) and summarized in <ref type="table" target="#tab_5">Table  4</ref>. Unless otherwise stated, the target task is SPR1 (classification  SPR1 model (SPR1) with randomly initialized word embeddings (SPR1-RAND). The results (Ta- ble 5) reveal substantial gains from the use of pre- trained embeddings; this is likely due to the com- paratively small size of the SPR1 training data.</p><p>Experiment 1a: Multi-task Pretraining We pretrained the BiLSTM encoder with two separate auxiliary tasks: French Translation and Prop- Bank Role Labeling. There are three settings: (1) Translation pretraining only (MT:SPR1), (2) Prop- Bank pretraining only (PB:SPR1), and (3) Transla- tion pretraining followed by PropBank pretraining (MT:PB:SPR1). In each case, after pretraining, the SPRL decoder is trained end-to-end, as in Experi- ment 0 (on SPR1 data).</p><p>Experiment 1b: Multi-task Concurrent One auxiliary task (Supersense or SPR2) is trained concurrently with SPR1 training. In one epoch of training, a training example is sampled at ran- dom (without replacement) from either task un- til all training instances have been sampled. The loss from the auxiliary task (which, in both cases, has more training instances than the target SPRL task) is down-weighted in proportion to ratio of the dataset sizes:  The auxiliary task loss is further down- weighted by a hyperparameter λ ∈ {1, 10 −1 , 10 −2 , 10 −3 , 10 −4 } which is chosen based on dev results. We apply this training regime with the auxiliary task of Supersense prediction (SPR1+WSD) and the scalar SPR2 prediction task (SPR1+SPR2), described in Experiment 2.</p><formula xml:id="formula_8">α = |target task| |auxiliary task| SPR</formula><p>Experiment 1c: Multi-task Combination This setting is identical to Experiment 1b, but includes MT pretraining (the best-performing pretraining setting on dev), as described in 1a. Accord- ingly, the two experiments are MT:SPR1+WSD and MT:SPR1+SPR2.</p><p>Experiment 1d: Property-Specific Model Selec- tion (PS-MS) Experiments 1a-1c consider a va- riety of pretraining tasks, co-training tasks, and weight values, λ, in an effort to improve aggre- gate F1 for SPR1. However, the SPR properties are diverse, and we expect to find gains by choos- ing training settings on a property-specific basis. Here, for each property, we select from the set of models considered in experiments 1a-1c the one that achieves the highest dev F1 for the target property. We report the results of applying those property-specific models to the test data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="0.743">SPR2</head><p>0.591</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MT:SPR1S</head><p>0.753 MT:SPR2 0.577 PB:SPR1S 0.731 PB:SPR2 0.568 MT:PB:SPR1S 0.720 MT:PB:SPR2 0.564  <ref type="table" target="#tab_11">Tables 8 and 7</ref>. In this case, we treat SPR1 and SPR2 both as target tasks (separately). By including SPR1 as a target task, we are able to compare (1) SPR as a binary task and a scalar task, as well as (2) SPR1 and SPR2 as scalar tasks. These results constitute the first reported numbers on SPR2.</p><p>We observe a few trends. First, it is generally the case that properties with high F1 on the SPR1 binary task also have high Pearson correlation on the SPR1 scalar task. The higher scoring proper- ties in SPR1 scalar are also generally the higher scoring properties in SPR2 (where the SPR1 and SPR2 properties overlap), with a few notable ex- ceptions, like INSTIGATION. Overall, correlation values are lower in SPR2 than SPR1. This may be the case for a few reasons. (1) The underlying data in SPR1 and SPR2 are quite different. The former consists of sentences from the Wall Street Journal via <ref type="bibr">PropBank (Palmer et al., 2005</ref>), while the latter consists of sentences from the English Web Treebank ( <ref type="bibr">Bies et al., 2012</ref>) via the Univer- sal Dependencies; (2) certain filters were applied in the construction of the SPR1 dataset to remove instances where, e.g., predicates were embedded in a clause, possibly resulting in an easier task; (3) SPR1 labels came from a single annotator (after determining in pilot studies that annotations from this annotator correlated well with other annota- tors), where SPR2 labels came from 24 different annotators with scalar labels averaged over two- way redundancy.</p><p>Discussion With SPR1 binary classification as the target task, we see overall improvements from various multi-task training regimes (Experiments 1a-d, <ref type="table" target="#tab_7">Tables 5 and 6</ref>), using four different auxiliary tasks: machine translation into French, PropBank abstract role prediction, word sense disambigua- tion (WordNet supersenses), and SPR2. <ref type="bibr">16</ref> These auxiliary tasks exhibit a loose trade-off in terms of the quantity of available data and the seman- tic relatedness of the task: MT is the least related task with the most available (parallel) data, while SPR2 is the most related task with the smallest quantity of data. While we hypothesized that the relatedness of PropBank role labeling and word sense disambiguation tasks might lead to gains in SPR performance, we did not see substantial gains in our experiments (PB:SPR1, SPR1+WSD). We did, however, see improvements over the target- task only model (SPR1) in the cases where we added MT pretraining (MT:SPR1) or SPR2 con- current training (SPR1+2). Interestingly, combin- ing MT pretraining with SPR2 concurrent training yielded no further gains (MT:SPR1+2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Data</head><p>SPR1 The SPR1.0 ("SPR1") dataset introduced by <ref type="bibr">Reisinger et al. (2015)</ref> contains proto-role an- notations on 4,912 Wall Street Journal sentences from PropBank ( <ref type="bibr">Palmer et al., 2005</ref>) correspond- ing to 9,738 predicate-argument pairs with 18 properties each, in total 175,284 property annota- tions. All annotations were performed by a sin- gle, trusted annotator. Each annotation is a rating from 1 to 5 indicating the likelihood that the prop- erty applies, with an additional "N/A" option if the question of whether the property holds is nonsen- sical in the context.</p><p>To compare with prior work <ref type="bibr">(Teichert et al., 2017)</ref>, we treat the SPR1 data as a binary pre- diction task: the values 4 and 5 are mapped to True (property holds), while the values 1, 2, 3, and "N/A" are mapped to False (property does not hold). In additional experiments, we move to treat- ing SPR1 as a scalar prediction task; in this case, "N/A" is mapped to 1, and all other annotation val- ues remain unchanged. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>previous</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>11 We obtained the CRF dev system predictions of Teichert et al. (2017) via personal communication with the authors. 12 According to the reference, of the 1071 dev examples, 150 have physical contact and 350 have volition. The two models compared here differed in phy. contact on 62 positive and 44 negative instances and for volition on 43 positive and 54 negative instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>SPR1S</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>SPR2 The second SPR release (White et al., 2016) contains annotations on 2,758 sentences from the English Web Treebank (EWT) (Bies et al., 2012) portion of the Universal Dependen- cies (v1.2) (Silveira et al., 2014) 17 , corresponding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>SPR comparison to Teichert et al. (2017). 
Bold number indicate best F1 results in each row. 
Right-most column is pearson correlation coefi-
cient for a model trained and tested on the scalar 
regression formulation of the same data. 

ploits the benefits of large-scale pretraining, in 
the form of GloVe embeddings and MT pretrain-
ing, both absent in the CRF. Ablation experiments 
(Appendix A) show the advantages conferred by 
these features. In contrast, the discrete-featured 
CRF model makes use of gold dependency labels, 
as well as joint modeling of SPR attribute pairs 
with explicit joint factors, both absent in our neu-
ral model. Future SPRL work could explore the 
use of models like the LSTM-CRF (Lample et al., 
2016; Ma and Hovy, 2016) to combine the advan-
tages of both paradigms. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>a side-by-side comparison of 
our model with prior work. The full break-
down of F1 scores over each individual prop-
erty is provided. For every property except EX-
ISTED DURING, EXISTED AFTER, and CREATED 
we are able to exceed prior performance. For 
some properties, the absolute F1 gains are quite 
large: DESTROYED (+24.2), CHANGED POSSES-
SION (+19.2.0), CHANGED LOCATION (+10.1), 
STATIONARY (+26.0) and LOCATION (+35.3). We 
also report performance with a scalar regression 
version of the model, evaluated with Pearson cor-
relation. The scalar model is with respect to the 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Manual error analysis on a sample of in-
stances (80 for each property) where outputs of 
CRF and the binary model from Table 2 differ. 
Negative ∆ FALSE+ and ∆ FALSE-indicate the 
neural model represents a net reduction in type I 
and type II errors respectively over CRF. Posi-
tive values indicate a net increase in errors. Each 
row corresponds to one of several (overlapping) 
subsets of the 80 instances in disagreement: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Omri Abend and Ari Rappoport. 2017. The state of the art in semantic representation. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), vol- ume 1, pages 77-89.pages 758-764, Atlanta, Georgia. Associ- ation for Computational Linguistics.</head><label>Omri</label><figDesc></figDesc><table>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly 
learning to align and translate. arXiv preprint 
arXiv:1409.0473. 

Collin F Baker, Charles J Fillmore, and John B Lowe. 
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics-
Volume 1, pages 86-90. Association for Computa-
tional Linguistics. 

Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 
2012. English web treebank. Linguistic Data Con-
sortium, Philadelphia, PA. 

Claire Bonial, Julia Bonn, Kathryn Conger, Jena D. 
Hwang, and Martha Palmer. 2014. Propbank: Se-
mantics of new predicate types. In Proceedings 
of the Ninth International Conference on Language 
Resources and Evaluation (LREC'14), Reykjavik, 
Iceland. European Language Resources Association 
(ELRA). 

Chris Callison-Burch, Philipp Koehn, Christof Monz, 
and Josh Schroeder. 2009. Findings of the 2009 
Workshop on Statistical Machine Translation. In 
Proceedings of the Fourth Workshop on Statistical 
Machine Translation, pages 1-28, Athens, Greece. 
Association for Computational Linguistics. 

Hector Neri Castañeda. 1967. Comment on d. david-
sons "the logical forms of action sentences". In 
N. Rescher, editor, The Logic of Decision and Ac-
tion. University of Pittsburgh Press, Pittsburgh. 

Ronan Collobert and Jason Weston. 2008. A unified 
architecture for natural language processing: Deep 
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on 
Machine Learning, ICML '08, pages 160-167, New 
York, NY, USA. ACM. 

Donald Davidson. 1967. The logical forms of action 
sentences. In N. Rescher, editor, The Logic of De-
cision and Action. University of Pittsburgh Press, 
Pittsburgh. 

David Dowty. 1991. Thematic proto-roles and argu-
ment selection. Language, 67(3):547-619. 

Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. Bradford Books. 

Juri Ganitkevitch, Benjamin Van Durme, and Chris 
Callison-Burch. 2013. 
Ppdb: The paraphrase 
database. In Proceedings of the 2013 Conference of 

the North American Chapter of the Association for 
Computational Linguistics: Human Language Tech-
nologies, Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-
hamed. 2013. Hybrid speech recognition with deep 
bidirectional LSTM. In Automatic Speech Recogni-
tion and Understanding (ASRU), 2013 IEEE Work-
shop on, pages 273-278. IEEE. 

Kazuma Hashimoto, Yoshimasa Tsuruoka, Richard 
Socher, et al. 2017. A joint many-task model: Grow-
ing a neural network for multiple nlp tasks. In Pro-
ceedings of the 2017 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1923-
1933. 

Luheng He, Kenton Lee, Mike Lewis, and Luke Zettle-
moyer. 2017. Deep semantic role labeling: What 
works and whats next. In Proceedings of the 55th 
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages 
473-483, Vancouver, Canada. Association for Com-
putational Linguistics. 

Sepp Hochreiter and Jrgen Schmidhuber. 1997. 
Long short-term memory. Neural computation, 
9(8):1735-1780. 

Jeremy Howard and Sebastian Ruder. 2018. Universal 
language model fine-tuning for text classification. 
In Proceedings of the 56th Annual Meeting of the 
Association for Computational Linguistics (Volume 
1: Long Papers), pages 328-339, Melbourne, Aus-
tralia. Association for Computational Linguistics. 

N. Ide and J. Pustejovsky. 2017. Handbook of Linguis-
tic Annotation. Springer Netherlands. 

Edward Kako. 2006. Thematic role properties of sub-
jects and objects. Cognition, 101(1):1-42. 

Diederik P. Kingma and Jimmy Ba. 2014. Adam: 
A method for stochastic optimization. 
CoRR, 
abs/1412.6980. 

Guillaume Klein, Yoon Kim, Yuntian Deng, Jean 
Senellart, and Alexander M. Rush. 2017. Open-
NMT: Open-source toolkit for neural machine trans-
lation. In Proc. ACL. 

Sigrid Klerke, Yoav Goldberg, and Anders Søgaard. 
2016. Improving sentence compression by learning 
to predict gaze. In Proceedings of the 2016 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 1528-1533, San Diego, 
California. Association for Computational Linguis-
tics. 

John D. Lafferty, Andrew McCallum, and Fernando 
C. N. Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML '01, pages 282-289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc. 

Guillaume Lample, Miguel Ballesteros, Sandeep Sub-
ramanian, Kazuya Kawakami, and Chris Dyer. 2016. 
Neural architectures for named entity recognition. 
In Proceedings of the 2016 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
pages 260-270, San Diego, California. Association 
for Computational Linguistics. 

Minh-Thang Luong, Hieu Pham, and Christopher D. 
Manning. 2015. Effective approaches to attention-
based neural machine translation. In Proceedings of 
the 2015 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1412-1421, Lis-
bon, Portugal. Association for Computational Lin-
guistics. 

Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol 
Vinyals, and Lukasz Kaiser. 2016. Multi-task se-
quence to sequence learning. In International Con-
ference on Learning Representations. 

Xuezhe Ma and Eduard Hovy. 2016. End-to-end se-
quence labeling via bi-directional lstm-cnns-crf. In 
Proceedings of the 54th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1: 
Long Papers), pages 1064-1074, Berlin, Germany. 
Association for Computational Linguistics. 

Nitin Madnani, Jordan Boyd-Graber, and Philip 
Resnik. 2010. Measuring transitivity using un-
trained annotators. In Proceedings of the NAACL 
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazons Mechanical Turk. 

Bryan McCann, James Bradbury, Caiming Xiong, and 
Richard Socher. 2017. Learned in translation: Con-
textualized word vectors. In I. Guyon, U. V. 
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 30, pages 6294-
6305. Curran Associates, Inc. 

Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, 
Lu Zhang, and Zhi Jin. 2016. How transferable are 
neural networks in nlp applications? In Proceed-
ings of the 2016 Conference on Empirical Methods 
in Natural Language Processing, pages 479-489. 

Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics, 
31(1):71-106. 

Terence Parsons. 1995. Thematic relations and argu-
ments. Linguistic Inquiry, pages 635-662. 

Matthew Peters, Mark Neumann, Mohit Iyyer, Matt 
Gardner, Christopher Clark, Kenton Lee, and Luke 
Zettlemoyer. 2018. Deep contextualized word rep-
resentations. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long Papers), pages 

2227-2237, New Orleans, Louisiana. Association 
for Computational Linguistics. 

Adam Poliak, Yonatan Belinkov, James Glass, and 
Benjamin Van Durme. 2018. On the evaluation 
of semantic phenomena in neural machine transla-
tion using natural language inference. In Proceed-
ings of the 2018 Conference of the North American 
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 2 
(Short Papers), volume 2, pages 513-523. 

Drew Reisinger, Rachel Rudinger, Francis Ferraro, 
Craig Harman, Kyle Rawlins, and Benjamin 
Van Durme. 2015. Semantic proto-roles. Transac-
tions of the Association for Computational Linguis-
tics, 3:475-488. 

Rachel Rudinger, Aaron Steven White, and Benjamin 
Van Durme. 2018. Neural models of factuality. In 
Proceedings of the 2018 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 
New Orleans, Louisiana. Association for Computa-
tional Linguistics. 

Roser Saurí and James Pustejovsky. 2009. Factbank: 
a corpus annotated with event factuality. Language 
Resources and Evaluation, 43(3):227. 

Sebastian Schuster and Christopher D. Manning. 2016. 
Enhanced english universal dependencies: An im-
proved representation for natural language under-
standing tasks. In Proceedings of the Tenth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2016), Paris, France. European Lan-
guage Resources Association (ELRA). 

Natalia Silveira, Timothy Dozat, Marie-Catherine 
de Marneffe, Samuel Bowman, Miriam Connor, 
John Bauer, and Christopher D. Manning. 2014. A 
gold standard dependency corpus for English. In 
Proceedings of the Ninth International Conference 
on Language Resources and Evaluation (LREC-
2014). 

Adam Teichert, Adam Poliak, Benjamin Van Durme, 
and Matthew R Gormley. 2017. Semantic proto-role 
labeling. In Thirty-First AAAI Conference on Artifi-
cial Intelligence (AAAI-17). 

Aaron Steven White, Drew Reisinger, Keisuke Sak-
aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, 
Kyle Rawlins, and Benjamin Van Durme. 2016. 
Universal decompositional semantics on universal 
dependencies. In Proceedings of the 2016 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 1713-1723, Austin, Texas. Asso-
ciation for Computational Linguistics. 

Sheng Zhang, Rachel Rudinger, and Benjamin Van 
Durme. 2017. An Evaluation of PredPatt and Open 
IE via Stage 1 Semantic Role Labeling. In Proceed-
ings of the 12th International Conference on Com-
putational Semantics (IWCS). Name 
# 
Description 

LR 

Logistic Regr. model, 
Reisinger et al. (2015) 

CRF 

CRF model, 
Teichert et al. (2017) 

SPR1 

0 
SPR1 basic model 

SPR1-RAND 

0 
SPR1, random word embeddings 

MT:SPR1 

1a SPR1 after MT pretraining 

PB:SPR1 

1a SPR1 after PB pretraining 

MT:PB:SPR1 

1a SPR1 after MT+PB pretraining 

SPR1+2 

1b SPR1 and SPR2 concurrently 

SPR1+WSD 

1b SPR1 and WSD concurrently 

MT:SPR1+2 

1b SPR1+2 after MT pretraining 

MT:SPR1+WSD 

1b SPR1+WSD after MT pretraining 

MT:SPR1S 

1c SPR1 scalar after MT pretraining 

PB:SPR1S 

1c SPR1 scalar after PB pretraining 

PS-MS 

1d SPR1 propty-specific model sel. 

SPR2 

3 
SPR2 basic scalar model 

MT:SPR2 

3 
SPR2 after MT pretraining 

PB:SPR2 

3 
SPR2 after PB pretraining 

MT:PB:SPR2 

3 
SPR2 after MT+PB pretraining 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Name and short description of each ex-
perimental condition reported. MT: indicates pre-
training with machine translation; PB: indicates 
pretraining with PropBank SRL. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Overall test performance for all settings 
described in Experiments 1 and 1a-d. The tar-
get task is SPR1 as binary classification. Micro-
and macro-F1 are computed over all properties. 
( ⋆ Baseline macro-F1 scores are computed from 
property-specific precision and recall values in Te-
ichert et al. (2017) and may introduce rounding er-
rors.) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Breakdown by property of binary classifi-
cation F1 on SPR1. All new results outperforming 
prior work (CRF) in bold. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>SPR1 and SPR2 as scalar prediction tasks. 
Pearson correlation between predicted and gold 
values. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>SPR1 and SPR2 as scalar prediction 
tasks. The overall performance for each experi-
mental setting is reported as the average Pearson 
correlation over all properties. Highest SPR1 and 
SPR2 results are in bold. 

Experiment 2: SPR as a scalar task In Exper-
iment 2, we trained the SPR decoder to predict 
properties as scalar instead of binary values. Per-
formance is measured by Pearson correlation and 
reported in </table></figure>

			<note place="foot" n="1"> SPRL and SPR refer to the labeling task and the underlying semantic representation, respectively.</note>

			<note place="foot" n="2"> Implementation available at https://github. com/decomp-sem/neural-sprl.</note>

			<note place="foot" n="5"> Splitting train/dev/test along Penn Treebank boundaries and casting the SPRL task as multi-label binary classification.</note>

			<note place="foot" n="6"> Observed to be state-of-the-art by Zhang et al. (2017). 7 using a modified version of OpenNMT-py (Klein et al.,</note>

			<note place="foot" n="13"> Note that this experiment does not make use of MT pretraining as was used for Table 2, to best highlight the impact of parameter sharing across attributes.</note>

			<note place="foot">DGE-1232825). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA, NSF, or the U.S. Government. 14 As we observed the same trend more clearly on the dev set, we suspect some over-fitting to the development data which was used for independently select a stopping epoch for each of the plotted points. 15 E.g., annotations do not include any questions relating to the origin or destination of an event.</note>

			<note place="foot" n="16"> Note that in some cases we treat SPR2 as an auxiliary task, and in others, the target task. 17 We exclude the SPR2 pilot data; if included, the SPR2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the JHU HLT-COE, DARPA AIDA, and NSF GRFP (Grant No.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PropBank</head><p>The PropBank project consists of predicate-argument annotations over corpora for which gold Penn TreeBank-style constituency parses are available. We use the Unified Prop- Bank release ( <ref type="bibr">Bonial et al., 2014;</ref><ref type="bibr">Ide and Pustejovsky, 2017)</ref>, which contains annotations over OntoNotes as well as the English Web TreeBank (EWT). Each predicate in each corpus is anno- tated for word sense, and each argument of each predicate is given a label such as ARG0, ARG1, etc., where the interpretation of the label is de- fined relative to the word sense. We use Prop- Bank Frames to map these sense-specific labels to 16 sense-independent labels such as PAG (proto- agent), PPT (proto-patient), etc., and then formu- late a task to predict the abstracted labels. Because our model requires knowledge of predicate and ar- gument head words, we ran the Stanford Univer- sal Dependencies converter <ref type="bibr">(Schuster and Manning, 2016</ref>) over the gold constituency parses to release contains annotations for 2,793 sentences. obtain Universal Dependency parses, which were then processed by the PredPatt framework ( <ref type="bibr">Zhang et al., 2017;</ref><ref type="bibr">White et al., 2016</ref>) to identify head words.</p><p>English-French Data The 10 9 French-English parallel corpus <ref type="bibr" target="#b0">(Callison-Burch et al., 2009</ref>) con- tains 22,520,376 French-English sentence pairs, made up of 811,203,407 French words and 668,412,817 English words. The corpus was con- structed by crawling the websites of international organizations such as the Canadian government, the European Union, and the United Nations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fr-En Corpus ; Callison-Burch</surname></persName>
		</author>
		<title level="m">Appendix A)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">this initialization resulted in raising micro-averaged F1 from 82.2 to 83.3 9 More recent discoveries on the usefulness of language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">300</biblScope>
		</imprint>
	</monogr>
	<note>Howard and Ruder, 2018) for RNN encoders suggest a promising direction for future SPRL experiments. uncased; glove.42B.300d from https://nlp.stanford.edu/projects/glove/</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">533 out-of-vocabulary words across all datasets were assigned a random embedding</title>
	</analytic>
	<monogr>
		<title level="m">Embeddings remained fixed during training</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>uniformly from [−.01, .01</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
