<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:07+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying the Effects of Text Duplication on Semantic Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Schofield</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
							<email>mimno@cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantifying the Effects of Text Duplication on Semantic Models</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2737" to="2747"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Duplicate documents are a pervasive problem in text datasets and can have a strong effect on unsupervised models. Methods to remove duplicate texts are typically heuristic or very expensive, so it is vital to know when and why they are needed. We measure the sensitivity of two latent semantic methods to the presence of different levels of document repetition. By artificially creating different forms of duplicate text we confirm several hypotheses about how repeated text impacts models. While a small amount of duplication is tolerable , substantial over-representation of subsets of the text may overwhelm meaningful topical patterns.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Different discussions of the same subject tend to use similar words. Unsupervised models such as latent semantic analysis (LSA) <ref type="bibr" target="#b11">(Deerwester et al., 1990</ref>) and latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b4">Blei et al., 2003</ref>) look for these statistical signatures of topicality in the form of repeated word co- occurrences. These methods have become increas- ingly popular because they are powerful and easy to apply to large unlabeled datasets. The appar- ent ease-of-use of LSA and LDA, however, makes it easy to overlook potential problems in text cor- pora. In this work, we focus on measuring the im- pact of one such issue: duplicate text.</p><p>Latent semantic methods look for patterns of repetition. But when text is repeated exactly, sta- tistical methods that look for patterns may be di- verted from more meaningful semantic groups: verbatim repetition looks, to the algorithm, more topical than actual topics. If not accounted for, re- peated text can change measures of fitness to over- value fit on repeated texts, or even "leak" held out data that is duplicated in the training data. At best, duplication may cause us to overestimate the ex- pressiveness and reliability of models. At worst, models skewed by text duplication may invalidate any conclusions drawn from them, and, by exten- sion, the method itself.</p><p>Text replication is a persistent and difficult problem in natural language corpora. In social media settings, partial duplication due to quota- tion and threading is ubiquitous. Of the 20k posts in the 20 Newsgroups corpus <ref type="bibr" target="#b16">(Lang, 1995)</ref>, 1151 are exact duplicates, and 25% of the remaining to- kens are quoted text from other newsgroup mes- sages. <ref type="bibr">1</ref> In literary corpora, different versions of the same document may also conflict: text files for Hamlet may differ slightly due to publisher information, line numbers, editorial changes be- tween Shakespeare's folios, and footnotes. Re- moving exactly identical duplicates of texts is pos- sible through direct lexicographic matching, but for lexical near-duplicates and partial textual over- lap, we may need more careful heuristics to detect duplicates, forcing researchers to make judgments about what text to remove and what to keep.</p><p>Evaluating what level of duplication is "safe" can therefore not only reduce the risk of false conclusions but also save great amounts of work spent identifying and removing duplication. In this work, we investigate the effect of text dupli- cation on LSA and LDA by experimentally am- plifying the magnitude of text duplication in a variety of corpora. We look both at how mod- els shift to over-represent repeated text and how that shift affects the model representation of doc- uments without repetition. To account for the va- riety of types of duplication, we look at exact du-plication of whole documents as well as repetition of a text segment across many documents. Finally, we recommend what aspects of text deduplication one should focus on to successfully reduce nega- tive effects, with different suggestions depending on the chosen model. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Text duplication and reuse is a well-established problem in textual corpora. The web is filled with pages of near-duplicate content ( <ref type="bibr" target="#b5">Broder et al., 1997;</ref><ref type="bibr" target="#b20">Manku et al., 2007)</ref>, journalistic reuse is common practice with the dissemination of information from news agencies to newspapers <ref type="bibr" target="#b9">(Clough et al., 2002;</ref><ref type="bibr" target="#b29">Smith et al., 2013)</ref>, and plagiarism is prevalent in student submissions ( <ref type="bibr" target="#b10">Clough et al., 2003)</ref>. However, past work has fo- cused on the identification of reuse instead of the effects that duplication has on semantic models.</p><p>The detection of text reuse relies on the abil- ity to measure similarity between documents or passages. In general, these techniques measure the similarity of textual content, though other sim- ilarity metrics for reuse identification have been proposed <ref type="bibr">(Bär et al., 2012</ref>). These measures can fall into two general groups: global and local. Global techniques measure the similarities of en- tire texts. These techniques are especially used for near-duplicate detection. A common approach of this form is fingerprinting <ref type="bibr" target="#b27">(Potthast and Stein, 2008)</ref>. This method involves transforming a doc- ument into a smaller representation (e.g. a set of n-grams) to measure similarity cheaply. Local techniques measure similarity at a finer granular- ity (e.g. paragraphs or sentences). In this setting, reuse may be mixed with text derived from other sources. These techniques often involve two steps: one aligning texts with some method <ref type="bibr" target="#b18">(Lee, 2007;</ref><ref type="bibr" target="#b29">Smith et al., 2013</ref>)m and one scoring similarity of aligned sequences, e.g. based on cosine similarity of the bag-of-words vector. All of these techniques require choices of hyperparameters such as simi- larity threshold and n-gram size that affect what the technique considers duplicate text. Our work focuses on understanding what types of document deduplication are important so that practitioners can make better-informed choices about how to calibrate these models.</p><p>While it is possible to evaluate semantic models as features in a downstream supervised task, they are harder to evaluate intrinsically as unsupervised models of data exploration. For LDA, it is stan- dard to consider held-out likelihood of a test set as a measure of model fit ( <ref type="bibr" target="#b31">Wallach et al., 2009b)</ref>. One can also use human evaluations to judge the interpretability of topic summaries <ref type="bibr" target="#b7">(Chang et al., 2009)</ref>, though this measure can also be approxi- mated with automated evaluations based on corpus statistics <ref type="bibr" target="#b0">(Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b17">Lau et al., 2014;</ref>). One can also evaluate individual topics based on how much they diverge from corpus-wide distributional expectations <ref type="bibr" target="#b1">(AlSumait et al., 2009)</ref>.</p><p>Because LSA does not yield semantically meaningful dimensions, intrinsic approaches to evaluation are focused on the spatial aspects of the model's word embedding into the real do- main. Word similarity tasks are perhaps the most common evaluation, which compare human "gold standard" judgments of word pair similarity to dis- tances between the corresponding word vectors ( <ref type="bibr" target="#b13">Finkelstein et al., 2001;</ref><ref type="bibr" target="#b6">Bruni et al., 2012;</ref><ref type="bibr" target="#b15">Hill et al., 2016)</ref>. However, the vagueness of defi- nitions of "similarity" and the contextual depen- dency of similarity have cast doubt on these as gold standards of evaluation <ref type="bibr" target="#b12">(Faruqui et al., 2016)</ref>. Solving word analogies using vector arithmetic is also sometimes used to evaluate neural word em- beddings, but LSA does not tend to produce this structure well ( <ref type="bibr" target="#b26">Pennington et al., 2014;</ref><ref type="bibr" target="#b22">Mikolov et al., 2013</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theorized Impact</head><p>The fundamental problem with repeated text in a distributional semantic model is the over- representation of specific word co-occurrences to a model. To understand this, we consider the ma- trix factorization representation of these models. Borrowing notation from <ref type="bibr" target="#b2">Arora et al. (2013)</ref>, we consider a corpus with M documents and vocab- ulary size V over which we want to learn a K- dimensional representation of each document and vocabulary term. We can build an M × V matrix C to represent our corpus, where C di is a function of the frequency of term i in document d. Both LSA and LDA represent factorizations of this ma- trix into two rank-K matrices, C = W A T , where W is an M × K matrix and A is a V × K ma- trix. In the case of LSA, we apply tf-idf weighting to C before producing a truncated singular value decomposition C = U Σ A T , with Σ a diagonal matrix of dimension K ×K, and U and A column- orthogonal matrices. We can reduce this to the fac- torization above by multiplying Σ with one of the two outer matrix factors, e.g. W = U Σ . LDA performs a non-negative matrix factorization on a smoothed stochastic version of C, producing row- stochastic matrices W and A.</p><p>Duplicate text implies that more rows in C will contain a particular signature of word frequencies. This implies that a low-rank matrix factorization will increasingly devote representative power to this particular textual signature in order to mini- mize loss in its representation. We expect to ob- serve two principal effects:</p><p>• As text is repeated more, to optimize model fit on the data, one or more topics/dimensions will converge to model the repeated text.</p><p>• Text that is not exactly or near-exactly re- peated (or singular text) will be modeled less effectively both in terms of model fit and in- terpretability.</p><p>These effects are based on the incentive of the model to overfit repeated text: topics and dimen- sions modeling solely the repeated text will leave less representational power for the remaining text, and combinations of repeated and singular text will likely yield less coherent topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Methods</head><p>We quantitatively examine several aspects of mod- els with varying forms and degrees of duplica- tion to determine the magnitude of the change produced by repeated text. It is important to note that our goal is simply to measure the dif- ference between models, and not to make norma- tive statements about the quality of topics. Indeed, many measures of topic quality such as word intru- sion ( <ref type="bibr" target="#b7">Chang et al., 2009</ref>) and word co-occurrence ( <ref type="bibr" target="#b25">Newman et al., 2010;</ref><ref type="bibr" target="#b17">Lau et al., 2014</ref>) may improve as a result of degener- ate, single-document topics: most documents are internally coherent, so a single document's word distribution may appear to be a sensible topic.</p><p>Loss The first aspect is model loss. As stated in Section 3, as a segment of text is repeated more, we anticipate that the fit over documents containing repeated text will improve, while the fit over documents not containing repeated text will worsen. To evaluate this for LSA, we exam- ine the Frobenius norm of the difference between the reconstruction W A T and C for the rows corre- sponding to documents with and without repeated text. For LDA, we estimate the perplexity of both the training data and held-out data without repeti- tions from the same corpus.</p><p>Concentration Secondly, we examine compo- nent (e.g. topic/dimension) concentration. Rep- etition of a document amplifies the co-occurrence between the terms contained in the document. As this signal grows stronger, we expect models to begin "memorizing" these words. We anticipate that affected models will develop a simpler la- tent representation for the repeated document, one concentrated over a small number of components. For example, if a model is devoting topic k to a repeated document, then instances of that docu- ment should have a high proportion of topic k.</p><p>Concentration measurements relate to loss, but fo- cuses specifically on the document-component or document-topic patterns, while loss also includes information about the topic-word dynamics.</p><p>The effect of components converging to a sin- gle piece of repeated text should be easily ob- served by examining how close topics are to the unigram language model induced by the repeated text. If we repeat multiple documents indepen- dently, however, we may also expect to see dis- tinct components correlated with disjoint subsets of the repeated texts. To account for this, we eval- uate component concentration separately for docu- ments with repeated text and without repeated text.</p><p>For LDA, we examine the entropy of document vectors. Information entropy represents the expec- tation of the representation length of a given out- come as a function of the probability distribution over outcomes:</p><formula xml:id="formula_0">E d = � k θ dk log θ dk</formula><p>where θ dk is the probability of a token generated in document d having topic k. Entropy is inverse to concentration: the entropy of text should lower as the text is repeated more, as all of their topi- cal mass would be concentrated in topics converg- ing to modeling duplicate texts. Conversely, doc- uments not containing repeated text may also have their entropy increase as text repetition increases, as topics will less adequately fit to the behavior of the singular documents.</p><p>In LSA, entropy is not as directly applicable: vectors in W = U Σ can be arbitrarily real- numbered. However, we still want to access a sim- ilar basic concept, the amount a vector representa- tion of a document is concentrated in a few dimen- sions. So, we examine the absolute dispersion of each row vector d in W :</p><formula xml:id="formula_1">D d = � k |d k | �d� 1 log |d k | �d� 1</formula><p>where d k is the kth component of d. Abso- lute dispersion measures the entropy of the L1- normalized masses of the vectors in W .</p><p>Expressivity The final aspect is expressivity of topics. If one topic converges to the unigram lan- guage model of repeated documents, the result- ing model has effectively lost one topic worth of expressive power by focusing on overly-specific themes. Someone looking to learn generalized se- mantic corpus patterns from a topic model will therefore have one fewer topic of interest avail- able. The frequency of terms in the repeated text may also overwhelm the most probable terms in many of the topics, again reducing the ability to interpret these topics or to understand their con- tent through a summarized representation. While expressivity in the form of topic summaries makes little sense for LSA, using LDA models, we may examine topic summaries, obtained as the top s most probable terms of a topic where s is a fixed parameter. We may select the same number of terms s from the most probable in a unigram lan- guage model of the repeated text, and determine what proportion of the tokens obtained from con- catenating topic similarities are the top terms of the repeated text language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Data We use two corpora: a sample of arti- cles from the New York Times Annotated Cor- pus <ref type="bibr" target="#b28">(Sandhaus, 2008)</ref> and a collection of Reuters newswires from the Spanish Language News Text Corpus (REUSL) <ref type="bibr" target="#b14">(Graff, 1995)</ref>. We choose news corpora because they provide well-curated text with repeated subjects but few exact document- level duplicates, though quotes and templated text may still cause text duplication. We can use these as a testbed for general duplication behaviors we see across a variety of corpora. Text is lower-cased and tokenized to only include tokens of three or more characters, allowing for contractions or hy- phenations as single tokens. New York Times arti- cles average 494.5 words in length, while Reuters newswires average 201.5 words.</p><p>To ensure our experiments are the only cause of exact duplication of text in our corpora, we use strict methods of text deduplication. When two or more documents have more than 70% uni- gram overlap, we remove all but the longest doc- ument. In addition, we delete 7-grams that ap- pear in more than 10 documents based upon ex- isting thresholds for plagiarism detection <ref type="bibr" target="#b8">(Citron and Ginsparg, 2015)</ref>. To account for stopwords, we remove all terms appearing in more than 80% of documents. Finally, we remove documents with fewer than 7 tokens after processing. We perform this process on a random sample of 30,000 docu- ments from each corpus to ensure we may obtain a sample of 25,000 curated documents for each of our two corpora. We also produce 10% samples of these corpora, containing 2,500 documents each, to measure the effect of corpus size.</p><p>Text Duplication Treatments We use our dedu- plicated news corpora to construct datasets with artificial text duplication. We examine two differ- ent duplication scenarios: exact document dupli- cation and template string duplication.</p><p>In exact document duplication, we randomly sample p% of the documents in the dataset and in- clude c copies of each sampled document in our final corpus along with one copy each of the re- maining documents, which we refer to as singu- lar documents. To test the extremes of this effect, we also perform single document tests for large c with only one repeated document. From these syn- thetically duplicative corpora, we can determine whether effects are triggered by the sheer volume of duplicated text or if they are influenced by the diversity of the copied documents.</p><p>In template string duplication, rather than du- plicating the sampled p% of documents, we prepend a fixed string to each document in the p% sample, producing what we refer to as templated documents or texts. As repeated text may be lex- ically similar or different from the non-repeated text of the corpus, we consider two different types of prepended string. The first is a randomly- sampled document from the deduplicated corpus but not included in the training set (Sampled Tem- plate), simulating repeated text that is lexically similar to the document content. The second is  <ref type="figure">Figure 1</ref>: Training perplexity with LDA models trained on the REUSL 25k corpus with 80 top- ics. Perplexity decreases significantly for the du- plicated documents with repetition, but the effect on singular documents is negligible with repeated proportion of the corpus smaller than 0.1.  Training We analyze two types of semantic models: LSA and LDA. LSA models are trained using tf-idf weighting on word-document matri- ces using custom Python code. 3 LDA models are trained using Mallet <ref type="bibr" target="#b21">(McCallum, 2002</ref>) with fixed hyperparameters α = 50/K and β = 0.01 for ease of comparison. To compute perplexity, we use log likelihood estimates from Mallet's built-in left-to-right estimation ( <ref type="bibr" target="#b30">Wallach et al., 2009a</ref>).</p><formula xml:id="formula_2">� � � � � � � � � � � � � � � � � � � � � � �������� ���� ���� ���� ���� ���� ���� ���� ���� ���������� � � � � � � � � � � � � � � � � � � � � �</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Because of the exponential combination of differ- ent experimental settings available, it would be un- feasible to examine all our metrics for all data. In- stead, we focus our analysis on specific examples <ref type="bibr">3</ref> Code uses scipy, numpy, and scikit-learn.  that we believe demonstrate the effects seen in the rest of the corpus. We use smaller sets of 2.5k documents for examining the effect of heavy du- plication and sets of 25k documents otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Loss</head><p>We begin with the case of exact document dupli- cation. In <ref type="figure">Figure 1</ref>, the perplexity of LDA de- creases substantially as documents are duplicated. This reduction is due to better fit to the dupli- cated documents. As fit improves in duplicated documents, however, we do not see a meaning- fully worse fit for singular documents. These doc- uments increase in perplexity, but the increase is not significant at low levels of duplication, such as when c = 2 or p = 0.001. In the single document case in <ref type="figure" target="#fig_0">Figure 2</ref>, this effect is emphasized: like- lihood on singular documents remains level even with heavy repetition in short corpora. The sheer volume of duplicated text does not by itself dam- age model fit, likely because the duplicated text can be easily modeled by a single topic. This effect is not solely due to LDA's spe- cific probabilistic model. We see a similar pat- tern in LSA. In <ref type="figure" target="#fig_1">Figure 3</ref>, we see that loss for duplicated documents decreases as duplication in- creases. However, the amount of decay depends on the proportion of the corpus replicated: the smaller the proportion size, the more dramatic the decay. In contrast, the loss for singular documents increases only slightly with more copies, though more for higher proportions of duplication.</p><p>To  <ref type="figure">Figure 4</ref>: Held-out data perplexity (in thousands) for different the NYT 25k corpus with varying numbers of topics K. Increasing the proportion of repetition for exact duplicate documents does not increase test perplexity. With repeated corpus proportion p = 0.001, however, repeating documents exactly 4 times (but not 2 or 8 times) significantly improves perplexity, potentially because it induces a new topic to model it. Held-out data contained no repeated documents.</p><formula xml:id="formula_3">� � � � � � � � � � � � � � � � � � � � � � �������� ��� ��� ��� ��� ��� ��� ��� ��� �������������������� � � � � � � � � � � � � � � � � � � � � � � �������� ���� ���� ���� ���� ���� ���� ������������������ � � �� �� �� �� ��� ���</formula><p>Figure 5: LSA model loss for the NYT 2.5k corpus with one duplicated document. Model loss for sin- gular documents is again unaffected by repetition, while loss for the duplicated document quickly falls to zero as repetition increases. The fall in loss signals the start of model "memorization."</p><p>times. From <ref type="figure">Figure 5</ref>, we see that the loss of sin- gular documents does not meaningfully change as the number of copies increases. We can observe the steep decline in loss for duplicated documents as the signal of when the top K components be- gin to "memorize" the duplicated document. The more components K, the fewer repetitions need for the overfitting to begin.</p><p>In the LDA case, we may also look at held out perplexity. <ref type="figure">Figure 4</ref> shows that the fit for held- out test data is not generally significantly affected by increased repetition. There is a pattern within the data, in which repeating documents 4 times seems to produce better perplexity for singular documents than 2 or 8, significantly so for a small fraction of the corpus. A theory for this is that at a sufficient level of repetition, LDA fits the repeated text to its own topic instead of trying to conflate it with other document contents, producing better topics. However, additional repetition further sat- urates these topics and adds noise to the meaning- ful co-occurrence signal. <ref type="figure">Figure 6</ref> demonstrates that, as before, perplex- ity is significantly higher as template repetition in- creases when there is a small number of topics K = 5. However, as the number of topics in-  <ref type="figure">Figure 6</ref>: LDA training perplexity for REUSL 2.5k with different types of templated text repe- tition. The effect of duplication is prominent for small numbers of topics but diminishes with more topics to sufficiently model the missing text. With the fraction of the corpus that contains duplicates p = 0.1, the perplexity of template documents is below that of untemplated texts.</p><p>creases, this disparity ceases to be significant. In- terestingly, however, with high enough proportion p of documents containing templates, the perplex- ity drops below that of documents not containing the duplicates at all numbers of topics.</p><p>For LSA, templated repetition has no apparent effect on the loss of untemplated texts. How- ever, the effect for templated texts is less straight- forward. <ref type="figure">Figure 7</ref> shows that for proportions p = 0.1 and p = 0.01 the loss of templated texts is smaller than for untemplated texts for all K component sizes. For proportion p = 0.001, though, templated loss is only smaller than un- templated loss when K is large, while templated loss is never significantly lower than untemplated loss for p = 0.0001. Lorem Template and Sam- ple Template also exhibit different behaviors tem- plated texts when p = 0.001 and K is large: loss is is significantly smaller for Lorem Template and has a larger drop in loss from K = 80 to K = 160.   <ref type="figure">Figure 8</ref>: Entropy for LDA with 80 topics de- creases for duplicated documents as the frequency of those documents increases, has little initial ef- fect on the entropy of the singular documents.</p><p>This may indicate that LSA is able to more ef- fectively model templated text when the templates have a distinctively different language model than the original documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Concentration</head><p>We expect the effect of duplication on entropy will be inversely correlated with its effect on model loss. As we increase the proportion of the cor- pus that is repeated, the model will devote more resources to duplicate text, leaving less modeling power for the remaining text. We therefore ex- pect dispersion to increase with p for duplicate documents and decrease with p for singular docu- ments. In <ref type="figure">Figure 8</ref>, the first effect clearly holds for LDA, but the second does not: there is a negligible  <ref type="figure">Figure 9</ref>: Absolute dispersion for LSA with 80 components increases slightly when first produc- ing duplicates (c = 2), but falls off for smaller proportions of repetition p = 0.01 and p = 0.001 at higher frequencies. Increasing c has a compar- atively small effect on the absolute dispersion of singular documents. <ref type="figure">Figure 10</ref>: When a single document in the short corpora is repeated enough to comprise the major- ity of the corpus, the LDA entropy decreases over singular documents.</p><formula xml:id="formula_4">� � � � � � � � � � � � � � � � � � � � � � �������� ��� ��� ��� ��� ��� ��� ���������� � � � � � � � � � � � � � � � � � � � � � � �������� ��� ��� ��� ��� ��� ��� �������� � � �� �� �� �� ��� ���</formula><p>change in entropy with the number of repetitions of documents. <ref type="figure">Figure 9</ref> shows a subtler version of the same effect for LSA. Notably, the decrease in absolute dispersion for repeated documents is only visible in the short corpora. We can examine the extreme effects of the change in component concentration for singular documents by looking at its behavior in the sin- gle document treatment. In <ref type="figure">Figure 10</ref>, we see that while entropy remains level for repetitions com- prising smaller portions of the corpus, eventually the entropy drops for both repeated and singu- lar documents. This may be because most top- ics describe the repeated document, leaving few to model the remaining singular documents.</p><p>For LSA, absolute dispersion remains level for all repetitions tested for the single document treat- ment. This result highlights a key difference be- tween LDA topics and LSA components: while changing the number of topics in LDA influences  <ref type="figure">Figure 11</ref>: LDA entropy for the REUSL 25k cor- pus with Sample Template and Lorem Template treatments. With few topics, templated documents have lower entropy than untemplated documents, but with many topics, their entropy is higher. In the mid range of topics for Lorem Template, higher proportions of sampled text p produce higher en- tropy, but for Sample Template, lower p produces higher entropy. the prior to raise or lower entropy over topics, the components of LSA are fixed. Increasing the num- ber of components increases dimensionality, but never alters preexisting dimensions.</p><p>The effect is more subtle when templated text is repeated within documents. <ref type="figure">Figure 11</ref> shows that with K = 20 LDA topics, if we apply the Sample Template to a small fraction of documents (p = 0.001), it produces a higher entropy than corpora with larger template inclusion proportion p. This is not surprising: though the template text and the original document are similar in style, with high probability they will still have different top- ics, which the model will have trouble fitting well without more observations. The Lorem Template has the reverse effect: the language is sufficiently disjoint from the content of the documents that few topics or even a single topic can model the repeated text fully, leading to low entropy. When the language model of duplicated text is disjoint from that of the text of interest, the template can be modeled by one or a few topics or components without significantly affecting other text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Expressivity</head><p>Quantitative analyses of model fit and topic uncer- tainty are helpful in analyzing the effect of differ- ent settings, but do not necessarily tell us whether topics from corpora with repeated documents are useful. Analysis of expressivity helps us fill in some of the gaps in our explanations above as to what is happening at the individual topic level. In <ref type="figure" target="#fig_0">Figure 12</ref>, we see that for a moderate number of topics, increased repetition of documents impacts a substantial portion of the top-ranked words, or most probable terms of topics. The saturation ef- fect has some relation to the number of repeated documents. With a single document repeated, as in <ref type="figure" target="#fig_1">Figure 13</ref>, as the number of topics increases, the ratio of top-ranked words belonging to the un- igram language model drops. We also notice that with few topics, there is a clear "saturation point" where the topic begins to be represented more, which remains level until half the short corpora are represented by the duplicate document. The pattern overwhelmingly shows that single texts are easily fit by single topics.</p><p>In the case of the Lorem Template input, where little textual overlap exists between the template and original text, a few topics quickly fill in the repeated text, producing a limited effect on most topics. In <ref type="table">Table 1</ref>, the number of topics containing "lorem" and "ipsum" remains small as the num- ber of topics grows. Regardless of topic count of proportion, topics containing "lorem ipsum" are entirely broken Latin: the top probable terms of an example 320-topic model with p = 0.1 are est justo donec iaculis sit ipsum quam lorem tris- tique sed amet eget pharetra curabitur fringilla non consequat mattis nec nascetur, a direct sam- ple of words from the template text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The presence of duplicated strings, either docu- ments or duplicated text within documents, is a se- rious but not insurmountable problem. Duplicate text can substantially alter the dimensions learned by distributional semantic models. The effect of duplication depends on several factors: the num- ber of distinct repeated strings, the similarity of repeated strings to the rest of the corpus, and the  <ref type="figure" target="#fig_0">Figure 12</ref>: With 80-topic LDA models of our larger datasets, we see that increased repetition leads to significant increases in the amount of rep- resentation of repeated text in the top keys of top- ics.  number of repetitions. We find that different algo- rithms are affected in different ways, but that there are methods to alleviate the effect of duplication without exhaustively removing all duplicated doc- uments. We provide the following specific conclu- sions and recommendations:</p><formula xml:id="formula_5">� � � � � � � � � � � � � � � � � � � � � � �������� ��� ��� ��� ��� ��� ��� ��� ��� ��� ��� ���������� � � � � � � � � � � � � � � � � � � � � �</formula><p>LDA accommodates low rates of document du- plication for many documents. We find that with more frequent repetition, the algorithm is able to sequester repeated text into small num- bers of topics if certain conditions hold. To handle this case, the model must have many topics avail- able relative to the number of repeated strings, and the language of the repeated text must be suffi- ciently distinct. If these conditions are met, re- peated text will affect a small number of topics that can be identified by their similarity to specific documents, or automatically based on lower than expected inter-document variability within a topic (Mimno and Blei, 2011) or distance from specific corpus-word or document-word distributions <ref type="bibr" target="#b1">(AlSumait et al., 2009)</ref>. We therefore suggest train- ing a model first with slightly more topics than desired, then evaluating if there are any signs of repeated texts overwhelming several topics due to low coherence or corpus statistics. If no such in- dications occur, or if the duplication remains in one or two topics, then there is no need to modify the corpus or retrain the model, as the duplicate- capturing topic may be ignored.</p><p>LSA permits high rates of document duplica- tion so long as few unique texts are repeated.</p><p>Repeating one document will likely only affect one or a few components regardless of how many repetitions occur. However, if there are many dif- ferent repeated documents, more components will be used to model them, which worsens the model fit more as the number of unique repeated texts in- creases. In this case, it may be preferable to look for near-duplicate documents more aggressively and worry less about exact duplicates. Unigram- count-based deduplication may be appropriate in this case, using a simple threshold of cosine sim- ilarity between the vectors of unigram counts be- tween two documents to deduplicate.</p><p>Repeated text templates for LSA and LDA are sequestered by the model so long as they do not overlap heavily with topics of interest. In a topic model, it may be easy to identify the tem- plated text based upon it appearing in one topic. However, if there is a concern that there is sys- tematic use of text templates in documents (such as page headers or publication information) that may be too close to the language model, the n- gram removal approach inspired by <ref type="bibr" target="#b8">Citron and Ginsparg (2015)</ref> is an expensive but straightfor- ward way to ensure these strings are detected and deleted. The combination of unigram dedupli- cation, n-gram deletion, and the inherent ability of semantic models to separate co-occurring text should reduce the negative effects of text duplica- tion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Perplexity from duplicating a single document remains largely unaffected for singular documents until the number of repetitions is c = 4096, when duplicate texts outnumber singular texts. There is also a subtle inflection point for smaller numbers of topics K at c = 256, approximately 1/10th of the corpus, but this effect is not visible with more topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model loss for LSA models with 80 components. Loss for duplicated documents decreases as the number of repetitions c increases. The frequency of replication affects loss at a much smaller scale for singular documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>�</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Top keys of LDA topics for only a single repeated document remain concentrated in only a few topics in models with K &gt; 5, negligibly impacting the top keys of remaining topics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>gain a better understanding of how dupli- cation affects LSA, we look at the effects of re- peating a single document an extreme number of</figDesc><table>� 

� 
� 
� 

�������� 

��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 

����������������� 

� 
� 
� 
� 

�������� 

��� 
��� 
��� 
��� 
��� 
��� 
��� 

������������������ 

� 
� 
� 
� 

�������� 

��� 
��� 
��� 
��� 
��� 
��� 

������������������ 

� 
� 
� 
� 

�������� 

��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 

������������������ 

� 
� 
� 
� 

�������� 

��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 

������������������ 

���������� 

����� 
���� 
��� 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>� �� �� �� �� ������ ��� ��� ��� ��� ��� ��� ��� ��� ��� ���������������������� ����� ���� ��� � �� �� �� �� ������ ��� ��� ��� ������������������������ � �� �� �� �� ������</head><label></label><figDesc></figDesc><table>�������� 

��� 
��� 
��� 
��� 
��� 
��� 
��� 
��� 

������������������������� 

� �� �� �� �� ������ 

�������� 

��� 

��� 

��� 

��������������������������� 

</table></figure>

			<note place="foot" n="1"> Computed using scikit-learn&apos;s 20 Newsgroups API: http://scikit-learn.org/stable/ datasets/twenty_newsgroups.html</note>

			<note place="foot" n="2"> Code for our experiments can be found at https://github.com/heraldicsandfox/ semantic-text-duplication.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our reviewers for their helpful feedback and suggestions. We would also like to thank Jack Hessel, Måns Magnusson, and Ana Smith for their reviews and feedback. This material is based on work supported by the DoD, Air Force Office of Scientific Research, Na-tional Defense Science and Engineering Gradu-ate (NDSEG) Fellowship, 32 CFR 168a; National Science Foundation grants #1526155, #1652536 (CAREER), and #DGE-1144153 (GRFP); and a faculty research fellowship from the Alfred P. Sloan Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Computational Semantics</title>
		<meeting>the 10th International Conference on Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Topic significance ranking of LDA generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loulwah</forename><surname>Alsumait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Gentle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlotta</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="67" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="280" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text reuse detection using a composition of text similarity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bär</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irnya</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics</title>
		<meeting>the 24th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="167" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Syntactic clustering of the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Mark S Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks and ISDN Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8-13</biblScope>
			<biblScope unit="page" from="1157" to="1166" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Patterns of text reuse in a scientific corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Citron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ginsparg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="30" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meter: Measuring text reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gaizauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Old and new challenges in automatic plagiarism detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Clough</surname></persName>
		</author>
		<ptr target="Http://ir.shef.ac.uk/cloughie/index.html" />
	</analytic>
	<monogr>
		<title level="m">National Plagiarism Advisory Service</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">W</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">391</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Problems with evaluation of word embeddings using word similarity tasks. Proceedings of the 1st Workshop on Evaluating VectorSpace Representations for NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on the World Wide Web</title>
		<meeting>the 10th International Conference on the World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spanish news text. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gallegos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
		<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A computational model of text reuse in ancient literary texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th</title>
		<meeting>the 45th</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Detecting near-duplicates for web crawling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurmeet</forename><surname>Singh Manku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish Das</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on the World Wide Web</title>
		<meeting>the 16th International Conference on the World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MALLET: a machine learning for language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew K Mccallum</surname></persName>
		</author>
		<ptr target="http://mallet.cs.umass.edu" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bayesian checking for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="227" to="237" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the Meeting of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processin</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processin</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">New issues in near-duplicate detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Analysis, Machine Learning and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="601" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The New York Times annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Sandhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2009" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Infectious texts: Modeling text reuse in nineteenth-century newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth Maddock</forename><surname>Cordell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Big Data</title>
		<meeting>the IEEE International Conference on Big Data</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="86" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking LDA: Why priors matter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hanna M Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew K</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1973" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation methods for topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mimno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
