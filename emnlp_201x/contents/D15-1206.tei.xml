<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
							<email>chenyunchuan11@mails.ucas.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Software Institute</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichan-nel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F 1-score of 83.7%, higher than competing methods in the literature.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation classification is an important NLP task. It plays a key role in various scenarios, e.g., in- formation extraction <ref type="bibr" target="#b26">(Wu and Weld, 2010)</ref>, ques- tion answering <ref type="bibr" target="#b28">(Yao and Van Durme, 2014</ref>), med- ical informatics ( <ref type="bibr" target="#b24">Wang and Fan, 2014</ref>), ontol- ogy learning ( <ref type="bibr" target="#b27">Xu et al., 2014</ref>), etc. The aim of relation classification is to categorize into pre- defined classes the relations between pairs of marked entities in given texts. For instance, in the sentence "A trillion gallons of <ref type="bibr">[water]</ref> e 1 have been poured into an empty <ref type="bibr">[region]</ref> e 2 of outer * Corresponding authors. space," the entities water and region are of rela- tion Entity-Destination(e 1 , e 2 ).</p><p>Traditional relation classification approaches rely largely on feature representation <ref type="bibr" target="#b15">(Kambhatla, 2004)</ref>, or kernel design ( <ref type="bibr" target="#b32">Zelenko et al., 2003;</ref><ref type="bibr" target="#b2">Bunescu and Mooney, 2005</ref>). The former method usually incorporates a large set of features; it is difficult to improve the model performance if the feature set is not very well chosen. The latter ap- proach, on the other hand, depends largely on the designed kernel, which summarizes all data infor- mation. Deep neural networks, emerging recently, provide a way of highly automatic feature learning ( <ref type="bibr" target="#b1">Bengio et al., 2013)</ref>, and have exhibited consid- erable potential ( <ref type="bibr" target="#b33">Zeng et al., 2014;</ref><ref type="bibr" target="#b7">dos Santos et al., 2015)</ref>. However, human engineering-that is, incorporating human knowledge to the network's architecture-is still important and beneficial.</p><p>This paper proposes a new neural network, SDP-LSTM, for relation classification. Our model utilizes the shortest dependency path (SDP) be- tween two entities in a sentence; we also design a long short term memory (LSTM)-based recurrent neural network for information processing. The neural architecture is mainly inspired by the fol- lowing observations.</p><p>• Shortest dependency paths are informative ( <ref type="bibr" target="#b9">Fundel et al., 2007;</ref><ref type="bibr" target="#b4">Chen et al., 2014</ref>). To determine the two entities' relation, we find it mostly sufficient to use only the words along the SDP: they concentrate on most relevant information while diminishing less relevant noise. <ref type="figure" target="#fig_0">Figure 1</ref> depicts the dependency parse tree of the aforementioned sentence. Words along the SDP form a trimmed phrase (gal- lons of water poured into region) of the orig- inal sentence, which conveys much informa- tion about the target relation. Other words, such as a, trillion, outer space, are less infor- mative and may bring noise if not dealt with properly.</p><p>• Direction matters. Dependency trees are a kind of directed graph. The dependency re- lation between into and region is PREP; such relation hardly makes any sense if the di- rected edge is reversed. Moreover, the enti- ties' relation distinguishes its directionality, that is, r(a, b) differs from r(b, a), for a same given relation r and two entities a, b. There- fore, we think it necessary to let the neu- ral model process information in a direction- sensitive manner. Out of this consideration, we separate an SDP into two sub-paths, each from an entity to the common ancestor node. The extracted features along the two sub- paths are concatenated to make final classi- fication.</p><p>• Linguistic information helps. For exam- ple, with prior knowledge of hyponymy, we know "water is a kind of substance." This is a hint that the entities, water and region, are more of Entity-Destination rela- tion than, say, Communication-Topic.</p><p>To gather heterogeneous information along SDP, we design a multichannel recurrent neu- ral network. It makes use of information from various sources, including words them- selves, POS tags, WordNet hypernyms, and the grammatical relations between governing words and their children.</p><p>For effective information propagation and inte- gration, our model leverages LSTM units during recurrent propagation. We also customize a new dropout strategy for our SDP-LSTM network to alleviate the problem of overfitting. To the best of our knowledge, we are the first to use LSTM- based recurrent neural networks for the relation classification task.</p><p>We evaluate our proposed method on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than com- peting methods in the literature.</p><p>In the rest of this paper, we review related work in Section 2. In Section 3, we describe our SDP- LSTM model in detail. Section 4 presents quan- titative experimental results. Finally, we have our conclusion in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation classification is a widely studied task in the NLP community. Various existing meth- ods mainly fall into three classes: feature-based, kernel-based, and neural network-based.</p><p>In feature-based approaches, different sets of features are extracted and fed to a chosen classifier (e.g., logistic regression). Generally, three types of features are often used. Lexical features concen- trate on the entities of interest, e.g., entities per se, entity POS, entity neighboring information. Syn- tactic features include chunking, parse trees, etc. Semantic features are exemplified by the concept hierarchy, entity class, entity mention. <ref type="bibr" target="#b15">Kambhatla (2004)</ref> uses a maximum entropy model to combine these features for relation classification. However, different sets of handcrafted features are largely complementary to each other (e.g., hyper- nyms versus named-entity tags), and thus it is hard to improve performance in this way ( <ref type="bibr" target="#b10">GuoDong et al., 2005</ref>).</p><p>Kernel-based approaches specify some measure of similarity between two data samples, with- out explicit feature representation. <ref type="bibr" target="#b32">Zelenko et al. (2003)</ref> compute the similarity of two trees by utilizing their common subtrees. <ref type="bibr" target="#b2">Bunescu and Mooney (2005)</ref> propose a shortest path depen- dency kernel for relation classification. Its main idea is that the relation strongly relies on the de- pendency path between two given entities. <ref type="bibr" target="#b25">Wang (2008)</ref> provides a systematic analysis of several kernels and show that relation extraction can bene-fit from combining convolution kernel and syntac- tic features. <ref type="bibr" target="#b20">Plank and Moschitti (2013)</ref> introduce semantic information into kernel methods in ad- dition to considering structural information only. One potential difficulty of kernel methods is that all data information is completely summarized by the kernel function (similarity measure), and thus designing an effective kernel becomes crucial.</p><p>Deep neural networks, emerging recently, can learn underlying features automatically, and have attracted growing interest in the literature. <ref type="bibr" target="#b21">Socher et al. (2011)</ref> propose a recursive neural network (RNN) along sentences' parse trees for sentiment analysis; such model can also be used to clas- sify relations <ref type="bibr" target="#b22">(Socher et al., 2012)</ref>. <ref type="bibr" target="#b11">Hashimoto et al. (2013)</ref> explicitly weight phrases' importance in RNNs to improve performance. <ref type="bibr" target="#b8">Ebrahimi and Dou (2015)</ref> rebuild an RNN on the dependency path between two marked entities. <ref type="bibr" target="#b33">Zeng et al. (2014)</ref> explore convolutional neural networks, by which they utilize sequential information of sen- tences. dos <ref type="bibr" target="#b7">Santos et al. (2015)</ref> also use the convo- lutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in <ref type="bibr">SemEval-2010 Task 8.</ref> In addition to the above studies, which mainly focus on relation classification approaches and models, other related research trends include in- formation extraction from Web documents in a semi-supervised manner ( <ref type="bibr" target="#b3">Bunescu and Mooney, 2007;</ref><ref type="bibr" target="#b0">Banko et al., 2007)</ref>, dealing with small datasets without enough labels by distant super- vision techniques <ref type="bibr" target="#b18">(Mintz et al., 2009)</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed SDP-LSTM Model</head><p>In this section, we describe our SDP-LSTM model in detail. Subsection 3.1 delineates the overall ar- chitecture of our model. Subsection 3.2 presents the rationale of using SDPs. Four different infor- mation channels along the SDP are explained in Subsection 3.3. Subsection 3.4 introduces the re- current neural network with long short term mem- ory, which is built upon the dependency path. Sub- section 3.5 customizes a dropout strategy for our network to alleviate overfitting. We finally present our training objective in Subsection 3.6. First, a sentence is parsed to a dependency tree by the Stanford parser; 1 the shortest dependency path (SDP) is extracted as the input of our net- work. Along the SDP, four different types of information-referred to as channels-are used, including the words, POS tags, grammatical rela- tions, and WordNet hypernyms. (See <ref type="figure" target="#fig_1">Figure 2a.</ref>) In each channel, discrete inputs, e.g., words, are mapped to real-valued vectors, called embeddings, which capture the underlying meanings of the in- puts. Two recurrent neural networks ( <ref type="figure" target="#fig_1">Figure 2b</ref>) pick up information along the left and right sub-paths of the SDP, respecitvely. (The path is separated by the common ancestor node of two entities.) Long short term memory (LSTM) units are used in the recurrent networks for effective information prop- agation. A max pooling layer thereafter gathers information from LSTM nodes in each path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The pooling layers from different channels are concatenated, and then connected to a hidden layer. Finally, we have a softmax output layer for classification. (See again <ref type="figure" target="#fig_1">Figure 2a</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Shortest Dependency Path</head><p>The dependency parse tree is naturally suitable for relation classification because it focuses on the ac- tion and agents in a sentence ( <ref type="bibr" target="#b23">Socher et al., 2014</ref>). Moreover, the shortest path between entities, as discussed in Section 1, condenses most illuminat- ing information for entities' relation.</p><p>We also observe that the sub-paths, separated by the common ancestor node of two entities, provide strong hints for the relation's directionality. Take <ref type="figure" target="#fig_0">Figure 1</ref> as an example. Two entities water and region have their common ancestor node, poured, which separates the SDP into two parts:</p><formula xml:id="formula_0">[water] e 1 → of → gallons → poured and poured ← into ← [region] e 2</formula><p>The first sub-path captures information of e 1 , whereas the second sub-path is mainly about e 2 . By examining the two sub-paths sepa- rately, we know e 1 and e 2 are of relation Entity-Destination(e 1 , e 2 ), rather than Entity-Destination(e 2 , e 1 ).</p><p>Following the above intuition, we design two recurrent neural networks, which propagate bottom-up from the entities to their common an- cestor. In this way, our model is direction- sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Channels</head><p>We make use of four types of information along the SDP for relation classification. We call them channels as these information sources do not inter- act during recurrent propagation. Detailed channel descriptions are as follows.</p><p>• Word representations. Each word in a given sentence is mapped to a real-valued vector by looking up in a word embedding table. Un- supervisedly trained on a large corpus, word embeddings are thought to be able to well capture words' syntactic and semantic infor- mation ( <ref type="bibr" target="#b17">Mikolov et al., 2013b</ref>).</p><p>• Part-of-speech tags. Since word embed- dings are obtained on a generic corpus of a large scale, the information they contain may not agree with a specific sentence. We deal with this problem by allying each input word with its POS tag, e.g., noun, verb, etc.</p><p>In our experiment, we only take into use a coarse-grained POS category, containing 15 different tags.</p><p>• Grammatical relations. The dependency relations between a governing word and its children makes a difference in meaning. A same word pair may have different depen- dency relation types. For example, "beats nsubj − −− → it" is distinct from "beats dobj − −− → it." Thus, it is necessary to capture such gram- matical relations in SDPs. In our experi- ment, grammatical relations are grouped into 19 classes, mainly based on a coarse-grained classification <ref type="bibr" target="#b6">(De Marneffe et al., 2006</ref>).</p><p>• WordNet hypernyms. As illustrated in Sec- tion 1, hyponymy information is also useful for relation classification. (Details are not re- peated here.) To leverage WordNet hyper- nyms, we use a tool developed by <ref type="bibr" target="#b5">Ciaramita and Altun (2006)</ref>. <ref type="bibr">2</ref> The tool assigns a hy- pernym to each word, from 41 predefined concepts in WordNet, e.g., noun.food, verb.motion, etc. Given its hypernym, each word gains a more abstract concept, which helps to build a linkage between dif- ferent but conceptual similar words.</p><p>As we can see, POS tags, grammatical rela- tions, and WordNet hypernyms are also discrete (like words per se). However, no prevailing em- bedding learning method exists for POS tags, say. Hence, we randomly initialize their embeddings, and tune them in a supervised fashion during train- ing. We notice that these information sources con- tain much fewer symbols, 15, 19, and 41, than the vocabulary size (greater than 25,000). Hence, we believe our strategy of random initialization is fea- sible, because they can be adequately tuned during supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recurrent Neural Network with Long Short Term Memory Units</head><p>The recurrent neural network is suitable for mod- eling sequential data by nature, as it keeps a hid-  den state vector h, which changes with input data at each step accordingly. We use the recurrent net- work to gather information along each sub-path in the SDP <ref type="figure" target="#fig_1">(Figure 2b</ref>).</p><p>The hidden state h t , for the t-th word in the sub-path, is a function of its previous state h t−1 and the current word x t . Traditional recurrent net- works have a basic interaction, that is, the input is linearly transformed by a weight matrix and non- linearly squashed by an activation function. For- mally, we have</p><formula xml:id="formula_1">h t = f (W in x t + W rec h t−1 + b h )</formula><p>where W in and W rec are weight matrices for the input and recurrent connections, respectively. b h is a bias term for the hidden state vector, and f h a non-linear activation function (e.g., tanh).</p><p>One problem of the above model is known as gradient vanishing or exploding. The train- ing of neural networks requires gradient back- propagation. If the propagation sequence (path) is too long, the gradient may probably either grow, or decay, exponentially, depending on the magnitude of W rec . This leads to the difficulty of training.</p><p>Long short term memory (LSTM) units are pro- posed in <ref type="bibr" target="#b14">Hochreiter (1998)</ref> to overcome this prob- lem. The main idea is to introduce an adaptive gat- ing mechanism, which decides the degree to which LSTM units keep the previous state and memo- rize the extracted features of the current data in- put. Many LSTM variants have been proposed in the literature. We adopt in our method a variant introduced by <ref type="bibr" target="#b31">Zaremba and Sutskever (2014)</ref>, also used in <ref type="bibr" target="#b34">Zhu et al. (2015)</ref>.</p><p>Concretely, the LSTM-based recurrent neural network comprises four components: an input gate i t , a forget gate f t , an output gate o t , and a mem- ory cell c t (depicted in <ref type="figure" target="#fig_4">Figure 3</ref> and formalized through Equations 1-6 as bellow).</p><p>The three adaptive gates i t , f t , and o t depend on the previous state h t−1 and the current input x t <ref type="figure" target="#fig_0">(Equations 1-3</ref>). An extracted feature vector g t is also computed, by Equation 4, serving as the candidate memory cell.</p><formula xml:id="formula_2">i t = σ(W i ·x t + U i ·h t−1 + b i )<label>(1)</label></formula><formula xml:id="formula_3">f t = σ(W f ·x t + U f ·h t−1 + b f )<label>(2)</label></formula><formula xml:id="formula_4">o t = σ(W o ·x t + U o ·h t−1 + b o )<label>(3)</label></formula><formula xml:id="formula_5">g t = tanh(W g ·x t + U g ·h t−1 + b g ) (4)</formula><p>The current memory cell c t is a combination of the previous cell content c t−1 and the candidate content g t , weighted by the input gate i t and forget gate f t , respectively. (See Equation 5 below.)</p><formula xml:id="formula_6">c t = i t ⊗ g t + f t ⊗ c t−1<label>(5)</label></formula><p>The output of LSTM units is the the recur- rent network's hidden state, which is computed by Equation 6 as follows.</p><formula xml:id="formula_7">h t = o t ⊗ tanh(c t )<label>(6)</label></formula><p>In the above equations, σ denotes a sigmoid function; ⊗ denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Dropout Strategies</head><p>A good regularization approach is needed to al- leviate overfitting. Dropout, proposed recently by <ref type="bibr" target="#b13">Hinton et al. (2012)</ref>, has been very successful on feed-forward networks. By randomly omitting feature detectors from the network during train- ing, it can obtain less interdependent network units and achieve better performance. However, the conventional dropout does not work well with re- current neural networks with LSTM units, since dropout may hurt the valuable memorization abil- ity of memory units.</p><p>As there is no consensus on how to drop out LSTM units in the literature, we try several dropout strategies for our SDP-LSTM network:</p><p>• Dropout embeddings;</p><p>• Dropout inner cells in memory units, includ- ing i t , g t , o t , c t , and h t ; and</p><p>• Dropout the penultimate layer.</p><p>As we shall see in Section 4.2, dropping out LSTM units turns out to be inimical to our model, whereas the other two strategies boost in perfor- mance.</p><p>The following equations formalize the dropout operations on the embedding layers, where D de- notes the dropout operator. Each dimension in the embedding vector, x t , is set to zero with a prede- fined dropout rate.</p><formula xml:id="formula_8">i t = σ(W i ·D(x t ) + U i ·h t−1 + b i )<label>(7)</label></formula><formula xml:id="formula_9">f t = σ(W f ·D(x t ) + U f ·h t−1 + b f )<label>(8)</label></formula><formula xml:id="formula_10">o t = σ(W o ·D(x t ) + U o ·h t−1 + b o )<label>(9)</label></formula><formula xml:id="formula_11">g t = tanh W g ·D(x t ) + U g ·h t−1 + b g<label>(10)</label></formula><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.6 Training Objective</head><p>The SDP-LSTM described above propagates in- formation along a sub-path from an entity to the common ancestor node (of the two entities). A max pooling layer packs, for each sub-path, the recurrent network's states, h's, to a fixed vector by taking the maximum value in each dimension. Such architecture applies to all channels, namely, words, POS tags, grammatical relations, and WordNet hypernyms. The pooling vectors in these channels are concatenated, and fed to a fully connected hidden layer. Finally, we add a softmax output layer for classification. The training objec- tive is the penalized cross-entropy error, given by</p><formula xml:id="formula_12">J = − nc i=1 t i log y i +λ ω i=1 W i 2 F + υ i=1 U i 2 F</formula><p>where t ∈ R nc is the one-hot represented ground truth and y ∈ R nc is the estimated probability for each class by softmax. (n c is the number of target classes.) · F denotes the Frobenius norm of a matrix; ω and υ are the numbers of weight matri- ces (for W 's and U 's, respectively). λ is a hyper- parameter that specifies the magnitude of penalty on weights. Note that we do not add 2 penalty to biase parameters. We pretrained word embeddings by word2vec (Mikolov et al., 2013a) on the English Wikipedia corpus; other parameters are initialized randomly. We apply stochastic gradient descent (with mini- batch 10) for optimization; gradients are computed by standard back-propagation. Training details are further introduced in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present our experiments in de- tail. Our implementation is built upon <ref type="bibr" target="#b19">Mou et al. (2015)</ref>. Section 4.1 introduces the dataset; Section 4.2 describes hyperparameter settings. In Section 4.3, we compare SDP-LSTM's performance with other methods in the literature. We also analyze the effect of different channels in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The SemEval-2010 Task 8 dataset is a widely used benchmark for relation classification <ref type="bibr" target="#b12">(Hendrickx et al., 2009</ref>). The dataset contains 8,000 sentences for training, and 2,717 for testing. We split 1/10 samples out of the training set for validation.</p><p>The target contains 19 labels: 9 directed rela- tions, and an undirected Other class. The di- rected relations are list as below.</p><p>• Cause-Effect</p><formula xml:id="formula_13">• Component-Whole • Content-Container • Entity-Destination • Entity-Origin • Message-Topic • Member-Collection • Instrument-Agency • Product-Producer</formula><p>In the following are illustrated two sample sen- tences with directed relations.</p><p>[People] e 1 have been moving back into <ref type="bibr">[downtown]</ref>  The target labels are Entity-Destination (e 1 , e 2 ), and Cause-Effect(e 1 , e 2 ), respec- tively.</p><p>The dataset also contains an undirected Other class. Hence, there are 19 target labels in total. The undirected Other class takes in entities that do not fit into the above categories, illustrated by the following example. We use the official macro-averaged F 1 -score to evaluate model performance. This official mea- surement excludes the Other relation. Nonethe- less, we have no special treatment of Other class in our experiments, which is typical in other stud- ies.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hyperparameters and Training Details</head><p>This subsection presents hyperparameter tuning for our model. We set word-embeddings to be 200-dimensional; POS, WordNet hyponymy, and grammatical relation embeddings are 50- dimensional. Each channel of the LSTM network contains the same number of units as its source embeddings (either 200 or 50). The penultimate hidden layer is 100-dimensional. As it is not fea- sible to perform full grid search for all hyperpa- rameters, the above values are chosen empirically. We add 2 penalty for weights with coefficient 10 −5 , which was chosen by validation from the set {10 −2 , 10 −3 , · · · , 10 −7 }.</p><p>We thereafter validate the proposed dropout strategies in Section 3.5. Since network units in different channels do not interact with each other during information propagation, we herein take one channel of LSTM networks to assess the ef- ficacy. Taking the word channel as an example, we first drop out word embeddings. Then with a fixed dropout rate of word embeddings, we test the effect of dropping out LSTM inner cells and the penultimate units, respectively.</p><p>We find that, dropout of LSTM units hurts the model, even if the dropout rate is small, 0.1, say <ref type="figure">(Figure 4b)</ref>. Dropout of embeddings im- proves model performance by 2.16% ( <ref type="figure">Figure 4a)</ref>; dropout of the penultimate layer further improves by 0.16% <ref type="figure">(Figure 4c</ref>). This analysis also provides, for other studies, some clues for dropout in LSTM networks. <ref type="table">Table 4</ref> compares our SDT-LSTM with other state- of-the-art methods. The first entry in the ta- ble presents the highest performance achieved by traditional feature engineering. <ref type="bibr" target="#b12">Hendrickx et al. (2009)</ref> leverage a variety of handcrafted features, and use SVM for classification; they achieve an F 1 -score of 82.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Neural networks are first used in this task in <ref type="bibr" target="#b22">Socher et al. (2012)</ref>. They build a recursive neural network (RNN) along a constituency tree for re- lation classification. They extend the basic RNN with matrix-vector interaction and achieve an F 1 - score of 82.4%. <ref type="bibr" target="#b33">Zeng et al. (2014)</ref> treat a sentence as sequen- tial data and exploit the convolutional neural net- work (CNN); they also integrate word position information into their model. dos <ref type="bibr" target="#b7">Santos et al. (2015)</ref> design a model called CR-CNN; they pro- pose a ranking-based cost function and elaborately diminish the impact of the Other class, which is not counted in the official F 1 -measure. In this way, they achieve the state-of-the-art result with the F 1 - score of 84.1%. Without such special treatment, their F 1 -score is 82.7%. <ref type="bibr" target="#b30">Yu et al. (2014)</ref> propose a Feature-rich Com- positional Embedding Model (FCM) for relation classification, which combines unlexicalized lin- guistic contexts and word embeddings. They achieve an F 1 -score of 83.0%.</p><p>Our proposed SDT-LSTM model yields an F 1 - score of 83.7%. It outperforms existing compet- ing approaches, in a fair condition of softmax with cross-entropy error.</p><p>It is worth to note that we have also conducted two controlled experiments: (1) Traditional RNN without LSTM units, achieving an F 1 -score of 82.8%; (2) LSTM network over the entire depen- dency path (instead of two sub-paths), achieving an F 1 -score of <ref type="bibr">82</ref> the effectiveness of LSTM and directionality in re- lation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Different Channels</head><p>This subsection analyzes how different channels affect our model. We first used word embeddings only as a baseline; then we added POS tags, gram- matical relations, and WordNet hypernyms, re- spectively; we also combined all these channels into our models. Note that we did not try the latter three channels alone, because each single of them (e.g., POS) does not carry much information.</p><p>We see from <ref type="table" target="#tab_0">Table 2</ref> that word embeddings alone in SDP-LSTM yield a remarkable perfor- mance of 82.35%, compared with CNNs 69.7%, RNNs 74.9-79.1%, and FCM 80.6%.</p><p>Adding either grammatical relations or Word- Net hypernyms outperforms other existing meth- ods (data cleaning not considered here). POS tag- ging is comparatively less informative, but still boosts the F 1 -score by 0.63%.</p><p>We notice that, the boosts are not simply added when channels are combined. This suggests that these information sources are complementary to each other in some linguistic aspects. Nonethe- less, incorporating all four channels further pushes the F 1 -score to 83.70%.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Channels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel neural network model, named SDP-LSTM, for relation classifi- cation. It learns features for relation classifica- tion iteratively along the shortest dependency path. Several types of information (word themselves, POS tags, grammatical relations and WordNet hy- pernyms) along the path are used. Meanwhile, we leverage LSTM units for long-range infor- mation propagation and integration. We demon- strate the effectiveness of SDP-LSTM by evalu- ating the model on SemEval-2010 relation clas- sification task, outperforming existing state-of-art methods (in a fair condition without data clean- ing). Our result sheds some light in the relation classification task as follows.</p><p>• The shortest dependency path can be a valu- able resource for relation classification, cov- ering mostly sufficient information of target relations.</p><p>• Classifying relation is a challenging task due to the inherent ambiguity of natural lan- guages and the diversity of sentence expres- sion. Thus, integrating heterogeneous lin- guistic knowledge is beneficial to the task.</p><p>• Treating the shortest dependency path as two sub-paths, mapping two different neural net- works, helps to capture the directionality of relations.</p><p>• LSTM units are effective in feature detec- tion and propagation along the shortest de- pendency path.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The dependency parse tree corresponding to the sentence "A trillion gallons of water have been poured into an empty region of outer space." Red lines indicate the shortest dependency path between entities water and region. An edge a → b refers to a being governed by b. Dependency types are labeled by the parser, but not presented in the figure for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 depicts</head><label>2</label><figDesc>Figure 2 depicts the overall architecture of our SDP-LSTM network. First, a sentence is parsed to a dependency tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) The overall architecture of SDP-LSTM. (b) One channel of the recurrent neural networks built upon the shortest dependency path. The channels are words, part-of-speech (POS) tags, grammatical relations (abbreviated as GR in the figure), and WordNet hypernyms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A long short term memory unit. h: hidden unit. c: memory cell. i: input gate. f : forget gate. o: output gate. g: candidate cell. ⊗: element-wise multiplication. ∼: activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A</head><label></label><figDesc>misty [ridge] e 1 uprises from the [surge] e 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F</head><label></label><figDesc>Figure 4: F 1-scores versus dropout rates. We first evaluate the effect of dropout embeddings (a). Then the dropout of the inner cells (b) and the penultimate layer (c) is tested with word embeddings being dropped out by 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>e 2 .</head><label>2</label><figDesc></figDesc><table>Financial [stress] e 1 is one of the main 
causes of [divorce] e 2 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>.2%. These results demonstrate</head><label></label><figDesc></figDesc><table>Classifier 

Feature set 
F 1 

SVM 
POS, WordNet, prefixes and other morphological features, 
82.2 
depdency parse, Levin classes, PropBank, FanmeNet, 
NomLex-Plus, Google n-gram, paraphrases, TextRunner 

RNN 
Word embeddings 
74.8 
Word embeddings, POS, NER, WordNet 
77.6 

MVRNN 
Word embeddings 
79.1 
Word embeddings, POS, NER, WordNet 
82.4 

CNN 
Word embeddings 
69.7 
Word embeddings, word position embeddings, WordNet 
82.7 
Chain CNN Word embeddings, POS, NER, WordNet 
82.7 

FCM 
Word embeddings 
80.6 
Word embeddings, depedency parsing, NER 
83.0 

CR-CNN 
Word embeddings 
82.8  † 
Word embeddings, position embeddings 
82.7 
Word embeddings, position embeddings 
84.1  † 

SDP-LSTM 
Word embeddings 
82.4 
Word embeddings, POS embeddings, WordNet embeddings, 
83.7 
grammar relation embeddings 

Table 1: Comparison of relation classification systems. The " †" remark refers to special treatment for 
the Other class. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Effect of different channels.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://nlp.stanford.edu/software/lex-parser.shtml</note>

			<note place="foot" n="2"> http://sourceforge.net/projects/supersensetag</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Open information extraction for the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of twentieth International Joint Conference on Artificial Intelligence</title>
		<meeting>twentieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to extract relations from the web using minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="576" to="583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokan</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="242" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="594" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 53rd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>53rd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chain based rnn for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1244" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relexłrelation extraction using dependency parse trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Fundel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Küffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="371" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Guodong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simple customization of recursive neural networks for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions)</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>Fuzziness and Knowledge-Based Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions, page 22. Association for Computational Linguistics</title>
		<meeting>the ACL 2004 on Interactive Poster and Demonstration Sessions, page 22. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Discriminative neural sentence modeling by tree-based convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01106</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Embedding semantic similarity in tree kernels for domain adaptation of relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grounded compositional semantics for finding and describing images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="207" to="218" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Medical relation extraction with manifold models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="828" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A re-examination of dependency path kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Joint Conference on Natural Language Processing</title>
		<meeting>the Third International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="841" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning non-taxonomic relations on demand for ontology extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Software Engineering and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">08</biblScope>
			<biblScope unit="page" from="1159" to="1175" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Information extraction over structured data: Question answering with freebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuchen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="956" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Factor-based compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Learning Semantics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning to execute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
