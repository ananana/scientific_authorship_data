<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
							<email>asiddhan@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
							<email>zlipton@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2904" to="2909"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2904</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Several recent papers investigate Active Learning (AL) for mitigating the data-dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement , using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While over the past several years, deep learning has pushed the state of the art on numerous tasks, its extreme data-dependence presents a formidable obstacle under restricted annotation budgets. Ac- tive Learning (AL) presents one promising ap- proach to reduce deep learning's data require- ments ( <ref type="bibr" target="#b2">Cohn et al., 1996)</ref>. Strategically selecting points to annotate over alternating rounds of label- ing and learning, an active learner is hoped to out- perform budget-matched i.i.d. labeling. Typical acquisition functions select examples for which the current predictor is most uncertain. However, how precisely to quantify uncertainty, especially for neural networks, remains an open question.</p><p>Classical approaches interpret either the en- tropy or the negative argmax of the predictive (e.g. softmax) distribution as the model's uncertainty, yielding the maximum entropy and least confi- dence heuristics, respectively. These approaches account for aleatoric but not epistemic uncertainty ( <ref type="bibr" target="#b11">Kendall and Gal, 2017)</ref>. Several recent Bayesian formulations of deep learning provide alternative techniques for extracting uncertainty estimates from deep networks, including a dropout-based approach ( <ref type="bibr" target="#b5">Gal and Ghahramani, 2016b)</ref>, previ- ously employed in Deep Active Learning (DAL) for image classification (  and named entity recognition <ref type="bibr" target="#b21">(Shen et al., 2018</ref>), and Bayes-by-Backprop ( <ref type="bibr" target="#b0">Blundell et al., 2015)</ref>. To our knowledge, our paper is the first to apply Bayes- by-Backprop in the context of DAL.</p><p>While the results in recent papers hint at DAL's potential, its suitability in practice has yet to be proven. That's because papers often address just a single task, just a single model, and sometimes just one or two datasets. However, it's not enough to look back retrospectively after a final round of experiments and declare that one acquisition func- tion outperforms an i.i.d. baseline. To apply DAL in practice, we must be confident that the tech- nique will work correctly-the first time-on a dataset that we have never seen before. Otherwise, we might exhaust the annotation budget while per- forming worse than an i.i.d. baseline. Once we've exhausted our resources for labeling, there's no going back. Moreover, many DAL papers suffer from implicit target leaks. The architectures and hyper-parameters are often tuned using the full dataset, before concealing the labels and simulat- ing AL.</p><p>In this paper, we present a large-scale study 1 , comparing various acquisition functions across multiple tasks: Sentiment Classification (SC), Named Entity Recognition (NER), and Semantic Role Labeling (SRL). For each task we consider, with multiple datasets, multiple models, and mul- tiple acquisition functions. Moreover, in all ex- periments, we set hyper-parameters on warm-start data, allowing for a more honest assessment. This paper does not seek to champion any one approach but instead to ask, is there any single method that we can reliably expect to work out-of-the-box on a new problem?</p><p>To our surprise, we find that BALD <ref type="bibr" target="#b9">(Houlsby et al., 2011</ref>), which measures uncertainty by the frequency over multiple Monte Carlo draws from a stochastic model with which the drawn models disagree with the plurality, proved effective across all combinations of task, dataset, and model. Moreover both variants of the approach, draw- ing samples according to the dropout method (  and from a Bayes-by-Backprop net- work ( <ref type="bibr" target="#b0">Blundell et al., 2015)</ref>, performed similarly well across most tasks, datasets, and models.</p><p>Related Work Only a few papers have ad- dressed DAL for NLP, notably <ref type="bibr" target="#b21">Shen et al. (2018)</ref> for NER and <ref type="bibr" target="#b24">Zhang et al. (2017)</ref> who address text classification, proposing to select examples according to the expected magnitude of updates to word embeddings. In this paper, we do not consider the latter heuristic because we address sequence tagging tasks, where the difficulty of marginalizing over all possible labels blows up ex- ponentially with sequence length. While both pre- vious papers do conduct experiments on multiple datasets (2 and 3, respectively) they each consider just one task and just one model.  apply the dropout-based un- certainty estimates due to ( <ref type="bibr" target="#b4">Gal and Ghahramani, 2016a</ref>) together with the BALD framework due to <ref type="bibr" target="#b9">(Houlsby et al., 2011</ref>) for image classification with convolutional neural networks. They obtain significant improvement over classic uncertainty- based acquisition functions on the MNIST dataset and for diagnosing skin cancer from lesion images (ISIC2016 task). Our work builds on theirs, both by offering a large-scale evaluation of BALD for NLP tasks and models, and by exploring BALD with another method for estimating uncertainty: the uncertainty of the weights as modeled by a Bayes-by-Backprop network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bayesian Deep Learning</head><p>While space constraints preclude an extensive dis- cussion of the various Bayesian formulations of neural nets, we briefly summarize the methods compared in this paper, pointing out various de- sign decisions that are important for reproducing our results.</p><p>Monte Carlo Dropout According to <ref type="bibr" target="#b5">(Gal and Ghahramani, 2016b</ref>), the dropout regularization techniques for neural networks can be interpreted as a Bayesian approximation to Gaussian pro- cesses <ref type="bibr" target="#b19">(Rasmussen, 2004</ref>). Here, unlike standard uses of dropout, we apply it at prediction time. Uncertainty estimates are produced by compar- ing the output of a trained neural network using T different stochastic passes through the neural network. The extension to CNNs is straightfor- ward. To apply dropout to RNNs, we follow the approach due to ( <ref type="bibr" target="#b6">Gal and Ghahramani, 2016c</ref>), who extended their variational analysis to RNNs, arguing that dropout ought to be applied to the re- current layers (and not just the synchronous con- nections, per previous standard practice ( <ref type="bibr">Zaremba et al., 2014)</ref>) by applying identical dropout masks at each sequence step.</p><p>Bayes by Backprop In this approach due to <ref type="bibr" target="#b0">Blundell et al. (2015)</ref>, instead of maintaining a point estimate for each weight, we main- tain a probability distribution over the weights.</p><formula xml:id="formula_0">A standard L-layer MLP model P (y|x, w) is parametrized by weights w = {W l , b l } L l=1 ∈ R d . Then, ˆ y = φ(W L · ... · +φ(W 1 · x + b 1 ) + .. + b L )</formula><p>where φ is an activation function such as tanh or ReLU. Bayes-by-Backprop represents imposes a prior over the weights, p(w) and seeks to learn the posterior distribution p(w|D) given training data</p><formula xml:id="formula_1">D = {x i , y i } N i=1</formula><p>. To deal with intractability, Bayes-by-Backprop approximates p(w|D) by a variational distribution q(w|θ), typically choosing q to be a Gaussian with diagonal covariance and each weight sampled from N (µ i , σ 2 i ). To enforce non-negativity, the σ i are further parametrized via the softplus function</p><formula xml:id="formula_2">σ i = log(1 + exp(ρ i )) giving variational parameters θ = {µ i , ρ i } d i=1</formula><p>. Our objective in optimizing the variational parameters is to minimize the KL divergence between q(θ) and p(w|D).</p><p>Some simpli- fication of the objective gives L(D, θ) = N j=1 log q(w j |θ) − log p(w j ) − log p(D|w j ) , where w j denotes the j-th Monte Carlo sam-ple drawn from q(w|θ) (we use N = 1). In Bayes-by-Backprop, the parameters are opti- mized by stochastic gradient descent, using the re-parameterization trick popularized by <ref type="bibr" target="#b14">Kingma and Welling (2014)</ref>.</p><p>Extending Bayes-by- Backprop to CNNs and RNNs is straightforward with the latter requiring minor adjustments for truncated back-propagation through time ( <ref type="bibr" target="#b3">Fortunato et al., 2017)</ref>. Uncertainty estimates calculated via Bayes-by-Backprop have been shown to be useful for efficient exploration in reinforcement learning ( ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Acquisition functions</head><p>In this work, we consider only uncertainty-based acquisition. In particular, we consider least confidence (LC) for classification and maximum length-normalized log probability (MNLP) for se- quence labeling tasks <ref type="bibr" target="#b21">(Shen et al., 2018)</ref>. LC chooses that example with for which the predic- tion has lowest predicted probability. MNLP ex- tends this to sequences, selecting by log probabil- ity normalized by length, removing the bias for the model to preferentially select longer sequences.</p><p>BALD We briefly articulate the details of the Bayesian Active Learning by Disagreement (BALD) approach due to <ref type="bibr" target="#b9">Houlsby et al. (2011)</ref>, upon which both our Bayesian approaches are based. We denote Monte Carlo Dropout Disagree- ment by DO-BALD and its Bayes-by-Backprop counterpart as BB-BALD. BALD originally se- lects samples that maximise the information gained about the model parameters. This boils down to choosing data points which each stochas- tic forward pass through the model would have the highest probability assigned to a different class ( . Our measure of uncertainty is the fraction of models, across MC samples from the network, that that disagree with most popular choice. This can be mathematically represented as arg max</p><formula xml:id="formula_3">j 1 − count(mode(˜ y (1) j , ..., ˜ y (T ) j )) T Here˜yHere˜ Here˜y (t)</formula><p>j represents the prediction (argmax) ap- plied to the tth forward pass on jth sample˜ysample˜ sample˜y</p><formula xml:id="formula_4">(t) j = argmax(ˆ y (t) j ).</formula><p>We resolve ties by choosing the least confident predictions as determined by the mean probability assigned to the consensus class.</p><p>For sequences, we look at agreement on the entire sequence tag, noting that this may exhibit a bias to preferentially sample longer sentences. Because we measure the budget at each round in words (not sentences), while this constitutes a bias, it does not constitute an unfair advantage. Moreover, we note that all AL necessarily consists of biased sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training details</head><p>The active learning process begins with a ran- dom acquisition of 2% warmstart samples from the dataset. We train an initial model on this data. Then based on this model's uncertainty estimates, we apply our chosen acquisition function to sam- ple an additional 2% of examples and train a new model based on this data In each round, we train from scratch to avoid badly overfitting the data collected in earlier rounds per observations by <ref type="bibr" target="#b10">Hu et al. (2018)</ref>. We continue with alternating rounds of labeling and training until we have annotated 50% of the dataset. For classification tasks, the we measure the budget in sentences while for se- quence labeling, we measure the budget by the number of words because the annotator must pro- vide one tag per word.</p><p>In each iteration, we train each model to con- vergence, decided based on early stopping with a patience of 1 epoch, or 25 epochs (whichever comes earlier). For datasets with fixed validation sets such as Conll 2003, instead of using the en- tire validation set for early stopping, we use the percentage of validation data equivalent to that in our current training pool. Our motivation here is to keep the simulation realistic. Essentially, we as- sume that given a large annotation budget, one will collect both a larger training set and a larger vali- dation set. As a motivating example, it seems un- reasonable that a practitioner might have only 500 training examples but 10,000 examples available for early stopping. Our reported results are aver- aged over 3 runs with different warmstart samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sentence Classification</head><p>We use two datasets for simulation: one question classification dataset TrecQA ( <ref type="bibr" target="#b20">Roth et al., 2002</ref>) and one sentiment analysis dataset <ref type="bibr" target="#b17">(Pang and Lee, 2005</ref>) and two architectures for training: CNNs and BiLSTMs. For implementation of the CNNs on both these datasets, we follow the setup of <ref type="bibr" target="#b12">Kim (2014)</ref>   <ref type="figure">Figure 3</ref>: Performance of different acquisition functions on SRL task for two datasets use 300-dimensional glove embeddings <ref type="bibr" target="#b18">(Pennington et al., 2014</ref>) pretrained on 6B tokens for all 4 settings, a dropout rate of 0.5, and the Adam opti- mizer ( <ref type="bibr" target="#b13">Kinga and Adam, 2015</ref>) with initial learn- ing rate 1e-3. We use a batch size set to be either 50 or the number required for at least 10 updates whichever is lower. This is done to ensure that when the training pool is small, the batch size is not too large and models get sufficient number of updates in an epoch. We also train a Unigram + Bigram + Linear SVM model with LC acquisition as a shallow AL baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Named Entity Recognition</head><p>Again, we use two datasets: <ref type="bibr">CoNLL 2003 (Tjong Kim Sang and</ref><ref type="bibr" target="#b22">De Meulder, 2003</ref>) and OntoNotes 5.0. The two architectures used for training are CNN-BiLSTM-CRF (CNN for character-level en- coding, BiLSTM for word-level encoding, and CRF for decoding) <ref type="bibr" target="#b16">(Ma and Hovy, 2016)</ref> and CNN-CNN-LSTM (CNN for character-level en- coding, CNN for word-level encoding, and LSTM for decoding) <ref type="bibr" target="#b21">(Shen et al., 2018)</ref>. We follow the exact experimental settings of these papers ex- cept that batch size is 16 for CoNLL and 80 for OntoNotes (minimum 10 updates heuristic is fol- lowed here too).</p><p>We note that our NER models consist of multi- ple modular components, and that we only train a subset of those units in a Bayesian fashion. In both DO-BALD and BB-BALD, we apply dropout/stochastic weights on the word-level lay- ers, but not on the character-level encoders or de- coding layers. For example, with DO-BALD, we apply recurrent dropout in the BiLSTM word-level component of CNN-BiLSTM-CRF and we apply normal dropout in the word-level (middle) CNN layer of the CNN-CNN-LSTM. For NER, as a shallow AL baseline, we have a linear chain CRF model with MNLP acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Semantic Role Labeling</head><p>We consider two datasets: <ref type="bibr">CoNLL 2005 (Carreras and</ref><ref type="bibr" target="#b1">M` arquez, 2005</ref>) and CoNLL 2012, focusing only on an LSTM-based model this time. Our model resembles <ref type="bibr" target="#b8">He et al. (2017)</ref>, but instead of using contained A* decoding, we use a CRF de- coder, noting that while this causes a 2% drop in performance (at 100% annotation), our goal is to compare acquisition functions, not achieve record- setting performance. We follow the experimental setup of the paper but use a higher dropout rate of 0.25, adjusting the batch size according to the minimum update heuristic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Results</head><p>We plot the performance for various annotation budgets for all combinations of dataset, model, and acquisition function, for the SC, NER, and SRL tasks in <ref type="figure">Figures 1, 2, and 3</ref>, respectively. In all cases, the active learning methods perform bet- ter than random i.i.d. baseline. We note that across the board, DAL methods show significant im- provement over shallow baselines. The Bayesian acquisition functions, DO-BALD and BB-BALD consistently outperform classic uncertainly sam- pling, although in a few cases including the setting considered by <ref type="bibr" target="#b21">Shen et al. (2018)</ref>, the improvement is only marginal. This finding underscores the im- portance of examining proposed AL methods on a broad set of representative tasks and with a broad set of representative models.</p><p>In general, we find that the advantages of DAL can be substantial. For example, on NER tasks, we achieve roughly 98-99% of the full-dataset perfor- mance while labeling only 20% of the samples for both CNN-BiLSTM-CRF and CNN-CNN-LSTM models. By comparison, the i.i.d. baseline re- quires 50% of the data to achieve comparable F score. While the reduction in the percentage of data required is not as dramatic in the classifi- cation datasets (possibly owing to their compar- atively small size), the relative improvement over i.i.d. baselines remains significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper set out to investigate the practical util- ity of DAL for NLP. Our study consisted of over 40 experiments, each repeated for 3 times to av- erage results and consisting of roughly 25 rounds of retraining, adding up to 3000 training runs to completion. Our goal was not to champion any one approach, but to ask if there was any consis- tent story at all: can active learning be applied on a new dataset with an arbitrarily architecture, without peeking at the labels to perform hyper- parameter tuning? To our surprise, we found that across many tasks, both classic uncertainty sam- pling and Bayesian approaches outperform i.i.d. baselines and that DO-BALD and BB-BALD con- sistently perform best.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>90 Accuracy Dataset: TREC, Model= Linear SVM</head><label></label><figDesc>and for BiLSTMs, we use a single layer model with 300 hidden units for both datasets. We</figDesc><table>0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.65 

0.70 

0.75 

0.80 

0.85 

0.RAND 
LC 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

Accuracy 

Dataset: TREC, Model= CNN 

RAND 
LC 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

Accuracy 

Dataset: TREC, Model= BiLSTM 

RAND 
LC 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.66 

0.68 

0.70 

0.72 

0.74 

0.76 

0.78 

0.80 

0.82 

Accuracy 

Dataset: MaReview, Model= Linear SVM 

RAND 
LC 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.66 

0.68 

0.70 

0.72 

0.74 

0.76 

0.78 

0.80 

0.82 

Accuracy 

Dataset: MAReview, Model= CNN 

RAND 
LC 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

0.66 

0.68 

0.70 

0.72 

0.74 

0.76 

0.78 

0.80 

0.82 

Accuracy 

Dataset: MAReview, Model= BiLSTM 

RAND 
LC 
DO_BALD 
BB_BALD 
100 % Data 

Figure 1: Performance of various models and acquisition functions for two SC datasets 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

76 

78 

80 

82 

84 

86 

88 

90 

F-score 

Dataset: CoNLL 2003, Model= CRF 

RAND 
MNLP 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

76 

78 

80 

82 

84 

86 

88 

90 

F-score 

Dataset: CoNLL 2003, Model= CNN_CNN_LSTM 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

76 

78 

80 

82 

84 

86 

88 

90 

F-score 

Dataset: CoNLL 2003, Model= CNN_BiLSTM_CRF 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

70 

72 

74 

76 

78 

80 

82 

84 

86 

F-score 

Dataset: OntoNotes, Model= CRF 

RAND 
MNLP 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

70 

72 

74 

76 

78 

80 

82 

84 

86 

F-score 

Dataset: OntoNotes, Model= CNN_CNN_LSTM 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

70 

72 

74 

76 

78 

80 

82 

84 

86 

F-score 

Dataset: OntoNotes, Model= CNN_BiLSTM_CRF 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

Figure 2: Performance of various models and acquisition functions for two NER datasets 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

50 

55 

60 

65 

70 

75 

80 

F-score 

Dataset: CoNLL 2005 SRL, Model= BiLSTM_CRF 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

0 
10 
20 
30 
40 
50 
Percentage of Data Used 

50 

55 

60 

65 

70 

75 

80 

F-score 

Dataset: CoNLL 2012 SRL, Model= BiLSTM_CRF 

RAND 
MNLP 
DO_BALD 
BB_BALD 
100 % Data 

</table></figure>

			<note place="foot" n="1"> Code for all of our models and for running active learning experiments can be found at https://github.com/ asiddhant/Active-NLP</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to the and shared task: Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lluís</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>David A Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02798</idno>
		<title level="m">Bayesian recurrent neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian convolutional neural networks with Bernoulli approximate variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR) Workshop Track</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Máté</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07427</idno>
		<title level="m">Active learning with partial feedback</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BBQ-networks: Efficient exploration in deep reinforcement learning for task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Zachary C Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01354</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">directional LSTM-CNNsCRF. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gaussian processes in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced lectures on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Question-answering via enhanced understanding of questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chad</forename><forename type="middle">M</forename><surname>Cumby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Morie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramya</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Rizzolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Retrieval Conference (TREC)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep active learning for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakov</forename><surname>Kronrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik F Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active discriminative text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
