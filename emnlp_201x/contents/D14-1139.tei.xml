<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Learning with Cost-Augmented Contrastive Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-Supervised Learning with Cost-Augmented Contrastive Estimation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1329" to="1341"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We generalize contrastive estimation in two ways that permit adding more knowledge to unsupervised learning. The first allows the modeler to specify not only the set of corrupted inputs for each observation , but also how bad each one is. The second allows specifying structural preferences on the latent variable used to explain the observations. They require setting additional hyperparameters, which can be problematic in unsupervised learning, so we investigate new methods for unsuper-vised model selection and system combination. We instantiate these ideas for part-of-speech induction without tag dictionaries , improving over contrastive estimation as well as strong benchmarks from the PASCAL 2012 shared task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised NLP aims to discover useful struc- ture in unannotated text. This structure might be part-of-speech (POS) tag sequences <ref type="bibr" target="#b39">(Merialdo, 1994)</ref>, morphological segmentation <ref type="bibr" target="#b12">(Creutz and Lagus, 2005</ref>), or syntactic structure ( <ref type="bibr" target="#b32">Klein and Manning, 2004</ref>), among others. Unsupervised systems typically improve when researchers incor- porate knowledge to bias learning to capture char- acteristics of the desired structure. <ref type="bibr">1</ref> There are many successful examples of adding knowledge to improve learning without labeled examples, including: sparsity in POS tag distri- butions <ref type="bibr" target="#b30">(Johnson, 2007;</ref><ref type="bibr" target="#b49">Ravi and Knight, 2009;</ref><ref type="bibr" target="#b22">Ganchev et al., 2010</ref>), short attachments for dependency parsing <ref type="bibr" target="#b55">(Smith and Eisner, 2006</ref>), agreement of word alignment models ( <ref type="bibr" target="#b37">Liang et al., 2006</ref>), power law effects in lexical distribu- tions <ref type="bibr" target="#b3">(Blunsom and Cohn, 2010;</ref><ref type="bibr" target="#b4">Blunsom and Cohn, 2011</ref>), multilingual constraints ( <ref type="bibr" target="#b56">Smith and Eisner, 2009;</ref><ref type="bibr" target="#b21">Ganchev et al., 2009;</ref><ref type="bibr" target="#b13">Das and Petrov, 2011)</ref>, and orthographic cues ( <ref type="bibr" target="#b60">Spitkovsky et al., 2010c;</ref><ref type="bibr" target="#b62">Spitkovsky et al., 2011b</ref>), inter alia.</p><p>Contrastive estimation (CE; <ref type="bibr">Smith and Eisner, 2005</ref>) is a general approach to weakly-supervised learning with a particular way of incorporating knowledge. CE increases the likelihood of the ob- servations at the expense of those in a particular neighborhood of each observation. The neighbor- hood typically contains corrupted versions of the observations. The latent structure is marginalized out for both the observations and their corruptions; the intent is to learn latent structure that helps to explain why the observation was generated rather than any of the corrupted alternatives.</p><p>In this paper, we present a new objective func- tion for weakly-supervised learning that general- izes CE by including two types of cost functions, one on observations and one on output structures. The first ( §4) allows us to specify not only the set of corrupted observations, but also how bad each corruption was. We use n-gram language models to measure the severity of each corruption.</p><p>The second ( §5) allows us to specify prefer- ences on desired output structures, regardless of the input sentence. For POS tagging, we attempt to learn language-independent tag frequencies by computing counts from treebanks for 11 languages not used in our POS induction experiments. For example, we encourage tag sequences that contain adjacent nouns and penalize those that contain ad- jacent adpositions.</p><p>We consider several unsupervised ways to set hyperparameters for these cost functions ( §7), in- cluding the recently-proposed log-likelihood esti- mator of <ref type="bibr" target="#b1">Bengio et al. (2013)</ref>. We also circumvent hyperparameter selection via system combination, developing a novel voting scheme for POS induc- tion that aligns tag identifiers across runs.</p><p>We evaluate our approach, which we call cost- augmented contrastive estimation (CCE), on POS induction without tag dictionaries for five languages from the PASCAL shared task <ref type="bibr" target="#b23">(Gelling et al., 2012</ref>). We find that CCE improves over both standard CE as well as strong baselines from the shared task. In particular, our final average accu- racies are better than all entries in the shared task that use the same number of tags.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Weakly-supervised techniques can be roughly cat- egorized in terms of whether they influence the model, the learning procedure, or explicitly target the output structure. Examples abound in NLP; we focus on those that have been applied to POS tagging.</p><p>There have been many efforts at biasing models, including features <ref type="bibr" target="#b53">(Smith and Eisner, 2005a;</ref><ref type="bibr" target="#b2">Berg-Kirkpatrick et al., 2010)</ref>, sparse priors <ref type="bibr" target="#b30">(Johnson, 2007;</ref><ref type="bibr" target="#b27">Goldwater and Griffiths, 2007;</ref><ref type="bibr" target="#b67">Toutanova and Johnson, 2007)</ref>, sparsity in tag transition distributions <ref type="bibr" target="#b49">(Ravi and Knight, 2009)</ref>, small models via minimum description length criteria ( <ref type="bibr" target="#b70">Vaswani et al., 2010;</ref><ref type="bibr" target="#b46">Poon et al., 2009</ref>), a one-tag-per-type constraint <ref type="bibr" target="#b4">(Blunsom and Cohn, 2011)</ref>, and power law effects via Bayesian nonparametrics <ref type="bibr" target="#b69">(Van Gael et al., 2009;</ref><ref type="bibr" target="#b3">Blunsom and Cohn, 2010;</ref><ref type="bibr" target="#b4">Blunsom and Cohn, 2011)</ref>.</p><p>We focus below on efforts that induce bias into the learning ( §2.1) or more directly in the output structure ( §2.2), as they are more closely related to our contributions in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Biasing Learning</head><p>Some unsupervised methods do not change the model or attempt to impose structural bias; rather, they change the learning. This may involve op- timizing a different objective function for the same model, e.g., by switching from soft to hard EM ( <ref type="bibr" target="#b59">Spitkovsky et al., 2010b)</ref>. Or it may in- volve changing the objective during learning via annealing ( <ref type="bibr" target="#b52">Smith and Eisner, 2004</ref>) or more gen- eral multi-objective techniques ( <ref type="bibr" target="#b61">Spitkovsky et al., 2011a;</ref><ref type="bibr" target="#b63">Spitkovsky et al., 2013)</ref>.</p><p>Other learning modifications relate to automatic data selection, e.g., choosing examples for genera- tive learning <ref type="bibr" target="#b58">(Spitkovsky et al., 2010a</ref>) or automat- ically generating negative examples for discrimi- native unsupervised learning ( <ref type="bibr" target="#b34">Li et al., 2010;</ref><ref type="bibr" target="#b72">Xiao et al., 2011)</ref>. CE does both, automatically generating nega- tive examples and changing the objective function to include them. Our observation cost function al- ters CE's objective function, sharpening the effec- tive distribution of the negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structural Bias</head><p>Our output cost function is used to directly spec- ify preferences on desired output structures. Sev- eral others have had similar aims. For dependency grammar induction, <ref type="bibr" target="#b55">Smith and Eisner (2006)</ref> fa- vored short attachments using a fixed-weight fea- ture whose weight was optionally annealed during learning. Their bias could be implemented as an output cost function in our framework.</p><p>Posterior regularization (PR; <ref type="bibr" target="#b22">Ganchev et al., 2010</ref>) is a general framework for declaratively specifying preferences on model outputs. <ref type="bibr" target="#b41">Naseem et al. (2010)</ref> proposed universal syntactic rules for unsupervised dependency parsing and used them in a PR regime; we use analogous universal tag sequences in our cost function.</p><p>Our output cost is similar to posterior regular- ization. The difference is that we specify pref- erences via an arbitrary cost function on output structures, while PR uses expectation constraints on posteriors of the model. We compare to the PR tag induction system of <ref type="bibr" target="#b28">Graça et al. (2011)</ref> in our experiments, improving over it in several settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Exploiting Resources</head><p>Much of the work mentioned above also benefits from leveraging existing resources. These may be curated or crowdsourced resources like the Wik- tionary ( <ref type="bibr" target="#b35">Li et al., 2012</ref>), or traditional annotated treebanks for languages other than those under in- vestigation . In this paper, we use tag statistics from treebanks for 11 languages to impose our structural bias for a different set of languages used in our POS induction experiments.</p><p>Substantial recent work has improved many NLP tasks by leveraging multilingual or paral- lel text <ref type="bibr" target="#b10">(Cohen and Smith, 2009;</ref><ref type="bibr" target="#b71">Wang and Manning, 2014</ref>), including un- supervised POS tagging ( <ref type="bibr" target="#b13">Das and Petrov, 2011;</ref><ref type="bibr" target="#b65">Täckström et al., 2013;</ref><ref type="bibr" target="#b20">Ganchev and Das, 2013)</ref>. This sort of multilingual guidance could also be captured by particular output cost functions, though we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Unsupervised Structure Learning</head><p>We consider a structured unsupervised learning setting. We use X to denote our set of possible structured inputs, and for a particular x ∈ X, we use Y(x) to denote the set of valid structured outputs for x. We are given a dataset of inputs</p><formula xml:id="formula_0">{x (i) } N i=1</formula><p>. To map inputs to outputs, we start by building a model of the joint probability distribu- tion p θ (x, y). We use a log-linear parameteriza- tion with feature vector f and weight vector θ:</p><formula xml:id="formula_1">p θ (x, y) = exp θ f (x, y) x ∈X,y ∈Y(x ) exp θ f (x , y )</formula><p>where the sum in the denominator ranges over all possible inputs and all valid outputs for them.</p><p>In this paper, we consider ways of learning the parameters θ. Given θ, at test time we output a y for a new x using, e.g., Viterbi or minimum Bayes risk decoding; we use the latter in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">EM and Contrastive Estimation</head><p>We start by reviewing two ways of choosing θ. The expectation-maximization algorithm (EM; <ref type="bibr" target="#b15">Dempster et al., 1977</ref>) finds a local optimum of the marginal (log-)likelihood of the observations</p><formula xml:id="formula_2">{x (i) } N i=1</formula><p>. The marginal log-likelihood is a sum over all x (i) of the gain function γ EM (x (i) ):</p><formula xml:id="formula_3">γ EM (x (i) ) = log y∈Y(x (i) ) p θ (x (i) , y) = log y∈Y(x (i) ) exp θ f (x (i) , y) − log x ∈X,y ∈Y(x ) exp θ f (x , y ) Z(θ)</formula><p>The difficulty is the final term, log Z(θ), which requires summing over all possible inputs and all valid outputs for them. This summation is typically intractable for structured problems, and may even diverge. For this reason, EM is typi- cally only used to train log-linear model weights when Z(θ) = 1, e.g., for hidden Markov models, probabilistic context-free grammars, and models composed of locally-normalized log-linear mod- els <ref type="bibr" target="#b2">(Berg-Kirkpatrick et al., 2010</ref>), among others. There have been efforts at approximating the summation over elements of X, whether by limit- ing sequence length ( <ref type="bibr" target="#b29">Haghighi and Klein, 2006</ref>), only summing over observations in the training data <ref type="bibr" target="#b50">(Riezler, 1999)</ref>, restricting the observation space based on the task ), or us- ing Gibbs sampling to obtain an unbiased sample of the full space (Della <ref type="bibr" target="#b14">Pietra et al., 1997;</ref><ref type="bibr" target="#b51">Rosenfeld, 1997</ref>).</p><p>Contrastive estimation (CE) addresses this chal- lenge by using a neighborhood function N : X → 2 X that generates a set of inputs that are "corrup- tions" of an input x; N(x) always includes x. Us- ing shorthand N i for N(x (i) ), CE corresponds to maximizing the sum over inputs x (i) of the gain</p><formula xml:id="formula_4">γ CE (x (i) ) = log Pr(x (i) | N i ) = log y∈Y(x (i) ) p θ (x (i) , y) x ∈N i y ∈Y(x ) p θ (x , y ) = log y∈Y(x (i) ) exp θ f (x (i) , y) − log x ∈N i y ∈Y(x ) exp θ f (x , y )</formula><p>Two log Z(θ) terms cancel out, leaving the sum- mation over input/output pairs in the neighbor- hood instead of the full summation over pairs. Two desiderata govern the choice of N. One is to make the summation over its elements computa- tionally tractable. If N(x) = X for all x ∈ X, we obtain EM, so a smaller neighborhood typically must be used in practice. The second considera- tion is to target learning for the task of interest. For POS tagging and dependency parsing, <ref type="bibr">Eisner (2005a, 2005b</ref>) used neighborhood func- tions that corrupted the observations in systematic ways, e.g., their TRANS1 neighborhood contains the original sentence along with those that result from transposing a single pair of adjacent words. The intent was to force the learner to explain why the given sentences were observed at the expense of the corrupted sentences.</p><p>Next we present our modifications to con- trastive estimation. Both can be viewed as adding specialized cost functions that penalize some part of the structured input/output pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Modeling Corruption Costs</head><p>While CE allows us to specify a set of corrupted x for each x (i) via the neighborhood function N, it says nothing about how bad each corruption is. The same type of corruption might be harmful in one context and not harmful in another.</p><p>This fact was suggested as the reason why cer- tain neighborhoods did not work as well for POS tagging as others <ref type="bibr" target="#b53">(Smith and Eisner, 2005a)</ref>. One poorly-performing neighborhood consisted of sen- tences in which a single word of the original was deleted. Deleting a single word in a sen- tence might not harm grammaticality. By contrast, neighborhoods that transpose adjacent words led to better results. These kinds of corruptions are ex- pected to be more frequently harmful, at least for languages with relatively rigid word order. How- ever, there may still be certain transpositions that are benign, at least for grammaticality.</p><p>To address this, we introduce an observation cost function ∆ : X × X → R ≥0 that indicates how much two observations differ. Using ∆, we define the following gain function</p><formula xml:id="formula_5">γ CCE 1 (x (i) ) = log y∈Y(x (i) ) exp θ f (x (i) , y) − log x ∈N i y ∈Y(x ) exp θ f (x , y ) + ∆(x (i) , x )</formula><p>The function ∆ inflates the score of neighbor- hood entries with larger differences from the ob- served x (i) . This gain function is inspired by ideas from structured large-margin learning ( <ref type="bibr" target="#b66">Taskar et al., 2003;</ref><ref type="bibr" target="#b68">Tsochantaridis et al., 2005</ref>), specifi- cally softmax-margin ( <ref type="bibr" target="#b48">Povey et al., 2008;</ref><ref type="bibr" target="#b24">Gimpel and Smith, 2010)</ref>. Softmax-margin extends con- ditional likelihood by allowing the user to specify a cost function to give partial credit for structures that are partially correct. Conditional likelihood, by contrast, treats all incorrect structures equally. While softmax-margin uses a cost function to specify how two output structures differ, our gain function γ CCE 1 uses a cost function ∆ to specify how two inputs differ. But the motivations are sim- ilar: since poor structures have their scores artifi- cially inflated by ∆, learning pays more attention to them, choosing weights that penalize them more than the lower-cost structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Observation Cost Functions</head><p>What types of cost functions should we consider? For efficient inference, we want to ensure that ∆ decomposes additively across parts of the cor- rupted input x in the same way as the features; we assume unigram and bigram features in this paper.</p><p>In addition, the choice of the observation cost function ∆ is tied to the choice of neighborhood function. In our experiments, we use neighbor- hoods that change the order of words in the obser- vation but not the set of words. Our first cost func- tion simply counts the number of novel bigrams introduced when corrupting the original:</p><formula xml:id="formula_6">∆ I (x (i) , x) = α |x|+1 j=1 I x j−1 x j / ∈ 2grams(x (i) )</formula><p>where x j is the jth word of sentence x, x 0 is the start-of-sentence marker, x |x|+1 is the end-of- sentence marker, 2grams(x) returns the set of bi- grams in x, I[] returns 1 if its argument is true and 0 otherwise, and α is a constant to be tuned. We call this cost function MATCH. Only x (i) (which is always contained in N i ) is guaranteed to have cost 0. In the TRANS1 neighborhood, corrupted sequences will be penalized more if their transpo- sitions occur in the middle of the sentence rather than at the beginning or end.</p><p>We also consider a version that weights the in- dicator by the negative log probability of the novel bigram:</p><formula xml:id="formula_7">∆ LM (x (i) , x) = α |x|+1 j=1 −log P(x j |x j−1 )I x j−1 x j / ∈ 2grams(x (i) )</formula><p>where P(x j |x j−1 ) is obtained from a bigram lan- guage model. Among novel bigrams in the cor- ruption x, if the second word is highly surprising conditioned on the first, the bigram will incur high cost. We refer to ∆ LM (x (i) , x) as MATLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expressing Structural Preferences</head><p>Our second modification to CE allows us to spec- ify structural preferences for outputs y. We first note that there exist objective functions for su- pervised structure prediction that never require computing the feature vector for the true output y (i) . Examples include Bayes risk ( <ref type="bibr" target="#b31">Kaiser et al., 2000;</ref><ref type="bibr" target="#b47">Povey and Woodland, 2002</ref>) and structured ramp loss ( <ref type="bibr" target="#b17">Do et al., 2008</ref>). These two objec- tives do, however, need to compute a cost func- tion cost(y (i) , y), which requires the true output y (i) . We start with the following form of struc- tured ramp loss from <ref type="bibr" target="#b25">Gimpel and Smith (2012)</ref>, transformed here to a gain function:</p><formula xml:id="formula_8">max y∈Y(x (i) ) θ f (x (i) , y) − cost(y (i) , y) − max y ∈Y(x (i) ) θ f (x (i) , y ) + cost(y (i) , y )<label>(1)</label></formula><p>Maximizing this gain function for supervised learning corresponds to increasing the model score of outputs that have both high model score (θ f ) and low cost, while decreasing the model score of outputs with high model score and high cost. For unsupervised learning, we do not have y (i) , so we simply drop y (i) from the cost function. The result is an output cost function π : Y → R ≥0 which captures our a priori knowledge about de- sired output structures. The value of π(y) should be large for outputs y that are far from the ideal. In this paper, we consider POS induction and use intrinsic evaluation; however, in a real-world sce- nario, the output cost function could use signals derived from the downstream task in which the tags are being used.</p><p>Given π, we convert each max to a log exp in</p><p>Eq. 1 and introduce the contrastive neighborhood into the second term, defining our new gain func-</p><formula xml:id="formula_9">tion γ CCE 2 (x (i) ) = log y∈Y(x (i) ) exp θ f (x (i) , y) − π(y) − log x ∈N i y ∈Y(x ) exp θ f (x , y ) + π(y )</formula><p>Gimpel <ref type="formula" target="#formula_8">(2012)</ref> found that using such "softened" versions of the ramp losses worked better than the original versions (e.g., Eq. 1) when training ma- chine translation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Output Cost Functions</head><p>The output cost π should capture our desider- ata about y for the task of interest. We con- sider universal POS tag subsequences analogous to the universal syntactic rules of <ref type="bibr" target="#b41">Naseem et al. (2010)</ref>. In doing so, we use the universal tags of <ref type="bibr" target="#b45">Petrov et al. (2012)</ref>: NOUN, VERB, ADJ (ad- jective), ADV (adverb), PRON (pronoun), DET (determiner), ADP (pre/postposition), NUM (nu- meral), CONJ (conjunction), PRT (particle), '.' (punctuation), and X (other). We aimed for a set of rules that would be ro- bust across languages. So, we used treebanks for 11 languages from the CoNLL 2006/2007 shared tasks ( <ref type="bibr" target="#b8">Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b43">Nivre et al., 2007)</ref> other than those used in our POS induc- tion experiments. In particular, we used Arabic, Bulgarian, Catalan, Czech, English, Spanish, Ger- man, Hungarian, Italian, Japanese, and Turkish. We replicated shorter treebanks a sufficient num- ber of times until they were a similar size as the largest treebank. Then we counted gold POS tag unigrams and bigrams from the concatenation. Where #(y) is the count of tag y in the treebank concatenation, the cost of y is</p><note type="other">tag unigram count cost X 50783 3.83 NUM 174613 2.59 PRT 179131 2.57 ADV 330210 1.96 CONJ 436649 1.68 PRON 461880 1.62 DET 615284 1.33 ADJ 694685 1.21 ADP 906922 0.95 VERB 1018989 0.83 . 1042662 0.81 NOUN 2337234 0 tag bigram count cost DET PRT 109 84.41 DET CONJ 518 68.82 NUM ADV 1587 57.63 NOUN NOUN 409828 2.09 DET NOUN 454980 1.04 NOUN . 504897 0</note><formula xml:id="formula_10">u(y) = log max y #(y ) #(y)</formula><p>and, where #(y 1 , y 2 ) is the count of tag bigram y 1 , y 2 , the cost of y 1 , y 2 is</p><formula xml:id="formula_11">u(y 1 , y 2 ) = 10×log max y 1 ,y 2 #(y 1 , y 2 ) #(y 1 , y 2 )</formula><p>We use a multiplier of 10 in order to exaggerate count differences among bigrams, which gener- ally are closer together than unigram counts. In <ref type="table" target="#tab_0">Table 1</ref>, we show counts and costs for all tag uni- grams and selected tag bigrams. <ref type="bibr">2</ref> Given these costs for individual tag unigrams and bigrams, we use the following π function, which we call UNIV:</p><formula xml:id="formula_12">π(y) = β |y|+1 j=1 u(y j ) + u(y j−1 , y j )</formula><p>where β is a constant to be tuned and y j is the jth tag of y. We define y 0 to be the beginning- of-sentence marker and y |y|+1 to be the end-of- sentence marker (which has unigram cost 0).</p><p>Many POS induction systems use one-tag- per-type constraints ( <ref type="bibr" target="#b4">Blunsom and Cohn, 2011;</ref><ref type="bibr" target="#b23">Gelling et al., 2012)</ref>, which often lead to higher</p><formula xml:id="formula_13">max θ N i=1 log y∈Y(x (i) ) exp θ f (x (i) , y) − log x ∈N i y ∈Y(x ) exp θ f (x , y )<label>(2)</label></formula><formula xml:id="formula_14">max θ N i=1 log y∈Y(x (i) ) exp θ f (x (i) , y) − π(y) − log x ∈N i y ∈Y(x ) exp θ f (x , y ) + ∆(x (i) , x ) + π(y )<label>(3)</label></formula><p>Figure 1: Contrastive estimation (Eq. 2) and cost-augmented contrastive estimation (Eq. 3). L2 regular- ization terms ( C 2 |θ| j=1 θ 2 j ) are not shown here but were used in our experiments.</p><p>accuracies even though the gold standard is not constrained in this way. This constraint can be en- coded as an output cost function, though it would require approximate inference ( <ref type="bibr" target="#b46">Poon et al., 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Cost-Augmented CE</head><p>We extended the objective function underlying CE by defining two new types of cost functions, one on observations ( §4) and one on outputs ( §5).</p><p>We combine them into a single objective, which we call cost-augmented contrastive estimation (CCE), shown as Eq. 3 in <ref type="figure">Figure 1</ref>. If the cost functions ∆ and π factor in the same way as the features f , then it is straightforward to implement CCE atop an existing CE implemen- tation. The additional terms in the cost functions can be implemented as features with fixed weights (albeit where the weight differs depending on the context).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Model Selection</head><p>Our modifications give increased flexibility, but require setting new hyperparameters. In addition to the choice of the cost functions, each has a weight: α for ∆ and β for π. We need ways to set these weights that do not require labeled data.</p><p>Smith and Eisner (2005a) chose the hyperpa- rameter values that yielded the best CE objec- tive on held-out development data. We use their strategy, though we experiment with two others as well. <ref type="bibr">3</ref> In particular, we estimate held-out data log- likelihood via the method of <ref type="bibr" target="#b1">Bengio et al. (2013)</ref> and also consider ways of combining outputs from multiple models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Estimating Held-Out Log-Likelihood</head><p>Bengio et al. <ref type="bibr">(2013)</ref> recently proposed ways to efficiently estimate held-out data log-likelihood for generative models. They showed empirically that a simple, biased version of their conserva- tive sampling-based log-likelihood (CSL) estima- tor can be useful for model selection.</p><p>The biased CSL requires a Markov chain on the variables in the model (i.e., x and y) as well as the ability to compute p θ (x|y). It generates con- secutive samples of y from a Markov chain ini- tialized at each x in a development set D, with S Markov chains run for each x. We compute and sum p θ (x|y j ) for each sampled y j , then sum over all x in D. The result is a biased estimate for the log-likelihood of D. <ref type="bibr">Bengio et al.</ref> showed that these biased estimates could give the same model ranking as unbiased estimates, though more effi- ciently. They also showed that taking the single, initial sample from the S Markov chains resulted in the same model ranking as using many samples from each chain. We follow suit here.</p><p>Our Markov chain is a blocked Gibbs sam- pler in which we alternate between sampling from p θ (y|x) and p θ (x|y). Since we only use a sin- gle sample from each Markov chain and initialize each chain to x, this simply amounts to drawing S samples from p θ (y|x). To sample from p θ (y|x), we use the exact algorithm obtained by running the backward algorithm and then performing left- to-right sampling of tags using the local features and requisite backward terms to define the local tag distributions.</p><p>We then compute p θ (x|y) for each sampled y. If there are no features in f that look at more than one word (which is the case with the features used in our experiments), then this probability factors:</p><formula xml:id="formula_15">p θ (x|y) = |y| k=1 p θ (x k |y k )</formula><p>This is easily computable assuming that we have normalization constants Z(y) cached for each tag y. To compute each Z(y), we sum over all words observed in the training data (replacing some with a special UNK token; see below). We can then compute likelihoods for individual words and mul-tiply them across the words in the sentence to com- pute p θ (x|y). To summarize, we get a log-likelihood estimate for development set D = {x (i) } |D| i=1 by sampling S times from p θ (y|x (i) ) for each x (i) , getting sam- ples</p><formula xml:id="formula_16">{{y (i),j } S j=1 } |D| i=1 , then we compute |D| i=1 S j=1 log p θ (x (i) |y (i),j )</formula><p>We used values of S ∈ {1, 10, 100}, finding that the ranking of models was consistent across S val- ues. We used S = 10 in all results reported below.</p><p>We note that this estimator was originally pre- sented for generative models, and that (C)CE is not a generative training criterion. It seeks to max- imize the conditional probability of an observation given its neighborhood. Nonetheless, when imple- menting our log-likelihood estimator, we treat the model as a generative model, computing the Z(y) constants by summing over all words in the vocab- ulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">System Combination</head><p>We can avoid choosing a single model by com- bining the outputs of multiple models via system combination. We decode test data by using poste- rior decoding. To combine the outputs of multiple models, we find the max-posterior tag under each model, then choose the highest vote-getter, break- ing ties arbitrarily.</p><p>However, when doing POS induction without a tag dictionary, the tags are simply unique identi- fiers and may not have consistent meaning across runs. To address this, we propose a novel voting scheme that is inspired by the widely-used 1-to-1 accuracy metric for POS induction <ref type="bibr" target="#b29">(Haghighi and Klein, 2006</ref>). This metric maps system tags to gold tags to maximize accuracy with the constraint that each gold tag is mapped to at most once. The optimal mapping can be found by solving a maxi- mum weighted bipartite matching problem.</p><p>We adapt this idea to map tags between two sys- tems, rather than between system tags and gold tags. Given k systems that we want to combine, we choose one to be the backbone and map the re- maining k − 1 systems' outputs to the backbone. <ref type="bibr">4</ref> After mapping each system's output to the back- bone system, we perform simple majority voting among all k systems. To choose the backbone, we consider each of the k systems in turn as back- bone and maximize the sum of the weights of the weighted bipartite matching solutions found. This is a heuristic that attempts to choose a backbone that is similar to all other systems. We found that highly-weighted matchings often led to high POS tagging accuracy metrics. We call this vot- ing scheme ALIGN. To see the benefit of ALIGN, we also compare to a simple scheme (NA¨IVENA¨IVE) that performs majority voting without any tag map- ping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experiments</head><p>Task and Datasets We consider POS induction without tag dictionaries using five freely-available datasets from the PASCAL shared task <ref type="bibr" target="#b23">(Gelling et al., 2012)</ref>. <ref type="bibr">5</ref> These include Danish (DA), using the Copenhagen Dependency Treebank v2 (Buch- <ref type="bibr" target="#b7">Kromann et al., 2007)</ref>; Dutch (NL), using the Alpino treebank ( <ref type="bibr" target="#b5">Bouma et al., 2001</ref>); Por- tuguese (PT), using the Floresta Sintá(c)tica tree- bank ( <ref type="bibr" target="#b0">Afonso et al., 2002</ref>); Slovene (SL), us- ing the jos500k treebank ( <ref type="bibr" target="#b19">Erjavec et al., 2010)</ref>; and Swedish (SV), using the Talbanken tree- bank ( <ref type="bibr" target="#b42">Nivre et al., 2006</ref>). We use their provided training, development, and test sets.</p><p>Evaluation We fix the number of tags in our models to 12, which matches the number of uni- versal tags from <ref type="bibr" target="#b45">Petrov et al. (2012)</ref>. We use both many-to-1 (M-1) and 1-to-1 (1-1) accuracy as our evaluation metrics, using the universal tags for the gold standard (which was done for the of- ficial evaluation for the shared task). <ref type="bibr">6</ref> We note that our π function assigns identities to tags (e.g., tag 1 is assumed to be NOUN), so we could use actual tagging accuracy when training with the π cost function. But we use M-1 and 1-1 accuracy to enable easier comparison both among different settings and to prior work. Baselines From the shared task, we compare to all entries that used 12 tags. These include <ref type="bibr">5</ref> http://wiki.cs.ox.ac.uk/ InducingLinguisticStructure/SharedTask 6 It is common to use a greedy algorithm to com- pute 1-to-1 accuracy, e.g., as in the shared task scor- ing script (http://www.dcs.shef.ac.uk/ ˜ tcohn/ wils/eval.tar.gz), though the optimal mapping can be computed efficiently via the maximum weighted bipartite matching algorithm, as stated above. We use the shared task scorer for all results here for ease of comparison. When we instead evaluate using the optimal mapping, we find that ac- curacies are usually only slightly higher than those found by the greedy algorithm. BROWN clusters <ref type="bibr" target="#b6">(Brown et al., 1992)</ref>, clusters ob- tained using the mkcls tool <ref type="bibr" target="#b44">(Och, 1995)</ref>, and the featurized HMM with sparsity constraints trained using posterior regularization (PR), described by <ref type="bibr" target="#b28">Graça et al. (2011)</ref>. The PR system achieved the highest average 1-1 accuracy in the shared task.</p><p>We restrict our attention to systems that use 12 tags because the M-1 and 1-1 metrics are highly dependent upon the number of hypothesized tags. In general, using more tags leads to higher M-1 and lower 1-1 <ref type="figure">(Gelling et al., 2012)</ref>. By keep- ing the number of tags fixed, we hope to provide a cleaner comparison among approaches.</p><p>We compare to two other baselines: an HMM trained with 500 iterations of EM and an HMM trained with 100 iterations of stepwise EM <ref type="bibr" target="#b36">(Liang and Klein, 2009)</ref>. We used random initialization as done by Liang and Klein: we set each param- eter in each multinomial to exp{1 + c}, where c ∼ U <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>, then normalized to get probability distributions. For stepwise EM, we used mini- batch size 3 and stepsize reduction power 0.7.</p><p>For all models we trained, including both base- lines and CCE, we used only the training data during training and used the unannotated devel- opment data for certain model selection criteria. No labels were used except for final evaluation on the test data. Therefore, we need a way to handle unknown words in test data. When running EM and stepwise EM, while reading in the final 10% of sentences in the training set, we replace novel words with the special token UNK. We then re- place unknown words in test data with UNK.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">CCE Setup</head><p>Features We use standard indicator features on tag-tag transitions and tag-word emissions, the spelling features from <ref type="bibr" target="#b53">Smith and Eisner (2005a)</ref>, and additional emission features based on Brown clusters. The latter features are simply indicators for tag-cluster pairs-analogous to tag-word emis- sions in which the word is replaced by its Brown cluster identifier. We run Brown clustering <ref type="bibr" target="#b38">(Liang, 2005</ref>) on the POS training data for each language, once with 12 clusters and once with 40, then add tag-cluster emission features for each clustering and one more for their conjunction. 7 <ref type="bibr">7</ref> To handle unknown words: for words that only appear in the final 10% of training sentences, we replace them with UNK when firing their tag-word emission features. We use special Brown cluster identifiers reserved for UNK. But we still use all spelling features derived from the actual word Learning We solve Eq. 2 and Eq. 3 by running LBFGS until convergence on the training data, up to 100 iterations. We tag the test data with mini- mum Bayes risk decoding and evaluate.</p><p>We use two neighborhood functions:</p><p>• TRANS1: the original sentence along with all sentences that result from doing a single trans- position of adjacent words.</p><p>• SHUFF10: the original sentence along with 10 random permutations of it.</p><p>We use L2 regularization, adding C 2 |θ| j=1 θ 2 j to the objectives shown in <ref type="figure">Figure 1</ref>. We use a fixed (untuned) C = 0.0001 for all experiments re- ported below. <ref type="bibr">8</ref> We initialize each CE model by sampling weights from N(0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cost Functions</head><p>The cost functions ∆ and π have constants α and β which balance their con- tributions relative to the model score and must be tuned. We consider the ways proposed in Sec- tion 7, namely tuning based on the contrastive es- timation criterion computed on development data (CE), the log-likelihood estimate on development data with S = 10 (LL), and our two system com- bination algorithms: na¨ıvena¨ıve voting (NA¨IVENA¨IVE) and aligned voting (ALIGN), both of which use as in- put the 4 system outputs whose hyperparameters led to the highest values for the CE criterion on development data.</p><p>We used α ∈ {3 × 10 −4 , 10 −3 , 3 × 10 −3 , 0.01, 0.03, 0.1, 0.3} and β ∈ {3 × 10 −6 , 10 −5 , 3 × 10 −5 , 10 −4 , 3 × 10 −4 }. Setting α = β = 0 gives us CE, which we also compare to. When using both MATLM and UNIV simul- taneously, we first choose the best two α values by the LL criterion and the best two β values by the CE criterion when using only those individual costs. This gives us 4 pairs of values; we run ex- periments with these pairs and choose the pair to report using each of the model selection criteria.</p><p>For system combination, we use the 4 system out- puts resulting from these 4 pairs.</p><p>For training bigram language models for the MATLM cost, we use the language's POS train- ing data concatenated with its portion of the Eu- roparl v7 corpus ( <ref type="bibr" target="#b33">Koehn, 2005</ref>) and the text of its type. For unknown words at test time, we use the UNK emis- sion feature, the Brown cluster features with the special UNK cluster identifiers, and the word's actual spelling features. <ref type="bibr">8</ref> In subsequent experiments we tried C ∈ {0.01, 0.001} for the baseline CE setting and found minimal differences.  <ref type="table">Table 2</ref>: Results for observation cost functions. The CE baseline corresponds to rows where cost="none". Other rows are CCE. Best score for each column and each neighborhood is bold.</p><formula xml:id="formula_17">neigh- cost mod. DA NL PT SL SV avg borhood sel. M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 M-1 1-1 SHUFF10 none N/A</formula><p>Wikipedia. The word counts for the Wikipedias used range from 18M for Slovene to 1.9B for Dutch. We used modified Kneser-Ney smoothing as implemented by SRILM <ref type="bibr" target="#b64">(Stolcke, 2002</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Results</head><p>We present two sets of results. First we compare our MATCH and MATLM observation cost func- tions for our two neighborhoods and two ways of doing model selection. Then we do a broader com- parison, comparing both types of costs and their combination to our full set of baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation Cost Functions</head><p>In <ref type="table">Table 2</ref>, we show results for observation cost functions. We note that the TRANS1 neighborhood works much better than the SHUFF10 neighborhood, but we find that using cost functions can close the gap in certain cases, particularly for Dutch and Slovene for which the SHUFF10 MATLM scores approach or exceed the TRANS1 scores without a cost. Since the SHUFF10 neighborhood exhibits more diversity than TRANS1, we expect to see larger gains from using observation cost functions. We do in fact see larger gains in M-1, e.g., average improvements are 1.6-2.6 for SHUFF10 and 0.4- 1.3 for TRANS1, though 1-1 gains are closer.</p><p>For TRANS1, while MATCH does reach a slightly higher average M-1 than MATLM, the lat- ter does much better in 1-1 (49.9 vs. 47.6 when using LL for model selection). For SHUFF10, MATLM consistently does better than MATCH. Nonetheless, we suspect MATCH works as well as it does because it at least differentiates the obser- vation (which is always part of the neighborhood) from the corruptions.</p><p>We find that the LL model selection criterion consistently works better than the CE criterion for model selection. When using LL model selection and fixing the neighborhood, all average scores are better than their CE baselines. For M-1, the aver- age improvement is 1.0 to 2.6 points, and for 1-1 the average improvement ranges from 0.4 to 2.7.</p><p>We find the best overall performance when us- ing MATLM with LL model selection with the TRANS1 neighborhood, and we report this setting in our subsequent experiments. <ref type="table" target="#tab_3">Table 3</ref> shows results when using our UNIV output cost function, as well as our full set of baselines. All (C)CE experiments used the TRANS1 neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output Cost Function</head><p>We find that our contrastive estimation baseline (cost="none") has a higher average M-1 (61.8) than all results from the shared task, but its average 1-1 accuracy is lower than that reached by poste- rior regularization, the best system in the shared task according to 1-1. Using an observation cost function increases both M-1 and 1-1: MATLM yields an average 1-1 of 49.9, nearing the 50.1 of PR while exceeding it in M-1 by nearly 2 points.</p><p>When using the UNIV cost function, we see some variation in performance across model selec- tion criteria, but we find improvements in both M- 1 and 1-1 accuracy under most settings. When do- ing model selection via ALIGN voting, we roughly match the average 1-1 of PR, and when using the CE criterion, we beat it by 1 point on average (51.3 vs. 50.1).</p><p>Combined Costs When using the UNIV cost, we find that model selection via CE works bet- ter than LL. So for the combined costs, we took the two best MATLM weights (α values) accord- ing to LL and the two best UNIV weights (β val- ues) according to CE and ran combined cost ex- periments (MATCHLM+UNIV) with the four pairs of hyperparameters. Then from among these four <ref type="table" target="#tab_0">, system   DA  NL  PT  SL  SV  avg  M-1  1-1  M-1  1-1  M-1  1-1  M-1  1-1  M-1  1-1  M-1  1-1  HMM,</ref>   The results are shown in the lower part of Ta- ble 3. We find different trends in M-1 and 1- 1 depending on whether we use CE or LL for model selection, which may be due to our lim- ited hyperparameter search stemming from com- putational constraints. However, by comparing NA¨IVENA¨IVE to ALIGN, we see a consistent benefit from aligning tags before voting, leading to our highest average accuracies. In particular, using MATCHLM+UNIV and ALIGN, we improve over CE by 2.5 in M-1 and 4.5 in 1-1, also improving over the best results from the shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have shown how to modify contrastive estima- tion to use additional sources of knowledge, both in terms of observation and output cost functions. We adapted a recently-proposed technique for es- timating the log-likelihood of held-out data, find- ing it to be effective as a model selection criterion when using observation cost functions. We im- proved tagging accuracy by using weak supervi- sion in the form of universal tag frequencies. We proposed a system combination method for POS induction systems that consistently performs bet- ter than na¨ıvena¨ıve voting and circumvents hyperpa- rameter selection. We reported results on par with or exceeding the best systems from the PASCAL 2012 shared task.</p><p>Contrastive estimation has been shown effective for numerous NLP tasks, including dependency grammar induction <ref type="bibr" target="#b54">(Smith and Eisner, 2005b</ref>), bilingual part-of-speech induction <ref type="bibr" target="#b9">(Chen et al., 2011</ref>), morphological segmentation ( <ref type="bibr" target="#b46">Poon et al., 2009)</ref>, and machine translation <ref type="bibr" target="#b72">(Xiao et al., 2011</ref>). The hope is that our contributions can benefit these and other applications of weakly-supervised learn- ing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Counts and costs for universal tags based 
on treebanks for 11 languages not used in POS in-
duction experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Unsupervised POS tagging accuracies for five languages, showing results for three systems from 
the PASCAL shared task as well as three other baselines (EM, stepwise EM, and contrastive estimation). 
All (C)CE results use the TRANS1 neighborhood. The best score in each column is bold. 

we again chose results by CE, LL, and both voting 
schemes. 
</table></figure>

			<note place="foot" n="1"> We note that doing so strains the definition of the term unsupervised. Hence we will use the term weakly-supervised to refer to methods that do not explicitly train on labeled examples for the task of interest, but do use some form of taskspecific knowledge.</note>

			<note place="foot" n="2"> The complete tag bigram list is provided in the supplementary material.</note>

			<note place="foot" n="3"> When using their strategy for CCE, we compute the CE criterion only, omitting the costs. We do so because the weights of the cost terms can have a large impact on the magnitude of the objective, making it difficult to do a fair comparison of models with different cost weights.</note>

			<note place="foot" n="4"> We use the LEMON C++ toolkit (Dezs et al., 2011) to solve the maximum weighted bipartite matching problems.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their in-sightful comments and Waleed Ammar, Chris Dyer, David McAllester, Sasha Rush, Nathan Schneider, Noah Smith, and John Wieting for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Floresta sintá(c)tica: a treebank for Portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bounding the test log-likelihood of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.6184</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A hierarchical PitmanYor process HMM for unsupervised part of speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alpino: Wide-coverage computational analysis of Dutch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-based N-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Copenhagen Danish-English dependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buch-Kromann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wedekind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elming</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>treebank v. 2.0. code.google.com/p/copenhagendependency-treebank</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised bilingual POS tagging with Markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the First Workshop on Unsupervised Learning in NLP</title>
		<meeting>of the First Workshop on Unsupervised Learning in NLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised structure prediction with non-parallel multilingual guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Helsinki University of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Creutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lagus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised part-ofspeech tagging with bilingual graph-based projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LEMON-an open source C++ graph template library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dezs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jüttner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kovács</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Notes Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tighter bounds for structured estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised word alignment with arbitrary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The JOS linguistically tagged corpus of Slovene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erjavec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ledinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-lingual discriminative learning of sequence models with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dependency grammar induction via bitext projection constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The PASCAL challenge on grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT Workshop on the Induction of Linguistic Structure</title>
		<meeting>of NAACL-HLT Workshop on the Induction of Linguistic Structure</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Softmax-margin CRFs: Training log-linear models with cost functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured ramp loss minimization for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Discriminative Feature-Rich Modeling for Syntax-Based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fully Bayesian approach to unsupervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Controlling complexity in partof-speech induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coheur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototype-driven learning for sequence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLTNAACL</title>
		<meeting>of HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Why doesn&apos;t EM find good HMM POS-taggers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A novel loss function for the overall risk criterion based discriminative training of HMM models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Horvat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kacic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICSLP</title>
		<meeting>of ICSLP</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Corpus-based induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of MT Summit</title>
		<meeting>of MT Summit</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised discriminative language model training for machine translation using simulated confusion sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wiki-ly supervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online EM for unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Alignment by agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tagging English text with a probabilistic model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Multilingual part-of-speech tagging: Two unsupervised approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>JAIR</publisher>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Using universal linguistic knowledge to guide grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Talbanken05: A Swedish treebank with phrase structure and dependency annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Maximum-likelihood-schätzung von wortkategorien mit verfahren der kombinatorischen optimierung. Bachelor&apos;s thesis (Studienarbeit)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Friedrich-Alexander-Universität ErlangenNürnburg</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised morphological segmentation with log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT: NAACL</title>
		<meeting>of HLT: NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Minimum phone error and I-smoothing for improved discrimative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Boosted MMI for model and feature space discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Visweswariah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Minimized models for unsupervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Probabilistic Constraint Logic Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Riezler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Universität Tübingen</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A whole sentence maximum entropy language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Annealing techniques for unsupervised statistical language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive estimation: Training log-linear models on unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Guiding unsupervised grammar induction using contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI Workshop on Grammatical Inference Applications</title>
		<meeting>of IJCAI Workshop on Grammatical Inference Applications</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Annealing structural bias in multilingual weighted grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING-ACL</title>
		<meeting>of COLING-ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Parser adaptation and projection with quasi-synchronous features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">From Baby Steps to Leapfrog: How &quot;Less is More&quot; in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Profiting from mark-up: Hyper-text annotations for guided parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Punctuation: Making a point in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">SRILM-an extensible language modeling toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICSLP</title>
		<meeting>of ICSLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Token and type constraints for crosslingual part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Maxmargin Markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS 16</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Bayesian LDA-based model for semi-supervised part-ofspeech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">The infinite HMM for unsupervised POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Efficient optimization of an MDL-inspired objective function for unsupervised part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Cross-lingual projected expectation regularization for weakly supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Fast generation of translation forest for large-scale SMT discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
