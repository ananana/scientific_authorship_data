<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LANGPRO: Natural Language Theorem Prover</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
							<email>L.Abzianidze@rug.nl</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CLCG</orgName>
								<orgName type="institution" key="instit2">University of Groningen</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LANGPRO: Natural Language Theorem Prover</title>
					</analytic>
					<monogr>
						<title level="j" type="main">EMNLP System Demonstrations</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="115" to="120"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>LangPro is an automated theorem prover for natural language. 1 Given a set of premises and a hypothesis, it is able to prove semantic relations between them. The prover is based on a version of analytic tableau method specially designed for natural logic. The proof procedure operates on logical forms that preserve linguistic expressions to a large extent. The nature of proofs is deductive and transparent. On the FraCaS and SICK textual en-tailment datasets, the prover achieves high results comparable to state-of-the-art.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays many formal logics come with their own proof systems and with the automated theo- rem provers based on these systems. If we share Montagues's famous belief that there is "no im- portant theoretical difference between natural lan- guages and the artificial languages of logicians", then there plausibly exists a proof system for nat- ural languages too. On the other hand, studies on Natural Logic seek a formal logic whose formu- las are as close as possible to linguistic expres- sions. Inspired by these research ideas, <ref type="bibr">Muskens (2010)</ref> proposed an analytic tableau system for natural logic, where higher-order logic based on a simple type theory is used as natural logic and a version of analytic tableau method is designed for it. <ref type="bibr">Later, Abzianidze (2015b</ref><ref type="bibr" target="#b2">,a, 2016a</ref>) made the tableau system suitable for wide-coverage reason- ing by extending it and implementing a theorem prover based on it.</p><p>This paper presents the Prolog implementation of the theorem prover, called <ref type="bibr">LangPro</ref>  <ref type="figure">Figure 1</ref>: LangPro checks whether a set of premises p 1 , . . . , p n entails (), contradicts (⊥) or is neutral (#) to a hypothesis h.</p><p>of the system description. The rest of the paper is organized as follows. First, we briefly intro- duce the tableau system and the employed natu- ral logic. Then we characterize the architecture and functionality of LangPro (see <ref type="figure">Figure 1</ref>). Be- fore concluding, we briefly compare the prover to the related textual entailment systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Natural Tableau</head><p>An analytic tableau method is a proof procedure which searches a model, i.e. a possible situ- ation, satisfying a set of logic formulas. The search is performed by gradually applying infer- ence rules, also called tableau rules, to the for- mulas. A tableau rule has antecedents and conse- quent and is easy to read, e.g., according to NO T in <ref type="figure">Figure 3</ref>, if no A is B, then for any entity c, ei- ther it is not A or it is not B. A tableau proof, in short a tableau, is often depicted as an upside- down tree with initial formulas at its root <ref type="figure">(Fig- ure 2)</ref>. After each rule application, new inferred formulas are introduced in the tableau. Depend- ing on the applied rules, the tableau can branch or grow in depth. A tableau branch models a situ- ation that satisfies all the formulas in the branch.</p><p>Closed branches, marked with ×, correspond to inconsistent situations. The search for a possi- ble situation fails if all branches are closed-the tableau is closed. The natural tableau is a tableau method for a 1 several pug bark : T 2 every(which bark dog)(be vicious) : T 3 no pug(be evil) : T × <ref type="bibr">[4,</ref><ref type="bibr">13]</ref> AUX <ref type="bibr">[14]</ref> × <ref type="bibr">[12,</ref><ref type="bibr">16]</ref> ∀T <ref type="bibr">[2]</ref> ∧F <ref type="bibr">[6]</ref> NOT <ref type="bibr">[3]</ref> Figure 2: The tableau proves: several pugs bark. every dog which barks is vicious. ⊥ no pug is evil.</p><p>version of natural logic. <ref type="bibr">2</ref> The terms of the nat- ural logic, called Lambda Logical Forms (LLFs), are simply typed λ-terms built up from variables and constant lexical terms with the help of func- tion application and λ-abstraction. The format of a tableau entry, i.e. node, is a tuple consisting of a modifier list, an LLF, an argument list and a truth sign. The parts are delimited with a colon. The empty lists are omitted for conciseness. For ex- ample, the entries (1) and (2) both mean that it is true that c barks loudly in Paris, where (α, β) is a functional type that expects an argument of type α and returns a value of type β. 3 in np,vp,vp Paris np : loudly vp,vp bark vp : c e :</p><formula xml:id="formula_0">T (1) (in np,vp,vp Paris np )(loudly vp,vp bark vp c e ) : T (2)</formula><p>In order to prove a certain logical relation between premises and a hypothesis, the natural tableau searches a situation for the counterexam- ple of the relation. The relation is proved if the situation is not found, otherwise it is refuted. An example of a closed natural tableau is shown in <ref type="figure">Figure 2</ref>. It proves the contradiction relation as it fails to find a situation for the counterexample- the premises and the hypothesis being true. In or- der to facilitate reading tableau proofs, type infor- mation is omitted, the entries are enumerated and arcs are labeled with tableau rule applications. For example, 4 and 5 are obtained by applying ∃ T to 1 : if it is true that several pugs bark, then there is some entity c which is a pug and which barks.</p><formula xml:id="formula_1">C A B : [ #- C] : F A : [ #- C] : F B : [ #- C] : F ∧ F C ∈ {and, which} A AUX B : [ #- C] : X B : [ #- C] : X AUX Q A B : T A : c : T B : c : T ∃ T Q ∈ {several, a, . . .} c is fresh A : [ #- C] : T B : [ #- C] : F × × A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LLF Generator</head><p>A Natural Tableau-based theorem prover for nat- ural language requires automatic generation of LLFs from raw text. To do so, we implement a module, called LLFgen, that generates LLFs from syntactic derivations of Combinatory Cate- gorial Grammar (CCG, Steedman 2000). Given a CCG derivation, LLFgen returns several LLFs that model different orders of quantifier scopes (see <ref type="figure">Figure 4</ref>). 4 <ref type="figure" target="#fig_1">Figure 5</ref> displays a CCG deriva- tion where VP i abbreviates S i \NP .</p><p>LLFs are obtained from a CCG tree in three major steps ( <ref type="figure">Figure 4</ref>): (i) removing directionality from CCG trees, (ii) correcting semantically inad- equate analyses, and (iii) type-raising quantified NPs (QNPs). Below we briefly describe each of these steps and give corresponding examples.</p><p>Directionality information encoded in CCG cat- egories and combinatory rules is redundant from a semantic perspective, therefore we discard it in</p><formula xml:id="formula_2">ba[S dcl ] fa[VP dcl ] fa[VP ng ]</formula><p>fa <ref type="bibr">[PP ]</ref> lx <ref type="bibr">[NP, N ]</ref> water  <ref type="figure">Figure 6</ref>). These rules are destructive from a compositional point of view. We designed 13 schematic rewriting rules of general type that correct CCG terms-make them semantically more adequate and transparent. The rules make use of types, part-of-speech (POS) and named entity (NE) tags to match semantically in- adequate analyses: <ref type="bibr">5</ref> • Certain non-compositional multiword expres- sions are treated as constant terms: a lot of, in front of, a few, because of, next to, etc.</p><formula xml:id="formula_3">N water NN with PP /NP with IN fa[VP ng /PP ] fa[NP ] steak N steak NN a NP/N a DT rinsing (VP ng /PP )/NP rinse VBG is VP dcl /VP ng be VBZ</formula><p>• Type-changing rules are explained by changing lexical types, decomposing terms or inserting new terms. This step carries out conversions like [europe n ] np europe np , [nobody n ] np no n,np person n , and [water n ] np a n,np water n (see <ref type="figure" target="#fig_3">Figure 7</ref>). Inserted a n,np merely plays a role of an existential quantifier.</p><p>• Several CCG analyses are altered in order to reflect formal semantics, e.g., attributive modifiers are pushed under a relative clause: big (which run mouse) which run (big mouse); and PPs are attached to nouns rather than NPs: in (a box)(every pug) every (in (a box) pug).</p><p>5 To handcraft the rules, we used a development set of 1.7K CCG derivations obtained by parsing the sentences from FraCaS ( <ref type="bibr">Cooper et al., 1996</ref>) and the trial portion of SICK ( <ref type="bibr">Marelli et al., 2014</ref>) with CCG-based parsers: C&amp;C <ref type="bibr" target="#b5">(Clark and Curran, 2007)</ref> and EasyCCG ( <ref type="bibr">Lewis and Steedman, 2014</ref>).   <ref type="figure">Figure 6</ref>: The CCG term obtained from the CCG tree of <ref type="figure" target="#fig_1">Figure 5</ref>. NB: the lexical rule remains. </p><formula xml:id="formula_4">s dcl np person n person NN no</formula><note type="other">n, np no DT vp dcl vp ng pp np water n water NN a n, np a DT with np, pp with IN pp, vp ng np steak n steak NN a n, np a DT rinsing np, pp, vp ng rinse VBG is vp ng , vp</note><p>Since LLFs encode instructions for semantic composition, they can be used to composition- ally derive semantics in other meaning represen- tations <ref type="figure">(Figure 4</ref>), e.g., first-order logic (FOL) or Discourse Representation Theory (DRT). For this application, LLFgen can be used as an independent tool. Given a CCG derivation in the Prolog format (supported by both C&amp;C and EasyCCG), LLFgen can return LLFs in XML, HTML or L A T E Xformats. For a CCG tree, it is also possible to get either only the first LLF, e.g., (3), often reflecting the natural order of quantifiers, or a list of LLFs with various quantifier scope orders (possibly including seman- tically equivalent LLFs, like <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Natural Logic Theorem Prover</head><p>The tableau theorem prover for natural logic (NLogPro) represents a core part of LangPro <ref type="figure">(Fig- ure 1)</ref>. It is responsible for checking a set of lin- guistic expressions on (in)consistency. NLogPro consists of four components: the Proof Engine builds tableau proofs by applying the rules from the inventory of Rules; the rule applications are validated by the properties of lexical terms (en- coded in the Signature) and the lexical knowledge (available from the Knowledge Base). We used the same development data for LLFgen and NLogPro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Signature</head><p>The signature (SG) lists lexical terms that have algebraic properties relevant for inference, e.g., monotonicity, intersectivity, and implicativity. The lexical items in the SG come with an argument structure where each argument position is associ- ated with a set of algebraic properties. For exam- ple, every is characterized in the SG as <ref type="bibr">[dw,up]</ref>, meaning that in its first argument every is down- ward monotone while being upward monotone in the second one. Currently, the SG lists about 20 lexical items, mostly generalized quantifiers (GQs), that were found in the development data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Inventory of Rules</head><p>The inventory of rules (IR) contains all inference rules used by the prover. Currently there are ca. 80 rules in the IR (some in <ref type="figure">Figure 3)</ref>. Around a quar- ter of the rules are from Muskens (2010) and the rest are manually collected while exploring the de- velopment data. The rules cover a plethora of phe- nomena. Some of them are of a formal nature like Boolean connectives and monotonicity and others of linguistic nature: adjectives, prepositions, defi- nite NPs, expletives, open compound nouns, light verbs, copula, passives and attitude verbs.</p><p>The IR involves around 25 derivable rules-the rules that represent shortcuts of several rule ap- plications. One such rule is (NO n T ) in <ref type="figure">Figure 3</ref>, which is a specific version of (NO T ). Use of deriv- able rules yields shorter tableau proofs but raises a problem of performing the same rule application several times. NLogPro avoids this by maintain- ing a subsumption relation between the rules and keeping track of rule applications per branch.</p><p>A user can introduce new rules in the IR as Pro- log rules (Code 1): the head of the rule encodes antecedent nodes ===&gt; consequent nodes, and the body is a list of Prolog goals specifying the condi- tions the rule has to meet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Knowledge Base</head><p>The knowledge base (BS) is based on the Pro- log version of WordNet 3.0 <ref type="bibr">(Fellbaum, 1998)</ref>. At this moment only the hyponymy/hypernymy, sim- ilarity and antonymy relations are included in the KB. For simplicity, LangPro does not do any word sense disambiguation (WSD) but allows multi- ple word senses for a lexical term. For example, A B iff SynSet A is a hyponym of SynSet B , or there are similar Sense A and Sense B , where Sense A ∈ SynSet A and Sense B ∈ SynSet B . In the prover, a user can restrict the number of word senses per word by specifying a cutoff N , i.e. the N most frequent senses per word.</p><p>In addition to the WN relations, a user can in- troduce new lexical relations in the KB as Prolog facts, e.g., is_(crowd, group).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Proof Engine</head><p>The proof engine (PE) is the component that builds proof trees. While applying rules it takes into ac- count computational efficiency of each rule where the efficiency depends on the following categories:</p><p>• Branching: a rule is either branching (e.g., ∀ T ) or non-branching (e.g., AUX).</p><p>• Semantic equivalence: this depends whether the antecedents of a rule is semantically equivalent to its consequents. For example, (∧ F ) encodes the semantic equivalence while (NO T ) does not.</p><p>• Producing: depending on whether a rule pro- duces a fresh entity, it is a producer or a non- producer. (∃ T ) is a producer while (∀ T ) is not.</p><p>• Consuming: a rule is a consumer iff it employs an old entity from the branch during application. The consumer rules are (∀ T ) and (NO T ) but (∃ T ).</p><p>The most efficient combination of these features is non-branching, semantic equivalence, non- producing and non-consuming. Depending on a priority order between these categories, called an efficiency criterion, one can define a partial effi- ciency order over the rules. In particular, (6) is one of the best efficiency criteria on SICK (Abzian- idze, 2016a, Ch. 6). According to (6), (∧ F ) is more efficient than (NO n T ) since the equivalence is the most prominent category in (6), and (∧ F ) is equiv- alence in contrast to (NO n T ) [equi, nonBr, nonProd, nonCons]</p><p>A user can change the default criterion (6) by pass- ing a criterion via the Prolog predicate effCr/1. The PE builds two structures: a tree (see <ref type="figure">Fig- ure 2)</ref> and a list. The latter represents a list of the tree branches. The list structure is the main data structure that guides the computation process while the tree structure is optional (activated with the predicate prooftree/0) and is used for dis- playing proofs in a compact way. A few of the predicates that control the proof procedure are:</p><p>• ral/1 sets a rule application limit to n, which means that after n rules are applied the proof is terminated. n = 400 by default.</p><p>• thE/0 always permits existential import from definite NPs: it makes (∃ T ) applicable to the en- try the n,vp,s dog n bark vp : F.</p><p>• allInt/0 allows to treat lexical modifiers of the form c VB.|JJ|NN n,n as intersective by default un- less stated differently in the SG. This permits to infer baby n : c : T and kangaroo n : c : T from baby n,n kangaroo : c : T, for better or worse.</p><p>• the/0, a2the/0, and s2the/0 are used as flags and treat bare, indefinite, and plural NPs as def- inite NPs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LangPro: Natural Language Prover</head><p>The tableau-based theorem prover for natural lan- guage is obtained by chaining a CCG parser, LLFgen and NLogPro. In order to detect a se- mantic relation between a set of premises {p i } n i=1</p><p>and a hypothesis h, first the corresponding LLFs {P i } n i=1 and H are obtained via a CCG parser and LLFgen (i.e. for simplicity, a single LLF per sen- tence). Then based on the lexical terms of the LLFs, relevant sets of relations K and rules R are collected from the KB and the IR, respectively. To refute both entailment and contradiction relations NLogPro builds two proof trees using K and R. One starts with the counterexample (7) for entail- ment and another with the counterexample (8) for contradiction. The semantic relation which could not be refuted (i.e. its tableau for the counterexam- ple was closed) is said to be proved. The relation is considered to be neutral iff both tableaux have the same closure status: open or closed.</p><p>{P 1 : T, . . . , P n : T, H : F} (7)</p><p>{P 1 : T, . . . , P n : T, H : T} (8) Entailment relations often do not depend on semantics of phrases shared by premises and hypotheses. To bypass analyzing the common phrases, LangPro can use an optional CCG term aligner in LLFgen <ref type="figure">(Figure 1)</ref>, which identifies the common CCG sub-terms and treats them as con- stants. The sub-terms that are downward mono- tone or indefinite NPs are excluded from align- ments as they do not behave semantically as constants. After aligning CCG terms, aligned LLFs are obtained from them via the type-raising. Tableau proofs with aligned LLFs are shorter. Thus, first, a tableau with aligned LLFs is built, and if the tableau did not close, then non-aligned LLFs are used since alignment might prevent the tableau from closing. On SICK, the aligner boosts the accuracy by 1%. If stronger alignment is used (i.e. aligning indefinite NPs), the accuracy on SICK is increased by 2%. Both weak and strong alignment options can be chosen in <ref type="bibr">LangPro.</ref> The parser component of LangPro can be filled by C&amp;C or EasyCCG. This results in two versions of LangPro, ccLangPro and easyLangPro respec- tively. Both versions achieve similar results on FraCaS and SICK, and a simple aggregation of their judgments (coLangPro) improves the accu- racy on the unseen portion of SICK by 1%.</p><p>With respect to its rule-based nature, LangPro is fast. Given ready CCG derivations, on average 100 SICK problems are classified in 3.5 seconds. <ref type="bibr">7</ref> Details about speed and impact of parameters on the performance are given in <ref type="bibr" target="#b2">Abzianidze (2016a)</ref>.</p><p>In addition to an entailment judgment, LangPro can output the actual tableau proof trees (similar to <ref type="figure">Figure 2</ref>) in three formats: a drawing of a proof tree via the XPCE GUI, a L A T E X source code, an XML output, or an HTML file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Theorem proving techniques <ref type="bibr" target="#b4">(Bos and Markert, 2005</ref>) or ideas from Natural Logic <ref type="bibr">(MacCartney, 2009)</ref> were already used in recognizing textual entailment (RTE). But the combination of these two is a novel approach to RTE. The underlying higher-order logic of LangPro guarantees sound reasoning over several premises, including some complex semantic phenomena. This is in con- trast to the RTE systems that cannot reason over several premises or cannot account for Booleans and quantifiers, including the ones <ref type="bibr">(MacCartney, 2009</ref>) inspired by Natural Logic, and in contrast to those ones that use FOL representations and cannot cover higher-order phenomena like gener- alized quantifiers or subsective adjectives.</p><p>LangPro achieves state-of-the-art semantic competence (with accuracy of 87%) on the FraCaS sections commonly used for evaluation <ref type="bibr">(Abzianidze, 2016b,a)</ref>. On SICK, the prover obtains 82.1% of accuracy <ref type="bibr" target="#b0">(Abzianidze, 2015a</ref><ref type="bibr" target="#b2">(Abzianidze, , 2016a</ref>) while state-of-the-art systems score in the range of 81-87% and average performance of human on the dataset is around 84%. Detailed comparison of LangPro to the related RTE systems is discussed in (Abzianidze, 2015a, 2016b,a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The presented natural language prover involves a unique combination of natural logic, higher-order logic and a tableau method. Its natural logic side simplifies generation of the logical forms and makes the prover to be relatively easily scaled up. Due to its higher-order virtue, the prover easily accounts for complex semantic phenomena untameable in FOL. Because of its high reliabil- ity (less than 3% of its entailment and contradic- tion judgments are incorrect), the judgments of the prover can be successfully borrowed by other RTE systems. Further scaling-up for longer sentences (e.g., newswire text) and automated knowledge ac- quisition present future challenges to the prover.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The inference rules employed in the tableau proof of Figure 2. An entity term is old (fresh) wrt a branch iff it is (not) in the branch. LLFgen CCG Tree CCG Term Corrected CCG Term LLFs FOL DRT Removing directionality Correcting analyses Type-raising quantified NPs Figure 4: The LLF generator produces a list of LLFs from a single CCG derivation tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The CCG tree by C&amp;C for nobody is rinsing a steak with water (SICK-1379).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The corrected version of the CCG term of Figure 6, with inserted and decomposed terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Code 1: The Prolog format of tableau rules. Feats denotes efficiency features, ConstIndx and KB are the KB and indexing of constants re- spectively (fixed for every rule), and KeyWrd de- notes fixed lexical terms occurring in the rule. Each branch maintains its own signature of enti- ties introduced during the proof.</figDesc><table>r(Name, Feats, ConstIndx, KeyWrd, KB, 
br([nd(Mod1, LLF1, Arg1, Sign1),... 
nd(ModN, LLFN, ArgN, SignN)], 
Signature) ===&gt; 
[br([nd(Mod3, LLF3, Arg3, Sign3),...], 
Signature3), 
br([nd(Mod4, LLF4, Arg4, Sign4),...], 
Signature4)] 
:-Goal1, ..., GoalN. %conditions 

</table></figure>

			<note place="foot" n="2"> It is an extended version of Muskens&apos; original tableau system. The extension is three-fold and concerns the type system, the format of tableau entries and the inventory of tableau rules (Abzianidze, 2015b). 3 LLFs are typed with syntactic and semantic types. Interaction between these types is established via the subtyping relation, e.g., entities being a subtype of NPs, e &lt;: np, makes barkvp ce well-formed, where vp abbreviates (np, s).</note>

			<note place="foot" n="4"> See Abzianidze (2016a, Ch. 3) for a detailed description.</note>

			<note place="foot" n="6"> The LLFs use the following abbreviations: S = aq steakn, W = aq watern, and N = noq person n , where q = (n, vp, s).</note>

			<note place="foot" n="7"> This is measured on 8 × 2.4 GHz CPU machine, when proving problems in parallel (via the paralle/0 predicate) with the strong aligner option and the rule application limit 50-the configuration that achieves high performance both in terms of speed and accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported by the NWO-VICI grant "Lost in Translation-Found in Meaning" (288-89-003).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A tableau prover for natural logic and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2492" to="2502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a wide-coverage tableau method for natural logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lenls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurisin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaba</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Frontiers in Artificial Intelligence: Revised Selected Papers of JSAI-isAI 2014 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="66" to="82" />
		</imprint>
	</monogr>
	<note>Tsuyoshi Murata, Koji Mineshima, and Daisuke Bekki</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A natural proof system for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Tilburg University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural solution to fracas entailment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasha</forename><surname>Abzianidze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Fifth Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognising textual entailment with logical inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Widecoverage efficient statistical parsing with CCG and log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
