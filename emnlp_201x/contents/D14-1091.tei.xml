<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Pate</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Language Technology</orgName>
								<orgName type="institution">Macquarie University Sydney</orgName>
								<address>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="844" to="853"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Syllable weight encodes mostly the same information for English word segmentation as dictionary stress Abstract Stress is a useful cue for English word segmentation. A wide range of computational models have found that stress cues enable a 2-10% improvement in segmen-tation accuracy, depending on the kind of model, by using input that has been annotated with stress using a pronouncing dictionary. However, stress is neither invariably produced nor unambiguously identifiable in real speech. Heavy syllables, i.e. those with long vowels or syllable codas, attract stress in English. We devise Adaptor Grammar word segmentation models that exploit either stress, or syllable weight, or both, and evaluate the utility of syllable weight as a cue to word boundaries. Our results suggest that syllable weight encodes largely the same information for word segmentation in English that annotated dictionary stress does.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the first skills a child must develop in the course of language acquisition is the ability to seg- ment speech into words. Stress has long been recognized as a useful cue for English word seg- mentation, following the observation that words in English are predominantly stress-initial <ref type="bibr" target="#b7">(Cutler and Carter, 1987)</ref>, together with the result that 9- month-old English-learning infants prefer stress- initial stimuli <ref type="bibr" target="#b15">(Jusczyk et al., 1993)</ref>. A range of statistical ( <ref type="bibr" target="#b9">Doyle and Levy, 2013;</ref><ref type="bibr" target="#b5">Christiansen et al., 1998;</ref><ref type="bibr" target="#b0">Börschinger and Johnson, 2014</ref>) and rule-based <ref type="bibr" target="#b24">(Yang, 2004;</ref><ref type="bibr" target="#b18">Lignos and Yang, 2010</ref>) models have used stress information to improve word segmentation. However, that work uses stress-marked input prepared by marking vowels that are listed as stressed in a pronouncing dic- tionary. This pre-processing step glosses over the fact that stress identification itself involves a non- trival learning problem, since stress has many pos- sible phonetic reflexes and no known invariants <ref type="bibr" target="#b3">(Campbell and Beckman, 1997;</ref><ref type="bibr" target="#b10">Fry, 1955;</ref><ref type="bibr" target="#b11">Fry, 1958)</ref>. One known strong correlate of stress in English is syllable weight: heavy syllables, which end in a consonant or have a long vowel, at- tract stress in English. We present experiments with Bayesian Adaptor Grammars ( <ref type="bibr" target="#b14">Johnson et al., 2007</ref>) that suggest syllable weight encodes largely the same information for word segmentation that dictionary stress information does. <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> to compare the utility of syllable weight and stress cues for finding word boundaries, both individually and in combination. We describe how a shortcoming of Adaptor Grammars prevents us from comparing stress and weight cues in combination with the full range of phonotactic cues for word segmentation, and design two experiments to work around this limitation. The first experiment uses grammars that provide parallel analyses for syllable weight and stress, and learns initial/non-initial phonotac- tic distinctions. In this first experiment, syllable weight cues are actually more useful than stress cues at larger input sizes. The second experiment focuses on incorporating phonotactic cues for typical word-final consonant clusters (such as inflectional morphemes), at the expense of parallel structures. In this second experiment, weight cues merely match stress cues at larger input sizes, and the learning curve for the combined weight- and-stress grammar follows almost perfectly with the stress-only grammar. This second experiment suggests that the advantage of weight over stress in the first experiment was purely due to poor modeling of word-final consonant clusters by the stress-only grammar, not weight per se. All together, these results indicate that syllable weight is highly redundant with dictionary-based stress for the purposes of English word segmentation; in fact, in our experiments, there is no detectable difference between relying on syllable weight and relying on dictionary stress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specifically, we modify the Adaptor Grammar word segmentation model of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Stress is the perception that some syllables are more prominent than others, and reflects a com- plex, language-specific interaction between acous- tic cues (such as loudness and duration), and phonological patterns (such as syllable shapes). The details on how stress is assigned, produced, and perceived vary greatly across languages. Three aspects of the English stress system are relevant for this paper. First, although English stress can shift in different contexts <ref type="bibr" target="#b17">(Liberman and Prince, 1977)</ref>, such as from the first syllable of 'fourteen' in isolation to the second syllable when followed by a stressed syllable, it is largely stable across different tokens of a given word. Second, most words in English end up being stress-initial on a type and token basis. Third, heavy syllables (those with a long vowel or a consonant coda) at- tract stress in English.</p><p>There is experimental evidence that English- learning infants prefer stress-initial words from around the age of seven months <ref type="bibr" target="#b15">(Jusczyk et al., 1993;</ref><ref type="bibr">Juszcyk et al., 1999;</ref><ref type="bibr" target="#b15">Jusczyk et al., 1993;</ref><ref type="bibr" target="#b22">Thiessen and Saffran, 2003)</ref>. A variety of com- putational models have subsequently been devel- oped that take stress-annotated input and use this regularity to improve segmentation accuracy. The earliest Simple Recurrent Network (SRN) mod- eling experiments of <ref type="bibr" target="#b5">Christiansen et al. (1998)</ref> and <ref type="bibr" target="#b4">Christiansen and Curtin (1999)</ref> found that stress improved word segmentation from about 39% to 43% token f-score (see Evaluation). <ref type="bibr" target="#b21">Rytting et al. (2010)</ref> applied the SRN model to prob- ability distributions over phones obtained from a speech recognition system, and found that the en- tropy of the probability distribution over phones, as a proxy to local hyperarticulation and hence a stress cue, improved token f-score from about 16% to 23%. In a deterministic approach using pre- syllabified input, <ref type="bibr" target="#b24">Yang (2004)</ref>, with follow-ups in <ref type="bibr" target="#b18">Lignos and Yang (2010)</ref> and <ref type="bibr" target="#b19">Lignos (2011;</ref><ref type="bibr" target="#b20">2012)</ref>, showed that a 'Unique Stress Constraint' (USC), or assuming each word has at most one stressed syllable, leads to an improvement of about 2.5% boundary f-score.</p><p>Among explicitly probabilistic models, Doyle and Levy (2013) incorporated stress into <ref type="bibr">Goldwater et al.'s (2009)</ref> Bigram model. They did this by modifying the base distribution over lexical forms to generate not simply phone strings but a sequence of syllables that may or may not be stressed. The resulting model can learn that some sequences of syllables (in particular, sequences that start with a stressed syllable) are more likely than others. However, observed stress improved token f-score by only 1%. <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> used Adaptor Grammars ( <ref type="bibr" target="#b14">Johnson et al., 2007)</ref>, a generalization of <ref type="bibr">Goldwater et al.'s (2009)</ref> Bigram model that will be described shortly, and found a clearer 4-10% advantage in token f-score, depending on the amount of training data.</p><p>Together, the experimental and computational results suggest that infants in fact pay attention to stress, and that stress carries useful information for segmenting words in running speech. How- ever, stress identification is itself a non-trivial task, as stress has many highly variable, context- sensitive, and optional phonetic reflexes. How- ever, one strong phonological cue in English is syllable weight: heavy syllables attract stress. Heavy syllables, in turn, are syllables with a coda and/or a long vowel, which, in English, are tense vowels. <ref type="bibr" target="#b23">Turk et al. (1995)</ref> replicated the <ref type="bibr" target="#b15">Jusczyk et al. (1993)</ref> finding that English- learning infants prefer stress-initial stimuli (using non-words), and then examined how stress inter- acted with syllable weight. They found that sylla- ble weight was not a necessary condition to trig- ger the preference: infants preferred stress-initial stimuli even if the initial syllable was light. How- ever, they also found that infants most strongly preferred stimuli whose first syllable was both stressed and heavy: infants preferred stress-initial and heavy-initial stimuli to stress-initial and light- initial stimuli. This result suggests that infants are sensitive to syllable weight in determining typical stress and rythmic patterns in their language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Models</head><p>We will adopt the Adaptor Grammar framework used by <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> to ex- plore the utility of syllable weight as a cue to word segmentation by way of its covariance with stress. Adaptor Grammars are Probabilis- tic Context Free Grammars (PCFGs) with a spe-  cial set of adapted non-terminal nodes. We un- derline adapted non-terminals (X) to distinguish them from non-adapted non-terminals (Y). While a vanilla PCFG can only directly model regular- ities that are expressed by a single re-write rule, an Adaptor Grammar model caches entire subtrees that are rooted at adapted non-terminals. Adaptor Grammars can thus learn the internal structure of words, such as syllables, syllable onsets, and syl- lable rhymes, while still learning entire words as well.</p><p>In Adaptor Grammars, parameters are associ- ated with PCFG rules. While this has been a useful factorization in previous work, it makes it difficult to integrate syllable weight and syllable stress in a linguistically natural way. A syllable is typically analyzed as having an optional onset followed by a rhyme, with the rhyme rewriting to a nucleus (the vowel) followed by an optional coda, as in Fig- ure 1a. We expect stress and syllable weight to be useful primarily because initial syllables tend to be different from non-initial syllables. However, dis- tinguishing final from non-final codas should be useful as well, due to the frequency of suffixes in English, and the importance of edge phenomena in phonology more generally <ref type="bibr" target="#b1">(Brent and Cartwright, 1996)</ref>. These principles come into conflict when modeling monosyllabic words. If we say that a monosyllable is an Initial and Final SyllIF, and has an initial Onset and an initial Rhyme, as in <ref type="figure" target="#fig_1">Figure 1b</ref>, then we can learn the initial/non-initial generalization about stressed or heavy rhymes at the expense of the generalization about final and non-final codas. If we say that a monosyllable is an initial onset with a final rhyme, the reverse oc- curs: we can learn the final/non-final coda gen- eralization at the expense of the initial/non-initial regularities. If we split the symbols further, we'd generalize even less: we'd essentially have to learn the initial/non-initial patterns separately for mono- syllables and polysyllables.</p><p>The most direct solution would introduce fac- tors that are 'smaller' than a single PCFG rule. Es- sentially, we would compute the score of a PCFG rule in terms of multiple features of its right-hand side, rather than a single 'one-hot' feature identi- fying the expansion. We left this direction for fu- ture work and instead carried out two experiments using Adaptor Grammars that were designed to work around this limitation.</p><p>Our first experiment focuses on modeling the initial/non-initial distinction, leaving the final/non-final coda distinction unmodeled. The models in this experiment assume parallel struc- tures for syllable weight and stress, and focus on providing the most direct comparison between syl- lable weight and stress with a strictly initial/non- initial distinction. This first experiment shows that observing dictionary stress is better early in learn- ing, but that modeling syllable weight is better later in learning. However, it is possible that sylla- ble weight was more useful because modeling syl- lable weight involves modeling the characteristics of codas; the advantage may not have been due to weight per se but due to having learned something about the effects of suffixes on final codas.</p><p>Our second experiment focuses on modeling some aspects of final codas at the expense of main- taining a rigid parallelism in the structures for syl- lable weight and stress. The models in this exper- iment split only those symbols that are necessary to bring stress or weight patterns into the expres- sive power of the model, and focus on comparing richer models of syllable weight and stress that account for inital/internal/final distinctions. This second experiment shows that observing dictio- nary stress is better early in learning, and that modeling syllable weight merely catches up to</p><formula xml:id="formula_0">Sentence → Collocations3 + (1) Collocations3 → Collocations2 + (2) Collocations2 → Collocation + (3) Collocation → Word +<label>(4)</label></formula><p>Figure 2: Three levels of collocation; symbols fol- lowed by + may occur one or more times.</p><p>stress without surpassing it. Moreover, a com- bined stress-and-weight model does no better than a stress model, suggesting that the weight gram- mar's contribution is fully redundant, for the pur- poses of word segmentation, with the stress obser- vations.</p><p>Together, these experiments suggest that sylla- ble weight eventually encodes everything about word segmentation that dictionary stress does, and that any advantage that syllable weight has over observing dictionary stress is entirely redundant with knowledge of word-final codas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptor Grammars</head><p>We follow <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> in us- ing a 3-level collocation Adaptor Grammar, as in- troduced by  and presented in <ref type="figure">Figure 2</ref>, as the backbone for all models, including the baseline. A 3-level collo- cation grammar assumes that words are grouped into collocations of words that tend to appear with each other, and that the collocations themselves are grouped into larger collocations, up to three levels of collocations. This collocational struc- ture allows the model to capture strong word- to-word dependencies without having to group frequently-occuring word sequences into a single, incorrect, undersegmented 'word' as the unigram model tends to do  Word rewrites in different ways in Experiment I and Experiment II, which will be explained in the relevant experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Set-up</head><p>We applied the same experimental set-up used by <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref>, to their dataset, as described below. To understand how different modeling assumptions interact with corpus size, we train on prefixes of each corpus with increas- ing input size: 100, 200, 500, 1,000, 2,000, 5,000, and 10,000 utterances. Inference closely fol- lowed <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> and . We set our hyperpa- rameters to encourage onset maximization. The hyperparameter for syllable nodes to rewrite to an onset followed by a rhyme was 10, and the hyperparameter for syllable nodes to rewrite to a rhyme only was 1. Similarly, the hyperparame- ter for rhyme nodes to include a coda was 1, and the hyperparameter for rhyme nodes to exclude the coda was 10. All other hyperparameters spec- ified vague priors. We ran eight chains of each model for 1,000 iterations, collecting 20 samples with a lag of 10 iterations between samples and a burn-in of 800 iterations. We used the same batch- initialization and table-label resampling to encour- age the model to mix.</p><p>After gathering the samples, we used them to perform a single minimum Bayes risk decoding of a separate, held-out test set. This test set was constructed by taking the last 1,000 utterances of each corpus. We use a common test-set instead of just evaluating on the training data to ensure that performance figures are comparable across in- put sizes; when we see learning curves slope up- ward, we can be confident that the increase is due to learning rather than easier evaluation sets.</p><p>We measured our models' performance with the usual token f-score metric <ref type="bibr" target="#b2">(Brent, 1999)</ref>, the har- monic mean of how many proposed word tokens are correct (token precision) and how many of the actual word tokens are recovered (token recall). For example, a model may propose "the in side" when the true segmentation is "the inside." This segmentation would have a token precision of 1 3 , since one of three predicted words matches the true word token (even though the other predicted words are valid word types), and a token recall of 1 2 , since it correctly recovered one of two words, yield a token f-score of 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dataset</head><p>We evaluated on a dataset drawn from the Alex portion of the Providence corpus ( <ref type="bibr" target="#b8">Demuth et al., 2006</ref>). This dataset contains 17, 948 utterances with 72, 859 word tokens directed to one child from the age of 16 months to 41 months. We used a version of this dataset that contained annota- tions of primary stress that Börschinger and John- son (2014) added to this input using an extended    <ref type="bibr">1</ref> The mean number of syllables per word token was 1.2, and only three word tokens had more than five sylla- bles. Of the 40, 323 word tokens with a stressed syllable, 27, 258 were monosyllabic. Of the 13, 065 polysyllabic word tokens with a stressed syllable, 9, 931 were stress-initial. Turning to the 32, 536 word tokens with no stress (i.e., the func- tion words), all but 23 were monosyllabic (the 23 were primarily contractions, such as "couldn't").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experiment I: Parallel Structures</head><p>The goal of this first experiment is to provide the most direct comparison possible between gram- mars that attend to stress cues and grammars that attend to syllable weight cues. As these are both hypothesized to be useful by way of an initial/non- initial distinction, we defined a word to be an ini- tial syllable SyllI followed by zero to three sylla- bles, and syllables to consist of an optional onset </p><formula xml:id="formula_1">Word → SyllI (Syll) {0,3}<label>(5)</label></formula><p>SyllI → (OnsetI) RhymeI</p><p>Syll → (Onset) Rhyme</p><p>In the baseline grammar, presented in <ref type="figure" target="#fig_3">Figure 3c</ref>, rhymes rewrite to a vowel followed by an optional consonant coda. Rhymes then rewrite to be heavy or light in the weight grammar, as in <ref type="figure" target="#fig_3">Figure 3a</ref>, to be stressed or unstressed in the stress grammar, as in <ref type="figure" target="#fig_3">Figure 3b</ref>. In the combination grammar, rhymes rewrite to be heavy or light and stressed or un- stressed, as in <ref type="figure" target="#fig_3">Figure 3d</ref>. LongVowel and Short- Vowel both re-write to all vowels. An additional grammar that restricted them to rewrite to long and short vowels, respectively, led to virtually identi- cal performance, suggesting that vowel quantity can be learned for the purposes of word segmenta- tion from distributional cues. We will also present evidence that the model did manage to learn most of the contrast.  Vowel counts by quantity outperforms both the baseline and the weight-only grammar early in learning. The weight-only gram- mar rapidly improves in performance at larger training data sizes, increasing its advantage over the baseline, while the advantage of the stress- only grammar slows and appears to disappear at the largest training data size. At 10,000 utterances, the improvement of the weight-only grammar over the stress-only grammar is significant according to an independent samples t-test (t = 7.2, p &lt; 0.001, 14 degrees of freedom). This pattern suggests that annotated dictionary stress is easy to take advan- tage of at low data sizes, but that, with sufficient data, syllable weight can provide even more in- formation about word boundaries. The best over- all performance early in learning is obtained by the combined grammar, suggesting that syllable weight and dictionary stress provide information about word segmentation that is not redundant.</p><p>An examination of the final segmentation sug- gests that the weight grammar has learned that initial syllables tend to be heavy. Specifically, across eight runs, 98.1% of RhymeI symbols rewrote to HeavyRhyme, whereas only 54.5% of Rhyme symbols (i.e. non-initial rhymes) rewrote to HeavyRhyme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Mean TF Std. Dev. noweight:nostress 0.830 0.005 noweight:stress 0.831 0.008 weight:nostress 0.861 0.008 weight:stress 0.861 0.008 <ref type="table">Table 1</ref>: Segmentation Token F-score for Experi- ment I at 10,000 utterances across eight runs.</p><p>We also examined the final segmentation to see well the model learned the distinction between long vowels and short vowels. <ref type="figure" target="#fig_5">Figure 5</ref> presents a heatmap, with colors on a log-scale, showing how many times each vowel label rewrote to each pos- sible vowel in the (translated to IPA). Although the quantity generalisations are not perfect, we do see a general trend where ShortVowel rarely rewrites to diphthongs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Experiment II: Word-final Codas</head><p>Experiment I suggested that, under a ba- sic initial/non-initial distinction, syllable weight eventually encodes more information about word boundaries than does dictionary stress. This is a surprising result, since we initially investigated syllable weight as a noisy proxy for dictionary stress. One possible source of the 'extra' advan- tage that the syllable weight grammar exhibited has to do with the importance of word-final codas, which can encode word-final morphemes in En- glish <ref type="bibr" target="#b1">(Brent and Cartwright, 1996)</ref>. Even though the grammars did not explicitly model them, the weight grammar could implicitly capture a bias for or against having a coda in non-initial position, while the stress grammar could not. This is be- cause most word tokens are one or two syllables, and only one of the two rhyme types of the weight grammar included a coda. Thus, the HeavyRhyme symbol could simultaneously capture the most im- portant aspects of both stress and coda constraints.</p><p>To see if the extra advantage of the syllable weight grammar can be attributed to the influence of word-final codas, we formulated a set of gram- mars that model word-final codas and also can learn stress and/or syllable weight patterns. These grammars are more similar in structure to the ones that <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref> used. For the baseline and weight grammar, we again defined words to consist of up to four syllables with an ini- tial SyllI syllable, but this time distinguished final syllables SyllF in polysyllabic words. The non- stress grammars use the following rules for pro- ducing syllables:</p><formula xml:id="formula_4">Word → SyllIF (8) Word → SyllI (Syll) {0,2} SyllF (9) SyllIF → (OnsetI) RhymeI (10) SyllI → (OnsetI) RhymeI (11) Syll → (Onset) Rhyme (12)</formula><p>SyllF → (Onset) RhymeF <ref type="formula">(13)</ref> For the stress grammar, we followed Börschinger and Johnson (2014) in distin- guishing stressed and unstressed syllables, rather than simply stressed rhymes as in Experiment I, to allow the model to learn likely stress patterns at the word level. A word can consist of up to four syllables, and any syllable and any number of syllables may be stressed, as in <ref type="figure" target="#fig_6">Figure 6a</ref>.</p><p>The baseline grammar is similar to the previous one, except it distinguishes word-final codas, as in <ref type="figure" target="#fig_6">Figure 6b</ref>. The weight grammar, presented in <ref type="figure" target="#fig_6">Figure 6c</ref>, rewrites rhymes to a nucleus followed by an optional coda and distinguishes nuclei in open syllables according to their position in the word. The stress grammar, presented in <ref type="figure" target="#fig_6">Figure 6d</ref>, is the all-stress-patterns model (without the unique stress constraint) <ref type="bibr" target="#b0">Börschinger and Johnson (2014)</ref>. This grammar introduces additional distinctions at the syllable level to learn likely stress patterns, and distinguishes final from non-final codas. The combined model is identical to the stress model, except Vowel non-terminals in closed and word- internal syllables are replaced with Nucleus non- terminals, and Vowel non-terminals in word-inital (-final) open syllables are replaced with NucleusI (NucleusF) non-terminals.</p><p>To summarize, the stress models distinguish stressed and unstressed syllables in initial, final, and internal position. The weight models distin- guish the vowels of initial open syllables, the vow- els of final open syllables, and other vowels, al- lowing them to take advantage of an important cue from syllable weight for word segmentation: if an initial vowel is open, it should usually be long. <ref type="figure" target="#fig_7">Figure 7</ref> shows segmentation performance on the Alex corpus with these more complete models. While the performance of the weight grammars is virtually unchanged compared to <ref type="figure">Figure 4</ref>, the two grammars that do not model syllable weight im- prove dramatically. This result supports our pro- posal that much of the advantage of the weight  </p><formula xml:id="formula_5">Word → {SyllUIF|SyllSIF} Word → {SyllUI|SyllSI} {SyllU|SyllS} {0,2} {SyllUF|SyllSF} (a) The all-patterns stress model Rhyme → Vowel (Coda) RhymeF → Vowel (CodaF) (b) Baseline grammar RhymeI → NucleusI RhymeI → Nucleus Coda Rhyme → Nucleus (Coda) RhymeF → NucleusF RhymeF → Nucleus CodaF (c) Weight-sensitive grammar SyllSIF → OnsetI RhymeSF SyllUIF → OnsetI RhymeUF SyllSI → Onset RhymeS SyllUI → Onset RhymeU SyllSF → Onset RhymeSF SyllUF → Onset RhymeUF RhymeSI → Vowel Stress (Coda) RhymeUI → Vowel (Coda) RhymeS → Vowel Stress (Coda) RhymeU → Vowel (Coda) RhymeSF → Vowel Stress (CodaF) RhymeUF → Vowel (CodaF) (d) Stress-sensitive grammar</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Mean TF Std. Dev. noweight:nostress 0.846 0.007 noweight:stress 0.880 0.005 weight:nostress 0.865 0.011 weight:stress 0.875 0.005 <ref type="table">Table 2</ref>: Segmentation Token F-score for Experi- ment II at 10,000 utterances across eight runs.</p><p>grammars over stress in Experiment I was due to modeling of word-final coda phonotactics. <ref type="table">Table 2</ref> presents token f-score at 10,000 train- ing utterances averaged across eight runs, along with the standard deviation in f-score. We see that the noweight:nostress grammar is several standard deviations than the grammars that model sylla- ble weight and/or stress, while the syllable weight and/or stress grammars exhibit a high degree of overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented computational modeling exper- iments that suggest that syllable weight (eventu- ally) encodes nearly everything about word seg- mentation that dictionary stress does. Indeed, our experiments did not find a persistent advan- tage to observing stress over modeling syllable weight. While it is possible that a different mod- eling approach might find such a persistent advan- tage, this advantage could not provide more than 13% absolute F-score. This result suggests that children may be able to learn and exploit impor- tant rhythm cues to word boundaries purely on the basis of segmental input. However, this result also suggests that annotating input with dictionary stress has missed important aspects of the role of stress in word segmentation. As mentioned, <ref type="bibr" target="#b23">Turk et al. (1995)</ref> found that infants preferred ini- tial light syllables to be stressed. Such a prefer- ence obviously cannot be learned by attending to syllable weight alone, so infants who have learned weight distinctions must also be sensitive to non- segmental acoustic correlates to stress. There was no long-term advantage to observing stress in ad- dition to attending to syllable weight in our mod- els, however, suggesting that annotated dictionary stress does not capture the relevant non-segmental phonetic detail. More modeling is necessary to as- sess the non-segmental phonetic features that dis- tinguish stressed light syllables from unstressed light syllables. This investigation also highlighted a weakness of current Adaptor Grammar models: the 'small- est' factors are the size of one PCFG rule. Allow- ing further factorizations, perhaps using feature functions of a rule's right-hand side, would allow models to capture finer-grained distinctions with- out fully splitting the symbols that are involved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different ways to incorporate phonotactics. It is not possible to capture word-final codas and word initial rhymes in monosyllabic words with factors the size of a PCFG rule.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>RhymeI</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experiment I Grammars version of CMUDict (cmu, 2008). 1 The mean number of syllables per word token was 1.2, and only three word tokens had more than five syllables. Of the 40, 323 word tokens with a stressed syllable, 27, 258 were monosyllabic. Of the 13, 065 polysyllabic word tokens with a stressed syllable, 9, 931 were stress-initial. Turning to the 32, 536 word tokens with no stress (i.e., the function words), all but 23 were monosyllabic (the 23 were primarily contractions, such as "couldn't").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 presentsFigure 4 :</head><label>44</label><figDesc>Figure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctions between Stressed/Unstressed and Heavy/Light syllable rhymes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Heatmap of learned vowels in the Experiment I weight-only grammar. Each cell corresponds to the count of a particular vowel being analyzed as one of the three vowel types. Diphthongs are rarely ShortVowel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Experiment II Grammars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotactics that exploit Stress and Weight.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploring the role of stress in Bayesian word segmentation using Adaptor Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Börschinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="93" to="104" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributional regularity and phonotactic constraints are useful for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Brent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cartwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="93" to="125" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An efficient, probabilistically sound algorithm for segmentation and word discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="71" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stress, prominence, and spectral tilt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><surname>Beckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of an ESCA workshop</title>
		<meeting>an ESCA workshop<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The power of statistical learning: No need for algebraic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanne</forename><forename type="middle">L</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Curtin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st annual conference of the Cognitive Science Society</title>
		<meeting>the 21st annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to segment speech using multiple cues: A connectionist model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morten</forename><forename type="middle">H</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Seidenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Language and Cognitive Processes</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="221" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The CMU pronouncing dictionary</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The predominance of strong initial syllables in the English vocabulary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David M</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="133" to="142" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word-minimality, epenthesis, and coda licensing in the acquisition of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Demuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Culbertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Alter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="137" to="174" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining multiple information types in Bayesian word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2013</title>
		<meeting>NAACL 2013</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="117" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Duration and intensity as physical correlates of linguistic stress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D B</forename><surname>Fry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. of Am</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="765" to="768" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiments in the perception of stress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D B</forename><surname>Fry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="126" to="152" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Bayesian framework for word segmentation: Exploring the effects of context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="54" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving nonparametric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="317" to="325" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>B Schoelkopf, J Platt, and T Hoffmann</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Infants&apos; preference for the predominant stress patterns of English words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Peter W Jusczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><forename type="middle">J</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="675" to="687" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">and Mary Newsome. 1999. The beginnings of word segmentation in English-learning infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">M</forename><surname>Peter W Juszcyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Houston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="159" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On stress and linguistic rhythm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linguistic Inquiry</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="336" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recession segmentation: simpler online word segmentation using limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Lignos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2010</title>
		<meeting>ACL 2010</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="88" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling infant word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Lignos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifteenth conference on computational natural language learning</title>
		<meeting>the fifteenth conference on computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Infant word segmentation: An incremental, integrated model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Lignos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the West Coast Conference on Formal Linguistics</title>
		<meeting>the West Coast Conference on Formal Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Segmenting words from natural speech: subsegmental variation in segmental cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>C Anton Rytting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fosler-Lussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Child Language</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="543" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">When cues collide: use of stress and statistical cues to word boundaries by 7-to-9-month-old infants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">R</forename><surname>Thiessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saffran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="706" to="716" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do English-learning infants use syllable weight to determine stress?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louann</forename><surname>Jusczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language and Speech</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="158" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Universal grammar, statistics or both?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="451" to="456" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
