<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IXA NLP Group</orgName>
								<orgName type="institution">University of the Basque Country (UPV/EHU)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2289" to="2294"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Mapping word embeddings of different languages into a single space has multiple applications. In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary. In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bilingual word embeddings have attracted a lot of attention in recent times ( <ref type="bibr" target="#b14">Zou et al., 2013;</ref><ref type="bibr" target="#b5">Kočisk´Kočisk´y et al., 2014;</ref><ref type="bibr" target="#b0">Chandar A P et al., 2014;</ref><ref type="bibr" target="#b4">Gouws et al., 2014;</ref><ref type="bibr" target="#b3">Gouws and Søgaard, 2015;</ref><ref type="bibr" target="#b8">Luong et al., 2015;</ref><ref type="bibr" target="#b11">Wick et al., 2016)</ref>. A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that mini- mizes the distances between equivalences listed in a bilingual dictionary. The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation ( <ref type="bibr" target="#b10">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b13">Zhao et al., 2015)</ref>.</p><p>The first method to learn bilingual word em- bedding mappings was proposed by <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref>, who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries. Subsequent work has pro- posed alternative optimization objectives to learn better mappings. <ref type="bibr" target="#b12">Xing et al. (2015)</ref> incorporate length normalization in the training of word embed- dings and try to maximize the cosine similarity in- stead, introducing an orthogonality constraint to pre- serve the length normalization after the projection. <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> use canonical correlation analysis to project the embeddings in both languages to a shared vector space.</p><p>Beyond linear mappings, <ref type="bibr" target="#b7">Lu et al. (2015)</ref> apply deep canonical correlation analysis to learn a non- linear transformation for each language. Finally, ad- ditional techniques have been used to address the hubness problem in <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref>, both through the neighbor retrieval method ( ) and the training itself ( ). We leave the study of non-linear transformation and other additions for further work.</p><p>In this paper, we propose a general framework to learn bilingual word embeddings. We start with a basic optimization objective ( <ref type="bibr" target="#b10">Mikolov et al., 2013b)</ref> and introduce several meaningful and intuitive con- straints that are equivalent or closely related to pre- viously proposed methods <ref type="bibr" target="#b2">(Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b12">Xing et al., 2015)</ref>. Our framework provides a more general view of bilingual word embedding map- pings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alterna- tive theoretical interpretation for them. Our exper- iments on an existing English-Italian word transla- tion induction and an English word analogy task give strong empirical evidence in favor of our the- oretical reasoning, while showing that one of our models clearly outperforms previous alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Learning bilingual mappings</head><p>Let X and Z denote the word embedding matrices in two languages for a given bilingual dictionary so that their ith row X i * and Z i * are the word embed- dings of the ith entry in the dictionary. Our goal is to find a linear transformation matrix W so that XW best approximates Z, which we formalize minimiz- ing the sum of squared Euclidean distances follow- ing <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref>:</p><formula xml:id="formula_0">arg min W i X i * W − Z i * 2</formula><p>Alternatively, this is equivalent to minimizing the (squared) Frobenius norm of the residual matrix:</p><formula xml:id="formula_1">arg min W XW − Z 2 F</formula><p>Consequently, W will be the so called least- squares solution of the linear matrix equation XW = Z. This is a well-known problem in lin- ear algebra and can be solved by taking the Moore- Penrose pseudoinverse X + = X T X −1 X T as W = X + Z, which can be computed using SVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Orthogonality for monolingual invariance</head><p>Monolingual invariance is needed to preserve the dot products after mapping, avoiding performance degradation in monolingual tasks (e.g. analogy). This can be obtained requiring W to be an orthog- onal matrix (W T W = I). The exact solution un- der such orthogonality constraint is given by W = V U T , where Z T X = U ΣV T is the SVD factoriza- tion of Z T X (cf. Appendix A). Thanks to this, the optimal transformation can be efficiently computed in linear time with respect to the vocabulary size. Note that orthogonality enforces an intuitive prop- erty, and as such it could be useful to avoid degen- erated solutions and learn better bilingual mappings, as we empirically show in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Length normalization for maximum cosine</head><p>Normalizing word embeddings in both languages to be unit vectors guarantees that all training instances contribute equally to the optimization goal. As long as W is orthogonal, this is equivalent to maximiz- ing the sum of cosine similarities for the dictionary entries, which is commonly used for similarity com- putations:</p><formula xml:id="formula_2">arg min W i X i * X i * W − Z i * Z i * 2 = arg max W i cos (X i * W, Z i * )</formula><p>This last optimization objective coincides with <ref type="bibr" target="#b12">Xing et al. (2015)</ref>, but their work was motivated by an hypothetical inconsistency in <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref>, where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine. However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by <ref type="bibr" target="#b12">Xing et al. (2015)</ref> is equivalent to that used by <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref> with orthogonality constraint and unit vectors. In fact, our experiments show that orthogonality is more relevant than length normal- ization, in contrast to <ref type="bibr" target="#b12">Xing et al. (2015)</ref>, who intro- duce orthogonality only to ensure that unit length is preserved after mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mean centering for maximum covariance</head><p>Dimension-wise mean centering captures the intu- ition that two randomly taken words would not be expected to be semantically similar, ensuring that the expected product of two random embeddings in any dimension and, consequently, their cosine sim- ilarity, is zero. As long as W is orthogonal, this is equivalent to maximizing the sum of dimension- wise covariance for the dictionary entries:</p><formula xml:id="formula_3">arg min W C m XW − C m Z 2 F = arg max W i cov (XW * i , Z * i )</formula><p>where C m denotes the centering matrix</p><p>This equivalence reveals that the method pro- posed by <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> is closely re- lated to our framework. More concretely, <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> use Canonical Correlation Analysis (CCA) to project the word embeddings in both lan- guages to a shared vector space. CCA maximizes the dimension-wise covariance of both projections (which is equivalent to maximizing the covariance of a single projection if the transformations are con- strained to be orthogonal, as in our case) but adds an implicit restriction to the two mappings, making different dimensions have the same variance and be uncorrelated among themselves 1 :</p><formula xml:id="formula_4">arg max A,B i cov (XA * i , ZB * i ) s.t. A T X T C m XA = B T Z T C m ZB = I</formula><p>Therefore, the only fundamental difference be- tween both methods is that, while our model en- forces monolingual invariance, <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> do change the monolingual embeddings to meet this restriction. In this regard, we think that the restriction they add could have a negative im- pact on the learning of the bilingual mapping, and it could also degrade the quality of the monolingual embeddings. Our experiments (cf. Section 3) show empirical evidence supporting this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we experimentally test the proposed framework and all its variants in comparison with related methods. For that purpose, we use the trans- lation induction task introduced by <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref>, which learns a bilingual mapping on a small dictionary and measures its accuracy on pre- dicting the translation of new words. Unfortunately, the dataset they use is not public. For that reason, we use the English-Italian dataset on the same task provided by  2 . The dataset con- tains monolingual word embeddings trained with the word2vec toolkit using the CBOW method with neg- ative sampling (Mikolov et al., 2013a) 3 . The English embeddings were trained on a 2.8 billion word cor- pus (ukWaC + Wikipedia + BNC), while the 1.6 bil- lion word corpus itWaC was used to train the Italian <ref type="bibr">1</ref> While CCA is typically defined in terms of correlation (thus its name), correlation is invariant to the scaling of variables, so it is possible to constrain the canonical variables to have a fixed variance, as we do, in which case correlation and covariance become equivalent 2 http://clic.cimec.unitn.it/ ˜ georgiana. dinu/down/ <ref type="bibr">3</ref> The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the num- ber of negative samples to 10 embeddings. The dataset also contains a bilingual dictionary learned from Europarl, split into a train- ing set of 5,000 word pairs and a test set of 1,500 word pairs, both of them uniformly distributed in frequency bins. Accuracy is the evaluation measure.</p><p>Apart from the performance of the projected em- beddings in bilingual terms, we are also interested in the monolingual quality of the source language em- beddings after the projection. For that purpose, we use the word analogy task proposed by <ref type="bibr" target="#b9">Mikolov et al. (2013a)</ref>, which measures the accuracy on answer- ing questions like "what is the word that is similar to small in the same sense as biggest is similar to big?" using simple word vector arithmetic. The dataset they use consists of 8,869 semantic and 10,675 syn- tactic questions of this type, and is publicly avail- able <ref type="bibr">4</ref> . In order to speed up the experiments, we fol- low the authors and perform an approximate eval- uation by reducing the vocabulary size according to a frequency threshold of 30,000 <ref type="bibr" target="#b9">(Mikolov et al., 2013a</ref>). Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves. With these settings, we obtain a coverage of 64.98%.</p><p>We implemented the proposed method in Python using NumPy, and make it available as an open source project <ref type="bibr">5</ref> . The code for <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref> and <ref type="bibr" target="#b12">Xing et al. (2015)</ref> is not publicly available, so we implemented and tested them as part of the pro- posed framework, which only differs from the origi- nal systems in the optimization method (exact solu- tion instead of gradient descent) and the length nor- malization approach in the case of <ref type="bibr" target="#b12">Xing et al. (2015)</ref> (postprocessing instead of constrained training). As for the method by <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref>, we used their original implementation in Python and MAT- LAB 6 , which we extended to cover cases where the dictionary contains more than one entry for the same word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EN-IT EN AN.</head><p>Original embeddings - 76.66% Unconstrained mapping 34.93% 73.80% + length normalization 33.80% 73.61% + mean centering 38.47% 73.71% Orthogonal mapping 36.73% 76.66% + length normalization 36.87% 76.66% + mean centering 39.27% 76.59% <ref type="table">Table 1</ref>: Our results in bilingual and monolingual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results of our framework</head><p>The rows in <ref type="table">Table 1</ref> show, respectively, the results for the original embeddings, the basic mapping pro- posed by <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref> (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, in- crementally, mean centering. In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary. The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforc- ing a relevant property (monolingual invariance) that the transformation to learn should intuitively have. The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual perfor- mance without hurting monolingual performance. <ref type="table" target="#tab_0">Table 2</ref> shows the results for our best performing configuration in comparison to previous work. As discussed before, ( <ref type="bibr" target="#b10">Mikolov et al., 2013b)</ref> and <ref type="bibr" target="#b12">(Xing et al., 2015)</ref> were implemented as part of our frame- work, so they correspond to our uncostrained map- ping with no preprocessing and orthogonal mapping with length normalization, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison to other work</head><p>As it can be seen, the method by <ref type="bibr" target="#b12">Xing et al. (2015)</ref> performs better than that of <ref type="bibr" target="#b10">Mikolov et al. (2013b</ref>) in the translation induction task, which is in line with what they report in their paper. Moreover, thanks to the orthogonality constraint their mono- lingual performance in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with re- spect to the original embeddings.</p><p>Since <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> take advantage of EN-IT EN AN. Original embeddings - 76.66% <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref> 34.93% 73.80% <ref type="bibr" target="#b12">Xing et al. (2015)</ref> 36.87% 76.66% <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> 37.80% 69.64% Our method 39.27% 76.59% CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimen- sions). This beats the method by <ref type="bibr" target="#b12">Xing et al. (2015)</ref> in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality.</p><p>In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by dimension- wise mean centering that achieves the best accuracy in the word translation induction task. Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref>.</p><p>When compared to <ref type="bibr" target="#b12">Xing et al. (2015)</ref>, our results in <ref type="table">Table 1</ref> reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to <ref type="bibr" target="#b10">Mikolov et al. (2013b)</ref> comes solely from the orthogonality constraint, and not from solving any inconsistency.</p><p>It should be noted that the implementation by <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> also length-normalizes the word embeddings in a preprocessing step. Follow- ing the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref>, as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only dif- ference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make differ- ent dimensions have the same variance and be un- correlated among themselves. However, our model performs considerably better than any configuration from <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref> in both the monolin- gual and the bilingual task, supporting our hypoth- esis that these two constraints that are implicit in their method are not only conceptually confusing, but also have a negative impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper develops a new framework to learn bilin- gual word embedding mappings, generalizing previ- ous work and providing an efficient exact method to learn the optimal transformation. Our experi- ments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of <ref type="bibr" target="#b12">Xing et al. (2015)</ref> and <ref type="bibr" target="#b2">Faruqui and Dyer (2014)</ref>. It is the proposed method with the orthogonality constraint and a global preprocess- ing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods. In the future, we would like to study non-linear mappings ( <ref type="bibr" target="#b7">Lu et al., 2015)</ref> and the additional techniques in ( .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of solution under orthogonality</head><p>Constraining W to be orthogonal (W T W = I), the original minimization problem can be reformulated as follows (cf. Section 2.1):</p><p>arg min In the above expression, Tr(·) denotes the trace operator (the sum of all the elements in the main di- agonal), and the last equality is given by its cyclic property. At this point, we can take the SVD of Z T X as Z T X = U ΣV T , so Tr</p><formula xml:id="formula_5">Z T XW = Tr U ΣV T W = Tr ΣV T W U . Since V T ,</formula><p>W and U are orthogonal matrices, their product V T W U will also be an orthogonal matrix. In ad- dition to that, given that Σ is a diagonal matrix, its trace after an orthogonal transformation will be maximal when the values in its main diagonal are preserved after the mapping, that is, when the or- thogonal transformation matrix is the identity ma- trix. This will happen when V T W U = I in our case, so the optimal solution will be W = V U T .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of our method to other work. 

</table></figure>

			<note place="foot" n="4"> https://code.google.com/archive/p/ word2vec/ 5 https://github.com/artetxem/vecmap 6 https://github.com/mfaruqui/ crosslingual-cca</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the Eu-ropean Commision (QTLeap FP7-ICT-2013-10-610516), a Google Faculty Award, and the Span-ish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P). Mikel Artetxe enjoys a doctoral grant from the Spanish Ministry of Edu-cation, Culture and Sports.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An autoencoder approach to learning bilingual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR2015), workshop track</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR2015), workshop track</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving vector space word representations using multilingual correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple taskspecific bilingual word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.2455</idno>
		<title level="m">Bilbowa: Fast bilingual distributed representations without word alignments</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning bilingual word representations by marginalizing alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Kočisk´kočisk´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hubness and pollution: Delving into crossspace mapping for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="270" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep multilingual correlation for improved word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilingual word representations with monolingual quality in mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL Workshop on Vector Space Modeling for NLP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimally-constrained multilingual embeddings via artificial code-switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pallika</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Pocock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalized word embedding and orthogonal transform for bilingual word translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiye</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1006" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning translation models from monolingual continuous representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1527" to="1536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
