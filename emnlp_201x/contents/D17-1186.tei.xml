<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incorporating Relation Paths in Neural Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Physics</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="laboratory">State Key Laboratory of Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incorporating Relation Paths in Neural Relation Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1768" to="1777"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distantly supervised relation extraction has been widely used to find novel rela-tional facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which also provide rich useful information but not yet employed by relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on real-world datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with strong baselines. The source code of this paper can be obtained from https:// github.com/thunlp/PathNRE.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge Bases (KBs) provide effective structured information for real world facts and have been used as crucial resources for several nat- ural language processing (NLP) applications such as Web search and question answering. Typical KBs such as Freebase ( <ref type="bibr" target="#b2">Bollacker et al., 2008)</ref>, DB- pedia ( <ref type="bibr" target="#b0">Auer et al., 2007)</ref> and <ref type="bibr">YAGO (Suchanek et al., 2007)</ref> usually describe knowledge as multi- relational data and represent them as triple facts. As the real-world facts are infinite and increasing * * Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn). every day, existing KBs are still far from com- plete. Recently, petabytes of natural-language text containing thousands of different structure types are readily available, which is an important re- source for automatically finding unknown rela- tional facts. Hence, relation extraction (RE), de- fined as the task of extracting structured informa- tion from plain text, has attracted much interest.</p><p>Most existing supervised RE systems usually suffer from the issue that lacks sufficient labelled relation-specific training data. Manual annotation is very time consuming and labor intensive. One promising approach to address this limitation is distant supervision. ( <ref type="bibr" target="#b14">Mintz et al., 2009</ref>) gener- ates training data automatically by aligning a KB with plain text. They assume that if two target entities have a relation in KB, then all sentences that contain these two entities will express this relation and can be regarded as a positive train- ing instance. Since neural models have been ver- ified to be effective for classifying relations from plain text <ref type="bibr" target="#b22">(Socher et al., 2012;</ref><ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b20">dos Santos et al., 2015)</ref>, <ref type="bibr" target="#b30">(Zeng et al., 2015;</ref><ref type="bibr" target="#b13">Lin et al., 2016</ref>) incorporate neural networks method with distant supervision relation extraction. Fur- ther, ( <ref type="bibr" target="#b29">Ye et al., 2016</ref>) considers finer-grained in- formation, and achieves the state-of-the-art perfor- mance.</p><p>Although existing RE systems have achieved promising results with the help of distant super- vision and neural models, they still suffer from a major drawback: the models only learn from those sentences contain both two target entities. How- ever, those sentences containing only one of the entities could also provide useful information and help build inference chains. For example, if we know that "h is the father of e" and "e is the father of t", we can infer that h is the grandfather of t.</p><p>In this work, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, we intro- duce a path-based neural relation extraction model  with relation paths. First, we employ convolu- tional neural networks (CNN) to embed the se- mantics of sentences. Afterward, we build a rela- tion path encoder, which measures the probability of relations given an inference chain in the text. Finally, we combine information from direct sen- tences and relation paths to predict the relation. We evaluate our model on a real-world dataset for relation extraction. The experimental results show that our model achieves significant and con- sistent improvements as compared with baselines. Besides, with the help of those sentences contain- ing one of the target entities, our model is more robust and performs well even when the number of noisy instances increases. To the best of our knowledge, this is the first effort to consider the information of relation path in plain text for neu- ral relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distant Supervision</head><p>Distant supervision for RE is originally pro- posed in <ref type="bibr" target="#b3">(Craven et al., 1999</ref>). They focus on extracting binary relations between proteins us- ing a protein KB as the source of distant supervi- sion. Afterward, ( <ref type="bibr" target="#b14">Mintz et al., 2009</ref>) aligns plain text with Freebase, by using distant supervision . However, most of these methods heuristically transform distant supervision to traditional super- vised learning, by regarding it as a single-instance single-label problem, while in reality, one instance could correspond with multiple labels in differ- ent scenarios and vice versa. To alleviate the is- sue, ( <ref type="bibr" target="#b17">Riedel et al., 2010)</ref> regards each sentence as a training instance and allows multiple instances to share the same label but disallows more than one label. Further, ( <ref type="bibr" target="#b8">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b25">Surdeanu et al., 2012</ref>) adopt multi-instance multi- label learning in relation extraction. The main drawback of these methods is that they obtain most features directly from NLP tools with inevitable errors, and these errors will propagate to the rela- tion extraction system and limit the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Relation Extraction</head><p>Recently, deep learning <ref type="bibr" target="#b1">(Bengio, 2009)</ref> has been successfully applied in various areas, includ- ing computer vision, speech recognition and so on. Meanwhile, its effectiveness has also been verified in many NLP tasks such as sentiment analysis <ref type="bibr" target="#b19">(dos Santos and Gatti, 2014</ref>), parsing <ref type="bibr" target="#b21">(Socher et al., 2013</ref>), summarization <ref type="bibr" target="#b18">(Rush et al., 2015)</ref> and ma- chine translation ). With the advances of deep learning, there are grow- ing works that design neural networks for rela- tion extraction. ( <ref type="bibr" target="#b22">Socher et al., 2012</ref>) uses a recur- sive neural network in relation extraction, and ( <ref type="bibr" target="#b28">Xu et al., 2015;</ref><ref type="bibr" target="#b15">Miwa and Bansal, 2016</ref>) further use LSTM. ( <ref type="bibr" target="#b31">Zeng et al., 2014;</ref><ref type="bibr" target="#b20">dos Santos et al., 2015</ref>) adopt CNN in this task, and ( <ref type="bibr" target="#b30">Zeng et al., 2015;</ref><ref type="bibr" target="#b13">Lin et al., 2016</ref>) combine attention-based multi- instance learning which shows promising results. However, these above models merely learn from those sentences which directly contain both two target entities. The important information of those relation paths hidden in the text is ignored. In this paper, we propose a novel path-based neural RE model to address this issue. Besides, although we choose CNN to test the effectiveness of our model, other neural models could also be easily adapted to our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Relation Path Modeling</head><p>Relation paths have been taken into consider- ation on large-scale KBs for relation inference. Path Ranking algorithm (PRA) <ref type="bibr" target="#b9">(Lao and Cohen, 2010</ref>) has been adopted for expert finding <ref type="bibr" target="#b9">(Lao and Cohen, 2010)</ref>, information retrieval ( <ref type="bibr" target="#b11">Lao et al., 2012)</ref>, and further for relation classification based on KB structure <ref type="bibr" target="#b10">(Lao et al., 2011;</ref><ref type="bibr" target="#b5">Gardner et al., 2013)</ref>. ( <ref type="bibr" target="#b16">Neelakantan et al., 2015;</ref><ref type="bibr" target="#b12">Lin et al., 2015;</ref><ref type="bibr" target="#b4">Das et al., 2016;</ref><ref type="bibr" target="#b27">Wu et al., 2016</ref>) use recur- rent neural networks (RNN) to represent relation paths based on all involved relations in KBs.( <ref type="bibr" target="#b6">Guu et al., 2015)</ref> proposes an embedding-based com- positional training method to connect the triple knowledge for KB completion. Different from the above work of modeling relation paths in KBs, our model aims to utilize relation paths in text corpus, and help to extract knowledge directly from plain text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>Given a pair of target entities, a set of corre- sponding direct sentences S = {s 1 , s 2 , · · · , s n } which contains this entity pair, and a set of rela- tion paths P = {p 1 , p 2 , · · · , p m }, our model aims to measure the confidence of each relation for this entity pair. In this section, we will introduce our model in three parts: (1) Text Encoder. Given the sentence with two corresponding target enti- ties, we use a CNN to embed the sentence into a semantic space, and measure the probability of each relation given this sentence. (2) Relation Path Encoder. Given a relation path between the target entities, we measure the probability of each relation r, conditioned on the relation path. (3) Joint Model. We integrate the information from both direct sentences and relation paths, then pre- dict the confidence of each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Encoder</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, we use a CNN to extract in- formation from text. Given a set of sentences of an entity pair, we first transform each sentence s into its distributed representation s, and then predict re- lation using the most representative sentence via a multi-instance learning mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Input Vector</head><p>First, we transform the words {w 1 , w 2 , · · · , w l } in sentence s into vectors of dimension d. For each word w i , we use word embedding to encode its syntactic and semantic meanings, and use position embedding to encode its position information. We then concatenate both word embedding and posi- tion embedding to form the input vector of w i for CNN. (See <ref type="figure" target="#fig_1">Figure 2</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Convolution and Max-pooling Layers</head><p>When processing a sentence, it is a great chal- lenge that important information could probably appear in all parts of that sentence. In addition, the length l of a sentence could also vary a lot. Therefore, we apply CNN to encode all local fea- tures regardless sentence length. We first apply a convolution layer to extract all possible local fea- tures, and then select the most important one via max-pooling layer.</p><p>To extract local features, the convolution layer first concatenates a sequence of word embeddings within a sliding window to be vector q i of dimen- sion k × d:</p><formula xml:id="formula_0">q i = w [i−k+1:i] (1 ≤ i ≤ l + k − 1), (1)</formula><p>where k is the size of the window, and we also set all out-of-index words to be zero vec- tors. It then multiplies q i by a convolution ma- trix W ∈ R dc×(k×d) , where d c is the dimen- sion of sentence embeddings. Hence, the out- put of convolution layer could be expressed as h = {h 1 , h 2 , · · · , h l+k−1 }:</p><formula xml:id="formula_1">h i = Wq i + b,<label>(2)</label></formula><p>where b is a bias vector. Finally, the max-pooling layer takes a max operation, followed by a hyper- bolic tangent activation, over the sequence of h i to select the most important information, namely,</p><formula xml:id="formula_2">[s] j = tanh(max i [h i ] j ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Multi-Instance Learning</head><p>Next, we apply a softmax classifier upon the sentence representation s to predict the corre- sponding relation. We define the condition proba- bility of relation r as follows,</p><formula xml:id="formula_3">p(r|θ, s) = exp(e r ) nr i=1 exp(e i ) ,<label>(4)</label></formula><p>where e i , a component of e, measures how well this sentence matches relation r i , and n r is the number of relations. More specifically, e could be calculated from:</p><formula xml:id="formula_4">e = Us + v,<label>(5)</label></formula><p>where U ∈ R nr×dc is the coefficient matrix of relations and v ∈ R nr is a bias vector. We use multi-instance learning to alleviate the wrong-labeling issue in distant supervision, by choosing one sentence in the set of all direct sen- tences S = {s 1 , s 2 , · · · , s m } which corresponds to the entity pair (h, t). Similar to ( <ref type="bibr" target="#b30">Zeng et al., 2015)</ref>, we define the score function of this entity pair and its corresponding relation r as a max-one setting:</p><formula xml:id="formula_5">E(h, r, t|S) = max i p(r|θ, s i ).<label>(6)</label></formula><p>where E reflects the direct information we derive from sentences. We can also set a random setting as a baseline:</p><formula xml:id="formula_6">E(h, r, t|S) = p(r|θ, s i ),<label>(7)</label></formula><p>where s i is randomly selected from S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation Path Encoder</head><p>We use Relation Path Encoder to embed the in- ference information of relation paths. Relation Path Encoder measures the probability of each re- lation r given a relation path in the text. This will utilize the inference chain structure to help make predictions. More specifically, we define a path p 1 between (h, t) as {(h, e), (e, t)}, and the corre- sponding relations are r A , r B . Each of (h, e) and (e, t) corresponds to at least one sentence in the text. Our model calculates the probability of rela- tion r conditioned on p 1 as follows,</p><formula xml:id="formula_7">p(r|r A , r B ) = exp(o r ) nr i=1 exp(o i ) ,<label>(8)</label></formula><p>where o i measures how well relation r matches with the relation path (r A , r B ). Inspired by the work on relation path representation learning ( <ref type="bibr" target="#b12">Lin et al., 2015)</ref>, our model first transforms relation r to its distributed representation, i.e. vector r ∈ R d R , and builds the path embeddings by composi- tion of relation embeddings. Then, the similarity o i is calculated as follows:</p><formula xml:id="formula_8">o i = −−r i − (r A + r B ) L 1 .<label>(9)</label></formula><p>Therefore, if r i gets more similar to (r A + r B ), the conditioned predicting probability of r i will become larger. Here, we make an implicit assump- tion that if r i is semantically similar to relation path p i : h </p><formula xml:id="formula_9">G(h, r, t|p i ) = E(h, r A , e)E(e, r B , t)p(r|r A , r B ),<label>(10)</label></formula><p>where E(h, r A , e) and E(e, r B , t) measure the probabilities of relational facts (h, r A , e) and (e, r B , t) from text, and p(r|r A , r B ) measures the probability of relation r given relation path (r A , r B ).</p><p>In reality, there are usually multiple relation paths between two entities. Hence, we define the inferring correlation between relation r and sev- eral sentence paths P as,</p><formula xml:id="formula_10">G(h, r, t|P ) = max i G(h, r, t|p i ),<label>(11)</label></formula><p>where we use max operation to filter out those noisy paths and select the most representative path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Model</head><p>Given any entity pair (h, t), those sentences S directly mentioning them and relation paths P be- tween them, we define the global score function with respect to a candidate relation r as, L(h, r, t) = E(h, r, t|S) + αG(h, r, t|P ), <ref type="formula" target="#formula_1">(12)</ref> where E(h, r, t|S) models the correlation between r and (h, t) calculated from direct sentences, G(h, r, t|P ) models the inferring correlation be- tween relation r and several sentence paths P . α equals to (1 − E(h, r, t|S)) times a constant β. This term serves to depict the relative weight be- tween direct sentences and relation paths, since we don't need to pay much attention on extra infor- mation when CNN has already given a confident prediction</p><note type="other">, namely E(h, r, t|S) is large. One of the advantages of this joint model is to alleviate the issue of error propagation. The un- certainty of information from Text Encoder and Relation Path encoder is characterized by its con- fidence, and could be integrated and corrected in this joint model step. Furthermore, since we treat relation paths in a probabilistic way, our model could fully utilize all relation paths, i.e. those al- ways hold and those likely to hold.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization and Implementation Details</head><p>The overall objective function is defined as:</p><formula xml:id="formula_11">J(θ) = (h,r,t) log(L(h, r, t)),<label>(13)</label></formula><p>where the summing runs over the log loss of all entity pairs in text and θ represents the model pa- rameters. To solve this optimization problem, we use mini-batch stochastic gradient descent (SGD) to maximize our objective function. We initial- ize W E with the results from Skip-gram model, and initialize other parameters randomly. We also adopt dropout ( <ref type="bibr" target="#b23">Srivastava et al., 2014</ref>) upon the output layer of CNN. We implement our model using C++. We train our model on Intel(R) Xeon(R) CPU E5-2620, and the training roughly takes half a day. The word embedding and other parameters are updated via back-propagation simultaneously, while the rela- tion path structure is extracted before training and stored afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset</head><p>We build a novel dataset for evaluating relation extraction task. We first describe the most com- monly used previous dataset and then explain the reason and how we construct the new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Previous Datasets &amp; Reasons for New Dataset</head><p>A commonly used benchmark dataset for this task was developed by ( <ref type="bibr" target="#b17">Riedel et al., 2010)</ref>. This dataset was built by aligning Freebase <ref type="bibr">(Dec. 2009</ref> Snapshot) with New York Times corpus (NYT). There are 53 possible relationships between two entities, including a special relation type NA, meaning that there is no relation between head and tail entities. For each relational fact in a filtered Freebase dataset, a sentence from NYT would be regarded as a mention of this relation if both the head and tail entity appear in that sentence.</p><p>While this previous dataset has been frequently used for evaluating relation extraction systems, we observe some limitations of it. First, the relational facts are extracted from a 2009 snapshot of Free- base. Therefore, this dataset is too old to contain many updated facts. This will underestimate the performance of a relation extraction system, since some real-world facts are missing from the dataset and labeled as NA. Second, the relational facts in this dataset are scattered, i.e. there are not suffi- cient relation paths in this dataset, while relational facts in real-world always have connections with each other. Third, Freebase will no longer update after 2016.These limitations mean that this dataset is somewhat improper for evaluating RE systems.</p><p>Although other relation extraction datasets ex- ist, e.g. ACE 1 and ( <ref type="bibr" target="#b7">Hendrickx et al., 2009)</ref>, they are too small to train an effective neural relation extraction model. Moreover, each relational fact in ( <ref type="bibr" target="#b7">Hendrickx et al., 2009</ref>) only corresponds with one sentence, which prevents it from evaluating multi-instance relation extraction systems. Hence, we constructed a novel relation extraction dataset to address these issues, and will make it available to the community. Our dataset contains more updated facts and richer structures of relations, e.g. more relations / relation paths, as compared to existing similar datasets. The dataset is expected to be more simi- lar to real-world cases, and thus be more appropri- ate for evaluating RE systems' performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Construction</head><p>We build the dataset by aligning Wikidata 2 re-lations with the New York Times Corpus (NYT). Wikidata is a large, growing knowledge base, which contains more than 80 million triple facts and 20 million entities. Different from Freebase, Wikidata is still in maintenance and could be eas- ily accessed by APIs. We first pick those en- tities simultaneously appeared in both Wikidata and Freebase, and relational facts associated with them. Then, we filtered out a subset S, reserv- ing those facts associating with the 99 highest fre- quency relations. This results in 4, 574, 665 triples with 1, 045, 385 entities and 99 relations. Next, we align those facts with NYT corpus, following the assumption of distant supervision. For each pair of entities appearing in our S, we traverse the corpus and pick those sentences where both entities appear. These sentences will be re- garded as mentions of this fact, and labeled by this relation type. To simulate noise in the real world, we also add sentences corresponding to "No Relation" entity pairs into our dataset. To get those "No Relation" instances, we first cre- ate a fake knowledge base S − by randomly re- placing the head or tail entities in triples, i.e., S − = {(h , r, t)}∪{(h, r, t )} and then align them with NYT corpus. Finally, we randomly split all those selected sentences into training, validation and testing set, assuring that a relational fact could be only mentioned by sentences in one set. The statistics of our dataset and ( <ref type="bibr" target="#b17">Riedel et al., 2010)</ref> are listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Following the previous work <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref>, we evaluate our model by extracting rela- tional facts from the sentences in test set, and com- pare them with those in Wikidata. We report Pre- cision/Recall curves, Precision@N (P@N) and F1 scores for comparison in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Initialization and Parameter Settings</head><p>In this paper, we use the word2vec tool 3 to pre-train word embeddings on NYT corpus. We keep the words which appear more than 100 times in the corpus as vocabulary. We tune our model on the validation set, using grid search to determine the optimal parameters, which are shown in boldface. We select learning rate for SGD λ ∈ {0.1, 0.01, 0.001}, the sentence em- bedding size d c ∈ {50, 60, · · · , 230, · · · , 300}, the window size k ∈ {1, 2, 3, 4, 5}, and the mini-batch size B ∈ {40, 160, 640}.</p><p>Be- sides, we select the relation embeddings size d R ∈ {5, 10, · · · , 40, · · · , 60}, and the weight for information from relation paths β ∈ {0.01, 0.1, 0.2, 0.5, 1, · · · , 5}. For other parame- ters which have little effect on the system perfor- mance, we follow the settings used in ( <ref type="bibr" target="#b30">Zeng et al., 2015)</ref>: word embedding size d w is 50, position embedding size d p is 5 and dropout rate p is 0.5. For training, the iteration number over all training data is 25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness of Incorporating Relation Paths</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Precision-Recall Curve Comparison</head><p>To demonstrate the effect of our approach, we empirically compare it with other neural relation extraction methods via held-out evaluation. <ref type="formula">(1)</ref> CNN+rand represents the CNN model reported in ( <ref type="bibr" target="#b31">Zeng et al., 2014)</ref>. <ref type="formula" target="#formula_1">(2)</ref> CNN+max represents the CNN model with multi-instances learning used in ( <ref type="bibr" target="#b30">Zeng et al., 2015)</ref>. <ref type="formula" target="#formula_2">(3)</ref> Path+rand/max is our model with those two multi-instance settings. We implement (1), (2) by ourselves which achieve comparable results as reported in those papers.   <ref type="figure" target="#fig_3">3</ref> shows the precision/recall curves of all methods. From the figure, we can observe that: (1) Our methods outperform their counterpart meth- ods, achieving higher precision over almost en- tire range of recall. They also enhance recall by 20% without decrease of precision. These results prove the effectiveness of our approach. We no- tice that the improvements of our methods over  baselines are relatively small at small recall value, which corresponds to high predicting confidence. This phenomenon is intuitive since our joint model could dynamically leverage the importance of di- rect sentence and relation paths, and tends to trust the Text Encoder when the confidence is high. <ref type="formula" target="#formula_1">(2)</ref> As the recall increases, our models exhibit larger improvements compared with CNN in terms of percentage. This is due to the fact that sometimes CNNs cannot extract reliable information from di- rect sentences, while our methods could alleviate this issue by considering more information from inference chains, and thus still maintain high pre- cision. (3) Both CNN+max and Path+rand are variations of CNN+rand, aiming to alleviate the problem of noisy data. We see that Path+rand out- performs CNN+max over all range, which indi- cates that considering path information is a better way to solve this issue. Meanwhile, combining paths information and max operation, Path+max, gives the best performance. (4) Path+rand shows a larger improvement over CNN+rand, compared with those of Path+max and CNN+max. This fur- thermore proves the effectiveness of considering relation path information: CNN+rand has much more severe problem suffering from noise, so us- ing our method to incorporate paths information to alleviate this issue could perform better.</p><formula xml:id="formula_12">Test Settings (Noise) 75% 85% 95% P@N (%) 10% 20% 50% F1 10% 20% 50% F1 10% 20% 50% F1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison on Long Tail Situation</head><p>Ns  Real-world data follows long-tail distribution (power law). In the testing set, we also observe a fact that about 40% triple facts appear only in single sentence, and thus a multi-instance rela- tion extraction system, e.g. CNN+max, could only rely on limited information and the multi-instance mechanism will not work well. Our system, on the contrary, can still utilize information from relation paths in this case, and is expected to perform much better in the long tail situation.</p><p>We evaluate the models on different parts of the long-tail distribution. To get testing instances from different parts of the distribution, we extract all the triple facts appearing less than N s sentences in the testing set, and those sentences associating with them. All text related to 'No Relation' entity pair are also reserved in order to simulate noise. We then evaluate different models on those sampled testing set, and report the results of F1 score in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>From <ref type="table" target="#tab_5">Table 3</ref>, we could observe that: (1) Incor- porating relation paths is indeed effective in pre- dicting relations, and our models have significant improvements compared with the baselines. (2) Path+max indeed has larger improvements over CNN+max when N s is small, which is consistent with our previous expectation. Also notice that the gap between Path+rand and CNN+rand is rel- atively constant. This is due to the fact that both these methods only use one random sentence, re- gardless of how many sentences there are associ- ating with an entity pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Robustness under Different</head><p>Percentages of Noise</p><p>In the task of relation extraction, there are lots of noise in text which may hurt the model's per- formance. More specifically, "No Relation" entity pair is a kind of noise, since "No Relation" could actually contain many unknown relation types, and thus might confuse the relation extraction sys- tems. Therefore, it is important to verify the ro- bustness of our model in the presence of massive noise. Here, we evaluate those models in three set- tings, with the same relational facts and different percentages of "No Relation" sentences in the test- ing sets. In each experiment, we extract top 20,000 predicting relational facts according to the model's predicting scores, and report the precision @top 10%, @top 20%, @top 50% and F1 score in <ref type="table" target="#tab_3">Table  2</ref>.</p><p>From the table, we can see that: (1) In terms of all evaluations, our models achieve the best perfor-Relation Text Path #1 mother Rebecca gave birth to twin sons, Esau and Jacob, ... Path #2 has child ...Isaac's marriage to Rebecca, by whom he has two sons, Esau and jacob, ... Test spouse ... Isaac and Rebecca and the female and male evil spirits ... Path #1 shares border with ... in Somalia, ... soldiers and marines stationed in neighboring Djibouti ... Path #2 shares border with ... Ethiopia have had the effect of making neighboring Djibouti ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test shares border with</head><p>The next day, Ethiopia struck, its military pushing deep into Somalia ... Since we utilize more information to make predic- tions, our model is more robust to the presence of mass noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effectiveness of Learned Features in Zero-Shot Scenario</head><p>It has been proved that CNN could automat- ically extract useful features, encoding syntactic and semantic meaning of sentences. These fea- tures are sometimes fed to subsequent models to solve other tasks. In this experiment, we demon- strate the effectiveness of the extracted features from our model. Since CNN-based models have already succeeded in extracting relations from sin- gle sentences, we set our experiment in a new scenario: predicting the relation between entities which have not appeared in the same sentence.</p><p>A natural approach is to build a relation path between this zero-shot entity pair. We assume that we can make a prediction about (h, t), once we know the information of (h, e) and (e, t). There- fore, we build the training set by extracting all such relation paths and their sentences from train- ing text, and similar for testing set. To test the effectiveness of features, we encode sentences by CNN+rand/max, Path+rand/max respectively, and then feed the concatenation of sentence vectors to a logistic classifier.  From <ref type="table" target="#tab_8">Table 5</ref>, we could observe that: (1) The result using CNN+rand features is comparable to the result using CNN+max features. It shows that using max operation to train the features does not greatly improve the features' behavior in this task, even though it performs well in previous tasks. The reason is that, both CNN+rand and CNN+max only encode the information from a single sentence, and they are unable to capture the correlations between relations. (2) Feature from Path+rand/max shows its effectiveness over those from other methods. It indicates that our method is able to model the correlations between relations, while also keeps the syntactic and semantic mean- ing of a sentence. Therefore, the features extracted from Path+rand/max are useful for a wider range of applications, especially in those tasks which need the information from relations. <ref type="table" target="#tab_6">Table 4</ref> shows some representative inference chains from the testing dataset. These examples can not be predicted correctly by the original CNN model, but are later corrected using our model. We show the test instances and their correct relations, as well as the inference chains the model uses. In the first example, the test sentence does not di- rectly express the relation spouse, the proof of this relation appears in a further context in NYT. However, using path#1 and path#2, we could eas- ily infer that Rebecca and Issac are spouse. The second example doesn't show the relation either. But with the help of intermediate entity, Dijibouti, our model predicts that Somalia shares the bor- der with Ethiopia. Note that this inference chain doesn't always hold, but our model could capture this uncertainty well via a softmax operation. In general, our model can utilize common sense from inference chains. It helps make correct predictions even if the inference is not explicit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this paper, we propose a neural relation ex- traction model which encodes the information of relation paths. As compared to existing neural re- lation extraction models, our model is able to uti- lize the sentences which contain both two target entities and only one target entity and is more ro- bust for noisy data. Experimental results on real- world datasets show that our model achieves sig- nificant and consistent improvements on relation extraction as compared with baselines.</p><p>In the future, we will explore the following di- rections: (1) We will explore the combination of relation paths from both plain texts and KBs for relation extraction. (2) We may take advantages of probabilistic graphical model or recurrent neu- ral network to encode more complicated correla- tions between relation paths, e.g. multi-step rela- tion paths, for relation extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of our neural relation extraction model with relation paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of CNN used for text encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the embedding r i will be closer to the relation path embedding (r A + r B ). Finally, for this relation path p i : h r A −→ e r B −→ t, we define an relation-path score function,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Aggregate precision/recall curve for CNN+rand, CNN+max, Path+rand, Path+max.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig.</head><label></label><figDesc>Fig. 3 shows the precision/recall curves of all methods. From the figure, we can observe that: (1) Our methods outperform their counterpart methods, achieving higher precision over almost entire range of recall. They also enhance recall by 20% without decrease of precision. These results prove the effectiveness of our approach. We notice that the improvements of our methods over</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Path Encoder Spouse Sentences</head><label></label><figDesc></figDesc><table>Jack loves Mary. 

Jack is the father of Alice. Alice is kissing her mother, Mary. 

CNN 

Multi-Instances 
Learning 

CNN 

Multi-Instances 
Learning 

CNN 

Multi-Instances 
Learning 

Father 
Mother 

Text Encoder 

CNN Prediction 

Final Prediction 

(Father) 
(Mother) 

Path Encoder 

Spouse 

Direct Sentence 
Relation Path 

Final Prediction 

CNN Prediction 

Text Encoder 

Sentences 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : P@N and F1 for relation extraction in texts containing different percentage of no-relation facts.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 : F1 score for long tail situation.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Some representative examples of inference chians in NYT corpus. The bold is target entities. 

mance as compared with other methods in all test 
settings. It demonstrates the effectiveness of our 
approach. (2) Even though the scores of all mod-
els drop as the increasing of noise, we find that 
Path+rand/max's scores decrease much less than 
their counterparts. This result proves the effective-
ness of taking inference chains into consideration. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Accuracy of different models in zero-shot 
situation. 

</table></figure>

			<note place="foot" n="1"> https://catalog.ldc.upenn.edu/ LDC2006T06 2 https://www.wikidata.org/</note>

			<note place="foot" n="3"> https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing biological knowledge bases by extracting information from text sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Kumlien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISMB</title>
		<meeting>ISMB</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1999</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01426</idno>
		<title level="m">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving learning and inference in a large knowledge-base using latent syntactic cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">Pratim</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="833" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Traversing knowledge graphs in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="318" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLHLT</title>
		<meeting>ACLHLT</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reading the web with learned syntactic-semantic inference rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amarnag</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1017" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling relation paths for representation learning of knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Huanbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML-PKDD</title>
		<meeting>ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maıra</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classifying relations by ranking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cıcero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="626" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. Citeseer</title>
		<meeting>ACL. Citeseer</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW</title>
		<meeting>WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Knowledge representation via joint learning of sequential text and knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07075</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classifying relations via long short term memory networks along shortest dependency paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1785" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhunchen</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07602</idno>
		<title level="m">Jointly extracting relations with class ties via effective deep ranking</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
