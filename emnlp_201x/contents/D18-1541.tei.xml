<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Kasewa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="4977" to="4983"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>4977</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic grammatical errors would be difficult, one could learn the distribution of naturally-occurring errors and attempt to introduce them into other datasets. Initial work on inducing errors in this way using statistical machine translation has shown promise; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straightforward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5% F 0.5 score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 F 1 score, indicating that our model generates mostly human-like instances.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is an ever-growing number of people learn- ing English as a second language; providing them with quick feedback to facilitate their learning is a crucial, labour-intensive endeavour. Part of this process is identifying and correcting grammati- cal errors, and several computational techniques have been developed to automate it ( <ref type="bibr" target="#b6">Rozovskaya and Roth, 2014;</ref><ref type="bibr">Junczys-Dowmunt and Grundkiewicz, 2016</ref>). For example, given an erroneous sentence "I wanted to goes to the beach", the grammatical error correction task is to output the valid sentence "I wanted to go to the beach". The task can be cast as a two-stage process, de- tection and correction, which can either be per- formed sequentially ( <ref type="bibr" target="#b12">Yannakoudakis et al., 2017)</ref>, or jointly <ref type="bibr">(Napoles and Callison-Burch, 2017)</ref>.</p><p>Automated error correction performance is ar- guably still too low for practical considera- tion, perhaps limited by the amount of training data ( ). High quality annotations are expensive to procure, and foreign language learners and commercial entities may feel uncom- fortable granting access to their data. Instead, one could attempt to supplement existing manual an- notations with synthetic instances. Such artificial samples are beneficial only when they share struc- ture with the true distribution from which human errors are generated. Generative Adversarial Net- works ( <ref type="bibr">Goodfellow et al., 2014</ref>) could be used for this purpose, but they are difficult to train, and re- quire a large collection of sentences that are incor- rect. One might attempt self-training ( <ref type="bibr">McClosky et al., 2006</ref>), where new instances are generated by applying a trained model to unannotated data, using high-confidence predictions as ground truth labels. However, in such a scheme, the expectation is that the unlabelled text already contains errors, which is not usually the case for most freely avail- able text such as Wikipedia articles as they strive towards correctness.</p><p>In place of using machine translation (MT) to correct grammatical mistakes <ref type="bibr" target="#b14">(Yuan and Felice, 2013;</ref><ref type="bibr">Junczys-Dowmunt and Grundkiewicz, 2014;</ref><ref type="bibr" target="#b13">Yuan and Briscoe, 2016)</ref>, one might con- sider swapping the input and output streams, and instead learn to induce errors into error-free text, for the purpose of creating a synthetic training dataset <ref type="bibr">(Felice and Yuan, 2014</ref>). Recently,  used a statistical MT (SMT) sys- tem to induce errors into error-free text. Build- ing on this work, and leveraging recent advances in neural MT (NMT), we used an off-the-shelf at- tentive sequence-to-sequence model ( <ref type="bibr">Britz et al., 2017)</ref>, eliminating the need of specialised soft-ware such as a phrase-table generator, decoder, and part-of-speech tagger. We created multi- ple synthetic datasets from in-domain and out- of-domain sources, and found that stochastic to- ken sampling, and pruning redundant and low- likelihood sentences, were helpful in generating meaningful corruptions. Using the artificial sam- ples thus generated, we improved upon detec- tion results with simply a vanilla bi-directional LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref>. Us- ing a more powerful model, we established new state-of-the-art results, that improve on previously published F 0.5 scores by over 5%. Addition- ally, we confirm that our generated instances are human-like, as an annotator identifying generated sentences achieved a maximum F 1 score of 39.39.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In computer vision, images are blurred, rotated, or otherwise deformed inexpensively to create new training instances ( <ref type="bibr" target="#b10">Wang and Perez, 2017)</ref>, be- cause such manipulation does not significantly alter the image semantics. Similar coarse pro- cesses do not work in NLP since mutating even a single letter or a word can change a sentence's meaning, or render it nonsensical. Nonetheless, <ref type="bibr" target="#b9">Vinyals et al. (2015)</ref> employed a kind of self- training where they use noisy predictions for un- labelled instances output by existing state-of-the- art parsers as ground-truth labels, and improved syntactic parsing performance. <ref type="bibr" target="#b8">Sennrich et al. (2016)</ref> synthesised training instances by round- trip-translating a monolingual corpus with weaker versions of an NMT learner, and used them to im- prove the translation. <ref type="bibr">Bouchard et al. (2016)</ref> de- veloped an efficient algorithm to blend generated and true data for improving generalisation.</p><p>Grammar correction is a well-studied task in NLP, and early systems were rule-based pattern recognisers <ref type="bibr">(Macdonald, 1983)</ref> and dictionary- based linguistic analysis engines <ref type="bibr" target="#b4">(Richardson and Braden-Harder, 1988)</ref>. Later systems used sta- tistical approaches, addressing specific kinds of errors such as article insertion <ref type="bibr">(Knight et al., 1994)</ref> and spelling correction <ref type="bibr">(Golding and Roth, 1996)</ref>. Most recently, architectural innovations in neural sequence labelling (  raised error detection performance through improved ability to process unknown words and jointly learning a language model.</p><p>Early efforts for artificial error generation in- cluded generating specific types of errors, such as mass noun errors <ref type="bibr">(Brockett et al., 2006</ref>) and arti- cle errors <ref type="bibr" target="#b5">(Rozovskaya and Roth, 2010)</ref>, and lever- aging linguistic information to identify error pat- terns and transfer them onto grammatically correct text <ref type="bibr">(Foster and Andersen, 2009;</ref><ref type="bibr" target="#b14">Yuan and Felice, 2013)</ref>. <ref type="bibr">Imamura et al. (2012)</ref> investigated meth- ods to generate pseudo-erroneous sentences for er- ror correction in Japanese. Recently,  corrupted error-free text using SMT to cre- ate training instances for error detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neural error generation</head><p>To learn to introduce errors, we use an off-the- shelf attentive sequence-to-sequence neural net- work ( <ref type="bibr">Bahdanau et al., 2014)</ref>. Given an input se- quence, the encoder generates context vectors for each token. Then, the attention mechanism and the decoder work in tandem to emit a distribution over the target vocabulary. At every decoder time- step, the encoder context vectors are scored by the attention mechanism, and a weighted sum is sup- plied to the decoder, along with its propagated in- ternal state and last output symbol.</p><p>Corruption: Tokens from this distribution are sampled at every decoder time-step, either by argmax (AM), which emits the most likely word, or by a stochastic alternative such as temperature sampling (TS) as argmax cannot be relied on to generate rare words. A temperature parameter τ &gt; 0 sharpens or softens the distribution:</p><formula xml:id="formula_0">˜ p i = f τ (p) i = p 1 τ i j p 1 τ j</formula><p>where i are the components of the probability dis- tribution corresponding to words in the vocabu- lary. As one interpolates τ from 0 to 1, the be- haviour of˜pof˜ of˜p transitions from argmax to p, control- ling the diversity of the generated tokens. The sentence generated by TS might be a low probability sequence from the joint conditional distribution P (v|u), where u is the input sentence and v is the output sentence. One way around this is to use beam search (BS), which checks the like- lihood of every possible continuation of a sentence fragment, and maintains a list of the n best trans- lations generated up to the current time-step. AM, TS, and BS are indicative of the trade-off between increasing levels of model flexibility at the cost of computation; we compare them to assess whether</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Corruption</head><p>She promised to turn over a new leaf. She promissed to turn over a new leaf. At the moment I'm in Spain. During the moment I'm in Spain.  the additional computations were helpful in creat- ing high-quality synthetic instances.</p><p>Post-processing: Original and corrupted sen- tences are aligned at a word-level using Leven- shtein distance. Using the minimal alignment, words in the corrupted sentence are labelled cor- rect, 'c', or incorrect, 'i', as follows:</p><p>If the word is not aligned with itself, then 'i'. Else, if following a gap, then 'i', as at this point a human reader would notice that there is a word missing in the sentence. Else, if it is the last word, but it is not aligned to the last word of the source sentence, then 'i', as a human would realise that this sentence ends abruptly, Else, 'c'.</p><p>These token-labelled corrupted sentences now form an artificial dataset for training an error de- tector. Duplicate instances and corrupted sen- tences with more than 5 errors were dropped to remove noise from the downstream training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluated our approach on the First Certificate of English (FCE) error detection dataset <ref type="bibr" target="#b3">(Rei and Yannakoudakis, 2016)</ref>, as well as on two human- annotated test sets (CoNLL1, CoNLL2) from the CoNLL 2014 shared task ( <ref type="bibr">Ng et al., 2014</ref>). The CoNLL data sets pose a unique challenge; as they are different in style and domain from FCE, we have no matching training data. We compared the effect of different neural generation proce- dures (AM, TS, BS) and contrasted the down- stream performance of a bidirectional LSTM with an elaborate sequence labeller. ber of candidates, and n is the maximum length of a sentence. Empirically, BS with b = 11 took a factor of 11.3 more time than AM. Examples of generated errors are provided in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Error detection: We compare two error detec- tion models: a vanilla bi-directional LSTM (BiL- STM) <ref type="bibr" target="#b7">(Schuster and Paliwal, 1997)</ref>, and the state- of-the-art sequence labeller (SL) neural network used by . These models were trained on the binary-labelled FCE training set augmented with the corrupted instances. Wher- ever no model is explicitly stated, the SL model was used. During training, we alternate between the annotated FCE dataset and the synthetic col- lection. This alternating protocol prevents over- fitting on FCE; once it shifts back, it reinforces connections made from the helpful synthetic cor- ruptions while forgetting about the noisy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The results for our baselines and data augmenta- tion strategies can be found in Figure 2: Training with increasing amounts of cor- rupted data from FCE and SW.</p><p>able to induce useful errors into a corpus with a large unseen vocabulary and different syntactic bi- ases, and this in turn proved valuable for detect- ing errors in a third domain, suggesting that our method can transfer learned distributions across stylistic genres. Using EVP as a standard source, <ref type="figure" target="#fig_0">Figure 1</ref> illus- trates the variance of the different sampling meth- ods. All generation methods yield corruptions that significantly improve test performance, with in- stances sampled by beam-search consistently out- performing the alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Error distribution</head><p>The original FCE dataset was annotated using the error taxonomy specified in <ref type="bibr">Nicholls (2003)</ref>, and contains 75 unique error codes. We annotated samples of EVP corrupted by all three sampling methods, at a reduced resolution, to compare the distribution of errors across FCE and the synthetic corpora. These are presented in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>At a high level, NMT generates errors more of- ten among more common parts-of-speech, favour- ing errors in verbs and nouns, rather than in ad- verbs and conjunctions. It did not make spelling errors as often as in the source dataset; this is likely because it only observed the specific spelling errors present in FCE, and as the vocab- ulary is restricted to that dataset, it does not en- counter those words as frequently in EVP, and thus rarely makes the same spelling mistakes.</p><p>Additionally, the differences in these distribu- tions can partially be attributed to the implicit dif- ferences between us and the annotators of FCE.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with human errors</head><p>To check if the synthetic instances passed for human-like, we mixed 50 generated sentences among an equal number of actual ungrammatical instances from FCE-dev and tasked a human eval- uator to identify the artificial statements, in a sim- ple Turing-style test. We created three such sets, one for each of our sampling techniques, and the test subject aimed to identify synthetic samples with high confidence. Results of this test are pre- sented in <ref type="table">Table 4</ref>.</p><p>The high precision but low recall scores suggest that while it is still possible to spot some corrup- tions that are quite clearly artificial, the bulk of our samples do not betray their synthetic nature and are indistinguishable from naturally occurring er- roneous sentences. In order to fairly compare our work with earlier results, we intended to conduct such a test for sentences generated by the SMT of . Unfortunately, we were only able to source corruptions of FCE-train via this method; therefore, we decided not to perform this test as its results cannot be compared to ours.  <ref type="table">Table 4</ref>: Results of a Turing-style test, where a sub- ject was asked to distinguish between real and fake sentences, sampled from each of the different gen- erated corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and future work</head><p>We presented a novel data augmentation tech- nique for grammatical error detection using neu- ral machine translation to learn the distribution of language-learner errors, and induce such er- rors into grammatically correct text. We explored several different variants of sampling to improve the quality of our synthetic errors. After creat- ing artificial training instances with an off-the- shelf NMT, we bettered previous state-of-the-art results on the canonical test with even a basic BiL- STM, and established a new state of the art using a stronger model. Additionally, we demonstrated that we were able to leverage corruptions of an out-of-domain dataset to set new benchmarks on separate, also out-of-domain tests, without specif- ically optimising for either. Our work indicates that neural error genera- tion warrants further investigation with different datasets and architectures, both for error detec- tion and error correction. Among possible fu- ture work is using generative adversarial networks as corruption engines, and developing better se- quence alignment methods. Some preliminary re- sults with simple corruptions using word substitu- tion and word dropout <ref type="bibr">(Iyyer et al., 2015</ref>) appear to be promising, and may feature as components of a future corruption system. Finally, one could use such artificial error-prone corpora as source text for self-training an error detection system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 1</head><label>1</label><figDesc>Figure 1: Improvements using three different methods of generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Example sentences generated by our NMT pipeline.</head><label>1</label><figDesc></figDesc><table>Data augmentation strategy 
Model 
FCE (dev) FCE CoNLL1 CoNLL2 

Rei et al. (2017) FCE P AT + EVP P AT 
SL 
-47.8 
19.5 
28.5 
Rei et al. (2017) FCE SM T + EVP SM T 
SL 
-48.4 
19.7 
28.4 
Rei et al. (2017) FCE SM T +P AT + EVP SM T +P AT SL 
-49.1 
21.9 
30.1 

None 
BiLSTM 
47.9 43.6 
16.6 
24.3 
FCE T S 
BiLSTM 
51.2 47.1 
19.7 
28.9 
EVP BS 
BiLSTM 
52.1 50.1 
20.8 
29.0 
SW T S 
BiLSTM 
51.5 50.6 
24.2 
31.7 
FCE AM +T S +EVP AM +T S 
BiLSTM 
52.3 50.4 
22.1 
30.8 

None 
SL 
52.5 48.2 
17.4 
25.5 
FCE T S 
SL 
54.8 49.9 
20.9 
29.2 
EVP BS 
SL 
55.2 54.6 
23.3 
31.4 
SW T S 
SL 
53.8 52.7 
26.8 
34.3 
FCE AM +T S +EVP AM +T S 
SL 
56.9 54.6 
25.1 
33.0 
FCE AM +T S +EVP AM +T S +SW AM +T S 
SL 
56.5 55.6 
28.3 
35.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>F 0.5 scores on various tests contrasted with published results and unaugmented baseline models.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Augmented 
with our NMT generated data, even our vanilla 
downstream BiLSTM outperforms the SMT+PAT 
artificial error augmentation approach of Rei et al. 
(2017), indicating that our process better gener-
alises the error information in the source dataset. 
Using the more powerful SL network bests the 
previous state of the art by over 5% on the FCE 
test. Most intriguingly, we note a significant im-
provement for the CoNLL tests using corruptions 
from out-of-domain SW. Figure 2 illustrates how 
we gain performance on these tests with increas-
ing amounts of corrupted SW, which does not hold 
true for corrupted FCE. This shows that we were 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Error distribution across FCE and manu-
ally annotated samples of artificial data. Spelling 
errors are a % of all errors, while Part-of-speech, 
and Remedy Type are compared within their own 
categories to sum to 100%. 

</table></figure>

			<note place="foot" n="1"> https://github.com/google/seq2seq 2 https://github.com/skasewa/wronging</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Marek Rei and Mariano Felice for grant-ing access to their data and code. Also, we would like to thank the anonymous reviewers and Jo-hannes Welbl for valuable feedback and discus-sions. This work was supported by an Allen Dis-tinguished Investigator Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2121" to="2130" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attending to characters in neural sequence labeling models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gamal</forename><surname>Crichton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Artificial error generation with machine translation and syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the 12th Workshop on Innovative Use of NLP for Building Educational Applications<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="287" to="292" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Compositional sequence labeling models for error detection in learner writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The experience of developing a large-scale natural language text processing system: Critique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Braden-Harder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Conference on Applied Natural Language Processing</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training paradigms for correcting errors in grammar and usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">Roth</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human language technologies: The 2010 annual conference of the north american chapter of the association for computational linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="154" to="162" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a state-of-the-art grammatical error correction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Rozovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="419" to="434" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading esol texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural sequencelabelling models for grammatical error correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Øistein</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grammatical error correction using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="380" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constrained grammatical error correction using statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariano</forename><surname>Felice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning: Shared Task<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
