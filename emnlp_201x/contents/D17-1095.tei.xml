<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An empirical study on the effectiveness of images in Multimodal Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">TCTS Lab</orgName>
								<orgName type="institution">University of Mons</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Dupont</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">TCTS Lab</orgName>
								<orgName type="institution">University of Mons</orgName>
								<address>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An empirical study on the effectiveness of images in Multimodal Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="910" to="919"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In state-of-the-art Neural Machine Translation (NMT), an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions that they describe. In this paper, we compare several attention mechanism on the multimodal translation task (English, image → German) and evaluate the ability of the model to make use of images to improve translation. We surpass state-of-the-art scores on the Multi30k data set, we nevertheless identify and report different misbehavior of the machine while translating.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In machine translation, neural networks have at- tracted a lot of research attention. Recently, the attention-based encoder-decoder framework <ref type="bibr" target="#b22">(Sutskever et al., 2014;</ref>) has been largely adopted. In this approach, Recurrent Neural Networks (RNNs) map source sequences of words to target sequences. The attention mech- anism is learned to focus on different parts of the input sentence while decoding. Attention mecha- nisms have shown to work with other modalities too, like images, where their are able to learn to attend the salient parts of an image, for instance when generating text captions ( <ref type="bibr" target="#b25">Xu et al., 2015</ref>). For such applications, Convolutional Neural Net- works (CNNs) such as Deep Residual ( <ref type="bibr" target="#b11">He et al., 2016</ref>) have shown to work best to represent im- ages.</p><p>Multimodal models of texts and images em- power new applications such as visual question an- swering or multimodal caption translation. Also, the grounding of multiple modalities against each other may enable the model to have a better under- standing of each modality individually, such as in natural language understanding applications.</p><p>In the field of Machine Translation (MT), the ef- ficient integration of multimodal information still remains a challenging task. It requires combining diverse modality vector representations with each other. These vector representations, also called context vectors, are computed in order the capture the most relevant information in a modality to out- put the best translation of a sentence.</p><p>To investigate the effectiveness of informa- tion obtained from images, a multimodal machine translation shared task (  has been addressed to the MT community <ref type="bibr">1</ref> . The best results of NMT model were those of <ref type="bibr" target="#b12">Huang et al. (2016)</ref> who used LSTM fed with global visual features or multiple regional visual features fol- lowed by rescoring. Recently, <ref type="bibr" target="#b3">Calixto et al. (2017)</ref> proposed a doubly-attentive decoder that outper- formed this baseline with less data and without rescoring.</p><p>Our paper is structured as follows. In section 2, we briefly describe our NMT model as well as the conditional GRU activation used in the decoder. We also explain how multi-modalities can be im- plemented within this framework. In the following sections (3 and 4), we detail three attention mech- anisms and explain how we tweak them to work as well as possible with images. Finally, we report and analyze our results in section 5 then conclude in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural Machine Translation</head><p>In this section, we detail the neural machine trans- lation architecture by , im- plemented as an attention-based encoder-decoder framework with recurrent neural networks ( §2.1). We follow by explaining the conditional GRU layer ( §2.2) -the gating mechanism we chose for our RNN -and how the model can be ported to a multimodal version ( §2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-based NMT</head><p>Given a source sentence X = (x 1 , x 2 , . . . , x M ), the neural network directly models the condi- tional probability p(Y |X) of its translation Y = (y 1 , y 2 , . . . , y N ). The network consists of one en- coder and one decoder with one attention mecha- nism. The encoder computes a representation C for each source sentence and a decoder generates one target word at a time and by decomposing the following conditional probability :</p><formula xml:id="formula_0">log p(Y |X) = n t=1 log p(y t |y &lt; t, C) (1)</formula><p>Each source word x i and target word y i are a col- umn index of the embedding matrix E X and E Y . The encoder is a bi-directional RNN with Gated Recurrent Unit (GRU) layers <ref type="bibr" target="#b5">(Chung et al., 2014;</ref>, where a forward RNN − → Ψ enc reads the input sequence as it is ordered (from x 1 to x M ) and calculates a sequence of forward hidden states (</p><formula xml:id="formula_1">− → h 1 , − → h 2 , . . . , − → h M ). A backward RNN</formula><p>← − Ψ enc reads the sequence in the reverse order (from x M to x 1 ), resulting in a sequence of backward hidden states (</p><formula xml:id="formula_2">← − h M , ← − h M −1 , . . . , ← − h 1 ).</formula><p>We obtain an annotation for each word x i by con- catenating the forward and backward hidden state</p><formula xml:id="formula_3">h t = [ − → h t ; ← − h t ]</formula><p>. Each annotation h t contains the summaries of both the preceding words and the following words. The representation C for each source sentence is the sequence of annotations</p><formula xml:id="formula_4">C = (h 1 , h 2 , . . . , h M ).</formula><p>The decoder is an RNN that uses a condi- tional GRU (cGRU, more details in §2.2) with an attention mechanism to generate a word y t at each time-step t. The cGRU uses it's previous hidden state s t−1 , the whole sequence of source annotations C and the previously decoded symbol y t−1 in order to update it's hidden state s t :</p><formula xml:id="formula_5">s t = cGRU (s t−1 , y t−1 , C)<label>(2)</label></formula><p>In the process, the cGRU also computes a time- dependent context vector c t . Both s t and c t are further used to decode the next symbol. We use a deep output layer ( <ref type="bibr" target="#b17">Pascanu et al., 2014</ref>) to com- pute a vocabulary-sized vector :</p><formula xml:id="formula_6">o t = L o tanh(L s s t + L c c t + L w E Y [y t−1 ]) (3)</formula><p>where</p><formula xml:id="formula_7">L o , L s , L c , L w are model parameters.</formula><p>We can parameterize the probability of decoding each word y t as:</p><formula xml:id="formula_8">p(y t |y t−1 , s t , c t ) = Softmax(o t )<label>(4)</label></formula><p>The initial state of the decoder s 0 at time-step t = 0 is initialized by the following equation :</p><formula xml:id="formula_9">s 0 = f init (h M )<label>(5)</label></formula><p>where f init is a feedforward network with one hid- den layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional GRU</head><p>The conditional GRU 2 consists of two stacked GRU activations called REC 1 and REC 2 and an attention mechanism f att in between (called ATT in the footnote paper). At each time-step t, REC1 firstly computes a hidden state proposal s t based on the previous hidden state s t−1 and the previ- ously emitted word y t−1 :</p><formula xml:id="formula_10">z t = σ W z E Y [y t−1 ] + U z s t−1 r t = σ W r E Y [y t−1 ] + U r s t−1 s t = tanh W E Y [y t−1 ] + r t (U s t−1 ) s t =(1 − z t ) s t + z t s t−1<label>(6)</label></formula><p>Then, the attention mechanism computes c t over the source sentence using the annotations se- quence C and the intermediate hidden state pro- posal s t :</p><formula xml:id="formula_11">c t = f att C, s t (7)</formula><p>Finally, the second recurrent cell REC 2 , com- putes the hidden state s t of the cGRU by looking at the intermediate representation s t and context vector c t :</p><formula xml:id="formula_12">z t =σ W z c t + U z s t r t =σ W r c t + U r s t s t =tanh W c t + r t (U s t ) s t =(1 − z t ) s t + z t s t (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multimodal NMT</head><p>Recently, <ref type="bibr" target="#b3">Calixto et al. (2017)</ref> proposed a dou- bly attentive decoder (referred as the "MNMT" model in the author's paper) which can be seen as an expansion of the attention-based NMT model proposed in the previous section. Given a se- quence of second a modality annotations I = (a 1 , a 2 , . . . , a L ), we also compute a new context vector based on the same intermediate hidden state proposal s t :</p><formula xml:id="formula_13">i t = f att I, s t (9)</formula><p>This new time-dependent context vector is an ad- ditional input to a modified version of REC2 which now computes the final hidden state s t us- ing the intermediate hidden state proposal s t and both time-dependent context vectors c t and i t :</p><formula xml:id="formula_14">z t =σ W z c t + W z i t + U z s t r t =σ W r c t + W r i t + U r s t s t =tanh W c t + W i t + r t (U s t ) s t =(1 − z t ) s t + z t s t (10)</formula><p>The probabilities for the next target word (from equation 3) also takes into account the new context vector i t :</p><formula xml:id="formula_15">L o tanh(L s s t + L c c t + L i i t + L w E Y [y t−1 ]) (11)</formula><p>where L i is a new trainable parameter. In the field of multimodal NMT, the second modality is usually an image computed into fea- ture maps with the help of a CNN. The annotations a 1 , a 2 , . . . , a L are spatial features (i.e. each anno- tation represents features for a specific region in the image) . We follow the same protocol for our experiments and describe it in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attention-based Models</head><p>We evaluate three models of the image attention mechanism f att of equation 7. They have in com- mon the fact that at each time step t of the de- coding phase, all approaches first take as input the annotation sequence I to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word y t . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mech- anism, we propose two hand-picked illustrations showing where the attention is placed in an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Soft attention</head><p>Soft attention has firstly been used for syntactic constituency parsing by <ref type="bibr" target="#b23">Vinyals et al. (2015)</ref> but has been widely used for translation tasks ever since. One should note that it slightly differs from  where their attention takes as input the previous decoder hidden state instead of the current (intermediate) one as shown in equation 7. This mechanism has also been successfully investigated for the task of image description generation ( <ref type="bibr" target="#b25">Xu et al., 2015</ref>) where a model generates an image's description in natural language. It has been used in multimodal translation as well <ref type="bibr" target="#b3">(Calixto et al., 2017)</ref>, for which it constitutes a state-of-the-art.</p><p>The idea of the soft attentional model is to consider all the annotations when deriving the context vector i t . It consists of a single feed- forward network used to compute an expected alignment e t between modality annotation a l and the target word to be emitted at the current time step t. The inputs are the modality annotations and the intermediate representation of REC1 s t :</p><formula xml:id="formula_16">e t,l = v T tanh(U a s t + W a a l )<label>(12)</label></formula><p>The vector e t has length L and its l-th item con- tains a score of how much attention should be put on the l-th annotation in order to output the best word at time t. We compute normalized scores to create an attention mask α t over annotations:</p><formula xml:id="formula_17">α t,i = exp(e t,i ) L j=1 exp(e t,j )<label>(13)</label></formula><formula xml:id="formula_18">i t = L i=1 α t,i a i<label>(14)</label></formula><p>Finally, the modality time-dependent context vec- tor i t is computed as a weighted sum over the an- notation vectors (equation 14). In the above ex- pressions, v T , U a and W a are trained parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hard Stochastic attention</head><p>This model is a stochastic and sampling-based process where, at every timestep t, we are making a hard choice to attend only one annotation. This corresponds to one spatial location in the image. The mechanism f att is now a function that returns a sampled intermediate latent variables γ t,i based upon a multinouilli distribution parameterized by α:</p><formula xml:id="formula_19">γ t ∼ Multinoulli({α 1,...,L })<label>(15)</label></formula><p>where γ t,i an indicator one-hot variable which is set to 1 if the i-th annotation (out of L) is the one used to compute the context vector i t :</p><formula xml:id="formula_20">p(γ t,i = 1|γ &lt; t, I) =α t,i<label>(16)</label></formula><formula xml:id="formula_21">i t = L i=1 γ t,i a i<label>(17)</label></formula><p>Context vector i t is now seen as the random vari- able of this distribution. We define the variational lower bound L(γ) on the marginal log evidence log p(y|I) of observing the target sentence y given modality annotations I.</p><formula xml:id="formula_22">L(γ) = γ p(γ|I) log p(y|γ, I) ≤ log γ p(γ|I)p(y|γ, I) = log p(y|I)<label>(18)</label></formula><p>The learning rules can be derived by taking derivatives of the above variational free energy L(γ) with respect to the model parameter W :</p><formula xml:id="formula_23">∂L ∂W = γ p(γ|I) ∂ log p(y|γ, I) ∂W + log p(y|γ, I) ∂ log p(γ|I) ∂W<label>(19)</label></formula><p>In order to propagate a gradient through this process, the summation in equation 19 can then be approximated using Monte Carlo based sampling defined by equation 16:</p><formula xml:id="formula_24">∂L ∂W ≈ 1 N N n=1</formula><p>∂ log p(y|˜γy|˜γ n , I) ∂W + log p(y|˜γy|˜γ n , I) ∂ log p(˜ γ n |I) ∂W</p><p>To reduce variance of the estimator in equation 20, we use a moving average baseline estimated as an accumulated sum of the previous log likeli- hoods with exponential decay upon seeing the k-th mini-batch:  </p><formula xml:id="formula_26">b k = 0.9 × b k−1 + 0.1 × log p(y|˜γy|˜γ k , I) (21)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local Attention</head><p>In this section, we propose a local attentional mechanism that chooses to focus only on a small subset of the image annotations. Local Attention has been used for text-based translation ( <ref type="bibr" target="#b14">Luong et al., 2015)</ref> and is inspired by the selective attention model of <ref type="bibr" target="#b10">Gregor et al. (2015)</ref> for image generation. Their approach allows the model to select an image patch of varying location and zoom. Local attention uses instead the same "zoom" for all target positions and still achieved good performance. This model can be seen as a trade-off between the soft and hard attentional models. The model picks one patch in the annotation sequence (one spatial location) and selectively focuses on a small window of context around it. Even though an image can't be seen as a temporal sequence, we still hope that the model finds points of interest and selects the useful information around it. This approach has an advantage of being differentiable whereas the stochastic attention requires more complicated techniques such as variance reduction and rein- forcement learning to train as shown in section 3.2. The soft attention has the drawback to attend the whole image which can be difficult to learn, especially because the number of annotations L is usually large (presumably to keep a significant spatial granularity).</p><p>More formally, at every decoding step t, the model first generates an aligned position p t . Context vector i t is derived as a weighted sum over the annotations within the window [p t − N ; p t + N ] where N is a fixed model parameter chosen empirically 3 . These selected annotations correspond to a squared region in the attention maps around p t . The attention mask α t is of size 2N + 1. The model predicts p t as an aligned position in the annotation sequence (referred as Predictive alignment (local-m) in the author's paper) according to the following equation:</p><formula xml:id="formula_27">p t = S · sigmoid(v T tanh(U a s t ))<label>(22)</label></formula><p>where v T and U a are both trainable model pa- rameters and S is the annotation sequence length |I|. Because of the sigmoid, p t ∈ [0, S]. We use equation 12 and 13 respectively to compute the ex- pected alignment vector e t and the attention mask α t . In addition, a Gaussian distribution centered around p t is placed on the alphas in order to favor <ref type="bibr">3</ref> We pick N = |ai|/4 = 49 annotations near p t :</p><formula xml:id="formula_28">α t,i = α t,i exp − (i − p t ) 2 2σ 2<label>(23)</label></formula><p>where standard deviation σ = D 2 . We obtain con- text vector i t by following equation 14.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Image attention optimization</head><p>Three optimizations can be added to the attention mechanism regarding the image modality. All lead to a better use of the image by the model and improved the translation scores overall.</p><p>At every decoding step t, we compute a gat- ing scalar β t ∈ [0, 1] according to the previous decoder state s t−1 :</p><formula xml:id="formula_29">β t = σ(W β s t−1 + b β )<label>(24)</label></formula><p>It is then used to compute the time-dependent im- age context vector :</p><formula xml:id="formula_30">i t = β t L l=1 α t,l a l<label>(25)</label></formula><p>Xu et al. (2015) empirically found it to put more emphasis on the objects in the image descriptions generated with their model.</p><p>We also double the output size of trainable parameters U a , W a and v T in equation 12 when it comes to compute the expected annotations over the image annotation sequence. More formally, given the image annotation sequence I = (a 1 , a 2 , . . . , a L ), a i ∈ R D , the three ma- trices are of size D × 2D, D × 2D and 2D × 1 respectively. We noticed a better coverage of the objects in the image by the alpha weights.</p><p>Lastly, we use a grounding attention inspired by <ref type="bibr" target="#b6">Delbrouck and Dupont (2017)</ref>. The mech- anism merge each spatial location a i in the annotation sequence I with the initial decoder state s 0 obtained in equation 5 with non-linearity :</p><formula xml:id="formula_31">I =(f (a 1 + s 0 ), f (a 2 + s 0 ), . . . , f (a L + s 0 ))<label>(26)</label></formula><p>where f is tanh function. The new annota- tions go through a L2 normalization layer fol- lowed by two 1 × 1 convolutional layers (of size D → 512, 512 → 1 respectively) to obtain L × 1 weights, one for each spatial location. We nor- malize the weights with a softmax to obtain a soft attention map α. Each annotation a i is then weighted according to its corresponding α i :</p><formula xml:id="formula_32">I =(α 1 a 1 , α 2 a 2 , . . . , α L a L )<label>(27)</label></formula><p>This method can be seen as the removal of unnec- essary information in the image annotations ac- cording to the source sentence. This attention is used on top of the others -before decoding -and is referred as "grounded image" in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>For this experiments on Multimodal Machine Translation, we used the Multi30K dataset  which is an extended version of the Flickr30K Entities. For each image, one of the English descriptions was selected and manually translated into German by a professional transla- tor. As training and development data, 29,000 and 1,014 triples are used respectively. A test set of size 1000 is used for metrics evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training and model details</head><p>All our models are build on top of the nematus framework ( <ref type="bibr" target="#b18">Sennrich et al., 2017)</ref>. The encoder is a bidirectional RNN with GRU, one 1024D single-layer forward and one 1024D single-layer backward RNN. Word embeddings for source and target language are of 620D and trained jointly with the model. Word embeddings and other non-recurrent matrices are initialized by sampling from a Gaussian N (0, 0.01 2 ), recurrent matrices are random orthogonal and bias vectors are all initialized to zero.</p><p>To create the image annotations used by our decoder, we used a ResNet-50 pre-trained on ImageNet and extracted the features of size 14 × 14 × 1024 at its res4f layer ( <ref type="bibr" target="#b11">He et al., 2016</ref>). In our experiments, our decoder operates on the flattened 196 × 1024 (i.e L × D). We also apply dropout with a probability of 0.5 on the embeddings, on the hidden states in the bidirectional RNN in the encoder as well as in the decoder. In the decoder, we also apply dropout on the text annotations h i , the image features a i , on both modality context vector and on all components of the deep output layer before the readout operation. We apply dropout using one same mask in all time steps <ref type="bibr" target="#b9">(Gal and Ghahramani, 2016</ref>).</p><p>We also normalize and tokenize English and German descriptions using the Moses tokenizer scripts ( <ref type="bibr" target="#b13">Koehn et al., 2007)</ref>. We use the byte pair encoding algorithm on the train set to convert space-separated tokens into subwords (Sennrich et al., 2016), reducing our vocabulary size to 9226 and 14957 words for English and German respectively.</p><p>All variants of our attention model were trained with ADADELTA <ref type="bibr" target="#b26">(Zeiler, 2012)</ref>, with mini- batches of size 80 for our monomodal (text-only) NMT model and 40 for our multimodal NMT. We apply early stopping for model selection based on BLEU4 : training is halted if no improvement on the development set is observed for more than 20 epochs. We use the metrics BLEU4 ( <ref type="bibr" target="#b16">Papineni et al., 2002</ref>), METEOR <ref type="bibr" target="#b7">(Denkowski and Lavie, 2014</ref>) and TER ( <ref type="bibr" target="#b20">Snover et al., 2006</ref>) to evaluate the quality of our models' translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative results</head><p>We notice a nice overall progress over <ref type="bibr" target="#b3">Calixto et al. (2017)</ref> multimodal baseline, especially when using the stochastic attention. With im- provements of +1.51 BLEU and -2.2 TER on both precision-oriented metrics, the model shows a strong similarity of the n-grams of our candidate translations with respect to the references. The more recall-oriented metrics METEOR scores  <ref type="formula" target="#formula_5">(2017)</ref> scores as baseline and report our results accordingly (green for improvement and red for deterioration). In each of our experiments, Soft attention is used for text. The comparison is hence with respect to the attention mechanism used for the image modality.</p><p>are roughly the same across our models which is expected because all attention mechanisms share the same subsequent step at every time-step t, i.e. taking into account the attention weights of previous time-step t − 1 in order to compute the new intermediate hidden state proposal and there- fore the new context vector i t . Again, the largest improvement is given by the hard stochastic attention mechanism (+0.4 METEOR): because it is modeled as a decision process according to the previous choices, this may reinforce the idea of recall. We also remark interesting improvements when using the grounded mechanism, especially for the soft attention. The soft attention may benefit more of the grounded image because of the wide range of spatial locations it looks at, especially compared to the stochastic attention. This motivates us to dig into more complex grounding techniques in order to give the machine a deeper understanding of the modalities.</p><p>Note that even though our baseline NMT model is basically the same as <ref type="bibr" target="#b3">Calixto et al. (2017)</ref>, our experiments results are slightly better. This is probably due to the different use of dropout and subwords. We also compared our results to <ref type="bibr" target="#b2">Caglayan et al. (2016)</ref> because our multimodal models are nearly identical with the major ex- ception of the gating scalar (cfr. section 4). This motivated some of our qualitative analysis and hesitation towards the current architecture in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative results</head><p>For space-saving and ergonomic reasons, we only discuss about the hard stochastic and soft attention, the latter being a generalization of the local attention.</p><p>As we can see in <ref type="figure" target="#fig_7">Figure 7</ref>, the soft attention model is looking roughly at the same region of the image for every decoding step t. Because the words "hund"(dog), "wald"(forest) or "weg"(way) in left image are objects, they benefit from a high gating scalar. As a matter of fact, the attention mech- anism has learned to detect the objects within a scene (at every time-step, whichever word we are decoding as shown in the right image) and the gating scalar has learned to decide whether or not we have to look at the picture (or more accurately whether or not we are translating an object). Without this scalar, the translation scores undergo a massive drop (as seen in <ref type="bibr" target="#b2">Caglayan et al. (2016)</ref>) which means that the attention mechanisms don't really understand the more complex relationships between objects, what is really happening in the scene. Surprisingly, the It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders { ← − Ψ enc , − → Ψ enc } combined with a GRU layer and word-embeddings took care of the right trans- lation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and "gets lost", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:</p><p>Source: A child claps while riding on a woman 's shoulders . GT:</p><p>Ein Kind sitzt auf den Schultern einer Frau und klatscht . Mono: Ein Kind sitzt auf den Schultern einer Frau und schläft . Soft:</p><p>Ein Kind , das sich auf der Schultern eines Frau reitet , fährt auf den Schultern . Hard:</p><p>Ein Kind in der Haltung , während er auf den Schultern einer Frau fährt .</p><p>The monomodal translation has a sentence-level BLEU of 82.16 whilst the soft attention and hard stochastic attention scores are of 16.82 and 34.45 respectively. <ref type="figure" target="#fig_8">Figure 8</ref> shows the attention maps for both mechanism. Nevertheless, one has to concede that the use of images indubitably helps the translation as shown in the score tabular. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We have tried different attention mechanism and tweaks for the image modality. We showed im- provements and encouraging results overall on the Flickr30K Entities dataset. Even though we iden- tified some flaws of the current attention mecha- nisms, we can conclude pretty safely that images are an helpful resource for the machine in a trans- lation task. We are looking forward to try out richer and more suitable features for multimodal translation (ie. dense captioning features). An- other interesting approach would be to use visu- ally grounded word embeddings to capture visual notions of semantic relatedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>917</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Die beiden Kinder spielen auf dem Spielplatz .</figDesc><graphic url="image-1.png" coords="3,310.45,642.92,209.20,66.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Ein Junge sitzt auf und blickt aus einem Mikroskop .</figDesc><graphic url="image-2.png" coords="4,74.24,62.81,211.07,68.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Hard attention has previously been used in the context of object recognition (Mnih et al., 2014; Ba et al., 2015) and later extended to image description generation (Xu et al., 2015). In the context of multimodal NMT, we can follow Xu et al. (2015) because both our models involve the same process on images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Ein Mann sitzt neben einem Computerbildschirm .</figDesc><graphic url="image-3.png" coords="4,320.77,464.11,188.55,63.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ein Mann in einem orangefarbenen Hemd und mit Helm .</figDesc><graphic url="image-4.png" coords="4,316.09,592.58,197.93,68.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ein Mädchen mit einer Schwimmweste schwimmt im Wasser .</figDesc><graphic url="image-5.png" coords="5,312.00,160.77,206.10,63.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ein kleiner schwarzer Hund springtüberspringt¨springtüber Hindernisse .</figDesc><graphic url="image-6.png" coords="5,312.22,286.02,205.65,61.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Representative figures of the soft-attention behavior discussed in §5.3</figDesc><graphic url="image-7.png" coords="8,72.00,62.81,432.54,161.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Wrong detection for both Soft attention (top) and Hard stochastic attention (bottom)</figDesc><graphic url="image-8.png" coords="8,307.92,377.12,216.98,105.19" type="bitmap" /></figure>

			<note place="foot" n="1"> http://www.statmt.org/wmt16/multimodal-task.html 910</note>

			<note place="foot" n="2"> https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Does multimodality help human and machine for translation and image captioning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Aransa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mercedes</forename><surname>García-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lo¨ıclo¨ıc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joost</forename><surname>Van De Weijer</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2358" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Doubly-attentive decoder for multi-modal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>CoRR abs/1702.01287</idno>
		<ptr target="http://arxiv.org/abs/1702.01287" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Dupont</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08084</idno>
		<title level="m">Multimodal compact bilinear pooling for multimodal neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Statistical Machine Translation</title>
		<meeting>the EACL 2014 Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multi30k: Multilingual english-german image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Specia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29 (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/gregor15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning. PMLR</title>
		<editor>Francis Bach and David Blei</editor>
		<meeting>the 32nd International Conference on Machine Learning. PMLR<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1462" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz-Rung</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Effective approaches to attentionbased neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="doi">10.3115/1073083.1073135</idno>
		<ptr target="https://doi.org/10.3115/1073083.1073135" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA, ACL &apos;02</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nematus: a Toolkit for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Demonstrations at the 15th Conference of the European Chapter</title>
		<meeting>the Demonstrations at the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Jozef Mokry, and Maria Nadejde</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A shared task on multimodal machine translation and crosslingual image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalil</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desmond</forename><surname>Elliott</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/W/W16/W16-2346" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation. Association for Computational Linguistics</title>
		<meeting>the First Conference on Machine Translation. Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<editor>C. Cortes, N. D</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf" />
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings</title>
		<editor>David Blei and Francis Bach</editor>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15). JMLR Workshop and Conference Proceedings</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>CoRR abs/1212.5701</idno>
		<ptr target="http://arxiv.org/abs/1212.5701" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
