<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Are Tree Structures Necessary for Deep Learning of Representations?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>ehovy@andrew.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Language Technology Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When Are Tree Structures Necessary for Deep Learning of Representations?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing apples-to-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answer-phrases; (3) discourse parsing; (4) semantic relation extraction. Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require long-distance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning based methods learn low- dimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., <ref type="bibr" target="#b29">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b24">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b5">Collobert et al., 2011)</ref>), successfully capturing syntactic and semantic aspects of text.</p><p>For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compo- sitional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: re- current models and recursive models:</p><p>Recurrent models (also referred to as sequence models) deal successfully with time-series data <ref type="bibr" target="#b32">(Pearlmutter, 1989;</ref><ref type="bibr" target="#b8">Dorffner, 1996</ref>) like speech ( <ref type="bibr" target="#b33">Robinson et al., 1996;</ref><ref type="bibr" target="#b27">Lippmann, 1989;</ref><ref type="bibr" target="#b14">Graves et al., 2013</ref>) or handwriting recognition ( <ref type="bibr" target="#b13">Graves and Schmidhuber, 2009;</ref><ref type="bibr" target="#b15">Graves, 2012)</ref>. They were ap- plied early on to NLP <ref type="bibr" target="#b11">(Elman, 1990)</ref>, by modeling a sentence as tokens processed sequentially and at each step combining the current token with pre- viously built embeddings. Recurrent models can be extended to bidirectional ones from both left- to-right and right-to-left. These models generally consider no linguistic structure aside from word order.</p><p>Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequen- tially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequen- tial order (((the food) is) delicious). Many recur- sive models have been proposed (e.g., <ref type="bibr" target="#b31">(Paulus et al., 2014;</ref><ref type="bibr" target="#b21">Irsoy and Cardie, 2014)</ref>), and applied to various NLP tasks, among them entailment <ref type="bibr" target="#b3">(Bowman, 2013;</ref><ref type="bibr" target="#b1">Bowman et al., 2014</ref>), sentiment anal- ysis ( <ref type="bibr" target="#b38">Socher et al., 2013;</ref><ref type="bibr" target="#b20">Irsoy and Cardie, 2013;</ref><ref type="bibr" target="#b7">Dong et al., 2014</ref>), question-answering ( <ref type="bibr" target="#b22">Iyyer et al., 2014</ref>), relation classification ( <ref type="bibr" target="#b37">Socher et al., 2012;</ref><ref type="bibr" target="#b16">Hashimoto et al., 2013)</ref>, and discourse ( ).</p><p>One possible advantage of recursive models is their potential for capturing long-distance depen- dencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its correspond- ing direct object can be far away in terms of to- kens if many adjectives lies in between, but they are adjacent in the parse tree <ref type="bibr" target="#b20">(Irsoy and Cardie, 2013)</ref>. However we do not know if this advan- tage is truly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is rela- tively slow, domain-dependent, and can be error- ful.</p><p>On the other hand, recent progress in multi- ple subfields of neural NLP has suggested that re- current nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequence- to-sequence generation ) for machine translation (e.g., <ref type="bibr" target="#b23">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr">3;</ref><ref type="bibr" target="#b28">Luong et al., 2014)</ref>), pars- ing ( , and sentiment, where for example recurrent-based paragraph vectors ( <ref type="bibr" target="#b24">Le and Mikolov, 2014</ref>) outperform recursive models <ref type="bibr" target="#b38">(Socher et al., 2013</ref>) on the Stanford sentiment- bank dataset.</p><p>Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive mod- els offer specific advantages. We investigate four tasks with different properties.</p><p>• Binary sentiment classification at the sen- tence level ( <ref type="bibr" target="#b30">Pang et al., 2002</ref>) and phrase level <ref type="bibr" target="#b38">(Socher et al., 2013</ref>) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is com- prehensive.</p><p>• Phrase Matching on the UMD-QA dataset <ref type="bibr" target="#b22">(Iyyer et al., 2014</ref>) can help see the difference between outputs from intermediate compo- nents from different models, i.e., representa- tions for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether pars- ing is useful for finding similarities between question sentences and target phrases.</p><p>• Semantic Relation Classification on the <ref type="bibr">SemEval-2010</ref><ref type="bibr" target="#b17">(Hendrickx et al., 2009</ref>) data can help understand whether parsing is help- ful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.</p><p>• Discourse parsing (RST dataset) is useful for measuring the extent to which parsing im- proves discourse tasks that need to combine meanings of larger text units. Discourse pars- ing treats elementary discourse units (EDUs) as basic units to operate on, which are usually short clauses. The task also sheds light on the extent to which syntactic structures help acquire shot text representations.</p><p>The principal motivation for this paper is to un- derstand better when, and why, recursive models are needed to outperform simpler models by en- forcing apples-to-apples comparison as much as possible. This paper applies existing models to existing tasks, barely offering novel algorithms or tasks. Our goal is rather an analytic one, to inves- tigate different versions of recursive and recurrent models. This work helps understand the limita- tions of both classes of models, and suggest direc- tions for improving recurrent models.</p><p>The rest of this paper organized as follows: We detail versions of recursive/recurrent models in Section 2, present the tasks and results in Section 3, and conclude with discussions in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recursive and Recurrent Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>We assume that the text unit S, which could be a phrase, a sentence or a document, is com- prised of a sequence of tokens/words: S = {w 1 , w 2 , ..., w N S }, where N s denotes the num- ber of tokens in S. Each word w is associated with a K-dimensional vector embedding e w = {e 1 w , e 2 w , ..., e K w }. The goal of recursive and re- current models is to map the sequence to a K- dimensional e S , based on its tokens and their cor- respondent embeddings.</p><p>Standard Recurrent/Sequence Models suc- cessively take word w t at step t, combines its vec- tor representation e t with the previously built hid- den vector h t−1 from time t − 1, calculates the re-sulting current embedding h t , and passes it to the next step. The embedding h t for the current time t is thus:</p><formula xml:id="formula_0">h t = f (W · h t−1 + V · e t )<label>(1)</label></formula><p>where W and V denote compositional matrices. If N s denotes the length of the sequence, h Ns repre- sents the whole sequence S.</p><p>Standard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order. It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. For a given node η in the tree and its left child η left (with representation e left ) and right child η right (with representation e right ), the standard recursive network calculates e η as follows:</p><formula xml:id="formula_1">e η = f (W · e η left + V · e η right )<label>(2)</label></formula><p>Bidirectional Models ( <ref type="bibr" target="#b34">Schuster and Paliwal, 1997</ref>) add bidirectionality to the recurrent frame- work where embeddings for each time are calcu- lated both forwardly and backwardly:</p><formula xml:id="formula_2">h → t = f (W → · h → t−1 + V → · e t ) h ← t = f (W ← · h ← t+1 + V ← · e t )<label>(3)</label></formula><p>Normally, final representations for sentences can be achieved either by concatenating vectors calcu- lated from both directions</p><formula xml:id="formula_3">[e ← 1 , e → N S</formula><p>] or using fur- ther compositional operation to preserve vector di- mensionality</p><formula xml:id="formula_4">h t = f (W L · [h ← t , h → t ])<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">W L denotes a K × 2K dimensional matrix.</formula><p>Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x 1 , x 2 , ..., x n X }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as i t , f t and o t . We notation- ally disambiguate e and h: e t denotes the vector for individual text units (e.g., word or sentence) at time step t, while h t denotes the vector computed by the LSTM model at time t by combining e t and h t−1 . σ denotes the sigmoid function. The vector representation h t for each time-step t is given by:</p><formula xml:id="formula_6">    i t f t o t l t     =     σ σ σ tanh     W · h t−1 e t (5) c t = f t · c t−1 + i t · l t (6) h s t = o t · c t (7)</formula><p>where W ∈ R 4K×2K . Labels at the phrase/sentence level are predicted representations outputted from the last time step.</p><p>Tree LSTMs Recent research has extended the LSTM idea to tree-based structures ( <ref type="bibr" target="#b42">Zhu et al., 2015;</ref><ref type="bibr" target="#b40">Tai et al., 2015</ref>) that associate memory and forget gates to nodes of the parse trees.</p><p>Bi-directional LSTMs These combine bi- directional models and LSTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks.</p><p>• Binary sentiment classification on the <ref type="bibr" target="#b30">Pang et al. (2002)</ref> dataset. This addresses the is- sues where supervision only appears globally after a long sequence of operations.</p><p>• Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned.</p><p>• Sentence-Target Matching on the UMD- QA dataset <ref type="bibr" target="#b22">(Iyyer et al., 2014</ref>): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models.</p><p>• Semantic Relation Classification on the SemEval-2010 task ( <ref type="bibr" target="#b17">Hendrickx et al., 2009</ref>). Learns long-distance relationships between two words that may be far apart sequentially.</p><p>• Discourse Parsing ( <ref type="bibr" target="#b7">Li et al., 2014;</ref><ref type="bibr" target="#b18">Hernault et al., 2010)</ref>: Learns sentence-to-sentence re- lations based on calculated representations.</p><p>In each case we followed the protocols de- scribed in the original papers. We first group the algorithm variants into two groups as follows:</p><p>• Standard tree models vs standard sequence models vs standard bi-directional sequence models</p><p>• LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models.</p><p>We employed standard training frameworks for neural models: for each task, we used stochas- tic gradient decent using AdaGrad <ref type="bibr" target="#b9">(Duchi et al., 2011</ref>) with minibatches (Cotter et al., 2011). Pa- rameters are tuned using the development dataset if available in the original datasets or from cross- validation if not. Derivatives are calculated from standard back-propagation <ref type="bibr" target="#b12">(Goller and Kuchler, 1996)</ref>. Parameters to tune include size of mini batches, learning rate, and parameters for L2 pe- nalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test is adopted for sta- tistical significance testing <ref type="bibr" target="#b10">(Efron and Tibshirani, 1994)</ref>. Test scores that achieve significance level of 0.05 are marked by an asterisk (*).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Stanford Sentiment TreeBank</head><p>Task Description We start with the Stanford Sentiment TreeBank ( <ref type="bibr" target="#b38">Socher et al., 2013)</ref>. This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words.</p><p>Of course, any conclusions drawn from imple- menting sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected. Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations. For recursive models, we followed the proto- cols in <ref type="bibr" target="#b38">Socher et al. (2013)</ref> where node embed- dings in the parse trees are obtained from recur- sive models and then fed to a softmax classifier. We transformed the dataset for recurrent model use as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. Each phrase is recon- structed from parse tree nodes and treated as a sep- arate data point. As the treebank contains 11,855 sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level <ref type="bibr">(2,210 instances</ref>  Results are shown in <ref type="table">Table 1</ref> and 2 1 . When comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bi- sequences), but yields no significant improvement at the phrase level.</p><p>LSTM <ref type="bibr" target="#b40">Tai et al. (2015)</ref> discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models. We explore this task a bit more by training deeper and more sophisticated models. We exam- ine the following three models:</p><p>1. Tree-structured LSTM models <ref type="bibr" target="#b40">(Tai et al., 2015)</ref> 2 .</p><p>2. Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence.</p><p>3. Deep Bi-LSTM hierarchical sequence mod- els (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of sub- sentences by using a look-up table of punc- tuations (i.e., comma, period, question mark <ref type="bibr">1</ref> The performance of our implementations of recursive models is not exactly identical to that reported in <ref type="bibr" target="#b38">Socher et al. (2013)</ref>, but the relative difference is around 1% to 2%.   We consider the third model because the dataset used in <ref type="bibr" target="#b40">Tai et al. (2015)</ref> contains long sentences and the evaluation is performed only at the sen- tence root level. Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advan- tage could be approximated by using punctuation- based approximations to clause boundaries.</p><p>We run 15 iterations for each algorithm. Pa- rameters are harvested at the end of each itera- tion; those performing best on the development set are used on the test set. The whole process takes roughly 15-20 minutes on a single GPU ma- chine <ref type="bibr">3</ref> . For a more convincing comparison, we did not use the bootstrap test where parallel ex- amples are generated from one same dataset. In- stead, we repeated the aforementioned procedure for each algorithm 20 times and report accuracies 3 Tesla K40m, 2880 Cuda cores.</p><p>with standard deviation in <ref type="table" target="#tab_2">Table 3</ref>.  Tree LSTMs are equivalent or marginally bet- ter than standard bi-directional sequence model (two-tailed p-value equals 0.041*, and only at the root level, with p-value for the phrase level at 0.376). The hierarchical sequence model achieves the same performance with a p-value of 0.198.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>The results above suggest that clausal segmentation of long sentences offers a slight performance boost, a result also supported by the fact that very little difference exists between the three models for phrase-level sentiment eval- uation. Clausal segmentation of long sentences thus provides a simple approximation to parse-tree based models.</p><p>We suggest a few reasons for this slightly better performances introduced by clausal segmentation:</p><p>1. Treating clauses as basic units (to the extent that punctuation approximates clauses) pre- serves the semantic structure of text.</p><p>2. Semantic compositions such as negations or conjunctions usually appear at the clause level. Working on clauses individually and then combining them model inter-clause compositions.</p><p>3. Errors are back-propagated to individual to- kens using fewer steps in hierarchical models than in standard models. Consider a movie review "simple as the plot was , i still like it a lot". With standard recurrent models it takes 12 steps before the prediction error gets back to the first token "simple":</p><p>error→lot→a→it→like→still→i→,→was →plot→ the→as→simple In a hierarchical model, the second clause is compacted into one component, and the error propagation is thus given by:</p><formula xml:id="formula_7">error→ second-clause → first-clause → was→plot→the→as→simple.</formula><p>Propagation with clause segmentation con- sists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanish- ing problem, potentially yielding better per- formance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Binary Sentiment Classification (Pang)</head><p>Task Description: The sentiment dataset of <ref type="bibr" target="#b30">Pang et al. (2002)</ref> consists of sentences with a sentiment label for each sentence. We divide the original dataset into train- ing(8101)/dev(500)/testing <ref type="bibr">(2000)</ref>. No pre- training procedure as described in <ref type="bibr" target="#b36">Socher et al. (2011b)</ref> is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package <ref type="bibr">4</ref> . Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below:</p><p>Discussion Why don't parse trees help on this task? One possible explanation is the distance <ref type="bibr">4</ref>   of the supervision signal from the local composi- tional structure. The Pang et al. dataset has an av- erage sentence length of 22.5 words, which means it takes multiple steps before sentiment related ev- idence comes up to the surface. It is therefore un- clear whether local compositional operators (such as negation) can be learned; there is only a small amount of training data (around 8,000 examples) and the sentiment supervision only at the level of the sentence may not be easy to propagate down to deeply buried local phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question-Answer Matching</head><p>Task Description: In the question-answering dataset QANTA 5 , each answer is a token or short phrase. The task is different from standard gener- ation focused QA task but formalized as a multi- class classification task that matches a source question with a candidates phrase from a prede- fined pool of candidate phrases We give an illus- trative example here: Question: He left unfinished a novel whose title character forges his father's signature to get out of school and avoids the draft by feigning desire to join. Name this German author of The Magic Mountain and Death in Venice.</p><p>Answer: Thomas Mann from the pool of phrases. Other candidates might include George Washington, Charlie Chaplin, etc.</p><p>The model of <ref type="bibr" target="#b22">Iyyer et al. (2014)</ref> minimizes the distances between answer embeddings and node embeddings along the parse tree of the question. Concretely, let c denote the correct answer to ques- tion S, with embedding c, and z denoting any ran- dom wrong answer. The objective function sums over the dot product between representation for every node η along the question parse trees and the answer representations: <ref type="bibr">5</ref> http://cs.umd.edu/ ˜ miyyer/qblearn/. Be- cause the publicly released dataset is smaller than the version used in <ref type="bibr" target="#b22">(Iyyer et al., 2014</ref>) due to privacy issues, our numbers are not comparable to those in ( <ref type="bibr" target="#b22">Iyyer et al., 2014</ref>). where e η denotes the embedding for parse tree node calculated from the recursive neural model. Here the parse trees are dependency parses follow- ing <ref type="bibr" target="#b22">(Iyyer et al., 2014)</ref>.</p><formula xml:id="formula_8">L = η∈[parse tree] z max(0, 1− c·e η + z ·e η ) (8)</formula><p>By adjusting the framework to recurrent mod- els, we minimize the distance between the answer embedding and the embeddings calculated from each timestep t of the sequence:</p><formula xml:id="formula_9">L = t∈[1,Ns] z max(0, 1 − c · e t + z · e t ) (9)</formula><p>At test time, the model chooses the answer (from the set of candidates) that gives the lowest loss score. As can be seen from results presented in <ref type="table">Table 5</ref>, the difference is only significant for the LSTM setting between the tree model and the sequence model; no significant difference is ob- served for other settings.  <ref type="table">Table 5</ref>: Test set accuracies for UMD-QA dataset.</p><p>Discussion The UMD-QA task represents a group of situations where because we have in- sufficient supervision about matching (it's hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. <ref type="bibr" target="#b35">Socher et al. (2011a)</ref>).</p><p>The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree mod- els and different time steps for sequence models), components from sequence models are able to em- bed important information, despite the fact that se- quence model components are just sentence frag- ments and hence usually not linguistically mean- ingful components in the way that parse tree con- stituents are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantic Relationship Classification</head><p>Task Description: <ref type="bibr">SemEval-2010</ref><ref type="bibr">Task 8 (Hendrickx et al., 2009</ref>) is to find semantic rela- tionships between pairs of nominals, e.g., in "My <ref type="bibr">[apartment]</ref> e1 has a pretty large <ref type="bibr">[kitchen]</ref> e2 " classifying the relation between <ref type="bibr">[apartment]</ref> and <ref type="bibr">[kitchen]</ref> as component-whole. The dataset con- tains 9 ordered relationships, so the task is formal- ized as a 19-class classification problem, with di- rected relations treated as separate labels; see <ref type="bibr" target="#b17">Hendrickx et al. (2009;</ref><ref type="bibr" target="#b37">Socher et al. (2012)</ref> for details.</p><p>For the recursive implementations, we follow the neural framework defined in <ref type="bibr" target="#b37">Socher et al. (2012)</ref>. The path in the parse tree between the two nominals is retrieved, and the embedding is calcu- lated based on recursive models and fed to a soft- max classifier <ref type="bibr">6</ref> . Retrieved paths are transformed for the recurrent models as shown in <ref type="figure" target="#fig_7">Figure 5</ref>. Discussion Unlike for earlier tasks, here recur- sive models yield much better performance than the corresponding recurrent versions for all ver- sions (e.g., standard tree vs. standard sequence, p = 0.004). These results suggest that it is the need to integrate structures far apart in the sen- tence that characterizes the tasks where recursive models surpass recurrent models. In parse-based models, the two target words are drawn together much earlier in the decision process than in recur- rent models, which must remember one target un- til the other one appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discourse Parsing</head><p>Task Description: Our final task, discourse parsing based on the RST-DT corpus <ref type="table">(Carlson et Standard   LSTM  Tree</ref> 0.748 0.767 Sequence 0.712 (-0.036) 0.740 (-0.027) P-value 0.004* 0.020* Bi-Sequence 0.730 (-0.018) 0.752 (-0.014) P-value 0.017* 0.041* <ref type="table">Table 6</ref>: Test set accuracies on the SemEval-2010 Semantic Relationship Classification task. [e 1 , e 2 , ...] denote EDUs (elementary discourse units), each consisting of a sequence of tokens.</p><p>[r 12 , r 34 , r 56 ] denote relationships to be classified. A binary classification model is first used to decide whether two EDUs should be merged and a multi- class classifier is then used to decide the relation type.</p><p>al., 2003), is to build a discourse tree for a doc- ument, based on assigning Rhetorical Structure Theory (RST) relations between elementary dis- course units (EDUs). Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compo- sitional meaning than sentiment or nominal rela- tions. See <ref type="bibr" target="#b18">Hernault et al. (2010)</ref> for more details on discourse parsing and the RST-DT corpus.</p><p>Representations for adjacent EDUs are fed into binary classification (whether two EDUs are re- lated) and multi-class relation classification mod- els, as defined in <ref type="bibr" target="#b7">Li et al. (2014)</ref>. Related EDUs are then merged into a new EDU, the representa- tion of which is obtained through an operation of neural composition based on the previous two re- lated EDUs. This step is repeated until all units are merged.</p><p>Discourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sen- tences, with an average length of 7.2 words. Re- cursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for dis- course parsing. We use this task for two rea- sons: (1) to illustrate whether syntactic parse trees are useful for acquiring representations for short clauses. (2) to measure the extent to which pars- ing improves discourse tasks that need to combine the meanings of larger text units.</p><p>Models are traditionally evaluated in terms of three metrics, i.e., spans 7 , nuclearity 8 , and identi- fying the rhetorical relation between two clauses. Due to space limits, we only focus the last one, rhetorical relation identification, because (1) rela- tion labels are treated as correct only if spans and nuclearity are correctly labeled (2) relation identi- fication between clauses offer more insights about model's abilities to represent sentence semantics. In order to perform a plain comparison, no addi- tional human-developed features are added.  Discussion We see no large differences between equivalent recurrent and recursive models. We suggest two possible explanations. (1) EDUs tend to be short; thus for some clauses, parsing might not change the order of operations on words. Even for those whose orders are changed by parse trees, the influence of short phrases on the final represen- tation may not be great enough. (2) Unlike earlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm pre- sented here adopts additional levels of neural com- position during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions and Conclusions</head><p>We compared recursive and recurrent neural mod- els for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance <ref type="bibr" target="#b37">(Socher et al., 2012;</ref><ref type="bibr" target="#b38">Socher et al., 2013;</ref><ref type="bibr" target="#b7">Li et al., 2014;</ref><ref type="bibr" target="#b22">Iyyer et al., 2014</ref>). As with any comparison between models, our results come with some caveats: First, we ex- plore the most general or basic forms of recur-sive/recurrent models rather than various sophis- ticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of lay- ers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neural compositions-despite the fact that deep neural models with multiple layers give better results. Our conclusions might thus be limited to the al- gorithms employed in this paper, and it is unclear whether they can be extended to other variants or to the latest state-of-the-art. Second, in order to compare models "fairly", we force every model to be trained exactly in the same way: AdaGrad with minibatches, same set of initializations, etc. How- ever, this may not necessarily be the optimal way to train every model; different training strategies tailored for specific models may improve their per- formances. In that sense, our attempts to be "fair" in this paper may nevertheless be unfair.</p><p>Pace these caveats, our conclusions can be sum- marized as follows:</p><p>• In tasks like semantic relation extraction, in which single headwords need to be associ- ated across a long distance, recursive models shine. This suggests that for the many other kinds of tasks in which long-distance seman- tic dependencies play a role (e.g., translation between languages with significant reorder- ing like Chinese-English translation), syntac- tic structures from recursive models may of- fer useful power.</p><p>• Tree models tend to help more on long se- quences than shorter ones with sufficient su- pervision: tree models slightly help root level identification on the Stanford Sentiment Treebank, but do not help much at the phrase level. Adopting bi-directional versions of re- current models seem to largely bridge this gap, producing equivalent or sometimes bet- ter results.</p><p>• On long sequences where supervision is not sufficient, e.g., in Pang at al.,'s dataset (super- vision only exists on top of long sequences), no significant difference is observed between tree based and sequence based models.</p><p>• In cases where tree-based models do well, a simple approximation to tree-based models seems to improve recurrent models to equiv- alent or almost equivalent performance: (1) break long sentences (on punctuation) into a series of clause-like units, (2) work on these clauses separately, and (3) join them together. This model sometimes works as well as tree models for the sentiment task, suggesting that one of the reasons tree models help is by breaking down long sentences into more manageable units.</p><p>• Despite that the fact that components (out- puts from different time steps) in recur- rent models are not linguistically meaningful, they may do as well as linguistically mean- ingful phrases (represented by parse tree nodes) in embedding informative evidence, as demonstrated in UMD-QA task. Indeed, recent work in parallel with ours <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref> has shown that recurrent models like LSTMs can discover implicit recursive compositional structure.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>2 Tai et al.. achieved 0.510 accuracy in terms of fine- grained evaluation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Transforming Stanford Sentiment Treebank to Sequences for Sequence Models.</figDesc><graphic url="image-1.png" coords="5,136.77,62.81,324.00,128.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of two sequence models. A, B, C, D denote clauses or sub sentences separated by punctuation. and exclamation mark). The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to join the subsentences. Illustrations are shown in Figure2.</figDesc><graphic url="image-2.png" coords="5,109.14,226.28,144.00,104.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sentiment prediction using a onedirectional (left to right) LSTM. Decisions at each time step are made by feeding embeddings calculated from the LSTM into a softmax classifier.</figDesc><graphic url="image-3.png" coords="6,80.34,62.81,201.60,129.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of Models for Semantic Relationship Classification.</figDesc><graphic url="image-4.png" coords="7,308.41,251.31,216.00,147.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An illustration of discourse parsing. [e 1 , e 2 , ...] denote EDUs (elementary discourse units), each consisting of a sequence of tokens. [r 12 , r 34 , r 56 ] denote relationships to be classified. A binary classification model is first used to decide whether two EDUs should be merged and a multiclass classifier is then used to decide the relation type.</figDesc><graphic url="image-5.png" coords="8,105.54,177.64,151.20,100.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Fine-Grained 
Binary 
Tree 
0.433 
0.815 
Sequence 
0.420 (-0.013) 0.807 (-0.007) 
P-value 
0.042* 
0.098 
Bi-Sequence 0.435 (+0.08) 
0.816 (+0.002) 
P-value 
0.078 
0.210 

Table 1: Test set accuracies on the Stanford Senti-
ment Treebank at root level. 

Fine-Grained 
Binary 
Tree 
0.820 
0.860 
Sequence 
0.818 (-0.002) 0.864 (+0.004) 
P-value 
0.486 
0.305 
Bi-Sequence 0.826 (+0.06) 
0.862 (+0.002) 
P-value 
0.148 
0.450 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Test set accuracies on the Stanford Senti-
ment Treebank at phrase level. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Test set accuracies on the Stanford Sen- timent Treebank with deviations. For our exper- iments, we report accuracies over 20 runs with standard deviation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Test set accuracies on the Pang's senti</head><label>4</label><figDesc></figDesc><table>-
ment dataset using Standard model settings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Test set accuracies for relation identifica-
tion on RST discourse parsing data set. 

</table></figure>

			<note place="foot" n="6"> (Socher et al., 2012) achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering. Again, because MV-RNN is difficult to adapt to a recurrent version, we do not employ this state-of-the-art model, adhering only to the general versions of recursive models described in Section 2, since our main goal is to compare equivalent recursive and recurrent models rather than implement the state of the art.</note>

			<note place="foot" n="7"> on blank tree structures. 8 on tree structures with nuclearity indication.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Acknowledgments</head><p>We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, ad-vice, and suggestions. We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful ad-vice on various aspects of this work. We acknowl-edge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully ac-knowledge support from an Enlight Foundation Graduate Fellowship, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Lab-oratory (AFRL) contract no. FA8750-13-2-0040, and the NSF via award IIS-1514268. Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the authors and do not necessarily reflect the views of Bloomberg L.P., DARPA, AFRL, NSF, or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recursive neural networks for learning logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1827</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04834</idno>
		<title level="m">Tree-structured composition in neural networks without tree-structured architectures</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6192</idno>
		<title level="m">Can recursive neural tensor networks learn logical reasoning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynn</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ellen</forename><surname>Okurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current and New Directions in Discourse and Dialogue Text, Speech and Language Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better mini-batch algorithms via accelerated gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural networks for time series processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Dorffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Network World</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="347" to="352" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Supervised sequence labeling with recurrent neural networks, In Studies in Computational Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">385</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple customization of recursive neural networks for semantic relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Chikayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1372" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diarmuid´odiarmuid´</forename><forename type="middle">Diarmuid´o</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</title>
		<meeting>the Workshop on Semantic Evaluations: Recent Achievements and Future Directions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hilda: a discourse parser using support vector machine classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Hernault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Prendinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitsuru</forename><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dialogue &amp; Discourse</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.0493</idno>
		<title level="m">Bidirectional recursive neural networks for token-level labeling with structure</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep recursive neural networks for compositionality in language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2096" to="2104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A neural network for factoid question answering over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4053</idno>
		<title level="m">Distributed representations of sentences and documents</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A model of coherence based on distributed sentence representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for discourse parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rumeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2061" to="2069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Review of neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Richard P Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Global belief recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2888" to="2896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning state space trajectories in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="269" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The use of recurrent neural networks in continuous speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic speech and speaker recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="233" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="801" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semi-supervised recursive autoencoders for predicting sentiment distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7449</idno>
		<title level="m">Grammar as a foreign language</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobihani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
