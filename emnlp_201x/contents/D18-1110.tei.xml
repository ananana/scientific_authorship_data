<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Chen</surname></persName>
							<email>chenliwei@pku.edu.cn,vadims@us.ibm.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="918" to="924"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>918</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees. In this paper, we first propose to use the syntactic graph to represent three types of syntactic information , i.e., word order, dependency and constituency features; then employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of semantic parsing is to translate text to its formal meaning representations, such as logical forms or structured queries. Recent neural seman- tic parsers approach this problem by learning soft alignments between natural language and logical forms from (text, logic) pairs <ref type="bibr" target="#b7">(Jia and Liang, 2016;</ref><ref type="bibr" target="#b3">Dong and Lapata, 2016;</ref><ref type="bibr" target="#b9">Krishnamurthy et al., 2017)</ref>. All these parsers follow the conventional encoder-decoder architecture that first encodes the text into a distributional representation and then decodes it to a logical form. These parsers may differ in the choice of the decoders, such as se- quence or tree decoders, but they utilize the same encoder which is essentially a sequential Long Short-Term Memory network (SeqLSTM). This encoder only extracts word order features while neglecting useful syntactic information, such as dependency parse and constituency parse.</p><p>However, the syntactic features capture im- portant structural information of the natural lan- * Work done when the author was at IBM Research.</p><p>guage input, which complements the simple word sequence. For example, a dependency graph presents grammatical relations that hold among the words; and a constituent tree provides a phrase structure representation. Intuitively, by incorpo- rating such additional information, the encoder could produce a more meaningful and robust sen- tence representation. The combination of these features (i.e., sequence + trees) forms a general graph structure (see <ref type="figure">Figure 1</ref>). This inspires us to apply a graph encoder to produce a represen- tation of a graph-structured input. The graph en- coder also has the advantages that it could simulta- neously encode all types of syntactic contexts, and incorporate multiple types of syntactic structures in a unified way.</p><p>In this paper, we first introduce a structure, namely syntactic graph, to represent three types of syntactic information, i.e., word order, depen- dency and constituency features (see §2). We then employ a novel graph-to-sequence (Graph2Seq) model ( <ref type="bibr" target="#b26">Xu et al., 2018)</ref>, which consists of a graph encoder and a sequence decoder, to learn the rep- resentation of the syntactic graph (see §3). Specif- ically, the graph encoder learns the representation of each node by aggregating information from its K-hop neighbors. Given the learned node em- beddings, the graph encoder uses a pooling-based method to generate the graph embedding. On the decoder side, a Recurrent Neural Network (RNN) decoder takes the graph embedding as its initial hidden state to generate the logical form while employing an attention mechanism over the node embeddings. Experimental results show that our model achieves the competitive performance on Jobs640, ATIS, and Geo880 datasets.</p><p>Different from existing works, we also inves- tigate the robustness of our model by evaluating the model on two types of adversarial examples <ref type="bibr" target="#b1">(Belinkov and Bisk, 2017;</ref><ref type="bibr" target="#b2">Cheng et al., 2018</ref>  <ref type="figure">Figure 1</ref>: The syntactic graph for the Jobs640 question what are the jobs for programmer that has salary 50000 that uses c++ and not related with AI. Due to the space limitation, the constituent tree is partially shown here.</p><p>Experimental results show that the model cou- pling all syntactic features has the best robustness, achieving the best performance. Our code and data is available at https://github.com/IBM/ Text-to-LogicForm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Syntactic Graph</head><p>We represent three types of syntactic features, i.e., word order, dependency parse and constituency parse, as the syntactic graph (see <ref type="figure">Figure 1</ref>).</p><p>• Word Order Features. Previous neural seman- tic parsers mainly use these features by building a SeqLSTM that works on the word sequence. Our syntactic graph also incorporates this information by generating a node for each word and connect- ing them in the chain form. In order to capture the forward and backward contextual information, we link these nodes in two directions, that is, from left to right and from right to left.</p><p>• Dependency Features. A dependency parse de- scribes the grammatical relations that hold among words. <ref type="bibr" target="#b18">Reddy et al. ( , 2017</ref> have demon- strated that the dependency parse tree could be directly transformed to a logical form, which in- dicates that the dependency information (i.e., tree structure and dependency labels) is critical to the semantic parsing task. We incorporate this infor- mation into the syntactic graph by adding directed edges between the word nodes and assign them with dependency labels.</p><p>• Constituency Features. Similar to the depen- dency parse, the constituency parse represents the phrase structure, which is also important to the se- mantic parsing task. Take <ref type="figure">Figure 1</ref> as an example: given the constituent tree that explicitly annotates "not related with AI" (node #1) is a proposition phrase, the model could learn a meaningful em- bedding for this phrase by encoding this structure into the model. Motivated by this observation, we add the non-terminal nodes of the constituent tree and the edges describing their parent-child rela- tionships into the syntactic graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph-to-sequence Model for Semantic Parsing</head><p>After building the syntactic graph for the input text, we employ a novel graph-to-sequence model ( <ref type="bibr" target="#b26">Xu et al., 2018)</ref>, which includes a graph encoder and a sequence decoder with attention mechanism, to map the syntactic graph to the logical form. Conceptually, the graph encoder generates node embeddings for each node by accumulating infor- mation from its K-hop neighbors, and then pro- duces a graph embedding for the entire graph by abstracting all these node embeddings. Next, the sequence decoder takes the graph embedding as the initial hidden state, and calculates the atten- tion over all node embeddings on the encoder side to generate logical forms. Note that this graph encoder does not explicitly encode the edge label information, therefore, for each labeled edge, we add a node whose text attribute is the edge's label.</p><p>Node Embedding. Given the syntactic graph G = (V, E), we take the embedding generation process for node v ∈ V as an example to explain the node embedding generation algorithm 1 :</p><p>(1) We first transform node v's text attribute to a feature vector, a v , by looking up the embedding matrix W e ; (2) The neighbors of v are categorized into for- ward neighbors N (v) and backward neighbors N (v) according to the edge direction. In partic- ular, N (v) returns the nodes that v directs to and N (v) returns the nodes that direct to v; (3) We aggregate the forward representations of</p><formula xml:id="formula_0">v's forward neighbors {h k−1 u , ∀u ∈ N (v)} into a single vector, h k N (v)</formula><p>, where k∈{1, ..., K} is the iteration index. Specifically, this aggregator feeds each neighbor's vector to a fully-connected neural network and applies an element-wise max-pooling operation to capture different aspects of the neigh- bor set. Notice that, at iteration k, this aggregator only uses the representations generated at k − 1. The initial forward representation of each node is its feature vector calculated in step (1); (4) We concatenate v's current forward represen- tation h k−1 v with the newly generated neighbor- hood vector h k N (v) . The resulted vector is fed into a fully connected layer with nonlinear activation function σ, which updates the forward represen- tation of v, h k v , to be used at the next iteration; (5) We update the backward representation of v, h k v using the similar procedure as introduced in step <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref> except that operating on the back- ward representations; (6) We repeat steps (3)∼(5) K times, and the con- catenation of the final forward and backward rep- resentations is used as the final representation of v. Since the neighbor information from different hops may have different impacts on the node em- bedding, we learn a distinct aggregator at each it- eration. Graph Embedding. We feed the obtained node embeddings into a fully-connected neural net- work, and apply the element-wise max-pooling operation on all node embeddings. We did not find substantial performance improvement using mean-pooling. Sequence Decoding. The decoder is an RNN which predicts the next token y i given all the pre- vious words y &lt;i = y 1 , ..., y i−1 , the RNN hidden state s i for time-step i and the context vector c i that captures the attention of the encoder side. In particular, the context vector c i depends on a set of node representations (h 1 ,...,h V ) to which the encoder maps the input graph. The context vec- tor c i is dynamically computed using an attention mechanism over the node representations. The whole model is jointly trained to maximize the conditional log-probability of the correct descrip- tion given a source graph. In the inference phase, we use the beam search algorithm to generate a description with beam size = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings; Geo880, a set of 880 queries to a database of U.S. geography; and ATIS, a set of 5,410 queries to a flight booking system. We use the standard train/development/test split as previous works, and the logical form accuracy as our evaluation metric. The model is trained using the Adam optimizer ( <ref type="bibr" target="#b8">Kingma and Ba, 2014)</ref>, with mini-batch size 30. Our hyper-parameters are cross-validated on the training set for Jobs640 and Geo880, and tuned on the development set for ATIS. The learning rate is set to 0.001. The decoder has 1 layer, and its hid- den state size is 300. The dropout strategy <ref type="bibr" target="#b19">(Srivastava et al., 2014</ref>) with the ratio of 0.5 is ap- plied at the decoder layer to avoid overfitting. W e is initialized using GloVe word vectors from <ref type="bibr" target="#b14">Pennington et al. (2014)</ref> and the dimension of word embedding is 300. For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as <ref type="bibr">ReLU (Glorot et al., 2011</ref>), the parameters of the aggregators are randomly initial- ized. We use the Stanford CoreNLP tool ( ) to generate the dependency and con- stituent trees. Results and Discussion. <ref type="table" target="#tab_1">Table 1</ref> summarizes the results of our model and existing semantic parsers on three datasets. Our model achieves competitive performance on Jobs640, ATIS and Geo880. Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in <ref type="bibr" target="#b3">Dong and Lapata (2016)</ref>, which only uses limited syntactic information. Comparison with Baseline. To better demon- strate that our work is an effective way to uti- lize both multiple trees and the word sequence for semantic parsing, we compare with an addi-tional straightforward baseline method (referred as BASELINE in <ref type="table" target="#tab_1">Table 1</ref>). To deal with the graph input, the BASELINE decomposes the graph em- bedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embed- dings, W seq ; (2) two TreeLSTMs ( <ref type="bibr" target="#b21">Tai et al., 2015)</ref> to extract the dependency tree and constituency features while taking W seq as initial word embed- dings. The resulted word embeddings and non- terminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder.</p><note type="other">Method Jobs Geo ATIS Zettlemoyer and Collins (2007) 79.3 86.1 84.6 Kwiatkowski et al. (2011) - 88.6 82.8 Liang et al. (2011) 90.7 87.9 - Kwiatkowski et al. (2013) - 89.0 - Wang et al. (2014) - 90.4 91.3 Zhao and Huang (2015) 85.0 88.9 84.2 Jia and Liang (2016) - 85.0 76.3 Dong and Lapata (2016)-Seq2Seq 87.1 85.0 84.2 Dong and Lapata (2016)-Seq2Tree 90.0 87.1 84.6 Rabinovich et al. (2017) 92.9 85.7 85.3 Graph2Seq 91.2 88.1 85.9 w/o word order features 86.7 84.4 82.9 w/o dependency features 89.3 85.8 83.8 w/o constituency features 88.9 84.7 84.6 w/ ONLY word order features 88.0 84.8 83.1 BASELINE 88.1 84.9 83.0</note><p>We can see that our model significantly outper- forms the BASELINE. One possible reason is that our graph encoder jointly extracts these features in a unified model by propagating the dependency and constituency information to all nodes in the syntactic graph. However, BASELINE separately models these features using two distinct TreeL- STMs. As a result, the non-terminal tree nodes only retain only one type of syntactic information propagated from their descendants in the tree.</p><p>Ablation Study. In <ref type="table" target="#tab_1">Table 1</ref>, we also report the results of three ablation variants of our model, i.e., without word order features/dependency features/constituency features.</p><p>We find that Graph2Seq is superior to <ref type="bibr">Seq2Seq (Dong and Lapata, 2016</ref>) which is expected since Graph2Seq ex- ploits more syntactic information. Among these features, the word order feature have more impact on the performance than other two syntactic fea- tures. By incorporating either the dependency or the constituency features, the model could gain further performance improvement, which under- lines the importance of utilizing more aspects of syntactic information. Finally, removing both syn- tactic features (w/ ONLY word order) performs slightly worse compared to the Seq2Seq baseline. This shows that using K=10 hops is good enough for memorizing the sentences in our benchmarks, although still weaker compared to a bidirectional LSTM encoder.</p><p>A natural question here is on which type of queries our model could benefit from incorporat- ing these parse features. By analyzing the queries and our predicted logical forms, we find that the parse features mainly improve the prediction ac- curacy for the queries with complex logical forms. <ref type="table" target="#tab_2">Table 2</ref> gives some running examples of com- plicated queries in three datasets. We find that the model that exploits three syntactic information could correctly predict these logical forms while the model that only uses word order features may fail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complicated Query &amp; Predicted Logical Forms</head><p>Jobs Q: what are the jobs for programmer that has salary 50000 that uses c++ and not related with AI Pred: answer(J,(job(J),-((area(J,R),const(R,'ai'))), language(J,L),const(L,'c++'), title(J,P), const(P,'Programmer'),salary greater than(J, 50000,year)))). Geo Q: which is the density of the state that the largest river in the united states run through Pred: answer(A,(density(B,A),state(B), longest(C,(river(C),loc(C,D),const(D,id(usa)))), traverse(C,B)))) ATIS Q: please find a flight round trip from los angeles to tacoma washington with a stopover in san francisco not exceeding the price of 300 dollars for june tenth 1993  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness Study.</head><p>Different from previous works, we evaluate the robustness of our model by creating adversarial examples with the hope to in- vestigate the impact of introducing more syntactic information on robustness. Specifically, we cre- ate two types of adversarial examples and con- duct experiments on the ATIS dataset. Follow- ing <ref type="bibr" target="#b1">Belinkov and Bisk (2017)</ref>, we first experiment with the synthetic noise, SWAP, which swaps two letters (e.g. noise→nosie). It is common to see such noisy information when typing quickly. Given a text, we randomly perform swap on m ∈ {1, 2, 3, 4, 5} words that not correspond to the operators or arguments in logical forms, ensuring the meaning of the text is not changed. We train Graph2Seq on the training data and first evaluate it on the original development data, Dev ori . Then we use the same model but evaluate it on a vari- ant of Dev ori , whose queries contain m swapped words. <ref type="figure" target="#fig_1">Figure 2</ref> summarizes the results of our model on the first type of adversarial examples, i.e., the ATIS development set with the SWAP noise. From <ref type="figure" target="#fig_1">Figure 2</ref>, we can see that (1) the performance of our model on all combinations of features de- grade significantly when increasing the number of swapped words; (2) the model that uses three syn- tactic features (our default model) always achieves the best performance, and the performance gap compared to others increases when rising the num- ber of swapped words; (3) word order features are the most sensitive to the word sequence while the dependency and constituency features seem more robust to such noisy information; (4) thanks to the robustness of the dependency and constituency features, the default model performs significantly better than the one that only uses word order features on the noisy sentences. These findings demonstrate that incorporating more aspects of syntactic information could enhance the robust- ness of the model. We also experiment with the paraphrase of the input text as the second type of adversarial exam- ples. More specifically, we collect the paraphrase of a text by first translating it to the other language such as Chinese and then translating it back to English, using the Google Translate service. We use this method to collect a new variant of Dev ori whose queries are the paraphrases of the original ones. By manually reading these queries, we find 94% queries convey the same meaning as original ones. Similar to the first experiment, we still train the model on Dev ori and evaluate it on the newly created dataset.  Accpara denote the accuracy on the original and paraphrased development set of ATIS, respectively. <ref type="table" target="#tab_4">Table 3</ref> shows the results of our model on the second type of adversarial examples, i.e., the para- phrased ATIS development set. We also report the result of our model on the original ATIS devel- opment set. We can see that (1) no matter which feature our model uses, the performance degrades at least 2.5% on the paraphrased dataset; (2) the model that only uses word order features achieves the worst robustness to the paraphrased queries while the dependency feature seems more robust than other two features. (3) simultaneously utiliz- ing three syntactic features could greatly enhance the robustness of our model. These results again demonstrate that our model could benefit from in- corporating more aspects of syntactic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Existing works of generating text representation has evolved into two main streams. The first one is based on the word order, that is, either generating general purpose and domain independent embed- dings of word sequences ( <ref type="bibr" target="#b23">Wu et al., 2018a;</ref><ref type="bibr" target="#b0">Arora et al., 2017)</ref>, or building Bi-directional LSTMs over the text ( . These methods neglect other syntactic information, which, how- ever, has been proved to be useful in shallow se- mantic parsing, e.g., semantic role labeling <ref type="bibr" target="#b15">(Punyakanok et al., 2008)</ref>. To address this, recent works attempt to incorporate these syntactic infor- mation into the text representation. For example, <ref type="bibr" target="#b25">Xu et al. (2016)</ref> builds separated neural networks for different types of syntactic annotation. <ref type="bibr" target="#b5">Gormley et al. (2015)</ref>; <ref type="bibr">Wu et al. (2018b)</ref> decompose a graph to simpler sub-graphs and embed these sub- graphs independently. Our approach, compared to the above methods, provided a unified solution to arbitrary combinations of syntactic graphs. In parallel to syntactic features, other works leverage additional information such as dialogue and para- phrasing for semantic parsing ( <ref type="bibr" target="#b20">Su and Yan, 2017;</ref><ref type="bibr" target="#b6">Gur et al., 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Existing neural semantic parsers mainly leverage word order features while neglecting other valu- able syntactic information. To address this, we propose to build a syntactic graph which repre- sents three types of syntactic information, and fur- ther apply a novel graph-to-sequence model to map the syntactic graph to a logical form. Ex- perimental results show that the robustness of our model is improved due to the incorporating more aspects of syntactic information, and our model outperforms previous semantic parsing systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Pred: (lambda $0 e (and (flight $0) (round trip $0) (from $0 los angeles) (to $0 tacoma washington) (stop $0 san francisco) (&lt; (cost $0) 300) (day number $0 tenth) (month $0 june) (year $0 1993)))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The logical form accuracy on the development set of ATIS with various swapped words number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Sentence Level Feature 
Dependency Feature 
Constituency Feature 

what 
are 
the 
jobs 
for 
programmer 
that 
has 
salary 
50000 
that 
uses 
c++ 
and 
not 
related 
with 
AI 

nsubj 

cop 
det 

nmod 
nsubj 
acl:relcl 

nsubj 

dobj 
nmod 

acl:relcl 

nsubj 
dobj 
cc 

conj 
case 
neg 

nmod 

NP 

VBP 

CD 
IN 

PP 
VBN 
RB 

CC 
JJ 
ADJP 

ADJP 

WP 
VBP DT NNS 
IN 
NP 
WDT 
VBZ NN 

NP 
NP 

#1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Exact-match accuracy on Jobs640, Geo880 and 

ATIS. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Examples of complicated query and predicted logi-

cal forms. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Evaluation results on ATIS where Accori and</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Interested readers may refer to (Xu et al., 2018) for more implementation details.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Synthetic and natural noise both break neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02173</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01128</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language to logical form with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>abs/1601.01280</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>AIS- TATS 2011</idno>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="315" to="323" />
			<date type="published" when="2011-04-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improved relation extraction with feature-rich compositional embedding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Matthew R Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dialsql: Dialogue based structured query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1339" to="1349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1606.03622</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural semantic parsing with type constraints for semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1516" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lexical generalization in CCG grammar induction for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>John McIntyre Conference Centre</publisher>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The importance of syntactic parsing and inference in semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="287" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abstract syntax networks for code generation and semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1139" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Universal semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cross-domain semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05974</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Morpho-syntactic lexical generalization for ccg semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrienne</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1284" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Word mover&apos;s embedding: From word2vec to document embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">E H</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>En-Hsu Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangli</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04956</idno>
		<title level="m">Pradeep Ravikuma, and Michael Witbrock. 2018b. D2ke: From distance to kernel and embedding</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Question Answering on Freebase via Relation Extraction and Textual Evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph2seq: Graph to sequence learning with attention-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Sheinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00823</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02474</idno>
		<title level="m">Sentencestate lstm for text representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Type-driven incremental semantic parsing with polymorphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
