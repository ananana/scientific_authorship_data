<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NiuTrans Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NiuTrans Lab</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Nlp</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Science</orgName>
								<orgName type="laboratory">CT Lab</orgName>
								<orgName type="institution">University of Macau</orgName>
								<address>
									<settlement>Macau</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1432" to="1441"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a hierarchical atten-tional neural translation model which fo-cuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation (NMT) automatically learns the abstract features of and semantic re- lationship between the source and target sen- tences, and has recently given state-of-the-art re- sults for various translation tasks <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b18">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). The most widely used model is the encoder-decoder framework <ref type="bibr" target="#b18">(Sutskever et al., 2014)</ref>, in which the source sentence is encoded into a dense representation, followed by a decod- ing process which generates the target translation. By exploiting the attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>, the generation of target words is con- ditional on the source hidden states, rather than on the context vector alone. From a model archi- tecture perspective, prior studies of the attentive * Corresponding author encoder-decoder translation model are mainly di- vided into two types.</p><p>The sequence-to-sequence model treats a sen- tence as a sequence of tokens. The most funda- mental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summa- rizes the preceding words <ref type="bibr" target="#b18">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b3">Cho et al., 2014b)</ref>. Although <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref> used a bidirectional recurrent neural net- work (RNN) <ref type="bibr" target="#b14">(Schuster and Paliwal, 1997</ref>) to con- sider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpre- tations of sentence structure ( <ref type="bibr" target="#b5">Eriguchi et al., 2016;</ref><ref type="bibr" target="#b19">Tai et al., 2015)</ref>. By incorporating additional fea- tures into a sequential model,  and <ref type="bibr" target="#b17">Stahlberg et al. (2016)</ref> suggest that a greater amount of linguistic information can im- prove the translation performance.</p><p>The tree-to-sequence model encodes a source sentence according to a given syntactic tree over the sentence. The existing tree-based en- coders ( <ref type="bibr" target="#b19">Tai et al., 2015;</ref><ref type="bibr" target="#b5">Eriguchi et al., 2016;</ref><ref type="bibr" target="#b25">Zhou et al., 2016</ref>) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local infor- mation, while failing to capture the global mean- ing of a sentence. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the phrases "take up" 1 and "a position" 2 have differ- ent meanings in different contexts. However, in composing the representations h VP 3 and h NP 7 for phrases VP 3 and NP 7 , the current approaches do not account for the differences in meaning which arise as a result of ignoring the neighboring con- text as well as the remote context, i.e. h NP 7 ← h PP 8 (sibling) and h VP 3 ← h NP 7 (child of sibling). More specifically, at the encoding step t, the gen- erated phrase is based on the results at the previous time steps h t−1 and h t−2 , but has no information about the parent phrases h t for t &gt; t.</p><p>To address the above problems, we propose a novel architecture, a bidirectional hierarchical en- coder, which extends the existing attentive tree- structured models <ref type="bibr" target="#b5">(Eriguchi et al., 2016</ref>). In con- trast to the model of <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref>, we first use a bidirectional RNN <ref type="bibr" target="#b14">(Schuster and Paliwal, 1997)</ref> at lexical level to concatenate the forward and backward states as the hidden states of source words, to capture the preceding and following con- texts (described in Section 3.1). Secondly, we pro- pose a bidirectional tree-based encoder (described in Section 3.2), in which the original bottom-up encoding model is extended using an additional top-down encoding process. In the bidirectional hierarchical model, the vector representations of the sentence, phrases as well as words, are there- fore based on the global context rather than local information.</p><p>To effectively leverage hierarchical representa- tions in generating the target words, we adopt a variant weighted tree-based attention mecha- nism (described in Section 3.4) in which a time- dependent gating scalar is used to control the pro- portion of conditional information between the word and phrase vectors. To alleviate the out-of- vocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level</p><formula xml:id="formula_0">h p 2,3 h p 4,5 h p 7,8 h p 6,8 h p 4,8 h p 2,8 h p 1,8</formula><p>Figure 2: The tree-based model of <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> comprising a structured and sequential en- coder.</p><p>by integrating byte-pair encoding (BPE)  into the tree-based model (as de- scribed in Section 3.3). Experimental results for the NIST English-to-Chinese translation task re- veal that the proposed model significantly outper- forms the vanilla tree-based ( <ref type="bibr" target="#b5">Eriguchi et al., 2016)</ref> and sequential NMT models ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>) (Section 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tree-Based Neural Machine Translation</head><p>A neural machine translation system (NMT) aims to use a single neural network to build a transla- tion model, which is trained to maximize the con- ditional distribution of sentence pairs using a par- allel training corpus <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b18">Sutskever et al., 2014;</ref><ref type="bibr">Cho et al., 2014b,a)</ref>. By incorporating syntactic information, the tree- based NMT exploits an additional syntactic struc- ture of the source sentence to improve the trans- lation. Since most existing NMTs generate one target word at a time, given a source sentence x = (x 1 , ..., x N ) and its corresponding syntactic tree tr, the conditional probability of a target sen- tence y = (y 1 , ..., y M ) is formally expressed as:</p><formula xml:id="formula_1">p(y | x, tr) = M 1 p(y j | y 1 , ..., y j−1 , x, tr; θ),</formula><p>where θ represents the model parameters. A tree- based NMT consists of a tree-based encoder and a decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tree-Based Encoder</head><p>In a tree-based encoder, the source language x is encoded according to a given syntactic structure tr of the sentence. As shown in <ref type="figure">Figure 2</ref>, <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> employed a forward Long Short-Term Memory (LSTM) <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b7">Gers et al., 2000</ref>) recurrent neural network (RNN) to encode the lexical nodes and a tree- LSTM ( <ref type="bibr" target="#b19">Tai et al., 2015)</ref> to generate the phrase representations in a bottom-up fashion. In the present study, we utilize the gated recurrent unit (GRU) ( <ref type="bibr" target="#b3">Cho et al., 2014b</ref>) instead of an LSTM, in view of its comparable performance ( <ref type="bibr" target="#b4">Chung et al., 2014</ref>) and since it yields even better results for cer- tain tasks <ref type="bibr">(Józefowicz et al., 2015)</ref>. The lexical an- notation vectors (h l 1 , ..., h l N ) are sequentially gen- erated by using a GRU. The i-th leaf node vector is calculated as:</p><formula xml:id="formula_2">h l i = f l GRU (x i , h l i−1 ),<label>(1)</label></formula><p>where x i is the i-th source word embedding and h l i−1 denotes the previous hidden state. The parent hidden state h ↑ i,j summarizes its left child h ↑ i,k and right child h ↑ k+1,j (i &lt; k &lt; j) by applying the tree-GRU ( <ref type="bibr" target="#b25">Zhou et al., 2016</ref>) as follows:</p><formula xml:id="formula_3">z ↑ i,j = σ(U L (z) h ↑ i,k + U R (z) h ↑ k+1,j + b ↑ (z) ) r ↑ i,k = σ(U L (rL) h ↑ i,k + U R (rL) h ↑ k+1,j + b ↑ (rL) ) r ↑ k+1,j = σ(U L (rR) h ↑ i,k + U R (rR) h ↑ k+1,j + b ↑ (rR) ) h ↑ i,j = tanh(U L (h) (r ↑ i,k h ↑ i,k ) + U R (h) (r ↑ k+1,j h ↑ k+1,j ) + b ↑ (h) ) h ↑ i,j = z ↑ i,j h ↑ i,j + (1 − z ↑ i,j )(h ↑ i,k + h ↑ k+1,j ),</formula><p>where z ↑ i,j is the update gate; r ↑ i,k , r ↑ k+1,j are the reset gates for the left and right children; h ↑ i,j de- notes the candidate activation; U L (·) and U R (·) repre- sent weight matrices; b ↑ (·) denote bias vectors; σ is the logistic sigmoid function; and the operator denotes element-wise multiplication between vec- tors. The phrase representations are recursively built in an upward direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoding with a Tree-Based Attention Mechanism</head><p>In generating the target words, we employ a se- quential decoder with an input-feeding method ( <ref type="bibr" target="#b12">Luong et al., 2015</ref>) and attention mechanism ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). The conditional proba- bility of the j-th target word y j is calculated using a non-linear function f sof tmax :</p><formula xml:id="formula_4">p(y j | y 1 , ..., y j−1 , x, tr; θ) = f sof tmax (c j ),</formula><p>where c j is the composite hidden state, which con- sists of a target hidden state s j and a context vector d j :</p><formula xml:id="formula_5">c j = f tanh ([s j , d j ]).</formula><p>Given the previous target word y j−1 , the concate- nation of the previous hidden state s j−1 and the previous context vector c j−1 (input-feeding) (Lu- ong et al., 2015), s j , is calculated using a standard sequential GRU network:</p><formula xml:id="formula_6">s j = f dec gru (y j−1 , [s j−1 , c j−1 ]).</formula><p>The context vector d j is computed using an at- tention model which is used to softly summarize the attended part of the source-side representa- tions. <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref> adopted a tree-based attention mechanism to consider both the word and phrase vectors:</p><formula xml:id="formula_7">d j = N i=1 α j (i)h l i + N −1 k=1 α j (k)h p k ,<label>(2)</label></formula><p>where h l i is the i-th hidden state of the source word at leaf level, and h p k is the k-th hidden state of the source phrase. The weight α j (t) of node t is com- puted by:</p><formula xml:id="formula_8">α j (t) = exp(e t ) N i=1 exp(e l i ) + N −1 k=1 exp(e p k ) e t = (V a ) T tanh(U a s j + W a h t + b a ),</formula><p>where h t is the hidden state of the node. V a , U a , W a and b a are the model parameters.</p><formula xml:id="formula_9">h ↓ 1 h ↓ 2 h ↓ 3 h ↓ 4 h ↓ 5 h ↓ 6 h ↓ 7 h ↓ 8 h ↓ 2,3 h ↓ 4,5 h ↓ 7,8 h ↓ 6,8 h ↓ 4,8 h ↓ 2,8 h ↓ 1,8</formula><p>Figure 3: A top-down encoding process updates the hidden states recursively from root to leaf nodes. The red and blue lines denote the use of different learning parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bidirectional Leaf-Node Encoding</head><p>As discussed in Section 1, the unidirectional re- current neural network reads an input sequence in order, from the first symbol to the last. In order to generate leaf node annotation vectors which jointly take into account both preceding and following annotations, we exploit a bidirectional RNN encoder ( <ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref>. The hidden state of the i-th leaf node h l i is the concatenation of the forward and backward vectors:</p><formula xml:id="formula_10">h l i = [ − → h l i , ← − h l i ],</formula><p>where − → h l i is obtained by a rightward GRU, as shown in Equation 1, and a leftward GRU calcu- lates ← − h l i , as follows:</p><formula xml:id="formula_11">← − h l i = f ← GRU (x i , ← − h l i−1 ),</formula><p>where ← − h l i−1 is the previous hidden state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bidirectional Tree-Node Encoding</head><p>Since the hidden states of leaf nodes are derived in a sequential, context-sensitive way, by gen- erating phrase annotations in a bottom-up fash- ion, the sequential context can be propagated to tree nodes. However, the learned annotation vec- tors still fail to capture global information from the upper nodes. To enhance the representations with global semantic information, we propose to use a standard GRU recurrent network to update representations in a top-down fashion, as shown in <ref type="figure">Figure 3</ref>. The annotation vectors, which are learned by the previous encoding steps, are fed to the updating process. First, we treat the bottom-up hidden state of root h ↑ root , which covers the global meaning as well as the syntactic information of the source sentence, as the initial state of the top-down GRU network:</p><formula xml:id="formula_12">h ↓ root = h ↑ root .</formula><p>Given an updated hidden state of the parent node h ↓ i,j , the hidden states of left and right children h ↓ i,k and h ↓ k+1,j are calculated as:</p><formula xml:id="formula_13">h ↓ i,k = f ld GRU (h ↑ i,k , h ↓ i,j ) h ↓ k+1,j = f rd GRU (h ↑ k+1,j , h ↓ i,j ),</formula><p>where h ↑ i,k and h ↑ k+1,j are the left and right child annotation vectors generated via the bottom- up tree-GRU network. Contrary to the simi- lar top-down encoding for sentiment classifica- tion ( <ref type="bibr" target="#b11">Kokkinos and Potamianos, 2017)</ref>, which uses same weighting parameters to handle both left and right child nodes, f ld GRU and f rd GRU with different parameters are applied in the proposed model to distinguish the left and right structural informa- tion. According to the definition of a GRU ( <ref type="bibr" target="#b3">Cho et al., 2014b</ref>), f ld GRU uses an update gate z ↓ i,k , a reset gate r ↓ i,k and a candidate activation h ↓ i,k to generate h ↓ i,k , as follows:</p><formula xml:id="formula_14">z ↓ i,k = σ(W ld (z) h ↑ i,k + U ld (z) h ↓ i,j + b ld (z) ) r ↓ i,k = σ(W ld (r) h ↑ i,k + U ld (r) h ↓ i,j + b ld (r) ) h ↓ i,k = tanh(W ld (h) h ↑ i,k + U ld (h) (r ↓ i,k h ↓ i,j ) + b ld (h) ) h ↓ i,k = (1 − z ↓ i,k )h ↓ i,j + z ↓ i,k h ↓ i,k ,<label>(3)</label></formula><p>where W ld (·) and U ld (·) represent weight matrices, and b ld (·) denote bias vectors. f rd GRU is defined in a similar way.</p><p>From a linguistic point of view, in the top-down GRU network, the reset gate is able to retain the useful global information and drop irrelevant in- formation from the parent state h ↓ i,j , while the pro- portions of the global context from the top-down state h ↓ i,j , and the local context from the bottom- up state h ↑ i,k are controlled by the update gate. As it covers both the partial meaning of the phrase and the whole meaning of the sentence, h ↓ i,k is re- garded as the final representation of node i,k :</p><formula xml:id="formula_15">h p i,k = h ↓ i,k .</formula><p>With the propagation of information from root to leaf nodes, the i-th leaf node representation is up- dated as:</p><formula xml:id="formula_16">h l i = h ↓ i .</formula><p>As each source-side hidden state of the leaf nodes and tree nodes carries the hierarchical information of the sentence, we interpret such an encoded state as a hierarchical representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Handling Out-of-Vocabulary: Tree-Based Rare Word Encoding</head><p>In NMT, the translation of rare words and un- known words is an open problem, since the com- putational cost increases with the size of the vo- cabulary.  proposed a sim- ple and effective approach to handling out-of- vocabulary by representing rare words as a se- quence of sub-word units, which are segmented using byte-pair encoding (BPE) <ref type="bibr" target="#b6">(Gage, 1994)</ref>.</p><formula xml:id="formula_17">x 1 x 2 x 1 3</formula><note type="other">x 2 3 x 3 3 x 4 x 5 h 1 h 2 h 4 h 5 h 1 3 h 2 3 h 3 3 h 1,2 3 h 1,3 3 Figure 4: Encoding sub-word units with an addi- tional binary lexical tree, where x 1 3 , x 2 3 , x 3 3 are the sub-units of word x 3 .</note><p>We propose a variant tree-based rare word en- coding approach which extends the tree-based model to the sub-word level. Sub-word units are encoded following an additional binary lexical tree. For a sentence x = (x 1 , ..., x i , ..., x N ), BPE segments the word x i into a sequence of sub-word units (x 1 i , ..., x n i ). The binary lexical tree is sim- ply built by composing two nodes in a rightwards fashion, (((x 1 i , x 2 i ), x 3 i )...), x n i ), as shown in <ref type="figure">Fig- ure 4</ref>. From the i-th leaf node, the original syn- tactic tree is extended downwards using the binary lexical tree, and the set of leaf nodes are replen- ished as x = (x 1 , ..., x 1 i , x 2 i , ..., x n i , ..., x N ). Sub- word units can therefore be regarded as leaf nodes, and can be encoded using the proposed encoder, as illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. The experimental results in Section 4.1 demonstrate the effectiveness of this simple approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoder with Weighted Variant of Attention Mechanism</head><p>Since each representation carries both local and global information, in this case, attending fairly to the lexical and phrase representations in each h p</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4,6</head><p>h p 4,9</p><p>h p decoding step may cause the problem of over- translation (repeatedly attending and translating the same constituent of a sentence). An alterna- tive approach is to balance the attentive informa- tion between the lexical and phrase vectors in the context vector. To effectively leverage these hi- erarchical representations, we propose a weighted variant of the tree-based attention mechanism (the original is defined in Equation 2). Formally, the calculation of the context vector d j at step j is modified as:</p><formula xml:id="formula_18">d j = (1 − β j ) n i=1 α j (i)h l i + β j n−1 k=1 α j (k)h p k (4)</formula><p>where β j ∈ [0, 1] is used to weight the expected importance of the representations. Inspired by work on a multi-modal NMT ( <ref type="bibr" target="#b1">Calixto et al., 2017)</ref> which exploits a gating scalar ( <ref type="bibr" target="#b21">Xu et al., 2015)</ref> to weight the image context vector, we use such a scalar in our model in order to dynamically adapt the weighting scalar. The gating scalar β j at step j is calculated by :</p><formula xml:id="formula_19">β j = σ(W β c j−1 + b β ),</formula><p>where W β and b β represent the model parame- ters. In contrast with α, which denotes the cor- respondence between each source annotation and the current target hidden state, β is dominated by the target composite hidden state alone. In other words, β is a time-dependent scalar in relation to the current target word, and therefore enables the attention model to explicitly quantify how far the leaf and no-leaf states contribute to the word pre- diction at each time step. In the proposed model, the phrase and lexical context vectors are learned by a single attention model, meaning that they are dependent, and the gating scalar weights the phrase and lexical context vectors in complemen- tary fashion, as shown in Equation 4. This dis- tinguishes the model from that introduced by <ref type="bibr" target="#b1">Calixto et al. (2017)</ref>, in which the context vectors of the source sentence and image (bi-modal) are mea- sured using two independent attention models and the gating scalar is merely used to weight the im- age context vector. We evaluate the proposed model on an English- to-Chinese translation task. For reasons of com- putational efficiency, we extracted 1.4M sentence pairs, in which the maximum length of the sen- tence was 40, from the LDC parallel corpus 3 as our training data. The models were developed using NIST mt08 data and were examined using NIST mt04, mt05, and mt06 data. The num- ber of sentences in each dataset is shown in Ta- ble 1. On the English side, we used the constituent parser ( <ref type="bibr" target="#b24">Zeng et al., 2014</ref><ref type="bibr" target="#b23">Zeng et al., , 2015</ref> to produce a bi- nary syntactic tree for each sentence, in constrast to the use of the HPSG parser by <ref type="bibr" target="#b5">Eriguchi et al. (2016)</ref>. On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans ( <ref type="bibr" target="#b20">Xiao et al., 2012)</ref>.</p><p>To avoid data sparsity, words referring to time, date and number, which are low in frequency, are generalized as '$time', '$date' and '$number'. In addition, as described in Section 3.3, the vocab- ularies are further compressed by segmenting the rare words into sub-word units using BPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>As shown in <ref type="table">Table 2</ref>, which gives the statistics of the token types, we limit the source and target vo- <ref type="table">Table 2</ref>: The vocabulary size of the training set before and after applying the generalization and BPE segmentation.</p><formula xml:id="formula_20">Training Set Original Generalization BPE |V | in En 159k 120k 40k |V | in Zh 198k 125k 40k</formula><p>cabulary size to 40,000, in order to cover all the English and Chinese tokens. The dimensions of word embedding and hidden layer are respectively set as 620 and 1,000. Due to the concatenation in the bidirectional leaf-node encoding, the dimen- sions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500. In order to prevent over-fitting, the train- ing data is shuffled following each epoch. More- over, the model parameters are optimized using AdaDelta <ref type="bibr" target="#b22">(Zeiler, 2012)</ref>, due to its capability for dynamically adapting the learning rate. We set the mini-batch size to 16 and the beam search size to 5. The accuracy of the translation relative to a ref- erence is assessed using the BLEU metric <ref type="bibr" target="#b13">(Papineni et al., 2002</ref>). In order to give an equitable comparison, all the NMT models used for com- parison are implemented or re-implemented using GRU in our code, based on dl4mt 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Enhanced Hierarchical Representations</head><p>Firstly, the effectiveness of the enhanced hierar- chical representations is evaluated through a set of experiments, the results of which are summarized in <ref type="table">Table 3</ref>. Compared with the original tree-based en- coder ( <ref type="bibr" target="#b5">Eriguchi et al., 2016)</ref>, the model with bidi- rectional leaf-node encoding (described in Sec- tion 3.1) shows better performance. This also re- veals that the future context at leaf level can con- tribute to word prediction. Secondly, although the representations of leaf nodes are learned in a sequential, context-sensitive way, the transla- tion quality is further improved by considering the global semantic information in the top-down en- coding <ref type="figure">(Section 3.2)</ref>.</p><p>By incorporating the above enhancements into the model, the proposed hierarchical encoder yields significant improvements over both the se- quential and the tree-based models.  <ref type="table">Table 3</ref>: Translation results for the various models. The first column shows the models; the second column indicates whether the corresponding experiment uses BPE data. The number of parameters (M = millions) in each model is given in the third column. The remaining columns are the translation accuracies for the test sets and development set, evaluated using BLEU scores (%). "↑ / ⇑": indicates that the hierarchical encoder is significantly better than the vanilla tree-based encoder (p &lt; 0.05/p &lt; 0.01).</p><note type="other">top-down encoding no 101.1M 32.85 25.37 25.30 18.26 + tree-based rare word encoding yes 95.0M 33.02 25.62 25.24 18.59 hierarchical encoder (β = 0.5) no 104.1M 32.91↑ 25.55↑ 25.52↑ 18.46↑ hierarchical encoder (β = 0.5) yes 104.1M 33.81⇑ 26.47⇑ 26.31⇑ 19.41⇑ + gating scalar yes 105.1M 34.33⇑ 26.72⇑ 26.58⇑ 20.10⇑</note><p>based model to sub-word level (Section 3.3). In addition, we evaluate our tree-based rare word en- coding method against the conventional rare word encoding ( ) using the sequen- tial encoder ( <ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). The empir- ical results confirm that our proposed tree-based BPE method achieves performance comparable to that of the standard BPE in the sequential model, but is applicable to the tree-based NMT model. Overall, the proposed hierarchical encoder has demonstrated the ability to effectively model source-side representations from both the sequen- tial and structural context. The NMT systems based on the proposed model significantly outper- form those of conventional models using the se- quential encoder and the tree-based encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Weighted Attention Model</head><p>As discussed in Section 3.4, in order to effectively leverage hierarchical representations in generating the target word, we adopt a variant weighted tree- based attention mechanism which incorporates a scalar to control the proportion of conditional in- formation between the word and phrase vectors. By manually or automatically varying the weight β, the utilization of the weighted attention model is assessed for four cases:</p><p>• β = 0.0: We manually set the weight of phrase vectors to 0.0; in other words, the de- coder is forced to ignore the phrase vectors. The final translation is therefore generated by merely summarizing the leaf vectors.</p><p>• β = 0.5: The representations of non-leaf nodes and leaf nodes participate equally in the translation process. The decoder of this case therefore employs the same attention mechanism as that of the original model (Sec- tion 2.2).</p><p>• β = 1.0: In the reverse of the first case, the weight of the leaf nodes is manually set to 0.0. Thus, only the phrase vectors are used to predict the target words.</p><p>• Gating scalar (GS): A gating scalar is used for dynamically learning to control the pro- portion in which the lexical and phrase con- texts contribute to the generation of the target words (Section 3.4  The experimental results are shown in <ref type="table" target="#tab_3">Table 4</ref>. The model which attends only to lexical annota- tion vectors (β = 0.0) gives slightly better per- formance than that which uses equal weights for </p><formula xml:id="formula_21">α (10 −2 ) Our: 该 组织 不会 在 在 在 成员国 以外 的 地区 使用 武力 Ref: 该 组织 不会 在 成员国 以外 的 地区 动用 军队 tr-enc: 该 组织 不会 在 成员国 境外 使用 武力 sq-enc: 该 组织 不会 使用 其 成员国 以外 的 武装力量</formula><p>β: 0.17 0.14 0.22 0.22 0.27 0.22 0.19 0.44 0.14 0.56 <ref type="figure">Figure 6</ref>: Translations of an English sentence output using the NMT models with bidirectional hierar- chical model (our), sequential encoder (seq-enc) and original tree-based encoder (tr-enc). Ref indicates the reference Chinese sentence. The attention scores (α), which are noted over the source-side syntactic tree, are output by the bidirectional hierarchical model at the step where the fourth target word "在" is translated. The sequence of scores β denote the value of the gating scalar at each translation step.</p><p>lexical and phrase vectors (β = 0.5). The use of global information contributes to distinguish- ing the differences between word meanings, al- though the similar semantic information in the lexical and phrase representations aggravates the over-translation problem observed in the transla- tion results. However, we found that the model which attends only to phrase representations tends to generate shorter translation of an average of 21.13 words in length, as shown in the last column of the first row of <ref type="table" target="#tab_3">Table 4</ref>. Furthermore, the model that neglects the leaf representations (β = 1.0) is likely to underperform the others that are also conditioned on the leaf nodes. Even though the phrase representations are derived from the lexical level via a bottom-up encoding, we believe it is unable to fully capture the lexical information of the source sentence. Through the use of the gating scalar, the hierarchical model achieves progressive improvements, as shown in <ref type="table" target="#tab_3">Tables 3 and 4</ref>, the problem of over-translation is also alleviated. The representations of non-leaf nodes can be regarded as supplements in the translation process. <ref type="figure">Figure 6</ref> shows an English sentence and its binary tree representation, together with the correspond- ing Chinese translations produced by the different NMT models. All the models successfully give the correct Chinese translation "该 组 织 不 会" for the first three words of the English sentence "the organization wouldn't". Differences appear in the translation of the fourth word, and these lead to markedly different meanings. The translation "使用 其 成员国 以外 的 武装力量" output by the sequential model, means "use the armed forces other than its member states" where "other than its member states" is incorrectly interpreted as a complement to "armed forces". This is caused by the intrinsic limitations of the sequential model, whereby it is unable to properly interpret the syn- tactic relationship of words. By explicitly incorpo- rating the syntactic information, both the proposed hierarchical model and the tree-based model can accurately attend to the dashed section of <ref type="figure">Figure 6</ref>, and the translations can be correctly generated to reflect the meaning of the source sentence. The distinction between the translations produced by the original tree-based model and our hierarchical model is the interpretation of the words "areas out- side". The tree-based model interprets it into "境 外 (outside)", while our model correctly translates it into "以外 的 地区 (areas outside)". We believe that, with the help of global and local contextual information, our model is able to capture the short as well as long range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Qualitative Analysis</head><p>We conducted an in-depth analysis of the BPE segmented units of rare words. It was observed that the sub-word units could be categorized into three groups. The first group of units involve the phonetic Romanization (Pinyin) of Chinese. In translation, these are simply transliterated into the corresponding Chinese characters. As shown in the second row of <ref type="table">Table 5</ref>, "Liu/jing/min" is a person's name. The segmented units are the phonetic representations. Both models can suc- cessfully transliterate this into the Chinese equiv- alent, "刘/敬/民". The second group of sub-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>Hierarchical Sequential liu/jing/min 刘/敬/民 刘/敬/民 刘/敬/民 Liú/j` ıng/mín Liú/j` ıng/mín Liú/j` ıng/mín adventur/er 探险家 探险家 探险者 T` an xiˇanxiˇan ji¯ a T` an xiˇanxiˇan ji¯ a T` an xiˇanxiˇan zhě hi/k/ed 上调 上升 发生 Shàng tiáo Shàng sh¯ eng F¯ a sh¯ eng <ref type="table">Table 5</ref>: Translation examples of sub-words, where '/' indicates a separation between sub-word units. The first two columns show the segmented words and their Chinese references. The last two columns report the translations given by the hier- archical and sequential models respectively.</p><p>word units are likely to represent the word mor- phemes. The words are segmented into sub-word units, which are to some extent close to the lin- guistic word stems and suffixes. For example, the word "adventurer" is segmented into "adven- tur/er", which is correctly translated into the Chi- nese translations "探险/家" and "探险/者" respec- tively by the hierarchical and sequential models, while the third group of sub-word units offer no linguistic interpretation. It is easy to see, using the BPE algorithm, that the identification of sub- word units is merely based on their frequency in the training data, with the result that not all units are well-formed linguistic morphemes. However, an interesting finding arises regarding the transla- tion of these segmented units. In the sequential model, the word is incorrectly translated; how- ever, it can be correctly translated by the hierar- chical model. Taking "hi/k/ed" as an example, the sequential model gives an incorrect translation "发生(happened)", while the hierarchical model translates it into "上升(rise)" which is a synonym of "hiked". This result indicates that in our hi- erarchical model, the parent node of hierarchical representation for sub-word units "hi/k/ed" is bet- ter able to capture the meaning of the word as a whole; this cannot be captured independently by the sequential model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an improved NMT sys- tem with a novel bidirectional hierarchical en- coder, which enhances the source-side representa- tions of a sentence, that is, both phrases and words, with local and global context information. By in- troducing a tree-based rare word encoding, the hi- erarchical model is extended to sub-word level in order to alleviate the problem of OOVs. To ef- fectively leverage the enhanced hierarchical repre- sentations, we also propose a weighted variant of the attention model which dynamically adjusts the proportion of conditional information between the lexical and phrase annotation vectors. Experimen- tal results for NIST English-Chinese translation tasks demonstrate that the proposed model signif- icantly outperforms the vanilla tree-based and se- quential NMT models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Induction of phrase and sentence representations over the syntactic structure of a sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of the bidirectional hierarchical encoder: representations are enhanced by a bidirectional leaf-node encoding and a bidirectional tree-node encoding. The green nodes indicate the sub-word representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The problem of OOV is alleviated by further extending the tree-</figDesc><table>Model 

BPE # of params MT04 MT05 MT06 Dev. 
sequential encoder 
no 
86.8M 
31.26 
23.98 
24.02 
17.20 
+ sequential rare word encoding 
yes 
86.8M 
32.54 
25.09 
25.07 
18.19 
+ tree-based rare word encoding 
yes 
104.1M 
32.56 
25.30 
24.96 
18.33 
tree-based encoder 
no 
95.0M 
31.90 
24.68 
24.40 
17.63 
+ bidirectional leaf-node encoding 
no 
92.0M 
32.13 
24.94 
25.02 
18.12 
+ </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Model 
BLEU Perplexity Avg. Length 
β = 1.0 17.16 
98.65 
21.13 
β = 0.5 19.41 
94.73 
23.08 
β = 0.0 19.83 
94.68 
23.33 
GS 
20.10 
94.18 
23.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Translation results for the development 
set. The last column indicates the average length 
of translation sentences, and the average length of 
reference sentences is 23.19. 

</table></figure>

			<note place="foot" n="1"> Take up has the meanings of start doing something new, use space/time, accept an offer, etc. 2 Position has the meanings of location, job offer, rank/status, etc.</note>

			<note place="foot" n="3"> The Bidirectional Hierarchical Model Although the tree-based encoder of Eriguchi et al. (2016) has shown certain advantages in translation tasks involving distant language pairs, e.g. English-Japanese, the representation of a phrase relies solely on its child nodes, and the word representation at leaf level only takes into account the sequential information. We argue that the incorporation of more hierarchical information into the representations may contribute to an improvement in the translation. In particular, the use of global information can help in distinguishing the differences between word meanings. Based on this hypothesis, we propose an alternative architecture, the bidirectional hierarchical model, to enhance the source-side representations.</note>

			<note place="foot" n="3"> Our training data was selected from LDC2000T46, LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08 and LDC2005T10.</note>

			<note place="foot" n="4"> https://github.com/nyu-dl/ dl4mt-tutorial</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the Na-tional Natural Science Foundation of China </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Doubly-Attentive Decoder for Multi-modal Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Campbell</surname></persName>
		</author>
		<idno>abs/1702.01287</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
		<meeting>SSST@EMNLP 2014, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>Holger Schwenk, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS 2014) Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tree-to-Sequence Attentional Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A New Algorithm for Data Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to Forget: Continual Prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Empirical Exploration of Recurrent Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Structural Attention Neural Networks for Improved Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippos</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno>abs/1701.01811</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attentionbased Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bidirectional Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Syntactically Guided Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS 2014)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics, System Demonstrations</title>
		<meeting>the 52th Annual Meeting of the Association for Computational Linguistics, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph-Based Lexicon Regularization for PCFG With Latent Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexicon Expansion for Latent Variable Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Liangye He, and Qiuping Huang</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modelling Sentence Pairs with Tree-structured Attentive Encoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 26th International Conference on Computational Linguistics</title>
		<meeting>26th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2912" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
