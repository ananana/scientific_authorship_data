<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TAG Parsing with Neural Networks and Vector Representations of Supertags</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
							<email>jungo.kasai@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Linguistics</orgName>
								<orgName type="department" key="dep3">Dept. of Linguistics</orgName>
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">Yale University</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">DSI Columbia University</orgName>
								<orgName type="institution" key="instit5">LIF Université Aix Marseille</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frank</surname></persName>
							<email>robert.frank@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Linguistics</orgName>
								<orgName type="department" key="dep3">Dept. of Linguistics</orgName>
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">Yale University</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">DSI Columbia University</orgName>
								<orgName type="institution" key="instit5">LIF Université Aix Marseille</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Linguistics</orgName>
								<orgName type="department" key="dep3">Dept. of Linguistics</orgName>
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">Yale University</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">DSI Columbia University</orgName>
								<orgName type="institution" key="instit5">LIF Université Aix Marseille</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
							<email>rambow@ccls.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Linguistics</orgName>
								<orgName type="department" key="dep3">Dept. of Linguistics</orgName>
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">Yale University</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">DSI Columbia University</orgName>
								<orgName type="institution" key="instit5">LIF Université Aix Marseille</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Nasr</surname></persName>
							<email>Alexis.Nasr@lif.univ-mrs.fr</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of Computer Science</orgName>
								<orgName type="department" key="dep2">Dept. of Linguistics</orgName>
								<orgName type="department" key="dep3">Dept. of Linguistics</orgName>
								<orgName type="institution" key="instit1">Yale University</orgName>
								<orgName type="institution" key="instit2">Yale University</orgName>
								<orgName type="institution" key="instit3">Yale University</orgName>
								<orgName type="institution" key="instit4">DSI Columbia University</orgName>
								<orgName type="institution" key="instit5">LIF Université Aix Marseille</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TAG Parsing with Neural Networks and Vector Representations of Supertags</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1712" to="1722"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (ele-mentary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely , and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is &quot;almost parsing.&quot; We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distri-butional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work has applied Combinatory Categorial Grammar <ref type="bibr">(CCG, Steedman and Baldridge (2011)</ref>) to the problem of broad-coverage parsing in order to derive grammatical representations that are suf- ficiently rich to support tasks requiring deeper rep- resentation of a sentence's meaning ( <ref type="bibr" target="#b19">Lewis et al., 2015;</ref><ref type="bibr" target="#b30">Reddy et al., 2016;</ref><ref type="bibr" target="#b22">Nadejde et al., 2017</ref>). Yet CCG is only one of a number of mildly context-sensitive grammar formalisms that can provide such rich representations, and each has distinct advantages. In this paper we explore the applicability of another formalism, Tree Adjoin- ing Grammar (TAG, <ref type="bibr" target="#b17">Joshi and Schabes (1997)</ref>), to the task of broad-coverage parsing.</p><p>TAG and CCG share the property of lexicaliza- tion: words are associated with elementary units of grammatical structure which are composed dur- ing a derivation using one of a small set of oper- ations to produce a parse tree. The task of pars- ing involves the construction of a derivation tree that encodes the application of this set of actions to a set of elementary lexically-associated objects. TAG differs from CCG in having an even richer set of lexical units, so that the identification of these units in a derivation could be even more informa- tive for subsequent tasks involving semantic inter- pretation, translation and the like, which have been the focus of CCG-based work.</p><p>The elementary units of CCG and TAG (cate- gories for CCG, and elementary trees for TAG) determine a word's combinatory potential, in a way that is not the case for the usual part-of- speech tags used in parsing. Indeed, the assign- ment of elementary objects to the words in a sen- tence almost determines the possible parse for a sentence. The near uniqueness of a parse given a sequence of lexical units motivated <ref type="bibr" target="#b3">Bangalore and Joshi (1999)</ref> to decompose the parsing problem into two phases: supertagging, where elementary objects, or supertags, are assigned to each word, and stapling, where these supertags are combined together. They claim that given a perfect supertag- ger, a parse of a sentence follows from syntac- tic features provided by the supertags, and there- fore, supertagging is "almost parsing." This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: ( <ref type="bibr" target="#b2">Bangalore et al., 2009;</ref><ref type="bibr" target="#b9">Chung et al., 2016)</ref>, CCG: ( <ref type="bibr" target="#b20">Lewis et al., 2016)</ref>). However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re-sults in inaccuracies that prevent us from effec- tively utilizing syntactic information provided by the imperfect set of supertags that are assigned. This problem is even more severe for TAG parsing. TAG differs from CCG in having a smaller set of combinatory operations, but a more varied set of elementary objects: the TAG-annotated version of the Penn Treebank that we use <ref type="bibr" target="#b7">(Chen, 2001</ref>) in- cludes 4727 distinct supertags (2165 occur once) while the CCG-annotated version <ref type="bibr" target="#b16">(Hockenmaier and Steedman, 2007</ref>) includes 1286 distinct su- pertags (439 occur once). As a result, building a robust, broad-coverage TAG parser has proven difficult.</p><p>In this work, we show that robust supertagging- based parsing of TAG is indeed possible by us- ing a dense representation of supertags that is in- duced using neural networks. In the first half of the paper, we present a neural network supertag- ger based on a bi-directional LSTM (BLSTM) ar- chitecture, inspired by the work of Xu (2015) and <ref type="bibr" target="#b20">Lewis et al. (2016)</ref> in CCG, and we make cru- cial use of synchronized dropout ( <ref type="bibr" target="#b14">Gal and Ghahramani, 2016)</ref>. This supertagger achieves the state- of-the-art accuracy on the WSJ Penn Treebank. When combined with an existing TAG chart parser ( <ref type="bibr" target="#b2">Bangalore et al., 2009)</ref>, the LSTM-based su- pertagger already yields state-of-the-art unlabeled and labeled attachment scores.</p><p>In the second half of the work, we present a shift-reduce parsing model based on a feed- forward neural network that makes use of dense supertag embeddings. Although this approach has much in common with the approach to shift- reduce CCG parsing taken by <ref type="bibr" target="#b36">Zhang and Clark (2011)</ref>, it differs in its additive structures in su- pertag embeddings. When a CCG operation com- bines two supertags (categories), it yields a re- sulting category that is typically distinct from the two that are combined, and CCG shift-reduce parsers (e.g. Xu <ref type="bibr">(2015)</ref>) make use of this result to guide subsequent actions. When the resulting category is the same as some lexical category as- signment (for example when function application over (S\N P )/N P N P yields S\N P , the same as an intransitive verb), the parser will benefit from sharing statistics across these contexts. For TAG however, substitution or adjoining of one elemen- tary tree into another does not change the nature of the elementary tree into which the operation has taken place. Consequently, the results of partial derivations are not identified with atomic lexical entries, resulting in sparser data. We propose a so- lution to this problem for TAG by introducing vec- tor representations that are added to the supertag embedding when an operation has been applied to an elementary tree. Not only does this result in a TAG-parser with the best known performance over the WSJ Penn Treebank, but the resulting supertag embeddings turn out to contain linguistically sen- sible linear structure that we illustrate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TAG Parsing and Dependency Parsing</head><p>TAG is a tree-rewriting system, and typically the elementary structures of a TAG are phrase struc- ture trees. Thus, TAG-derived structures are also phrase structure trees. In addition, a TAG deriva- tion also yields a record of the derivational op- erations (substitutions, adjunctions) used to pro- duce the derived tree. Since these operations are context-free, this record also forms a tree, called the derivation tree, whose nodes are the elemen- tary objects and the edges are combinatory oper- ations. If we assume the TAG is lexicalized (i.e., each elementary structure is anchored by at least one terminal symbol), then we can label the nodes of the derivation tree with the tree names and also the anchors of the elementary trees, and we ob- tain what is formally a dependency tree. <ref type="bibr">1</ref> Since each node is also labeled with an elementary tree from the grammar, we can associate rich linguistic structure with that node, such as passive voice or empty subject.</p><p>In addition, it has long been observed that the derivation tree can also be interpreted linguisti- cally as a dependency tree <ref type="bibr" target="#b28">(Rambow and Joshi, 1997)</ref>, if certain assumptions are made about the shape of the elementary trees in the grammar <ref type="bibr" target="#b13">(Frank, 2001</ref>). The substitution operation corre- sponds to the obligatory addition of an argument, and adjunction is used to add adjuncts, as well as function words to a lexical head. The one exception is the treatment of long distance wh- movement in TAG. Here, a matrix clause is rep- resented by a predicative auxiliary tree which is adjoined into the embedded clause, so that the wh- element moved from the embedded clause can still be substituted locally into the tree headed by its verb. As a result, the dependency between the ma- trix and embedded verbs is inverted relative to the  <ref type="figure">Figure 1</ref>: TAG derivation tree (left) and closely related dependency tree (right) for The bill, which they failed to pass, would regulate emissions. Substitution edges are labeled SUBJ or OBJ, predicative auxiliary edges are labeled PREDAUX, while all other adjoining edges are labeled ADJ. We use the same edge labels in the dependency tree. The derivation tree also carries the name of the elementary tree used during the derivation, which can be used to look up rich syntactic information about that word in context. normally assumed dependency. This can be seen in <ref type="figure">Figure 1</ref>, where in the linguistically motivated dependency tree (right) pass depends on failed as the latter's object, while in the TAG deriva- tion tree (left), failed depends on pass, linked by an arc marked PREDAUX for predicative auxil- iary. These cases can be detected automatically because of the trees used; as a result of this in- version, there is almost no non-projectivity in En- glish. In summary, TAG parsing into derivation trees is very closely related to dependency pars- ing. In this paper, we are interested in extracting derivation trees, not the derived trees (which can be recovered from the derivation trees).</p><p>The corpus we use is obtained by extracting a TAG grammar from the WSJ part of the Penn Treebank corpus, resulting in a grammar and derivation trees labeled with the grammar <ref type="bibr" target="#b7">Chen (2001)</ref>. For example, in <ref type="figure">Figure 1</ref>, t27 is the ba- sic tree for a transitive verb (regulate), while t722 is the tree for a transitive verb which forms an ob- ject relative clause with an overt relative pronoun but an empty subject ( <ref type="figure" target="#fig_0">Figure 2</ref>). <ref type="bibr">2</ref> The corpus and grammar were iteratively refined to obtain linguis- tically plausible derivation trees which could serve as input for a generation task ( <ref type="bibr" target="#b4">Bangalore and Rambow, 2000</ref>). As a result, the dependency struc- ture is similar to Universal Dependency ( <ref type="bibr" target="#b23">Nivre et al., 2016)</ref>, apart from the different treatment of long-distance wh-movement noted above: the pri- mary dependencies are between the core meaning- bearing lexical words, while function words (aux- iliaries, determiners, complementizers) depend on their lexical head and have no dependents. <ref type="bibr">3</ref> We label verbal argument arcs with deep dependency labels: Subject, Object, and Indirect Object nor- malized for passive and dative shift. All other arcs are labeled as Adjuncts. This means that our label set is small, but determining the argument labels requires detection of voice alternations and dative shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">TAG Supertagging</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Long-Distance Dependencies and LSTMs</head><p>In CCG, a transitive verb is uniformly associated with the category (S\N P )/N P , and variation in the word order of a clause is addressed through the use of different combinatory operations. This results in greater parsing ambiguity given a se- quence of categories. In TAG, the set of opera- tions is more restricted. While this has the pos- itive effect of reducing parsing ambiguity given a sequence of elementary trees, it necessitates a pro- liferation in the number of elementary trees. For example, a TAG will associate different elemen- tary trees for the same transitive verb in order to derive canonical clauses, subject and object rela- tives, and subject and object questions. Not only does this lead to a larger number of supertags, it also means that the determination of the correct supertag requires sensitivity to long-distance de- pendencies. For example, in the question Who does Bill think Harry likes?, the category of the verb like requires sensitivity to the first word of the sentence. To address this problem, we make use of a supertagging model that is based on a recurrent network architecture, the Long Short-Term Mem- ory (LSTM, <ref type="bibr" target="#b15">Hochreiter and Schmidhuber (1997)</ref>), which is constructed so that its update rule avoids the vanishing/exploding gradient problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Supertagger Model</head><p>The model architecture we adopt is depicted in <ref type="figure" target="#fig_1">Figure 3</ref>, a BLSTM. The input for each word is represented via the concatenation of a 100-dimensional embedding of the word, a 5- dimensional embedding of a predicted part of speech tag, and a 10-dimensional embedding of a suffix vector (which encodes the presence of 1 and 2 character suffixes of the word). We ini- tialize the word embeddings to be the pre-trained GloVe vectors ( <ref type="bibr" target="#b26">Pennington et al., 2014</ref>); for words which do not have a corresponding GloVe vector, we initialize their embedding to a zero vector. The other embeddings are randomly initialized. Fea- tures for each word are fed into the BLSTMs. To produce an output for the network, we concatenate the output vectors from the two LSTM directions and apply an affine transformation before the soft- max function to obtain a probability distribution over the 4727 supertags. We train this network, including the embeddings, by optimizing the neg- ative log-likelihood of the observed sequences of supertags in a mini-batch stochastic fashion with the Adam optimization algorithm with l = 0.001 ( <ref type="bibr" target="#b18">Kingma and Ba, 2015)</ref>.</p><p>Since neural networks have numerous param- eters, regularization plays a key role in training. This is typically accomplished by using dropout <ref type="bibr" target="#b32">(Srivastava et al., 2014</ref>). Although dropout train- ing has been successful on feed-forward neural networks, performing dropout on recurrent neural networks has been problematic ( <ref type="bibr" target="#b14">Gal and Ghahramani, 2016)</ref>. Armed with a novel interpretation of dropout based on variational inference on pa- rameters, <ref type="bibr" target="#b14">Gal and Ghahramani (2016)</ref> propose that dropout noise should be shared across the time steps. We apply this technique to the training of our LSTM network, and achieve an improvement of approximately 2% in accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transition-based Parsing for TAG</head><p>As discussed in Section 2, TAG parsing into derivation trees is closely related to dependency parsing; it is natural to make use of techniques from dependency parsing to reconstruct a TAG derivation tree. We make use of this approach here, eschewing complete chart-based parsing al- gorithms in favor of greedy or beam-search-based explorations of possible parses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shift-Reduce Parsing Algorithm</head><p>We employ the arc-eager system of shift-reduce parsing, familiar from the MALT parser ( <ref type="bibr" target="#b25">Nivre et al., 2006</ref>). In this system, an oracle is trained to predict a sequence of transition operations from an initial state to a terminal state for each sentence. Each state is represented by c = (s, b, A) where s, b, and A denote the stack, buffer and set of de- pendency relations derived so far. Therefore, our objective is to predict a transition operation given the configuration set c. The initial configuration is defined as s = <ref type="bibr">[ROOT ]</ref>, b = [w 1 , · · · w n ], and A = ∅ where n is the number of tokens in the sen- tence w 1 w 2 · · · w n . At a particular state, denote the top ith element of the stack and the buffer by s i and b i respectively. The arc-eager system de- fines four types of operations with corresponding preconditions: LEFT-ARC, RIGHT-ARC, SHIFT and REDUCE. For the present parser, the LEFT- ARC and RIGHT-ARC operations are each further divided into seven different types depending on the derivational operation involved and the loca- tion: Substitution 0-4, Adjoining, and Co-anchor attachment. Substitution n represents an instance of substitution into an argument slot of an elemen- tary tree that is uniquely annotated with the num-ber n (we discuss the interpretation of such num- bers below). Adjoining represents an application of the adjoining operation. It is not further sub- divided, as the current parser does not distinguish among different loci of adjoining within an ele- mentary tree. Co-anchor attachment represents the substitution into a node that is construed as a co- head of an elementary tree. An example of this is the insertion of a particle into a verbally headed tree associated with a verb-particle construction, such as the insertion of up into the pick-headed tree to generate I picked up the book. The transi- tions terminate when the buffer is empty.</p><p>This system will fail to capture non-projective TAG derivation structures. However, as noted in Section 2, there is almost no non-projectivity in TAG derivation structures of English. Concretely, we find that WSJ Sections 01-22 contain only 26 non-projective sentences (0.065%), and those sen- tences are discarded during training. WSJ Section 00 does not have any non-projective sentences.</p><p>On the other hand, WSJ Sections 01-22 con- tain 0.6% of non-projective sentences in depen- dency grammar <ref type="bibr" target="#b6">(Chen and Manning, 2014)</ref>, an or- der of magnitude more than non-projectivity for TAG. This suggests that the problem of TAG pars- ing is more compatible with standard shift-reduce parsing than dependency grammar parsing is. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parser Model</head><p>In this work, we use a non-lexicalized parser, which does not have access to the identities of the words of the sentence to be parsed. <ref type="bibr">5</ref> Instead, the parser's decisions will be guided by the supertags of the top k elements from the stack and the first k elements of the buffer. Using these features as in- put, we build a two-layer feed-forward network to predict the action for the parser to take. As noted above, the identity of the supertag does not allow <ref type="bibr">4</ref> We recognize alternatives to shift-reduce parsing. For instance, <ref type="bibr" target="#b12">Dozat and Manning (2017)</ref> propose a graph-based parser that accommodates non-projectivity. It remains open whether such alternatives will work for TAG parsing, and we leave this for the future. We emphasize, however, that because of the nature of TAG derivations, the issue of non- projectivity is much less severe than dependency parsing. <ref type="bibr">5</ref> We have tried adding word embeddings as inputs to the parser with different choices of hyperparameters (e.g., the number of embedding dimensions). Unfortunately, our ex- periments yielded degraded performance. It should be noted, however, that TAG supertags typically provide enough infor- mation for deriving correct parses; the only cases that su- pertags cannot disambiguate are ambiguous attachments to identical nonterminals (e.g. The picture of my friend with green eyes). the parser to encode whether a particular node in an elementary tree has already been targeted by a substitution operation. In order to overcome this deficiency, we augment the parser's state with sub- stitution memory, which encodes for each possi- ble substitution site (from 0 to 4) in a supertag T whether that substitution has already applied in T .</p><p>Each</p><note type="other">supertag is mapped to a d-dimensional vector by an embedding matrix E ∈ R d×(N +2) where N denotes the number of supertags; we also have additional vectors representing the empty state and ROOT . Substitution memory is simi- larly transformed, with a substitution memory em- bedding matrix M ∈ R d×5 , to a d-dimensional vector that encodes in a distributed manner where substitution has applied. Each column in M is the vector corresponding to a specific substitution type. Each element from the stack and buffer is then represented by adding the supertag T embed- ding to the embedding associated with each vec- tor from M</note><p>corresponding to the substitution op- erations already performed on T , if any. Mathe- matically, suppose that we are at the configuration c = (s, b, A), and p (i) ∈ R 5 denotes the substitu- tion history of s i . p (i) is an indicator vector that p (i) j = 1 if and only if we have already performed substitution j into s i in the parser, and 0 otherwise. Define p (k+1) in the same way for b 1 . 6 Then, the input vector to the network can be expressed as</p><formula xml:id="formula_0">[Es 1 + M p (1) ; · · · ; Es k + M p (k) ; Eb 1 + M p (k+1) ; · · · ; Eb k ]</formula><p>This model with the additive substitution memory has several conceptual advantages. First, the ad- ditive structure gives us an unbounded scope of the past transition, avoiding making decisions that lead to substitution collisions without a computa- tionally expensive architecture such as an ensem- ble of LSTMs ( <ref type="bibr" target="#b35">Xu, 2015)</ref>. Moreover, as TAG su- pertags encode rich syntactic features, the parsing data for some supertags tend to become scarce. The most common 300 supertags in the Penn Tree Bank WSJ Sections 01-22 cover 96.8% of the data. In a situation of such data sparsity, it be- comes crucial to link, for example, the behaviors of intransitive verbs with those of transitive verbs. With substitution memory, the network can de- velop representations under which addition of ap- propriate substitution vectors serves to transform one supertag into another, allowing the generaliza- tion across these contexts. Indeed, as we will show in a later section, the substitution memory embed- dings and supertag embeddings turn out to yield interpretable and linguistically sensible structures.</p><p>Finally, we concatenate the vectors associated with the relevant elements from the stack and buffer into a 2dk dimensional vector and feed it to the network to obtain a probability distribution over the possible transition actions. The architec- ture is visualized in <ref type="figure" target="#fig_2">Figure 4</ref>. Following <ref type="bibr" target="#b6">Chen and Manning (2014)</ref>, we use the cube activation func- tion for the first layer, which could better capture interactions. We, again, optimize the negative log- likelihood in a mini-batch stochastic fashion with the Adam optimization algorithm with l = 0.001 ( <ref type="bibr" target="#b18">Kingma and Ba, 2015)</ref>. With regards to decoding, we consider both greedy parsing as well as a beam search algorithm, where we keep transition action hypotheses at each time step, in the experiments we report below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supertag Input to the Parser</head><p>We consider three types of supertag inputs to the neural network parser: gold supertags, 1-best su- pertags from the BLSTM supertagger, and 1-best supertags from the MICA chart parser ( <ref type="bibr" target="#b2">Bangalore et al., 2009)</ref>. MICA searches through n-best su- pertags with their corresponding probabilities and produces a full parse forest that abides by the TAG grammar. To generate the 1-best supertags from MICA, we first feed 10-best supertags from the BLSTM supertagger to the MICA chart parser, and retain only the supertags of the best parse. These supertags have the special property that there exists a feasible parse in the TAG gram- mar for every sentence, which does not necessarily hold for the 1-best supertags from the BLSTM su- pertagger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>In order to ensure comparability with past work on TAG parsing, we follow the protocol of <ref type="bibr" target="#b2">Bangalore et al. (2009)</ref> and <ref type="bibr" target="#b9">Chung et al. (2016)</ref>, and use the grammar and the TAG-annotated WSJ Penn Tree Bank described in Section 2. Following that work, we use Sections 01-22 as the training set, Section 00 as the development set, and Section 23 as the test set. The training, development, and test sets comprise 39832, 1921, and 2415 sentences, re- spectively. The development set contains 177 sen- tences with at least one supertag that was absent from the training set. We implement the networks in TensorFlow ( <ref type="bibr" target="#b0">Abadi et al., 2015)</ref>. During train- ing, we shuffle the order of the sentences in the training set to form mini-batches. Each mini-batch consists of 100 sentences, except the last which contains 32 sentences.</p><p>For supertagging, we first generate predicted POS tags for both the training set and the develop- ment set. The POS-tagger architecture is similar to that of the supertagger shown in <ref type="figure" target="#fig_1">Figure 3</ref>, ex- cept that, obviously, we do not feed it POS embed- dings. The BLSTMs each contain 128 units, and we do not apply dropout at this stage. To derive predicted POS tags for the supertagger training set, we perform 10-fold jackknife training over the training set. For the supertagger, each direction of LSTM computation involves two layers, and each LSTM contains 512 units. The hidden units, layer- to-layer, and input units dropout rates are 0.5, 0.5, and 0.2 respectively. After each training epoch, we test the parser on the development set. When classification accuracy does not improve on two consecutive epochs, we end the training.</p><p>For the parser, we initialize the supertag embedding matrix E and the substitution memory embedding matrix M according to</p><formula xml:id="formula_1">Uniform(− 1 √ d , 1 √ d</formula><p>). For all of the experiments reported here, we fix the hyper-parameters as follows: the embedding dimensions d for the supertag and substitution memory embeddings are 50, the number of units is 200 on both of the two hidden layers, and the input dropout rate is 0.2 and the hidden dropout rate is 0.3. We choose k = 3 or 5 for the stack/buffer scope. After each training epoch, we test the parser on the development set, and when the greedy accuracy fails to improving on two consecutive epochs, we terminate the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supertagging Results</head><p>We achieve on Section 00 supertagging accuracy of 89.32%, 90.67% if we disregard the 177 sen- tences that contain an unseen supertag. This performance surpasses previous results on this task: <ref type="bibr" target="#b2">Bangalore et al. (2009)</ref> report 88.52% ac- curacy using a maxent supertagger combined with a chart parser (MICA), which is the best result over a tag set of this complexity, though bet- ter results are reported for considerably smaller tag sets (on the order of 300 supertags). The n-best and β pruning accuracy <ref type="bibr" target="#b11">(Clark and Curran, 2007</ref>) are given in <ref type="figure" target="#fig_3">Figure 5</ref>. In the β prun- ing scheme, we pick supertags whose probabil- ities are greater than β times the probability of the most likely supertag. We show the results for β ∈ [0.075, 0.03, 0.01, 0.005, 0.001, 0.0001]. It is noteworthy that with β = 0.005, the average number of supertags picked for each token (ambi- guity level) is about 2, but the accuracy surpasses 98%, suggesting that incorporating the β pruning method in the stapling phase of TAG parsing will enhance the parser. We also obtain comparable ac- curacy of 89.44% on Section 23.</p><p>As discussed above, TAG supertags alone pro- vide rich syntactic information. In order to under- stand how much such information our supertag- ger sucessfully captures, we analyze the 1-best su- pertag results on the basis of the syntactic prop- erties of the elementary trees defined in <ref type="bibr" target="#b9">Chung et al. (2016)</ref>. Extending the notion of binary pre- cision and recall, we define the macro-averaging precision and recall as the simple average over precision or recall corresponding to each class ( <ref type="bibr" target="#b31">Sokolova and Lapalme, 2009</ref>). We also compute accuracy, which is simply the ratio of correctly classified examples to the entire number of exam- ples. <ref type="table">Table 1</ref> shows the results along with those for the maxent supertagger ( <ref type="bibr" target="#b2">Bangalore et al., 2009)</ref>. Recall tends to be lower than precision; we can attribute this pattern to the nature of the macro- averaging scheme that equally treats each class re- gardless of the size; poor recall performance on a small class, such as the class of dative shift verbs, influences the overall recall as much as perfor- mance on a large class. Observe, however, that the BLSTM supertagger yields significantly bet- ter performance on recall in general, and it outper-  forms the maxent supertagger by a large margin in handling long dependencies of wh-movement and relativization. Lastly, we interpret our supertagging perfor- mance in the context of prepositional phrase (PP) attachment ambiguity. Normally, in dependency parsing, PP attachment is resolved by the parser. However, in our case, it can be resolved before parsing, during the supertagging step. This is be- cause the supertags for prepositions vary depend- ing on the type of constituent modified by the PP containing the preposition; for example, t4 is the supertag for a preposition whose PP modifies an NP, while t13 is the supertag for a preposition whose PP modifies a VP.</p><p>To test how well our supertagger resolves PP at- tachment ambiguity, we used the dataset from <ref type="bibr" target="#b29">Ratnaparkhi et al. (1994)</ref> (derived from the PTB WSJ) to extract a test set of sentences with PPs that are ambiguous between attaching to a VP or to an NP. <ref type="bibr">7</ref> We then supertagged these sentences and checked whether the supertag for the preposition in the am- biguous PP is a VP modifier or an NP modifier. Of our test set of 1951 sentences, 1616 had supertags modifying the correct part of speech, to give an accuracy of 0.826. <ref type="table" target="#tab_2">Table 2</ref> compares this result to past work. The supertagger outperforms all other models besides the Word Vector model. Since this Word Vector model (like the MaxEnt model) is specifically trained for this task, and given that our supertagger is not trained for this particular task, the accuracy is reasonably encouraging. This re- sult suggests that TAG supertagging is a reason- able intermediate level between only resolving PP attachment and conducting full parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>PP Attachment Accuracy Malt ( <ref type="bibr" target="#b25">Nivre et al., 2006)</ref> 79.7* MaxEnt ( <ref type="bibr" target="#b29">Ratnaparkhi et al., 1994)</ref> 81.6* Word Vector ( <ref type="bibr" target="#b5">Belinkov et al., 2014)</ref> 88.7* Parsey McParseface ( <ref type="bibr" target="#b1">Andor et al., 2016)</ref> 82.3 BLSTM Supertagger 82.6 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Parsing Results</head><p>Parsing results and comparison with prior models are summarized in <ref type="table" target="#tab_4">Tables 3, 4</ref> (Section 00), and 5 (Section 23). From <ref type="table" target="#tab_5">Table 4</ref>, we see that the com- bination of the BLSTM supertagger, MICA chart parser, and the neural network parser achieves state-of-the-art performance, even compared to parsers that make use of lexical information, POS tags, and hand-engineered features. With gold su- pertags, the neural network parser with beam size 16 performs slightly better than the chart parser. As shown in <ref type="table" target="#tab_6">Table 5</ref>, our supertag-based parser outperforms SyntaxNet ( <ref type="bibr" target="#b1">Andor et al., 2016</ref>) with the computationally expensive global normaliza- tion. This suggests that, besides providing the grammars and linguistic features that can be used in downstream tasks in addition to derivation trees (Semantic Role Labeling: (Chen and Rambow, 2003), Textual Entailments: (Xu et al., 2017)), su- pertagging also improves parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learned Vector Representation</head><p>We motivated the use of embeddings in the parser to encode properties of the supertags and the sub- stitution operations performed on them. We can examine their structure in a way similar to what <ref type="bibr" target="#b21">Mikolov et al. (2013)</ref> did for word embeddings by performing analogy tests on the learned supertag embeddings. Consider, for example, the anal- ogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a transitive verb (t99) is to a subject relative headed by an intransitive verb (t109). Following <ref type="bibr" target="#b21">Mikolov et al. (2013)</ref>, we can express this analogy with the equation t27 − t81 ≈ t99 − t109, which can be rearranged as t27 − t81 + t109 ≈ t99. By seeing if this ap- proximate equality holds when the embeddings of the relevant supertags have been added and sub- tracted, we can test how well the embeddings cap- ture syntactic properties of the supertags.</p><p>To create a set of such analogies, we extracted all pairs (stag1, stag2) such that stag2 is the re- sult of excising exactly one substitution node from stag1. The idea here is that, once a substitution node is filled within a supertag, the result behaves like a supertag without that substitution node; for example, a transitive verb with its object filled behaves like an intransitive verb. We then cre- ate analogies by choosing two such pairs, (stag1, stag2) and (stag3, stag4), chosen so that stag1 and stag2 are related in the same way that stag3 and stag4 are related. From these two pairs we then form an equation of the form stag1 − stag2 + stag4 ≈ stag3.</p><p>We considered three different criteria for choos- ing which pairs of pairs can form analogies: A- 1, where both pairs must have the same deep syntactic role (Drole) for the excised substitution node; A-2, where both pairs must have the same Drole and POS for the excised substitution node; and A-3, where both pairs must have the same Drole and same POS for the excised substitution node, and the heads of all supertags in the analogy must have the same POS. For each analogy gen- erated, we computed the left hand side by adding and subtracting the relevant supertag embeddings and used cosine similarity to determine the most similar embeddings to the result and whether the intended right hand side was among the closest neighbors. We used four metrics for evaluation: Acc, the proportion of analogies for which the closest neighbor was the correct supertag; Acc- 300, the proportion of analogies for which the closest neighbor amongst the 300 most common supertags was the correct supertag; Avg Rank, the average position of the correct choice in the ranked list of the closest neighbors; and Avg Rank-300,    the average position of the correct choice in the ranked list of the closest neighbors amongst the 300 most common supertags. We expect that the embeddings for common su- pertags would be better representations than em- beddings for rare supertags. Thus, we restricted our experiment to analogies between supertags among the 300 most common ones in the train- ing set. (Indeed, experiments that included rare supertags in the analogies produced poor results.) <ref type="table" target="#tab_8">Table 6</ref> provides the results for the 3 types of analogies, which are very promising, particularly type A-3. We can visualize these results by per- forming PCA on the embedding vectors. <ref type="figure" target="#fig_4">Figure 6a</ref> shows the first 2 PCA components of A-3 analo- gies involving supertags containing transitive and intransitive predicates across a variety of struc- tures. We see that virtually all pairs differ from one another by a similar vector, and in fact this differ- ence is essentially the vector associated with sub- stitution 1 in the substitution embedding memory (shown in blue). <ref type="figure" target="#fig_4">Figure 6b</ref> shows the case of pairs of canonical sentence elementary trees (read in I read the book) and their subject relative analogs (read in the guy who read the book). This again shows a systematic mapping between grammati-  cally related embeddings, suggesting that the em- beddings encode relevant structural properties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We presented a state-of-the-art TAG supertagger and parser, the former based on a BLSTM archi- tecture, and the latter on a non-lexicalized shift- reduce parser using a feed-forward network. The parser makes crucial use of supertag embeddings that provide linguistically interpretable vector rep- resentations of the supertags. These positive re- sults suggest that TAG can provide the foundation of NLP systems for tasks requiring deeper anal- ysis than current dependency parsers provide, and we will apply our parser to such tasks in the future. Nonetheless, a large discrepancy remains in parser performance with gold supertags and predicted su- pertags, indicating that supertagging is still a bot- tleneck. We will explore ways to leverage our su- pertagger's high β-pruning accuracy in parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The elementary trees for t27 (left) and t722 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: BLSTM Supertagger Architecture.</figDesc><graphic url="image-1.png" coords="4,307.86,62.81,217.12,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Shift-Reduce Parser Neural Network Architecture.</figDesc><graphic url="image-2.png" coords="6,95.44,62.82,171.40,141.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Section 00 n-best accuracy (left), and β pruning accuracy (right). Sentences with unseen supertags are disregarded.</figDesc><graphic url="image-3.png" coords="7,307.34,62.81,107.71,80.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Embedding vector alignments.</figDesc><graphic url="image-5.png" coords="9,307.85,391.16,107.71,80.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Various PP attachment results. * denotes the results 
on a different dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>97.68±0.06 97.46±0.05 90.38 ±0.05 88.92 ±0.04 90.88±0.06 89.39±0.06</head><label></label><figDesc></figDesc><table>Gold Stags 

BLSTM 
BLSTM+Chart 
k B 
UAS 
LAS 
UAS 
LAS 
UAS 
LAS 
3 
1 
96.74±0.06 
96.47±0.06 
89.54±0.03 88.06±0.04 
90.03±0.02 
88.56±0.02 
3 16 97.62±0.06 
97.42±0.07 
90.31±0.04 88.85±0.04 
90.85±0.02 
89.38±0.02 
5 
1 
96.96±0.19 
96.67±0.20 
89.63±0.03 88.12±0.04 
90.07±0.06 
88.60±0.06 
5 16 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Parsing Results on Section 00. k is # of elements from stack and buffer used as input, B is the beam size. We show mean and standard deviation over 5 trials with different initialization for each configuration. BLSTM+Chart shows results obtained by feeding the 1-best supertag inputs from the MICA chart parser discussed in Section 4.3.</head><label>3</label><figDesc></figDesc><table>Gold Stags 
Predicted Stags 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Section 00 Performance Comparison with Prior Models. The P3 results are from Chung et al. (2016). P3 is based on 
the model described in Nivre et al. (2004). * denotes the results with gold POS tags. For the NN parser, k=5 and B=16. 

Model 
Stag Acc UAS 
LAS 
SyntaxNet 
-
90.47±0.05 
88.99±0.06 
Maxent+Chart 
86.85 
86.66 
84.90 
BLSTM+Chart 
89.44 
90.20 
88.66 
BLSTM+NN 
89.44 
90.31±0.03 
88.98±0.03 
BLSTM+Chart+NN 
89.71 
90.97±0.03 89.68±0.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Supertagging and Parsing Results on Section 23. For 
the NN parser, k=5 and B=16 throughout. We trained Syn-
taxnet (Andor et al., 2016) with global normalization beam 
size 16 using the TensorFlow toolkit. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 : Analogy Task Results.</head><label>6</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> For the difference between formal and linguistic dependency, see Rambow (2010).</note>

			<note place="foot" n="2"> Our full grammar is shown at http://mica.lif. univ-mrs.fr/d6.clean2-backup.pdf</note>

			<note place="foot" n="3"> One difference should be noted: UD considers prepositions always to be function words, while our TAG grammar treats them as core words unless the Penn Treebank marks them as closely related to the verb.</note>

			<note place="foot" n="6"> Notice that no substitution should have happened on b2, b3,. .. by construction.</note>

			<note place="foot" n="7"> We were unable to use the full test set because, in order to run the supertagger on the test set, we had to map the test examples back to their full sentences, but some of those original sentences are no longer available in PTB3.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<title level="m">TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow</title>
		<meeting><address><addrLine>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Computational Linguistics</title>
		<meeting>Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MICA: A Probabilistic Dependency Parser Based on Tree Insertion Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Boullier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2009</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supertagging: An Approach to Almost Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="237" to="266" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting a probabilistic hierarchical model for generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Computational Linguistics</title>
		<meeting>the 18th International Conference on Computational Linguistics<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring compositional architectures and word vector representations for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="561" to="572" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Fast and Accurate Dependency Parser using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards Efficient Statistical Parsing Using Lexicalized Grammatical Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
		<respStmt>
			<orgName>University of Delaware</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Use of Deep Linguistics Features for the Recognition and Labeling of Semantic Arguments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonchang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhas</forename><surname>Siddhesh Mhatre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Bangalore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Revisiting supertagging and parsing: How to use supertags in transition-based parsing</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+12)</title>
		<meeting>the 12th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+12)</meeting>
		<imprint>
			<biblScope unit="page" from="85" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Widecoverage semantic representations from a CCG parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Phrase Structure Composition and Syntactic Dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theoretically grounded application of dropout in recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani ; In</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6241-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1019" to="1027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Treeadjoining grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schabes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Formal Languages</title>
		<editor>G. Rozenberg and A. Salomaa</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="69" to="124" />
		</imprint>
	</monogr>
	<note>Beyond Words</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ADAM: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint a * CCG parsing and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">LSTM CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT 2016</title>
		<meeting>NAACLHLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Syntax-aware neural machine translation using CCG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>1702.01147v1</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Universal dependencies v1: A multilingual treebank collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Choukri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Declerck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Goggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marko</forename><surname>Grobelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bente</forename><surname>Maegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association (ELRA)<address><addrLine>Helene Mazo, Asuncion Moreno, Jan Odijk; Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Nicoletta Calzolari (Conference Chair)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL 2004 Workshop: Eighth Conference on Computational Natural Language Learning (CoNLL-2004)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maltparser: A data-driven parser-generator for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The simple truth about dependency and phrase structure representations: An opinion piece</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/N10-1049" />
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Trends in Meaning-Text Theory, John Benjamins, Amsterdam and Philadelphia</title>
		<editor>Leo Wanner</editor>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="167" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A maximum entropy model for prepositional phrase attachment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adwait</forename><surname>Ratnaparkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Reynar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ARPA Human Language Technology Workshop</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="250" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transforming dependency structures to logical forms for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A systematic analysis of performance measures for classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename><surname>Sokolova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="427" to="437" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combinatory categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-Transformational Syntax: Formal and Explicit Models of Grammar</title>
		<editor>Robert Borsley and Kersti Börjars</editor>
		<imprint>
			<publisher>WileyBlackwell</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TAG parsing evaluation using textual entailments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13)</title>
		<meeting>the 13th International Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+13)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LSTM shift-reduce CCG parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenduan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1754" to="1764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shift-Reduce CCG Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="683" to="692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
