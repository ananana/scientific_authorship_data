<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Translations via Matrix Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derry</forename><surname>Wijaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Callahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hewitt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer and Information Science Department</orgName>
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LIMSI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris-Saclay</orgName>
								<address>
									<postCode>91403</postCode>
									<settlement>Orsay</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Translations via Matrix Completion</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1452" to="1463"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine translation (MT) models typically re- quire large, sentence-aligned bilingual texts to learn good translation models ( <ref type="bibr" target="#b45">Wu et al., 2016;</ref><ref type="bibr" target="#b34">Sennrich et al., 2016a;</ref><ref type="bibr" target="#b20">Koehn et al., 2003)</ref>. How- ever, for many language pairs, such parallel texts may only be available in limited quantities, which is problematic. Alignments at the word-or subword-levels ( <ref type="bibr" target="#b35">Sennrich et al., 2016b</ref>) can be in- accurate in the limited parallel texts, which can in turn lead to inaccurate translations. Due to the low quantity and thus coverage of the texts, there may still be "out-of-vocabulary" words encountered at run-time. The Bilingual Lexicon Induction (BLI) task <ref type="bibr" target="#b27">(Rapp, 1995)</ref>, which learns word translations from monolingual or comparable corpora, is an at- tempt to alleviate this problem. The goal is to use plentiful, more easily obtainable, monolingual or comparable data to infer word translations and re- duce the need for parallel data to learn good trans- lation models. The word translations obtained by BLI can, for example, be used to augment MT systems and improve alignment accuracy, cover- age, and translation quality ( <ref type="bibr" target="#b12">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b2">Callison-Burch et al., 2006;</ref><ref type="bibr" target="#b5">Daumé and Jagarlamudi, 2011</ref>  <ref type="figure">Figure 1</ref>: Our framework allows us to use a di- verse range of signals to learn translations, in- cluding incomplete bilingual dictionaries, infor- mation from related languages (like Indonesian loan words from Dutch shown here), word embed- dings, and even visual similarity cues.</p><p>Previous research has explored different sources for estimating translation equivalence from mono- lingual corpora ( <ref type="bibr" target="#b33">Schafer and Yarowsky, 2002</ref>; <ref type="bibr" target="#b18">Klementiev and Roth, 2006;</ref><ref type="bibr">CallisonBurch, 2013, 2017)</ref>. These monolingual signals, when combined in a supervised model, can en- hance end-to-end MT for low resource languages ( <ref type="bibr" target="#b17">Klementiev et al., 2012a;</ref><ref type="bibr">Irvine and CallisonBurch, 2016</ref>). More recently, similarities between words in different languages have been approxi- mated by constructing a shared bilingual word em- bedding space with different forms of bilingual su- pervision ( <ref type="bibr" target="#b38">Upadhyay et al., 2016</ref>).</p><p>We present a framework for learning transla- tions by combining diverse signals of translation that are each potentially sparse or noisy. We use matrix factorization (MF), which has been shown to be effective for harnessing incomplete or noisy distant supervision from multiple sources of in- formation ( <ref type="bibr" target="#b7">Fan et al., 2014;</ref><ref type="bibr" target="#b32">Rocktäschel et al., 2015)</ref>. MF is also shown to result in good cross- lingual representations for tasks such as alignment ( <ref type="bibr" target="#b10">Goutte et al., 2004</ref>), QA ( <ref type="bibr" target="#b46">Zhou et al., 2013)</ref>, and cross-lingual word embeddings ( <ref type="bibr" target="#b36">Shi et al., 2015</ref>).</p><p>Specifically, we represent translation as a matrix with source words in the columns and target words in the rows, and model the task of learning trans- lations as a matrix completion problem. Starting from some observed translations (e.g., from exist- ing bilingual dictionaries,) we infer missing trans- lations in the matrix using MF with a Bayesian Personalized Ranking (BPR) objective <ref type="bibr" target="#b30">(Rendle et al., 2009</ref>). We select BPR for a number of reasons: (1) BPR has been shown to outperform traditional supervised methods in the presence of positive-only data ( <ref type="bibr" target="#b31">Riedel et al., 2013)</ref>, which is true in our case since we only observe positive translations. (2) BPR is easily extendable to incor- porate additional signals for inferring missing val- ues in the matrix <ref type="bibr">(He and McAuley, 2016)</ref>. Since observed translations may be sparse, i.e. the "cold start" problem in the matrix completion task, in- corporating additional signals of translation equiv- alence estimated on monolingual corpora is useful. (3) BPR is also shown to be effective for multi- lingual transfer learning ( <ref type="bibr" target="#b39">Verga et al., 2016</ref>). For low resource source languages, there may be re- lated, higher resource languages from which we can project available translations (e.g., translations of loan words) to the target language <ref type="figure">(Figure 1)</ref>.</p><p>We conduct large scale experiments to learn translations from both low and high resource lan- guages to English and achieve state-of-the-art per- formance on these languages. Our main contribu- tions are as follows:</p><p>• We introduce a MF framework that learns translations by integrating diverse bilingual and monolingual signals of translation, each potentially noisy/incomplete.</p><p>• The framework is easily extendable to incor- porate additional signals of translation equiv- alence. Since ours is a framework for integra- tion, each signal can be improved separately to improve the overall system.</p><p>• Large scale experiments on both low and high resource languages show the effectiveness of our model, outperforming the current state- of-the-art.</p><p>• We make our code, datasets, and output trans- lations publicly available. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Bilingual Lexicon Induction Previous research has used different sources for estimating transla- 1 http://www.cis.upenn.edu/%7Ederry/translations.html tions from monolingual corpora. Signals such as contextual, temporal, topical, and ortographic sim- ilarities between words are used to measure their translation equivalence ( <ref type="bibr" target="#b33">Schafer and Yarowsky, 2002;</ref><ref type="bibr" target="#b18">Klementiev and Roth, 2006;</ref><ref type="bibr">Callison-Burch, 2013, 2017)</ref>. With the increasing popularity of word em- beddings, many recent works approximate simi- larities between words in different languages by constructing a shared bilingual embedding space ( <ref type="bibr" target="#b19">Klementiev et al., 2012b;</ref><ref type="bibr" target="#b47">Zou et al., 2013;</ref><ref type="bibr" target="#b42">Vuli´cVuli´c and Moens, 2013;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b8">Faruqui and Dyer, 2014;</ref><ref type="bibr" target="#b4">Chandar A P et al., 2014;</ref><ref type="bibr" target="#b11">Gouws et al., 2015;</ref><ref type="bibr" target="#b23">Luong et al., 2015;</ref><ref type="bibr" target="#b22">Lu et al., 2015;</ref><ref type="bibr" target="#b38">Upadhyay et al., 2016</ref>). In the shared space, words from different languages are represented in a language-independent manner such that similar words, regardless of language, have similar repre- sentations. Similarities between words can then be measured in the shared space. One approach to in- duce this shared space is to learn a mapping func- tion between the languages' monolingual semantic spaces ( <ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b6">Dinu et al., 2014</ref>). The mapping relies on seed translations which can be from existing dictionaries or be reliably cho- sen from pseudo-bilingual corpora of compara- ble texts e.g., Wikipedia with interlanguage links. Vuli´c <ref type="bibr" target="#b43">Vuli´c and Moens (2015)</ref> show that by learning a linear function with a reliably chosen seed lexicon, they outperform other models with more expen- sive bilingual signals for training on benchmark data.</p><p>Most prior work on BLI however, either makes use of only one monolingual signal or uses unsu- pervised methods (e.g., rank combination) to ag- gregate the signals. <ref type="bibr" target="#b14">Irvine and Callison-Burch (2016)</ref> show that combining monolingual signals in a supervised logistic regression model produces higher accuracy word translations than unsuper- vised models. More recently, Vuli´c  show that their multi-modal model that employs a simple weighted-sum of word embeddings and vi- sual similarities can improve translation accuracy. These works show that there is a need for combin- ing diverse, multi-modal monolingual signals of translations. In this paper, we take this step further by combining the monolingual signals with bilin- gual signals of translations from existing bilingual dictionaries of related, "third" languages.</p><p>Bayesian Personalized Ranking (BPR) Our approach is based on extensions to the probabilis-tic model of MF in collaborative filtering <ref type="bibr" target="#b21">(Koren et al., 2009;</ref><ref type="bibr" target="#b30">Rendle et al., 2009)</ref>. We represent our translation task as a matrix with source words in the columns and target words in the rows <ref type="figure">(Fig- ure 1</ref>). Based on some observed translations in the matrix found in a seed dictionary, our model learns low-dimensional feature vectors that encode the latent properties of the words in the row and the words in the column. The dot product of these vectors, which indicate how "aligned" the source and the target word properties are, captures how likely they are to be translations.</p><p>Since we do not observe false translations in the seed dictionary, the training data in the matrix con- sists only of positive translations. The absence of values in the matrix does not imply that the cor- responding words are not translations. In fact, we seek to predict which of these missing values are true. The BPR approach to MF <ref type="bibr" target="#b30">(Rendle et al., 2009)</ref> formulates the task of predicting missing values as a ranking task. With the assumption that observed true translations should be given higher values than unobserved translations, BPR learns to optimize the difference between values assigned to the observed translations and values assigned to the unobserved translations.</p><p>However, due to the sparsity of existing bilin- gual dictionaries (for some language pairs such dictionaries may not exist), the traditional for- mulation of MF with BPR suffers from the "cold start" issue ( <ref type="bibr" target="#b9">Gantner et al., 2010;</ref><ref type="bibr">He and McAuley, 2016;</ref><ref type="bibr" target="#b39">Verga et al., 2016</ref>). In our case, these are situations in which some source words have no translations to any word in the target or related languages. For these words, additional in- formation, e.g., monolingual signals of translation equivalence or language-independent representa- tions such as visual representations, must be used.</p><p>We use bilingual translations from the source to the target language, English, obtained from Wikipedia page titles with interlanguage links. Since Wikipedia pages in the source language may be linked to pages in languages other than English, we also use high accuracy, crowdsourced transla- tions ( <ref type="bibr" target="#b26">Pavlick et al., 2014</ref>) from these third lan- guages to English as additional bilingual transla- tions. To alleviate the cold start issue, when a source word has no existing known translation to English or other third languages, our model backs- off to additional signals of translation equivalence estimated based on its word embedding and visual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe our framework for integrating bilingual and monolingual signals for learning translations. First we formulate the task of Bilingual Lexicon Induction, and introduce our model for learning translations given observed translations and additional monolingual/language- independent signals. Then we derive our learning procedure using the BPR objective function.</p><p>Problem Formulation Given a set of source words F , a set of target words E, the pair e, f where e ∈ E and f ∈ F is a candidate trans- lation with an associated score x e,f ∈ [0, 1] in- dicating the confidence of the translation. The input to our model is a set of observed transla- tions T := {{e, f | x e,f = 1}. These could come from an incomplete bilingual dictionary. We also add word identities to the matrix i.e., we define T identity := {{e, e}, where T identity ⊂ T . The task of Bilingual Lexicon Induction is then to gen- erate missing translations: for a given source word f and a set of target words {e | e, f / ∈ T }, pre- dict the score x e,f of how likely it is for e to be a translation of f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bilingual Signals for Translation</head><p>One way to predict x e,f is by using matrix factorization. The problem of predicting x e,f can be seen as a task of estimating a matrix X : E ×F . X is approximated by a matrix product of two low-rank matrices P : |E| × k and Q : |F | × k:</p><formula xml:id="formula_0">ˆ X := P Q T</formula><p>where k is the rank of the approximation. Each row p e in P can be seen as a feature vector describ- ing the latent properties of the target word e, and each row q f of Q describes the latent properties of the source word f . Their dot product encodes how aligned the latent properties are and, since these vectors are trained on observed translations, it en- codes how likely they are to be translation of each other. Thus, we can write this formulation of pre- dicted scoresˆxscoresˆ scoresˆx e,f with MF as:  Since each word can have multiple additional feature vectors, we can formulatê</p><formula xml:id="formula_1">ˆ x MF e,f = p T e q f = k i=1 p ei . q f i<label>(1</label></formula><formula xml:id="formula_2">NL ¢ ¢ ... ¢ ¢ ¢ ¢ ... ¢ ¢ ¢ ¢ ... ¢ ¢   ...     ...     ...</formula><formula xml:id="formula_3">x AU X u,i</formula><p>as a weighted combination of all the auxiliary features available to us:</p><formula xml:id="formula_4">ˆ x AU X u,i = ↵ 1 ✓ T u ✓ i + ↵ 2 T u i + ... + ↵ n T u i</formula><p>where ↵ m are parameters assigned to control the contribution of each auxiliary feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning with Bayesian Personalized Ranking</head><p>The objective of Bayesian Personalized Ranking (BPR) is to  The word tidur (id) is a cold word with no associated translation in the matrix. Auxiliary features ✓ f about the words can be used to predict translations for cold words.</p><p>Auxiliary Signals for Translation Because the observed bilingual translations may be sparse, the MF approach can suffer from the existence of cold items: words that have too few associated ob- served translations to estimate their latent dimen- sions accurately <ref type="figure" target="#fig_1">(Figure 2</ref>). Additional signals for measuring translation equivalence can allevi- ate this problem. Hence, in the case of cold words, we use a formulation ofˆxofˆ ofˆx u,i that involves auxiliary features about the words in the predictedˆxpredictedˆ predictedˆx u,i :</p><formula xml:id="formula_5">ˆ x AUX u,i = ✓ T u ✓ i + T ✓ i<label>(2)</label></formula><p>✓ i represents an auxiliary information about the cold word i e.g., its word embedding or visual fea- tures. ✓ u is a feature vector to be trained, whose dot product with ✓ i models the extent to which the word u matches the auxiliary features of word i. In practice, learning ✓ u amounts to learning a classi- fier, one for each target word u that learns weights ✓ u given the feature vectors ✓ i of its translations. models the target words' overall bias toward a given word i.</p><p>Since each word can have multiple additional feature vectors, we can formulatê</p><formula xml:id="formula_6">x AUX u,i</formula><p>as a weighted sum of available auxiliary features 1 :</p><formula xml:id="formula_7">ˆ x AUX u,i = ↵ 1 ✓ T u ✓ i + ↵ 2 T u i + ... + ↵ n T u i</formula><p>where ↵ m are parameters assigned to control the contribution of each auxiliary feature. In practice, we can combine the MF and auxil- iary formulations by defining:</p><formula xml:id="formula_8">ˆ x u,i = ˆ x MF u,i + ˆ x AUX u,i</formula><p>1 we omit bias terms for brevity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning with Bayesian Personalized Ranking</head><p>The objective of Bayesian Personalized Ranking (BPR) is to maximize the difference in scores assigned to the observed translations compared to those assigned to the unobserved translations. Given a training set D consisting of triples of the form hu, i, ji, where hu, ii 2 T and hu, ji / 2 T , BPR wants to maximizê x u,i,j , defined as:</p><formula xml:id="formula_9">ˆ x u,i,j = ˆ x u,i ˆ x u,j</formula><p>wherê x u,i andˆxandˆ andˆx u,j can be defined either by eq. 1 or eq. 2 (for cold words). Specifically, BPR optimizes ( <ref type="bibr" target="#b30">Rendle et al., 2009)</ref>:</p><formula xml:id="formula_10">X hu,i,ji2D ln (ˆ x u,i,j ) ⇥ ||⇥|| 2</formula><p>where is the logistic sigmoid function, ⇥ is the parameter vector ofˆxofˆ ofˆx u,i,j to be trained, and ⇥ is its hyperparameter vector. BPR can be trained using stochastic gradient ascent where a triple hu, i, ji is sampled from D and parameter updates are performed:</p><formula xml:id="formula_11">⇥ ⇥ + ⌘.((ˆ x u,i,j ) @ ˆ x u,i,j @⇥ ⇥ ⇥)</formula><p>⌘ is the learning rate. Hence, for the MF formula- tion ofˆxofˆ ofˆx u,i,j , we can sample a triple hu, i, ji from D and update its parameters as:</p><formula xml:id="formula_12">p u p u + ⌘.((ˆ x MF u,i,j )(q i q j ) P p u ) q i q i + ⌘.((ˆ x MF u,i,j )(p u ) Q + q i ) q j q j + ⌘.((ˆ x MF u,i,j )(p u ) Q q j )</formula><p>while for the auxiliary formulation ofˆxofˆ ofˆx u,i,j ; we we can sample a triple hu, i, ji from D and update its parameters as: </p><formula xml:id="formula_13">✓ u ✓ u + ⌘.((ˆ x AUX u,i,j )(✓ i ✓ j ) ⇥ ✓ u ) + ⌘.((ˆ x AUX u,i,j )(✓ i ✓ j ) )</formula><formula xml:id="formula_14">ˆ x AUX e,f = θ T e θ f + β T θ f<label>(2)</label></formula><p>θ f represents an auxiliary information about the cold word f e.g., its word embedding or visual fea- tures. θ e is a feature vector to be trained, whose dot product with θ f models the extent to which the word e matches the auxiliary features of word f . In practice, learning θ e amounts to learning a classifier, one for each target word e that learns weights θ e given the feature vectors θ f of its trans- lations. β models the targets' overall bias toward a given word f . Since each word can have multiple additional feature vectors, we can formulatê</p><formula xml:id="formula_15">x AUX e,f</formula><p>as a weighted sum of available auxiliary features 2 :</p><formula xml:id="formula_16">ˆ x AUX e,f = α 1 θ T e θ f + α 2 γ T e γ f + ... + α n δ T e δ f</formula><p>where α m are parameters assigned to control the contribution of each auxiliary feature. In practice, we can combine the MF and auxil- iary formulations by defining:</p><formula xml:id="formula_17">ˆ x e,f = ˆ x MF e,f + ˆ x AUX e,f</formula><p>2 We omit bias terms for brevity.</p><p>However, since bilingual signals that are input tôtô x MF e,f are often precise but sparse, while monolin- gual signals that are input tô x AUX e,f are often noisy and not sparse, in our model we only back-off to the less precisê x AUX e,f for cold source words that have none or too few associated translations (more details are given in the experiments, Section 4). For other source words, we usê</p><p>x MF e,f to predict. Learning with Bayesian Personalized Ranking Unlike traditional supervised models that try to maximize the scores assigned to positive instances (in our case, observed translations), the objec- tive of Bayesian Personalized Ranking (BPR) is to maximize the difference in scores assigned to the observed translations compared to those assigned to the unobserved translations. Given a training set D consisting of triples of the form e, f, g, where e, f ∈ T and e, g / ∈ T , BPR wants to maxi- mizê x e,f,g , defined as:</p><formula xml:id="formula_18">ˆ x e,f,g = ˆ x e,f − ˆ x e,g</formula><p>wherê x e,f andˆxandˆ andˆx e,g can be defined either by eq. 1 or eq. 2 (for cold words). Specifically, BPR optimizes ( <ref type="bibr" target="#b30">Rendle et al., 2009</ref>):</p><formula xml:id="formula_19">e,f,g∈D ln σ(ˆ x e,f,g ) − λ Θ ||Θ|| 2</formula><p>where σ is the logistic sigmoid function, Θ is the parameter vector ofˆxofˆ ofˆx e,f,g to be trained, and λ Θ is its hyperparameter vector. BPR can be trained using stochastic gradient ascent where a triple e, f, g is sampled from D and parameter updates are performed:</p><formula xml:id="formula_20">Θ ← Θ + η.(σ(−ˆ x e,f,g ) ∂ ˆ x e,f,g ∂Θ − λ Θ Θ)</formula><p>η is the learning rate. Hence, for the MF formula- tion ofˆxofˆ ofˆx e,f,g , we can sample a triple e, f, g from D and update its parameters as:</p><formula xml:id="formula_21">p e ← p e + η.(σ(−ˆ x MF e,f,g )(q f − q g ) − λ P p e ) q f ← q f + η.(σ(−ˆ x MF e,f,g )(p e ) − λ Q + q f ) q g ← q g + η.(σ(−ˆ x MF e,f,g )(−p e ) − λ Q − q g )</formula><p>while for the auxiliary formulation ofˆxofˆ ofˆx e,f,g , we can sample a triple e, f, g from D and update its parameters as:</p><formula xml:id="formula_22">θ e ← θ e + η.(σ(−ˆ x AUX e,f,g )(θ f − θ g ) − λ Θ θ e ) β ← β + η.(σ(−ˆ x AUX e,f,g )(θ f − θ g ) − λ β β)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To implement our approach, we extend the imple- mentation of BPR in LIBREC 3 which is a publicly available Java library for recommender systems. We evaluate our model for the task of Bilingual Lexicon Induction (BLI). Given a source word f , the task is to rank all candidate target words e by their predicted translation scoresˆxscoresˆ scoresˆx e,f . We con- duct large-scale experiments on 27 low-and high- resource source languages and evaluate their trans- lations to English. We use the 100K most frequent words from English Wikipedia as candidate En- glish target words (E).</p><p>At test time, for each source language, we eval- uate the top-10 accuracy (Acc 10 ): the percent of source language words in the test set for which a correct English translation appears in the top-10 ranked English candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Test sets</head><p>We use benchmark test sets for the task of bilin- gual lexicon induction to evaluate the performance of our model. The VULIC1000 dataset (Vulic and Moens, 2016) comprises 1000 nouns in Span- ish, Italian, and Dutch, along with their one-to-one ground-truth word translations in English.</p><p>We construct a new test set (CROWDTEST) for a larger set of 27 languages from crowdsourced dictionaries ( <ref type="bibr" target="#b26">Pavlick et al., 2014</ref>). For each lan- guage, we randomly pick up to 1000 words that have only one English word translation in the crowdsourced dictionary to be the test set for that language. On average, there are 967 test source words with a variety of POS per language. Since different language treats grammatical categories such as tense and number differently (for example, unlike English, tenses are not expressed by spe- cific forms of words in Indonesian (id); rather, they are expressed through context), we make our evaluation on all languages in CROWDTEST generic by treating a predicted English translation of a foreign word as correct as long as it has the same lemma as the gold English translation. To facilitate further research, we make CROWDTEST publicly available in our website.  served translations to both the source language and the target language, English. We first col- lect all interlingual links from English Wikipedia pages to pages in other languages. Using these links, we obtain translations of Wikipedia page titles in many languages to English e.g., id.wikipedia.org/wiki/Kulkas → fridge (en). The observed translations are projected to fill the missing translations in the matrix <ref type="figure" target="#fig_3">(Figure 3</ref>). We call these bilingual translations WIKI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Bilingual Signals for Translation</head><p>From the links that we have collected, we can also infer links from Wikipedia pages in the source language to other pages in non-target lan- guages e.g., id.wikipedia.org/wiki/Kulkas → it.wikipedia.org/wiki/Frigorifero. The ti- tles of these pages can be translated to English if they exist as entries in the dictionaries. These non-source, non-target language pages can act as yet another third language whose observed trans- lations can be projected to fill the missing transla- tions in the matrix <ref type="figure" target="#fig_3">(Figure 3)</ref>. We call these bilin- gual translations WIKI+CROWD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Monolingual Signals for Translation</head><p>We define cold source words in our experiments as source words that have no associated WIKI trans- lations and fewer than 2 associated WIKI+CROWD translations. For each cold source word f , we pre- dict the score of its translation to each candidate English word e using the auxiliary formulation ofˆx ofˆ ofˆx e,f (Equation 2). There are two auxiliary signals about the words that we use in our experiments: (1) bilingually informed word embeddings and (2) visual representations.</p><p>Bilingually Informed Word Embeddings For each language, we learn monolingual embed- dings for its words by training a standard mono- lingual word2vec skipgram model <ref type="bibr" target="#b25">(Mikolov et al., 2013b</ref>) on tokenized Wikipedia pages of that language using Gensim <ref type="bibr">( ˇ Rehůřek and Sojka, 2010)</ref>. We obtain 100-dimensional word embed- dings with 15 epochs, 15 negatives, window size of 5, and cutoff value of 5.</p><p>Given two monolingual embedding spaces R d F and R d E of the source and target languages F and E, where d f and d e denote the dimensionality of the monolingual embedding spaces, we use the set of crowdsourced translations that are not in the test set as our seed bilingual translations <ref type="bibr">4</ref> and learn a mapping function W ∈ R d E ×d F that maps the target language vectors in the seed translations to their corresponding source language vectors. <ref type="bibr">5</ref> We learn two types of mapping: linear and non- linear, and compare their performances. The linear mapping ( <ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b6">Dinu et al., 2014</ref>) minimizes:</p><formula xml:id="formula_23">||X E W−X F || 2</formula><p>F where, following the notation in <ref type="bibr" target="#b41">(Vuli´cVuli´c and Korhonen, 2016)</ref>, X E and X F are matrices obtained by respective concate- nation of target language and source language vec- tors that are in the seed bilingual translations. We solve this optimization problem using stochastic gradient descent (SGD).</p><p>We also consider a non-linear mapping ) using a simple four-layer neural net- work, W = (φ (1) , φ (2) , φ (3) , φ <ref type="bibr">(4)</ref> ) that is trained to minimize:</p><formula xml:id="formula_24">x f ∈X F xe∈X E ||x f − φ (4) s(φ (3) s(φ (2) s(φ (1) xe)))|| 2 where φ (1) ∈ R h 1 ×d E , φ (2) ∈ R h 2 ×h 1 , φ (3) ∈ R h 3 ×h 2 , φ (4) ∈ R d F ×h 3 , h n</formula><p>is the size of the hid- den layer, and s = tanh is the chosen non-linear function.</p><p>Once the map W is learned, all candidate target word vectors x e can be mapped into the source language embedding space R d F by computing x T e W. Instead of the raw monolingual word em- beddings x e , we use these bilingually-informed mapped word vectors x T e W as the auxiliary word features WORD-AUX to estimatê</p><p>x AUX e,f . Visual Representations Pilot Study Recent work  has shown that combin- ing word embeddings and visual representations of words can help achieve more accurate bilingual translations. Since the visual representation of a word seems to be language-independent (e.g. the concept of water has similar images whether ex- pressed in English or French <ref type="figure" target="#fig_4">(Figure 4)</ref>, the visual representations of a word may be useful for infer- ring its translation and for complementing the in- formation learned in text.</p><p>We performed a pilot study to include visual features as auxiliary features in our framework. We use a large multilingual corpus of labeled im- ages <ref type="bibr" target="#b1">(Callahan, 2017)</ref> to obtain the visual repre- sentation of the words in our source and target lan- guages. The corpus contains 100 images for up to 10k words in each of 100 foreign languages, plus images of each of their translations into English. For each of the images, a convolutional neural net- work (CNN) feature vector is also provided fol- lowing the method of <ref type="bibr" target="#b16">Kiela et al. (2015)</ref>. For each word, we use 10 images provided by this corpus and use their CNN features as auxiliary visual fea- tures VISUAL-AUX to estimatê</p><p>x AUX e,f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Combining Signals</head><p>During training, we trained the parameters ofˆxofˆ ofˆx MF e,f andˆxandˆ andˆx AUX e,f using a variety of signals:</p><formula xml:id="formula_25">• ˆ x MF−W e,f</formula><p>is trained using WIKI translations as the set of observed translations</p><formula xml:id="formula_26">T • ˆ x MF−W+C e,f</formula><p>is trained using WIKI+CROWD translations as the set of observed</p><formula xml:id="formula_27">T • ˆ x AUX−WE e,f</formula><p>is trained using the set of word identities T identity and WORD-AUX as θ f</p><formula xml:id="formula_28">• ˆ x AUX−VIS e,f</formula><p>is trained using the set of word identities T identity and VISUAL-AUX as θ f During testing, we use the following back-off scheme to predict translation scores given a source word f and a candidate target word e:</p><formula xml:id="formula_29">1457ˆx 1457ˆ 1457ˆx e,f =          ˆ x MF−W e,f if f has ≥ 1 associated WIKI, ˆ x MF−W+C e,f else if f has ≥ 2 associated WIKI+CROWD, ˆ x AUX e,f otherwise wherê x AUX e,f = α wê x AUX−WE e,f + α visˆxvisˆ visˆx AUX−VIS e,f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>We conduct experiments using the following vari- ants of our model, each of which progressively in- corporates more signals to rank candidate English target words. When a variant uses more than one formulation ofˆxofˆ ofˆx e,f , it applies them using the back- off scheme that we have described before.</p><p>•  We evaluate the performance of BPR WE against a baseline that is the state-of-the-art model of Vuli´c <ref type="bibr" target="#b41">Vuli´c and Korhonen (2016)</ref>, on benchmark VULIC1000 <ref type="table" target="#tab_4">(Table 1)</ref>. The baseline (MNN) learns a linear mapping between monolingual em- bedding spaces and finds translations in an unsu- pervised manner: it ranks candidate target words based on their cosine similarities to the source word in the mapped space. As seed translation pairs, MNN uses mutual nearest neighbor pairs (MNN) obtained from pseudo-bilingual corpora constructed from unannotated monolingual data of the source and target languages <ref type="bibr" target="#b44">(Vulic and Moens, 2016)</ref>. We train MNN and our models using the same 100-dimensional word2vec monolingual word embeddings.</p><p>As seen in <ref type="table" target="#tab_4">Table 1</ref>, we see the benefit of learning translations in a supervised manner. BPR+MNN uses the same MNN seed translations as MNN, obtained from unannotated monolingual data of English and the foreign language, to learn the linear mapping between their embedding spaces. However, unlike MNN, BPR+MNN uses the mapped word vectors to predict ranking in a su- pervised manner with BPR objective. This results in higher accuracies than MNN. Using seed trans- lations from crowdsourced dictionaries to learn the linear mapping (BPR LN) improves accuracies even further compared to using MNN seed trans- lations obtained from unannotated data. Finally, BPR WE that learns translations in a supervised manner and uses third language translations and non-linear mapping (trained with crowdsourced translations not in the test set) performs consis- tently and very significantly better than the state- of-the-art on all benchmark test sets. This shows that incorporating more and better signals of trans- lation can improve performance significantly.</p><p>Evaluating on CROWDTEST, we observe a similar trend over all 27 languages ( <ref type="figure" target="#fig_6">Figure 5</ref>). Par- ticularly, we see that BPR W and BPR W+C suffer from the cold start issue where there are too few or no observed translations in the matrix to make ac- curate predictions. Incorporating auxiliary infor- mation in the form of bilingually-informed word embeddings improves the accuracy of the predic- tions dramatically. For many languages, learn- ing these bilingually-informed word embeddings with non-linear mapping improves accuracy even more. The top accuracy scores achieved by the model vary across languages and seem to be in- fluenced by the amount of data i.e., Wikipedia to- kens and seed lexicons entries available for train- ing. Somali (so) for example, has only 0.9 million tokens available in its Wikipedia for training the word2vec embeddings and only 3 thousand seed translations for learning the mapping between the word embedding spaces. In comparison, Span- ish (es) has over 500 million tokens available in its Wikipedia and 11 thousand seed translations. We also believe that our choice of tokenization may not be suitable for some languages -we use a simple regular-expression based tokenizer for many languages that do not have a trained NLTK 6 tokenization model. This may influence perfor- mance on languages such as Vietnamese (vi) on which we have a low performance despite its large Wikipedia corpus.  Some example translations of an Indonesian word produced by different variants of our model are shown in <ref type="table" target="#tab_5">Table 2</ref>. Adding third language trans- lation signals on top of the bilingually-informed auxiliary signals improves accuracies even fur- ther. <ref type="bibr">7</ref> The accuracies achieved by BPR WE on these languages are significantly better than pre- viously reported accuracies <ref type="bibr">(Irvine and CallisonBurch, 2017</ref>) on test sets constructed from the same crowdsourced dictionaries <ref type="bibr" target="#b26">(Pavlick et al., 2014)</ref> 8 .</p><p>The accuracies across languages appear to im- prove consistently with the amount of signals be- ing input to the model. In the following exper- iments, we investigate how sensitive these im- provements are with varying training size.</p><p>In <ref type="figure" target="#fig_7">Figure 6</ref>, we show accuracies obtained by <ref type="bibr">7</ref> Actual improvement per language depends on the cover- age of the Wikipedia interlanguage links for that language <ref type="bibr">8</ref> The comparison however, cannot be made apples-to- apples since the way Irvine and Callison-Burch (2017) select test sets from the crowdsourced dictionaries maybe different and they do not release the test sets BPR WE with varying sizes of seed translation lex- icons used to train its mapping. The results show that a seed lexicon size of 5K is enough across languages to achieve optimum performance. This finding is consistent with the finding of Vuli´c <ref type="bibr" target="#b41">Vuli´c and Korhonen (2016)</ref> that accuracies peak at about 5K seed translations across all their models and lan- guages. For future work, it will be interesting to investigate further why this is the case: e.g., how optimal seed size is related to the quality of the seed translations and the size of the test set, and how the optimum seed size should be chosen. Lastly, we experiment with incorporating auxil- iary visual signals for learning translations on the multilingual image corpus <ref type="bibr" target="#b1">(Callahan, 2017)</ref>. The corpus contains 100 images for up to 10K words in each of 100 foreign languages, plus images of each of their translations into English. We train and test our BPR VIS model to learn translations of 5 low-and high-resource languages in this cor- pus. We use the translations of up to 10K words in each of these languages as test set and use up to  10 images (CNN features) of the words in this set as auxiliary visual signals to predict their transla- tions. In this experiment, we weigh auxiliary word embedding and visual features equally. To train the mapping of our word embedding features, we use as seeds crowdsourced translations not in test set.</p><p>We compare the quality of our translations with the baseline CNN-AVGMAX (Bergsma and Van Durme, 2011), which considers cosine simi- larities between individual images from the source and target word languages and takes average of their maximum similarities as the final similarity between a source and a target word. For each source word, the candidate target words are ranked according to these final similarities. This base- line has been shown to be effective for inducing translations from images, both in the uni-modal (Bergsma and Van Durme, 2011; <ref type="bibr" target="#b16">Kiela et al., 2015)</ref> and multi-modal models .</p><p>As seen in <ref type="table" target="#tab_6">Table 3</ref>, incorporating additional bilingual and textual signals to the visual signals improves translations. Accuracies on these image corpus' test sets are lower overall as they contain a lot of translations from our crowdsourced dictio- naries; thus we have much less seeds to train our word embedding mapping. Furthermore, these test sets contain 10 times as many translations as our previous test sets. Using more images instead of just 10 per word may also improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel framework for combining diverse, sparse and potentially noisy multi-modal signals for translations. We view the problem of learning translations as a matrix com- pletion task and use an effective and extendable matrix factorization approach with BPR to learn translations.</p><p>We show the effectiveness of our approach in large scale experiments. Starting from minimally- trained monolingual word embeddings, we con- sistently and very significantly outperform state- of-the-art approaches by combining these features with other features in a supervised manner using BPR. Since our framework is modular, each in- put to our prediction can be improved separately to improve the whole system e.g., by learning bet- ter word embeddings or a better mapping func- tion to input into the auxiliary component. Our framework is also easily extendable to incorporate more bilingual and auxiliary signals of translation equivalence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The word tidur (id) is a cold word with no associated translation in the matrix. Auxiliary features ✓ f about the words can be used to predict translations for cold words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The word tidur (id) is a cold word with no associated translation in the matrix. Auxiliary features θ f about the words can be used to predict translations for cold words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Wikipedia pages with observed translations to the source (id) and the target (en) languages act as a third language in the matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Five images for the French word eau and its top 4 translations ranked using visual simularities of images associated with English words (Bergsma and Van Durme, 2011)</figDesc><graphic url="image-7.png" coords="6,317.53,62.81,197.75,115.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>BPR W uses onlyˆxonlyˆ onlyˆx MF−W e,f • BPR W+C usesˆxusesˆ usesˆx MF−W e,f andˆxandˆ andˆx MF−W+C e,f • BPR LN uses onlyˆxonlyˆ onlyˆx AUX−WE e,f with linear mapping • BPR NN uses onlyˆxonlyˆ onlyˆx AUX−WE e,f with neural network (NN) mapping • BPR WE usesˆxusesˆ usesˆx MF−W e,f , ˆ x MF−W+C e,f , andˆx andˆ andˆx AUX−WE e,f with NN mapping • BPR VIS addsˆxaddsˆ addsˆx AUX−VIS e,f to BPR WE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Acc 10 on CROWDTEST across all 27 languages show that adding more and better signals for translation improves translation accuracies. The top accuracies achieved by our model: BPR WE vary across languages and appear to be influenced by the amount of data (Wikipedia tokens and seed translations) and tokenization available for the language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Acc 10 across different seed lexicon sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>)</head><label></label><figDesc></figDesc><table>Auxiliary Signals for Translation Because the 
observed bilingual translations may be sparse, the fridge 

house 

fridge 
house 

sleep 

sleep 
kulkas 
koki 
tidur 

hijzelf 
koelkast 

kokkien 

EN 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>4 Experiments</head><label>4</label><figDesc></figDesc><table>Data 

Results 

5 Results and conclusion 

References 

Sarath Chandar AP, Stanislas Lauly, Hugo Larochelle, 
Mitesh Khapra, Balaraman Ravindran, Vikas C 

Aligning words using matrix factorisation. In Pro 
ceedings of the 42nd Annual Meeting on Associa 
tion for Computational Linguistics. Association fo 
Computational Linguistics, page 502. 

Stephan Gouws, Yoshua Bengio, and Greg Corrado 
2014. Bilbowa: Fast bilingual distributed represen 
tations without word alignments. stat 1050:9. 

Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati 
Bowen Zhou, and Yoshua Bengio. 2016. Pointing 
the unknown words. In Proceedings of the 54th An 
nual Meeting of the Association for Computationa 
Linguistics (ACL). Association for Computationa 
Linguistics. 

Ruining He and Julian McAuley. 2016. Vbpr: Visua 
bayesian personalized ranking from implicit feed 
back. In Thirtieth AAAI Conference on Artificial In 
telligence. 

Ann Irvine and Chris Callison-Burch. 2013. Su 
pervised bilingual lexicon induction with multiple 
monolingual signals. Citeseer. 

Ann Irvine and Chris Callison-Burch. 2016. End 
to-end statistical machine translation with zero o 
small parallel texts. Natural Language Engineering 
22(04):517-548. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 1 : Acc 10 performance on VULIC1000</head><label>1</label><figDesc></figDesc><table>Baseline BPR+MNN BPR LN BPR WE 
(MNN) 
IT-EN 
78.8% 
79.4% 
81.3% 
86.0% 
ES-EN 
81.8% 
82.1% 
83.4% 
87.1% 
NL-EN 
80.8% 
81.6% 
83.2% 
87.2% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Top-5 translations of the Indonesian word kesadaran (awareness) using different model variants 

BPR W 
BPR LN 
BPR NN 
BPR WE 
kesadaran 
kesadaran 
kesadaran 
kesadaran 
consciousness consciousness consciousness 
conscience 
goddess 
awareness 
empathy 
awareness 
friendship 
empathy 
awareness 
understanding 
night 
perception 
perceptions 
consciousness 
nation 
mindedness 
perception 
acquaintance 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Acc 10 performance on the multilingual 
image corpus test set (Callahan, 2017) 

Baseline 
BPR VIS # Seeds 
(CNN-AvgMax) 
IT-EN 
31.4% 
55.8% 
581 
ES-EN 
33.0% 
58.3% 
488 
NL-EN 
35.5% 
69.2% 
1857 
FR-EN 
37.1% 
65.9% 
1697 
ID-EN 
36.9% 
45.3% 
462 

</table></figure>

			<note place="foot" n="4"> On average, there are 9846 crowdsourced translations per language that we can use as seed translations. 5 We find that mapping from target to source vectors gives better performances across models in our experiments.</note>

			<note place="foot" n="6"> http://www.nltk.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based in part on research spon-sored by DARPA under grant number HR0011-15-C-0115 (the LORELEI program). The U.S. Gov-ernment is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government. This work was also supported by the French Na-tional Research Agency under project ANR-16-CE33-0013, and by Amazon through the Amazon Academic Research Awards (AARA) program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Bilingual Lexicons Using the Visual Similarity of Labeled Web Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1764" to="1769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Image-based bilingual lexicon induction for low resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Callahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved Statistical Machine Translation Using Paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Philipp Koehn, and Miles Osborne</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Human Language Technology Conference of the NAACL, Main Conference</title>
		<meeting><address><addrLine>New York City</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Autoencoder Approach to Learning Bilingual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1853" to="1861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain Adaptation for Machine Translation by Mining Unseen Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagadeesh</forename><surname>Jagarlamudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving zero-shot learning by mitigating the hubness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR Workshop</title>
		<meeting>ICLR Workshop<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction with Matrix Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Miao Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Vector Space Word Representations Using Multilingual Correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning attribute-to-feature mappings for cold-start recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aligning words using matrix factorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Goutte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 502. Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics, page 502. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BilBOWA: Fast Bilingual Distributed Representations without Word Alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="748" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vbpr: Visual bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
	<note>AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervised Bilingual Lexicon Induction with Multiple Monolingual Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013)<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="518" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Endto-end statistical machine translation with zero or small parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Comprehensive Analysis of Bilingual Lexicon Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="310" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual Bilingual Lexicon Induction with Transferred ConvNet Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="148" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Toward Statistical Machine Translation without Parallel Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callisonburch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="130" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly Supervised Named Entity Transliteration and Discovery from Multilingual Comparable Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inducing Crosslingual Distributed Representations of Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012<address><addrLine>Mumbai, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1459" to="1474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matrix Factorization Techniques for Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Multilingual Correlation for Improved Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="250" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bilingual Word Representations with Monolingual Quality in Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The language demographics of Amazon Mechanical Turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kachaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="79" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying Word Translations in Non-Parallel Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd</title>
		<meeting>the 33rd</meeting>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="320" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Radimřehůřekradimˇradimřehůřek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Steffen Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relation Extraction with Matrix Factorization and Universal Schemas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Injecting Logical Background Knowledge into Embeddings for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<idno>COLING-02</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Learning</title>
		<meeting>the 6th Conference on Natural Language Learning<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving Neural Machine Translation Models with Monolingual Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Cross-lingual Word Embeddings via Matrix Co-factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="567" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Zero-Shot Learning Through Cross-Modal Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-lingual models of word embeddings: An empirical comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multilingual relation extraction using compositional universal schema</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="886" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-Modal Representations for Improved Bilingual Lexicon Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="188" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the Role of Seed Lexicons in Learning Bilingual Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="247" to="257" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1613" to="1624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bilingual Word Embeddings from Non-Parallel DocumentAligned Data Applied to Bilingual Lexicon Induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli´cvuli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="719" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bilingual Distributed Word Representations from DocumentAligned Comparable Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="953" to="994" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Statistical machine translation improves question retrieval in community question answering via matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="852" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
