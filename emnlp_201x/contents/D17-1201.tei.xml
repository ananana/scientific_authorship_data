<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Initializing Convolutional Filters with Semantic Features for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Chinese Information Processing</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UltraPower-BNU Joint Laboratory for Artificial Intelligence</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
							<email>tliu@ruc.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renfen</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">College of Chinese Language and Culture</orgName>
								<orgName type="institution">Beijing Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Initializing Convolutional Filters with Semantic Features for Text Classification</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1884" to="1889"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
					<note>Xiaoyong Du 4,5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Convolutional Neural Networks (CNNs) are widely used in NLP tasks. This paper presents a novel weight initialization method to improve the CNNs for text classification. Instead of randomly initializing the convolutional filters, we encode semantic features into them, which helps the model focus on learning useful features at the beginning of the training. Experiments demonstrate the effectiveness of the ini-tialization technique on seven text classification tasks, including sentiment analysis and topic classification.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, neural networks (NNs) dominate the state-of-the-art results on a wide range of natu- ral language processing (NLP) tasks. The com- monly used neural networks in NLP include Re- current NNs, Convolutional NNs, Recursive NNs and their combinations. NNs are known for their strong abilities to learn features automatically. However, the lack of data or inappropriate param- eter settings might greatly limit the generalization abilities of the models ( <ref type="bibr" target="#b0">Bengio et al., 2009;</ref><ref type="bibr" target="#b11">LeCun et al., 2015;</ref><ref type="bibr" target="#b9">Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b21">Srivastava et al., 2014</ref>). To enhance the performance, a lot of improved methods have been proposed, e.g. developing advanced structures ( <ref type="bibr" target="#b30">Zhao et al., 2015;</ref><ref type="bibr" target="#b27">Zhang et al., 2016a</ref>), introducing prior knowledge ( <ref type="bibr" target="#b6">Hu et al., 2016</ref>) and utilizing external resources ( <ref type="bibr" target="#b25">Xie et al., 2016;</ref><ref type="bibr" target="#b18">Qian et al., 2016)</ref>.</p><p>It is also noteworthy that the neural networks' performance is sensitive to weight initialization † Corresponding author.</p><p>because their objectives are non-convex <ref type="bibr" target="#b3">(Glorot and Bengio, 2010;</ref><ref type="bibr" target="#b19">Saxe et al., 2013;</ref><ref type="bibr" target="#b15">Mishkin and Matas, 2015)</ref>. In fact, initialization techniques even play a role of catalyst for the revival of neu- ral networks <ref type="bibr" target="#b4">(Hinton et al., 2006</ref>; <ref type="bibr" target="#b11">LeCun et al., 2015)</ref>. Most improvements on initializing weights are based on mathematical methods, e.g. xavier initialization <ref type="bibr" target="#b3">(Glorot and Bengio, 2010)</ref> and or- thogonal initialization <ref type="bibr" target="#b19">(Saxe et al., 2013</ref>). For NLP tasks, an influential technique is to use pre- trained word vectors to initialize embedding layers <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b1">Chen and Manning, 2014)</ref>. Consider the embedding layers could be initialized by pre- trained word vectors, how about weights in other layers that are still randomly initialized?</p><p>Inspired by this question, we propose a sim- ple yet effective method to improve CNNs by ini- tializing convolutional layers (filters). Unlike the previous weight initialization based on mathemati- cal methods, we encode semantic features into the filters instead of initializing them randomly. As CNNs exploit 1-D convolutional filters to extract n-gram features, our method aims at helping the filters focus on learning useful n-grams, e.g. "not bad" which is more useful than "watch a movie" for determining reviews' polarities. Specifically, we select n-grams from training data via a novel Naive Bayes (NB) weighting technique, and then cluster the n-gram embeddings with K-means al- gorithm. After that, we use the centroid vectors of the clusters to initialize the filters.</p><p>With this initialization method, CNN filters tend to extract important n-gram features at the begin- ning of the training process. By integrating our method into a classic CNN model for text classi- fication <ref type="bibr" target="#b7">(Kim, 2014)</ref>, we observe significant im-provements in sentiment analysis and topic classi- fication tasks. The advantages of our approach are as follows:</p><p>• Features are directly extracted from train- ing data without involving any external re- sources; • The computation brought by our method is relatively small, resulting in small additional training costs; • The filter initialization is task independent. It could be easily applied to other NLP tasks. Also, we further analyze the filters, shedding some light on the mechanism how our method influences the training process.</p><p>The source code is released at https://github.com/ shenshen-hungry/Semantic-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most recently, CNNs are becoming increasingly popular in a variety of NLP tasks. An influential one is the work of <ref type="bibr" target="#b7">(Kim, 2014)</ref>, where a simple CNN with a single layer of convolution is used for feature extraction. Despite its simple structure, the model achieves strong baselines on many sentence classification datasets. Following this work, sev- eral improved models are proposed. <ref type="bibr" target="#b29">Zhang and Wallace (2015)</ref> improve the model by optimiz- ing hyper-parameters and provide a detailed anal- ysis of the CNN <ref type="bibr" target="#b7">(Kim, 2014)</ref>. <ref type="bibr" target="#b26">Yin and Schütze (2016)</ref> and <ref type="bibr" target="#b28">Zhang et al. (2016b)</ref> exploit different pre-trained word embeddings (e.g. word2vec and GloVe) to enhance the model. In addition to initializing embedding layers with pre-trained word vectors, other pre-designed fea- tures also prove to be very effective in assisting the training of neural models. For example, in ( <ref type="bibr" target="#b6">Hu et al., 2016</ref>), neural models are harnessed by logic rules. <ref type="bibr" target="#b12">Li et al. (2016)</ref> propose to use pre-calculated words' weights to guide Paragraph Vector model. <ref type="bibr" target="#b2">Dai and Le (2015)</ref> combine the hidden layers of RNNs with linearly increasing weights. <ref type="bibr" target="#b25">Xie et al. (2016)</ref> use entity descriptions from knowledge bases (e.g. Freebase) to learn knowledge repre- sentations for entity classification and knowledge graph completion. <ref type="bibr" target="#b18">Qian et al. (2016)</ref> propose lin- guistically regularized LSTMs for sentiment anal- ysis with sentiment lexicons, negation words, and intensity words. In this work, we encode seman- tic features into convolutional layers by initializ- ing them with important n-grams. Being aware of which n-grams are important, CNN is able to ex- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>The intuition behind our method is simple: Since CNNs essentially capture semantic features of n- grams, we can use important n-grams to initial- ize the filters. As a result, the filters are able to focus on extracting those important n-gram fea- tures at the beginning of the training. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we use embeddings of "not" and "bad" to initialize the filter. A larger score will be ob- tained when the "not bad" filter matches the bi- gram "not bad" in the text, otherwise a relatively smaller score will be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">N-gram Selection</head><p>Firstly, we extract important n-grams from the training data. Intuitively, n-gram "not bad" is much more important than "watch a movie" for determining reviews' polarities. Naive Bayes (NB) weighting is an effective technique for de- termining the words' importance ( <ref type="bibr" target="#b14">Martineau and Finin, 2009;</ref><ref type="bibr" target="#b23">Wang and Manning, 2012)</ref>. NB weight r of a n-gram w in class c is calculated as follows:</p><formula xml:id="formula_0">r = (p w c + α)/||p c || 1 (p w ˜ c + α)/||p˜c||p˜ ||p˜c || 1</formula><p>where p w c is the number of texts that contain  n-gram w in class c, p w ˜ c is the number of texts that contain n-gram w in other classes, ||p c || 1 is the number of texts in class c, ||p˜c||p˜ ||p˜c || 1 is the number of texts in other classes, α is a smoothing param- eter. For positive class in movie review dataset, the ratios of n-grams like "amazing" and "not bad" should be large since they appear much more fre- quently in positive texts than in negative texts. For neutral n-grams like "of the" and "movie", their ratios should be around 1. For each class, we se- lect the n-grams whose ratios are much higher than 1 for filter initialization. We give examples of n- grams selected by our method in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Filter Initialization</head><p>We concatenate word embeddings to construct n- gram embeddings. For example, a tri-gram em- bedding has 3*100 dimensions when word embed- ding has 100 dimensions. This concatenation fol- lows the mechanism of convolutional filters,where a filter with n*d dimensions is able to capture n- gram features (d is the dimension of word embed- ding). Because the number of filters in CNNs is much smaller than the number of n-grams, a fil- ter tends to extract the features of a class of n- grams rather than an individual n-gram. Based on this observation, we don't use n-gram embed- dings to initialize the filters directly. Instead, we firstly use K-means to cluster features of the se- lected n-grams, and then use the clusters' centroid vectors to initialize the filters. In this work, we consider clustering uni-gram (word), bi-gram and tri-gram features. <ref type="figure" target="#fig_1">Figure 2</ref> shows two uni-gram cluster examples extracted from the location ques- tions in TREC dataset ( <ref type="bibr" target="#b13">Li and Roth, 2002</ref>).</p><p>After obtaining the n-gram clusters, we feed their centroid vectors into the center of the filters. The remaining positions are still initialized ran- domly. Taking filters with size 3, 4, 5 as examples, <ref type="figure" target="#fig_2">Figure 3</ref> shows how we fill uni, bi, and tri-gram features into the filters. By doing this, we encode semantic features into the filters. For example, in the TREC question classification task, the initial- ization will result in six types of filters which are sensitive to abbreviation, entity, description, hu- man, location and number questions respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Hyper-parameter Settings</head><p>CNN-non-static 1 (short for CNN) proposed by <ref type="bibr" target="#b7">Kim (2014)</ref> is used as our baseline, which con- sists of one embedding layer, one convolutional layer, one max pooling layer, and one fully con- nected layer. The model proposed by <ref type="bibr" target="#b7">Kim (2014)</ref> is a strong baseline in sentence classification. For details of the model, one can see <ref type="bibr" target="#b7">(Kim, 2014;</ref><ref type="bibr" target="#b29">Zhang and Wallace, 2015)</ref>. Pre-trained word em- beddings on Google News via word2vec toolkit 2 are used for initializing the convolutional filters, besides initializing the embedding layer of CNN as in <ref type="bibr" target="#b7">(Kim, 2014</ref>). For a fair comparison, we use the same seven datasets 3 and hyper-parameter set- ting with Kim (2014)'s work for training and test- ing. Uni, bi, and tri-gram features are used to ini- tialize the filters. For a K-way classification prob- lem, we select top 10% n-grams in each class ac- cording to NB weighting. Since 300 filters are used in <ref type="bibr" target="#b7">Kim (2014)</ref>'s work, we follow this set- ting and aggregate n-grams into 300/K clusters for each class. Centroid vectors are used for filling the filters. Taking binary classification dataset MR as an example, 150 "positive" filters and 150 "nega- tive" filters are obtained after initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of Filter Initialization</head><p>In this section, we demonstrate the effectiveness of our initialization technique. We respectively use uni, bi and tri-gram centroid vectors to fill the filters.  further improves the accuracies significantly on all datasets except MPQA. The results are con- sistent with ( <ref type="bibr" target="#b23">Wang and Manning, 2012)</ref>, where NB weighting produces little improvement over MPQA. We can also observe that the performance of uni, bi and tri-grams are comparable. None of them outperforms the others on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with</head><p>State-of-the-arts <ref type="table" target="#tab_1">Table 2</ref> lists the results of our model and other state-of-the-arts. Models in the first group are improved CNNs based on <ref type="bibr" target="#b7">(Kim, 2014</ref> Experiments show that our n-gram features make great contribution to both two-class and multi-class classification. Essentially, our method enables CNNs to obtain better generalization abil- ities. Furthermore, as the initialization does not rely on any external prior knowledge or resources, it could be easily applied to other NLP tasks or other languages.  <ref type="table">Table 3</ref>: "+" and "-" are used to denote the num- ber of positive and negative weights respectively. The data in the table are obtained from MR by the average of 10 times training. Every time we select 100 filters. 50 of them are initialized with positive n-grams and the rest are with negative n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Analysis of Filters</head><p>We further analyze the filter initialization with an example of binary sentiment classification. Through the initialization we have determined which filters are positive or negative in advance. The corresponding neurons of positive filters upon max-pooling layer are supposed to be activated by positive samples. Since positive (negative) samples have labels of 1 (0), the corresponding weights (in logistic regression) of those "positive" neurons tend to be positive. For the same reason, the negative filters tend to have negative weights. The results shown in <ref type="table">Table 3</ref> confirm our hypoth- esis: Positive/negative filters respectively tend to have positive(+)/negative(-) weights. The differ- ence between positive and negative filters are more obvious in bi-gram and tri-gram cases. It is be- cause bi and tri-gram centroid vectors could ini- tialize more parameters of filters than uni-gram.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, experiments show that different choices of uni, bi, and tri-grams have little influ- ence on the results. The following is our assump- tion: Compared to uni-grams (words), bi and tri- grams can cover more spaces of filters and intro- duce more NB information to filters. Filters ini- tialized by them thus pay more attention to NB information than filters initialized by uni-grams according to <ref type="table">Table 3</ref>. However, bi and tri-grams are also sparser in data than uni-grams. Their NB weights are not as accurate as those of uni-grams, even applied smoothing. As NB weight of a n- gram denotes its contribution to the classification, model initialized with tri-grams does not always perform the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a novel weight initialization technique for CNNs. We discover that convolu- tional filters that encode semantic features at the beginning of the training tend to produce better results than being randomly initialized. This has a similar effect with embedding layer initializa- tion via pre-trained word vectors. Experimental results demonstrate the effectiveness of the ini- tialization technique on multiple text classification tasks. In addition, our method requires few ex- ternal resources and relatively small calculation, making it attractive for scenarios where training costs may be an issue. In textual data, the features extracted by CNNs are n-grams. However, in fields like computer vi- sion, features extracted by filters are more difficult to interpret. It still requires further exploration to apply our method to fields beyond NLP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of CNN with one layer of convolution and pooling.</figDesc><graphic url="image-1.png" coords="2,312.09,62.81,208.63,235.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Uni-gram cluster examples.</figDesc><graphic url="image-3.png" coords="3,85.89,151.28,190.48,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Filter initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 lists the results. The CNN has provided very strong baselines. Our method</head><label>1</label><figDesc></figDesc><table>Model 

MR SST-1 SST-2 Subj TREC CR MPQA 
CNN-non-static 
81.5 
48.0 
87.2 
93.4 
93.6 
84.3 
89.5 
+UNI 
82.1 
50.8 
89.0 
93.7 
94.4 
86.0 
89.3 
+BI 
82.2 
50.7 
88.3 
93.7 
94.6 
85.8 
89.5 
+TRI 
82.1 
49.8 
88.2 
93.8 
94.2 
85.9 
89.2 

Table 1: Effectiveness of filter initialization. 

Model 
MR SST-1 SST-2 Subj TREC CR MPQA 
CNN-non-static (Kim, 2014) 
81.5 
48.0 
87.2 
93.4 
93.6 
84.3 
89.5 
MV-CNN (Yin and Schütze, 2016) 
-
49.6 
89.4 
93.9 
-
-
-
MGNC-CNN (Zhang et al., 2016b) 
-
48.7 
88.3 
94.1 
95.5 
-
-
CNN-Rule (Hu et al., 2016) 
81.7 
-
89.3 
-
-
85.3 
-
Our Model (CNN-non-static+UNI) 
82.1 
50.8 
89.0 
93.7 
94.4 
86.0 
89.3 
combine-skip (Kiros et al., 2015) 
76.5 
-
-
93.6 
92.2 
80.1 
87.1 
Adasent (Zhao et al., 2015) 
83.1 
-
-
95.5 
92.4 
86.3 
93.3 
DSCNN (Zhang et al., 2016a) 
82.2 
50.6 
88.7 
93.9 
95.6 
-
-
PV (Le and Mikolov, 2014) 
74.8 
48.7 
87.8 
90.5 
91.8 
78.1 
74.2 
NBSVM (Wang and Manning, 2012) 
79.4 
-
-
93.2 
-
81.8 
86.3 
Tree LSTM (Tai et al., 2015) 
-
51.0 
88.0 
-
-
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons of state-of-the-arts. 

</table></figure>

			<note place="foot" n="1"> The embedding layer in CNN-non-static is initialized with pre-trained vectors from word2vec toolkit and fine-tuned for each task. 2 https://code.google.com/p/word2vec/ 3 (MR (Pang and Lee, 2005), SST-1/2 (Socher et al., 2013), Subj (Pang and Lee, 2004), TREC (Li and Roth, 2002), CR (Hu and Liu, 2004), and MPQA (Wiebe et al., 2005))</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning deep architectures for ai. Foundations and trends® in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2015</title>
		<meeting>NIPS 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks. In Aistats</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD</title>
		<meeting>ACM SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Harnessing deep neural networks with logic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2015</title>
		<meeting>NIPS 2015</meeting>
		<imprint>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>Hinton</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2014</title>
		<meeting>ICML 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted neural bag-of-n-grams model: New baselines for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016</title>
		<meeting>COLING 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1591" to="1600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
		<meeting>ACL 2002</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Delta TFIDF: an improved feature space for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">All you need is a good init</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<idno>CoRR abs/1511.06422</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2004. Association for Computational Linguistics</title>
		<meeting>ACL 2004. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2005. Association for Computational Linguistics</title>
		<meeting>ACL 2005. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Linguistically regularized lstms for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03949</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>CoRR abs/1312.6120</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2013</title>
		<meeting>EMNLP 2013</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2012</title>
		<meeting>ACL 2012</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Annotating expressions of opinions and emotions in language. Language resources and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="165" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multichannel variable-size convolution for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno>CoRR abs/1603.04513</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dependency sensitive convolutional neural networks for modeling sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2016</title>
		<meeting>NAACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1512" to="1521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>CoRR abs/1603.00968</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>CoRR abs/1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-adaptive hierarchical sentence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2015</title>
		<meeting>IJCAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4069" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
