<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhuge</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Aston University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Guangzhou University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Key Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">ICT</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<title level="j" type="main">Association for Computational Linguistics</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">4046</biblScope>
							<biblScope unit="page" from="4046" to="4056"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Rapid growth of multi-modal documents on the Internet makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in text summarization. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images. In addition, our model can generate informative summaries of images.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Summarizing multi-modal documents to get multi-modal summaries is becoming an urgent need with rapid growth of multi-modal documents on the Internet. Text-Image summarization is to summarize a document with text and images to generate a summary with text and images. The summarization approach is different from pure text summarization. It is also different from image summarization which summarizes an image set to get a subset of images.</p><p>An image worths thousands of words <ref type="bibr" target="#b20">(Rossiter, et al., 2012)</ref>. Image plays an important role in information transmission. Incorporating images into text to generate text-image</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN</head><p>summaries can help people better understand, memorize, and express information. Most of recent research focuses on pure text summarization, or image summarization. Little has been done on text-image summarization. <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 2</ref> show an example of text- image summarization. <ref type="figure">Figure 1</ref> is the original multi-modal news with text and images. The news has 17 sentences (with 322 words) and 4 images each of which has a caption. <ref type="figure">Figure 2</ref> is the manually generated multi-modal summary. In the summary, the news is distilled to 3 sentences (with 36 words) and 2 images, and each summary sentence is aligned with an image.</p><p>To generate such a text-image summary, the following problems should be considered: How to generate the text part? How to measure the importance of images, and extract important images to form the image summary? How to align sentences with images?</p><p>In this paper, we propose a neural text-image summarization model based on the attentional hierarchical Encoder-Decoder model to solve the above problems. The attentional Encoder- Decoder model has been successfully used in sequence-to-sequence applications such as machine translation ), text summarization ( <ref type="bibr" target="#b3">Cheng and Lapata, 2016;</ref><ref type="bibr" target="#b24">Tan et al., 2017)</ref>, image captioning ( <ref type="bibr" target="#b15">Liu et al., 2017a)</ref>, and machine reading comprehension ( <ref type="bibr" target="#b5">Cui et al., 2016)</ref>.</p><p>At the encoding stage, we use the hierarchical bi-directional RNN to encode the sentences and the text document, use the RNN and the CNN to encode the image set. In the decoding stage, we combine text encoding and image encoding as the initial state, and use the attentional hierarchical decoder which attends original sentences, images and captions to generate the text summary. Each generated sentence is aligned with a sentence, an image, or a caption in the original document. Based on the alignment scores, images are selected and aligned with the generated sentences. In the inference stage, we adopt the multi-modal beam search algorithm which scores beams based on bigram overlaps of the generated sentences and the attended captions.</p><p>The main contributions are as follows: 1) We propose the text-image summarization task, and extend the standard DailyMail corpora by collecting images and captions of each news from the Web for the task.</p><p>2) We propose an RNN model to encode the ordered image set of the multi-model document as one of the initial states (the other is the text encoding) of the decoder. 3) We propose three multi-modal attentional mechanisms which attend the text and the images simultaneously when decoding. 4) Experiments show that attending images when decoding can improve text summarization, and that our model can generate informative image summaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent research on text summarization focuses on neural methods. Attentional Encoder-Decoder model is first proposed in ( <ref type="bibr" target="#b1">Bahdanau et al., 2014)</ref> and <ref type="bibr">(Luond et al., 2015)</ref> to align the original text and the translated text in machine translation. The attention model is applied to sentence summarization by considering the neural language model and the attention model when generating next words ( <ref type="bibr" target="#b21">Rush et al., 2015)</ref>. A selective Encoder-Decoder model that uses a selective gate network to control information from the encoder to the decoder for sentence summarization is proposed ( <ref type="bibr" target="#b32">Zhou et al., 2017)</ref>.</p><p>A neural document summarization model by extracting sentences and words is proposed <ref type="bibr" target="#b3">(Cheng and Lapata, 2016)</ref>. They use a CNN model to encode sentences, and then use a RNN model to encode documents. The model extracts sentences by computing the probability of sentences belonging to the summary based on an RNN model. The model extracts words from the original document based on an attentional decoder.</p><p>An RNN-based extractive summarization named SummaRuNNer, treating summarization as a sentence classification problem is proposed ( <ref type="bibr" target="#b19">Nallapati et al., 2016)</ref>. A logistic classifier is then applied using features computed based on the RNN model. A hierarchical Encoder-Decoder model, conserving the hierarchical structure of documents is proposed ( <ref type="bibr" target="#b13">Li et al., 2015)</ref>. A graph-based attentional Encoder-Decoder model using a PageRank algorithm to compute the attention is proposed ( <ref type="bibr" target="#b24">Tan et al., 2017)</ref>.</p><p>Image captioning generates a caption for an image. Text-image summarization is similar to image captioning in that both utilize image information to generate text. Images are encoded with CNN models such as VGGNet <ref type="bibr" target="#b22">(Simonyan and Zisserman, 2014</ref>), AlexNet ( <ref type="bibr">Krizhevsky et al., 2012</ref>) and GoogleNet ( <ref type="bibr" target="#b23">Szegedy et al., 2014</ref>) by extracting the last full-connected layers. An attentional model is used in image captioning by splitting an image into multiple parts which is attended in the decoding process ( <ref type="bibr" target="#b29">Xu et al., 2015)</ref>. Image tags was used as additional information, and semantic attention model which attends image tags when decoding was proposed ( <ref type="bibr" target="#b31">You et al., 2016</ref>). The attention-based alignment of image parts and text is studied ( <ref type="bibr" target="#b15">Liu et al., 2017a)</ref>, and the results show that the alignments is in high accordance with manual alignments. An image to an ordered recognized object set is encoded, and the attentional decoder is applied to generate captions ( <ref type="bibr" target="#b16">Liu et al., 2017b</ref>).</p><p>Multi-modal summarization summarizes text, images, videos, and etc. It is an important branch of automatic summarization. Traditional multi- modal summarization inputs multi-modal documents or pure text documents, and outputs multi-modal documents <ref type="bibr" target="#b27">(Wu, 2011;</ref><ref type="bibr" target="#b8">Greenbacker, 2011;</ref><ref type="bibr" target="#b30">Yan, 2012;</ref><ref type="bibr" target="#b0">Agrawal, 2011;</ref><ref type="bibr" target="#b33">Zhu, 2007;</ref><ref type="bibr" target="#b25">UzZaman, 2011</ref>). For example, <ref type="bibr" target="#b30">Yan et al., (2012)</ref> generate multi-modal timeline summaries for news sets by constructing a bi-graph between text and images, and apply a heterogeneous reinforcement ranking algorithm. Strategies to summarizing texts with images and the notion of summarization of things are proposed in <ref type="bibr" target="#b34">(Zhuge, 2016)</ref>. The deep learning related work ( ) treats text summarization as a sentence recommendation task and applies matrix factorization algorithm. They first retrieve images from Yahoo!, use the CNN to extract image features as the additional information of sentences, use Rouge maximization as the training object function which are trained with SGD. In test time, sentences are extracted based on the model and images are retrieved from the Search Engine. <ref type="figure" target="#fig_0">Figure 3</ref> shows the framework, a multi-modal attentional hierarchical encoder-decoder model. The hierarchical encoder-decoder is proposed in ( <ref type="bibr" target="#b13">Li et al., 2015</ref>) and extended by <ref type="bibr" target="#b24">(Tan et al. , 2017</ref>) for document summarization through bringing in the graph-based attentional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our model consists of three parts: a hierarchical RNN to encode the original sentences and the captions, a CNN+RNN encoder to encode the image set, and a multi-modal attentional hierarchical RNN decoder.</p><p>The input of our model is a multi-modal document MD = {D, PicSet}, where D is the main text of the multi-modal document and PicSet is the image-caption set ordered by the occurring order of images in the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Text Encoder</head><p>The </p><formula xml:id="formula_0">s i j i j i j h GRU h x      (1) , , , 1 ( , ) s i j i j i j h GRU h x      (2) ,1 , 1 [ , ] sent i i i enc h h     (3)</formula><p>where enc sent i denotes the vector representation of s i . It is the concatenation of ,1</p><formula xml:id="formula_1">i h  and , 1 i h   .</formula><p>We use enc sent i as inputs to the document encoder to encode the main text to vector representations. A bi-directional RNN is adopted as the document encoder:</p><formula xml:id="formula_2">1 ( , ) d sent i i i h GRU h enc      (4) 1 ( , ) d sent i i i h GRU h enc      (5) [ , ] i i i h h h    (6) 1 1 [ , ] doc d enc h h    (7)</formula><p>where enc doc denotes the vector representation of the D, and h i is the concatenated hidden state of s i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CaptionSet and ImageSet Encoder</head><p>The ordered image-caption set PicSet consists of an ordered image set and an ordered caption set which are ordered by the occurring order in the multi-modal document. The image occurring order makes sense because images are often put near the most related sentences, and the sentences have strict order in the document. We treat the ordered caption set as a document, and apply the sentence encoder and the document encoder to the caption document. Then, we get the hidden state h cap i and the vector representation enc cap of the caption document.</p><p>We use the CNN model to extract the vector representation of each image, and then use the RNN model to encode the ordered image set to vector representation. The CNN model we adopted is 19-layer VGGNet <ref type="bibr" target="#b22">(Simonyan and Zisserman, 2014</ref>). We drop the last dropout layer and keep the last full-connected layer as the image's vector representation, the dimension of which is 4096.</p><p>We then use a bi-directional RNN model to encode the ordered image set and the image features are used as inputs of the RNN model.</p><formula xml:id="formula_3">1 ( , ) img img img fea i i i h GRU h img      (8) 1 ( , ) img img img fea i i i h GRU h img      (9) [ , ] img img img i i i h h h    (10) 1 1 [ , ] img img img enc h h     (11)</formula><p>where img fea i is the vector representation of img i , enc img is the vector representation of the image set, and h img i is the hidden state of img i when encoding the image set.</p><p>To our best knowledge, we are the first to adopt the RNN model to encode the image set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>In the decoding state, we adopt the hierarchical RNN decoder to generate text summaries.</p><formula xml:id="formula_4">_ 0 _ _ tanh( ) dec doc doc dec img img dec cap cap h W enc V enc V enc      <label>(12)</label></formula><formula xml:id="formula_5"> _ 1 1 1, 1 ( , ) dec sent i i i h GRU h h     (13)  _ 2 ( , ) dec sent i i i h GRU h c  (14) _ , , 1 , 1 ( , ) dec word i j i j i j h GRU h y   <label>(15)</label></formula><p>max , , max( )   <ref type="formula" target="#formula_4">(12)</ref> to <ref type="formula">(16)</ref> are the equations of the hierarchical decoder which consists of a sentence decoder and a word decoder.</p><formula xml:id="formula_6">soft i j i j y soft W h b   (16)</formula><p>Equation <ref type="formula" target="#formula_4">(12)</ref> computes the initial state 0 h for the sentence decoder by combining the decoding of the main text information and the decoding of the image information of the multi-modal document. To represent image information, we can use both of the image set decoding and the caption set decoding, or only use one of them, depending on the multi-modal attention mechanism introduced in the next subsection.</p><p>The sentence decoder uses a two-level hidden output model ( <ref type="bibr" target="#b17">Luong et al., 2015)</ref> to generate the representation of the next sentence through equation <ref type="formula">(13)</ref> and equation <ref type="formula">(14)</ref>. The two-level hidden output model consistently improves the summarization performance on different datasets . In equation <ref type="formula">(14)</ref> The word decoder uses the sentence representation generated by the sentence decoder as the initial state, and use the &lt;sos&gt; (start of sentence) token as the initial input. Equation (15) and equation <ref type="formula">(16)</ref> generate the next hidden state and the next word. The output of the word decoder in the first step is a switch sign which is either &lt;neod&gt; token or &lt;eod&gt; token. The token &lt;neod&gt; means "not end of document", and the token &lt;eod&gt; means "end of document". If the first output is &lt;eod&gt;, the whole decoding process is finished. If the first output is &lt;neod&gt;, the token is used as the next input of the word decoder. The word decoding process is finished when it generates the &lt;eos&gt; token. The last hidden state of the word decoder is treated as the vector representation of the generated sentence and is used as next input of the sentence decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-Modal Attention</head><p>We propose three multi-modal attention mechanisms to compute the sentence decoding context i c .</p><p>Traditional attention mechanisms for text summarization computes the importance score of the sentence s j in the original document based on the relationship between the decoding hidden state  i h and the original sentence encoding hidden state j h . We call the traditional attention model as Text Attention (attT for short), which is computed by equation <ref type="formula" target="#formula_7">(17)</ref>, <ref type="formula">(18)</ref> and <ref type="formula">(19)</ref>:</p><formula xml:id="formula_7">  ( , ) tanh( ) T T T T i i j j att h h v W h U h  <label>(17)</label></formula><formula xml:id="formula_8">   | | 1 exp( ( , )) ( , ) exp( ( , )) T i j T i j D T i j j att h h h h att h h     (18)   | | 1 ( ) ( , ) D T T i i j j j c h h h h     (19)</formula><p>where</p><formula xml:id="formula_9"> ( , ) T i j att h h is the attention (Banahama et al., 2014),  ( , ) T i j h h </formula><p>is the normalized attention,</p><formula xml:id="formula_10">and  ( ) T i</formula><p>c h is the context. The problem is that the multi-modal document has images and captions besides the main text. Therefore, we propose three multi-modal attention mechanisms which take images and captions into consideration.</p><p>Text-Caption Attention (attTC for short). This attention model uses captions to represent the image information. attTC computes the attention score of the caption cap j based on the relationship between the caption encoding hidden state h cap i and the decoding hidden state  j h . </p><formula xml:id="formula_11">    | | | | 1 1 exp( ( , )) ( , ) exp(<label>( , )) exp( ( , ))</label></formula><formula xml:id="formula_12">TC i j TC i j D PicSet TC TC cap i i j j j j att h h h h att h h att h h        (20)     | | | | 1 1 exp(<label>( , )) ( , ) exp( ( , )) exp( ( , ))</label></formula><formula xml:id="formula_13">TC cap i j TC cap i j D PicSet TC TC cap i i j j j j att h h h h att h h att h h       <label>(21)</label></formula><formula xml:id="formula_14">   | | | | 1 1 ( ) ( , )<label>( , ) D</label></formula><formula xml:id="formula_15"> j h .     | | | | 1 1 exp(<label>( , )) ( , ) exp( ( , )) exp( ( , ))</label></formula><formula xml:id="formula_16">TI img i j TI img i j D PicSet TI TI img i i j j j j att h h h h att h h att h h       <label>(23)</label></formula><formula xml:id="formula_17">   | | | | 1 1 ( ) ( , )<label>( , )</label></formula><formula xml:id="formula_18">D PicSet TI TI TI img img i i i j j j j j j c h h h h h h h        <label>(24)</label></formula><p>Text-Image-Caption Attention (attTIC for short). This attention model uses both captions and images to represent the image information. attTIC computes the importance score of the caption cap j and the importance score of the image img j simultaneously, and then compute the context of the decoding hidden state  i h using equation <ref type="formula">(25)</ref>. The initial state of the decoder is computed by Equation (12) which can be adjusted according to different attention models.</p><formula xml:id="formula_19">    | | 1 | | 1 ( ) ( , )<label>( , ) ( , )</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Training</head><p>Since there are no existing manual text-image summaries, and most of the existing training and testing data have pure text summaries, we decide to use pure text summaries as training data to train our models. The sentence-image alignment relationships can be discovered through training the multi-modal attention models.</p><p>The loss function L of our summarization model is the negative log likelihood of generating text summaries over the training multi-modal document set MDS. <ref type="bibr">( , , )</ref> log ( | , )</p><formula xml:id="formula_20">D PicSet Y MDS L P Y D PicSet    <label>(26)</label></formula><p>where <ref type="formula">Y=</ref>  <ref type="bibr" target="#b11">Kingma and Ba, 2014</ref>) gradient-based optimization method to optimize the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Multi-Modal Beam Search Algorithm</head><p>There are two major problems of the generation of summaries: one is the out-of-vocabulary problem, and the other is the low quality of the generated texts including information incorrectness and repetitions.</p><p>For the OOV problem, we use the words in the attended sentences or captions in the original document to replace OOV tokens in the generated summary. Previous research uses the attended words to replace OOV tokens in the flatten encoder-decoder model which attends the words of the original word sequence <ref type="bibr">(Jean, et al., 2015)</ref>. Our model is hierarchical and multi-modal, and attends sentences, images, and captions when decoding. We use the following algorithm to find the replacement for the j th OOV in a generated sentence:</p><p>Step 1: Order the original sentences and captions by the attending scores in descending order.</p><p>Step 2: Return the j th OOV word in the ordered sentences and captions as the replacement.</p><p>For the attTI mechanism that attends images neglecting captions, we use captions instead of the attended images in the algorithm.</p><p>For the low-quality generated text problem, we adopt the hierarchical beam search algorithm <ref type="bibr">(Tan el al., 2017)</ref>. We extend the algorithm by adding caption-level and image-level beam search. The multi-modal hierarchical beam search algorithm comprises K-best word-level beam search and N- best sentence-caption-level beam search. In particular, we use the corresponding captions instead of images in beam search algorithm for the attTI mechanism which attends images. </p><formula xml:id="formula_21">1 * 1 * ( ) ( ) ( ( , )<label>( , )</label></formula><p>At the word-level search algorithm, we compute the score of generating word y t using equation <ref type="formula" target="#formula_22">(28)</ref> where ref is a function calculating the ratio of bigram overlap between two texts, s * is the attended sentence or caption, and γ is the weighting factor. The added term aims to increase the overlap of the generated summary and the original text.</p><p>At the sentence level and the caption level, we set the sentence beam width as N, and keep N-best previously un-referred sentences or captions which have highest attending scores. For each sentence beam, we try M sentences or captions and keep the one achieving best word-level scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Image Selection and Alignment</head><p>We rank the images, select several most important images as the image summary, and align each sentence with an image in the image summary. The score of images is computed by equation <ref type="bibr">(29)</ref>. where α i,j is the attention score of the j th image when generating the i th sentence of the text summary, and |TextSum| is the number of summary sentences. The images are ranked by the scores in descending order, and the top K images are selected to form the image summary ImgSum. We align each sentence i in TextSum to the image j in ImgSum such that α i,j is the biggest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data preparation</head><p>We extend the standard DailyMail corpora through extracting the images and the captions from the html-formatted documents. We call the corpora as E-DailyMail. The standard DailyMail and CNN datasets are two widely used datasets for neural document summarization, which are originally built in ( <ref type="bibr" target="#b9">Hermann et al., 2015</ref>) by collecting human generated highlights and news stories from the news websites. We only extend the DailyMail dataset because it has more images and is easier to collect than the CNN dataset does. We find that the text documents provided by the original DailyMail corpora contain captions. This is due to that all related texts are extracted from the html-formatted news when the corpora are created. We keep the original text documents unchanged in E-DailyMail. The split and statistics of E-DailyMail are shown in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>We preprocess the text of the E-DailyMail corpora by tokenizing the text and replacing the digits with the &lt;NUM&gt; token. The 40k most frequent words in the corpora are kept and other words are replaced with OOV.</p><p>Our model is implemented by using Google's open-source seq2seq-master project written with Tensorflow. We use one layer of the GRU cell. The dimension of the hidden state of the RNN decoder is 512. The dimension of the word embedding vector is 128. The dimension of the hidden state of the bi-directional RNN encoder is 256. We initialize the word embeddings with Google's word2vec tools ( <ref type="bibr" target="#b18">Mikolov et al., 2013</ref>) trained in the whole text of DailyMail/CNN corpora. We extract the 4096-dimension full-connected layer of 19-layer VGGNet ( <ref type="bibr" target="#b22">Simonyan and Zisserman, 2014</ref>) as the vector representation of images. We set the parameters of Adam to those provided in <ref type="bibr" target="#b11">(Kingma and Ba, 2014</ref>). The batch size is set to 5. Convergence is reached within 800k training steps. It takes about one day for training 40k ~ 50k steps depending on the models on a GTX-1080 TI GPU card. The sentence beam width and the word beam width are set as 2 and 5 respectively. M is set as 3. The parameter γ is set as 3 or 300 tuned on the validation set.</p><p>To train the multi-modal attention mechanism such as attTIC, we concatenate the matrix of text representations, image representations, and caption representations to one matrix</p><formula xml:id="formula_23">M = [h 1 , h 2 , ... h |D| , h cap 1 , h cap 2 , …, h cap |PicSet| , h img 1 , h img 2 , …, h img |PicSet| ]</formula><p>. The parameters of the attention mechanisms are trained simultaneously. This way the model training can converge faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation of Text Summarization</head><p>The widely used ROUGE <ref type="bibr" target="#b14">(Lin, 2004</ref>) is adopted to evaluate text summaries.</p><p>We compare four attention models. HNNattTC- 3, HNNattTIC-3, HNNattTI-3, and HNNattT-3 are our hierarchical RNN summarization models with the attTC, attIC, attTI, and attT attention mechanisms respectively, and 3 is the γ value.</p><p>HNNattT is similar to the model introduced in <ref type="bibr" target="#b24">(Tan et al., 2017</ref>) without the graph-based attention. We compare our models with HNNattT to show the influence of multi-modal attentions. The first 4 lines in <ref type="table" target="#tab_12">Table 2</ref>  reasons is that the text documents provided by the DailyMail corpora contain captions. Captions are already parts of the text documents. The other reason is that captions distract attentions and cannot attract sufficient attentions from the original sentences, which will be discussed in the next subsection.</p><p>We compare our methods with state-of-the-art neural summarization methods reported in recent papers on the DailyMail corpora. Extractive models include Lead which is a strong baseline using the leading 3 sentences as the summary, NN-SE ( <ref type="bibr" target="#b3">Cheng and</ref><ref type="bibr" target="#b3">Lapata, 2016), and</ref><ref type="bibr">SummaRuNNer-abs (Nallapati et al., 2017)</ref> which is trained on the abstractive summaries. Abstractive models include NN-ABS, NN-WE, LREG, though they are tested on 500 samples of the test set. LREG is a feature-based method using linear regression. NN-ABS is a simple hierarchical extension of ( <ref type="bibr" target="#b21">Rush et al., 2015)</ref>. NN- WE is the abstractive model restricting the generation of words from the original document. The results are shown in the last 6 rows in <ref type="table" target="#tab_12">Table 2</ref>. Our method HNNattTI outperforms the three extractive models and the three abstractive models.</p><p>We compare our models under the full-length F1 metric by setting the γ value as 300. According to <ref type="bibr" target="#b24">(Tan et al., 2017)</ref>, a large γ makes the generated summary has more overlaps with the attended texts, and thus partly overcome the repeated sentences problem in the generated summary. We do not incorporate the attention distraction mechanism  into our model, because we want to focus on our own model to see whether considering images improves text summarization. Results in <ref type="table" target="#tab_11">Table 3</ref> also show that HNNattTI performs better than HNNattT, HNNattTC, and HNNattTIC.</p><p>To show the influence of our OOV replacement mechanism, we eliminate the mechanism from our models, and show the evaluation results in <ref type="table" target="#tab_9">Table 4</ref> and <ref type="table" target="#tab_10">Table 5</ref>. We can see from the two tables that the scores are lower than the corresponding scores in <ref type="table" target="#tab_11">Table 2 and Table 3</ref>. Our OOV replacement mechanism improves the summarization models, though the mechanism is relatively simple.</p><p>In short, combining and attending images in the neural summarization model improves document summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Image Summarization</head><p>To evaluation the image summarization, the gold standard image summary is generated based on a greedy algorithm on the captions as follows: at each time i, choose img k to maximize      cap k is the caption of img k . The average number of images in summaries is 2.15. The average Rouge-1, Rouge-2, and Rouge-L scores of the caption summaries with respect to the ground truth summaries are <ref type="bibr">43.85, 19.70, and 36.30</ref> respectively.</p><formula xml:id="formula_24">Rouge({cap 1 ,…cap i-1 ,cap k }, Abs_Sum) ˗ Rouge({cap 1 ,…cap i-1 }, Abs_Sum))</formula><p>We use the 1-image and 2-image random selected image summaries as the baselines which we compare our models with. The top 1 or 2 images ranked by our model are selected out to form the summaries. Results in <ref type="table" target="#tab_9">Table 4</ref> show that HNNattTI outperforms the random baseline, while HNNattTC and HNNattTIC perform worse. This implies that attending images can generate better sentence-image alignment in the multi- modal summaries than the model attending captions does. And this can also partly explain why our summarization model attending images when decoding can generate better text summaries than the one attending captions does. <ref type="figure" target="#fig_3">Figure 4</ref> shows the text-image summary of the example demonstrated in <ref type="figure">Figure 1</ref> generated by the HNNattTI model. In the summary, there are 2 images and 3 generated sentences, and each sentence is aligned with an image. The image summary has one common image with <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Instance</head><p>The sentences are named by S1, S2, and S3 respectively. <ref type="table">Table 7</ref> shows the sentence-image alignment scores. The four images in the original document are numbered from top to bottom and left to right by IMG1, IMG2, IMG3, and IMG4. The summation of alignment scores for a summary sentence is less than 1, because the sentence is also aligned with the sentences in the original document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>This paper proposes the text-image summarization task to summarize and align texts and images simultaneously. Most previous research summarizes texts and images separately, and few has been done on text-image summarization. We propose the multi-modal attentional mechanism which attends original sentences, images, captions simultaneously in the hierarchical encoder-decoder model, use the RNN model to encode the ordered image set as the initial state of the decoder, and propose the multi- modal beam search algorithm which scores beams using the bigram overlaps of the generated sentences and the captions. The model is trained by using abstractive text summaries as the targets, and the attention scores of images are used to score images. The original DailyMail dataset is extended by collecting images and captions from the Web. Experiments show that our model attending images outperforms the models not attending images, three existing neural abstractive models and three existing extractive models. Experiments also show our model can generate informative summaries of images.  <ref type="table" target="#tab_9">IMG1  IMG2  IMG3  IMG4  S1</ref> 0.0947 0.1089 0.1157 0.1194 S2 0.0893 0.1020 0.1070 0.1052 S3 0.0853 0.0769 0.0946 0.0969 <ref type="table">Table 7</ref>: The sentence-image alignment scores of the generated summary for the news in <ref type="figure">Figure 1.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework of our neural text-image summarization model.</figDesc><graphic url="image-3.png" coords="4,111.60,72.96,370.80,288.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Equation</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The generated text-image summary of the example in Figure 1.</figDesc><graphic url="image-4.png" coords="9,74.40,448.08,211.44,278.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>main text D consists of sentences, each of which consists of words. Let D=[s 1 , s 2 , …, s |d| ], andis the word embedding of the j th word in the s i .</head><label></label><figDesc></figDesc><table>,1 
,2 
, 

[ , , , 
] 

i 

i 
i 
i 
i s 

s x x 
x 
 
 
where x i,j We use 
word2vec (Mikolov et al., 2013) to create word 
embeddings. GRU is used as the RNN cell (Cho 
et al., 2014). 
We use a hierarchical RNN encoder to encode 
the main text D to vector representation. The 
sentence encoder is adopted to encode sentences 
to vector representations. An &lt;eos&gt; token is 
appended to the end of each sentence. A bi-
directional RNN is used as the sentence encoder: 

, 
, 1 
, 

( 
, ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>[y 1 , y 2 , …, y |Y| ] is the word sequences of the summary corresponding to the main text D and the ordered image set PicSet, including</head><label></label><figDesc></figDesc><table>the tokens 
&lt;eos&gt;, &lt;neod&gt; and &lt;eod&gt;. 

| | 

1 
1 
1 

log ( | , 
) 
log ( |{ ,..., }, ; ) 

Y 

t 
t 
t 

P Y D PicSet 
P y y 
y 
c  

 
 

  

(27) 

where 

1 
1 

log ( | { ,..., }, ; ) 

t 
t 

P y y 
y 
c  

 

is modeled by 

the multi-modal encoder-decoder model. 

We use the Adam (</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>The results show that HNNattTI has considerable improvement over HNNattT. An interesting observation is that HNNattTC and HNNattTIC are not better than HNNattT. One of the</head><label></label><figDesc>are the results with summary length of 75 bytes.</figDesc><table>Train 
Dev 
Test 
196557 
12147 
10396 
D.L. 
S.L. 
I.N 
Sent.L Cap.L 
26.0 
3.84 
5.42 
26.86 
24.75 

Table 1: The split and statistics of the E-DailyMail 
corpora. D.L and S.L indicate the average number of 
sentences in the document and summary. I.N 
indicates the average number of images in the story. 
Sent.L and Cap.L indicates the average number of 
word in the sentence and the caption respectively. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>where Abs_Sum is the ground truth text summary and</head><label></label><figDesc></figDesc><table>num HNNattTI HNNattTC HNNattTIC Random 
1 
0.4978 
0. 4137 
0. 4362 
0. 4721 
2 
0.4783 
0. 3998 
0. 4230 
0. 4517 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Image summarization using the recall metric 
for the 1-image or 2-images summary. γ is set as 300. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison results using Rouge recall at 75 
bytes without OOV replacement. HNNattTI-3-OOV 
is the version of HNNattTI-3 without the OOV 
replacement mechanism. 

Method 
Rouge-1 Rouge-2 Rouge-L 
HNNattTI-300-OOV 
32.03 
11.52 
22.67 
HNNattTC-300-OOV 
26.13 
9.87 
19.03 
HNNattTIC-300-OOV 
30.11 
10.87 
21.12 
HNNattT-300-OOV 
30.74 
11.21 
22.28 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison results using full-length F1 
metric without OOV replacement. HNNattTI-300-
OOV is the version of HNNattTI-300 without the 
OOV replacement mechanism. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison results on the DailyMail test 
set using full-length F1metric. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison results on the DailyMail test 
set using Rouge recall at 75 bytes. </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching textbooks with images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Information and knowledge management</title>
		<meeting>the 20th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1847" to="1856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distraction-based neural networks for modeling documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016-07" />
			<biblScope unit="page" from="2754" to="2760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural summarization by extracting sentences and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07252</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Encoder-decoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Attention-over-attention neural networks for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating Copying Mechanism in Sequenceto-Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards a framework for abstractive summarization of multimodal documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Greenbacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
		<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2007</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the Acm</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2012</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Attention Correctness in Neural Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="4176" to="4182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MAT: A multimodal attentive translator for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05658</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is a Picture Worth a Thousand Words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rossiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Derwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tesol Quarterly</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="325" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Neural Attention Model for Abstractive Sentence Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abstractive document summarization with a graph-based attentional neural model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal summarization of complex sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on Intelligent user interfaces</title>
		<meeting>the 16th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A LowRank Approximation Approach to Learning Joint Embeddings of News Stories and Images for Timeline Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Toward extractive summarization of multimodal documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carberry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<title level="m">Proceedings of the Workshop on Text Summarization at the Canadian Conference on Artificial Intelligence</title>
		<meeting>the Workshop on Text Summarization at the Canadian Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="page" from="2048" to="2057" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams. CIKM2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image Captioning with Semantic Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<title level="m">Selective Encoding for Abstractive Sentence Summarization. Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1095" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A text-topicture synthesis system for augmenting communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1590" to="1595" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Multi-Dimensional Summarization in Cyber-Physical Society</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhuge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
