<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Who did What: A Large-Scale Person-Centered Cloze Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Onishi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Toyota Technological Institute at Chicago</orgName>
								<address>
									<postCode>60637</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Who did What: A Large-Scale Person-Centered Cloze Dataset</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2230" to="2235"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We have constructed a new &quot;Who-did-What&quot; dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gi-gaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles-an article given as the passage to be read and a separate article on the same events used to form the question. Second , we avoid anonymization-each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84% solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Researchers distinguish the problem of general knowledge question answering from that of read- ing comprehension ( <ref type="bibr" target="#b5">Hermann et al., 2015;</ref><ref type="bibr" target="#b6">Hill et al., 2016)</ref>. Reading comprehension is more diffi- cult than knowledge-based or IR-based question an- swering in two ways. First, reading comprehen- sion systems must infer answers from a given un- structured passage rather than structured knowledge sources such as Freebase ( <ref type="bibr" target="#b0">Bollacker et al., 2008)</ref> or the Google Knowledge Graph <ref type="bibr">(Singhal, 2012)</ref>. Second, machine comprehension systems cannot ex- ploit the large level of redundancy present on the web to find statements that provide a strong syntac- tic match to the question ( <ref type="bibr" target="#b8">Yang et al., 2015</ref>). In con- trast, a machine comprehension system must use the single phrasing in the given passage, which may be a poor syntactic match to the question.</p><p>In this paper, we describe the construction of a new reading comprehension dataset that we refer to as "Who-did-What". Two typical examples are shown in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="bibr">2</ref> The process of forming a prob- lem starts with the selection of a question article from the English Gigaword corpus. The question is formed by deleting a person named entity from the first sentence of the question article. An information retrieval system is then used to select a passage with high overlap with the first sentence of the question article, and an answer choice list is generated from the person named entities in the passage.</p><p>Our dataset differs from the CNN and Daily Mail comprehension tasks ( <ref type="bibr" target="#b5">Hermann et al., 2015</ref>) in that it forms questions from two distinct articles rather than summary points. This allows problems to be derived from document collections that do not con- tain manually-written summaries. This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.</p><p>To make the dataset more challenging we selec- tively remove problems so as to suppress four simple Passage: Britain's decision on Thursday to drop extradition proceedings against Gen. Augusto Pinochet and allow him to return to Chile is understandably frustrating ... Jack Straw, the home secretary, said the 84-year-old former dictator's ability to understand the charges against him and to direct his defense had been seriously impaired by a series of strokes. ... Chile's president-elect, Ricardo Lagos, has wisely pledged to let justice run its course. But the outgoing government of President Eduardo Frei is pushing a constitutional reform that would allow Pinochet to step down from the Senate and retain parliamentary immunity from prosecution. ...</p><p>Question: Sources close to the presidential palace said that Fujimori declined at the last moment to leave the country and instead he will send a high level delegation to the ceremony, at which Chilean President Eduardo Frei will pass the mandate to XXX.  baselines -selecting the most mentioned person, the first mentioned person, and two language model baselines. This is also intended to produce problems requiring deeper semantic analysis.</p><p>The resulting dataset yields a larger gap between human and machine performance than existing ones. Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN ( <ref type="bibr" target="#b1">Chen et al., 2016</ref>) and 82% for the CBT named entities task ( <ref type="bibr" target="#b6">Hill et al., 2016)</ref>. In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset. For ex- ample, the Attentive Reader ( <ref type="bibr" target="#b5">Hermann et al., 2015</ref>) achieves 63% on CNN but only 55% on Who-did- What and the Attention Sum Reader ( <ref type="bibr" target="#b1">Kadlec et al., 2016</ref>) achieves 70% on CNN but only 59% on Who- did-What.</p><p>In summary, we believe that our Who-did-What dataset is more challenging, and requires deeper se- mantic analysis, than existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our Who-did-What dataset is related to several re- cently developed datasets for machine comprehen- sion. The MCTest dataset ( <ref type="bibr">Richardson et al., 2013)</ref> consists of 660 fictional stories with 4 multiple choice questions each. This dataset is too small to train systems for the general problem of reading comprehension.</p><p>The bAbI synthetic question answering dataset ( <ref type="bibr" target="#b6">Weston et al., 2016</ref>) contains passages describing a series of actions in a simulation followed by a ques- tion. For this synthetic data a logical algorithm can be written to solve the problems exactly (and, in fact, is used to generate ground truth answers).</p><p>The Children's Book Test (CBT) dataset, created by <ref type="bibr" target="#b6">Hill et al. (2016)</ref>, contains 113,719 cloze-style named entity problems. Each problem consists of 20 consecutive sentences from a children's story, a 21st sentence in which a word has been deleted, and a list of ten choices for the deleted word. The CBT dataset tests story completion rather than reading compre- hension. The next event in a story is often not de- termined -surprises arise. This may explain why human performance is lower for CBT than for our dataset -82% for CBT vs. 84% for Who-did-What. The 16% error rate for humans on Who-did-What seems to be largely due to noise in problem forma- tion introduced by errors in named entity recogni- tion and parsing. Reducing this noise in future ver- sions of the dataset should significantly improve hu- man performance. Another difference compared to CBT is that Who-did-What has shorter choice lists on average. Random guessing achieves only 10% on CBT but 32% on Who-did-What. The reduction in the number of choices seems likely to be responsi- ble for the higher performance of an LSTM system on Who-did-What -contextual LSTMs (the atten- tive reader of <ref type="bibr" target="#b5">Hermann et al., 2015</ref>) improve from 44% on CBT (as reported by <ref type="bibr" target="#b6">Hill et al., 2016</ref>) to 55% on Who-did-What.</p><p>Above we referenced the comprehension datasets created from CNN and Daily Mail articles by <ref type="bibr" target="#b5">Hermann et al. (2015)</ref>. The CNN and Daily Mail datasets together consist of 1.4 million questions constructed from approximately 300,000 articles. Of existing datasets, these are the most similar to Who-did-What in that they consist of cloze-style question answering problems derived from news ar- ticles. As discussed in Section 1, our Who-did-What dataset differs from these datasets in not being de- rived from article summaries, in using baseline sup- pression, and in yielding a larger gap between ma- chine and human performance. The Who-did-What dataset also differs in that the person named entities are not anonymized, permitting the use of external resources to improve performance while remaining difficult for language models due to suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Construction</head><p>We now describe the construction of our Who-did- What dataset in more detail. To generate a problem we first generate the question by selecting a random article -the "question article" -from the Giga- word corpus and taking the first sentence of that ar- ticle -the "question sentence" -as the source of the cloze question. The hope is that the first sentence of an article contains prominent people and events which are likely to be discussed in other independent articles. To convert the question sentence to a cloze question, we first extract named entities using the Stanford NER system ( <ref type="bibr" target="#b4">Finkel et al., 2005</ref>) and parse the sentence using the Stanford PCFG parser ( <ref type="bibr" target="#b6">Klein and Manning, 2003)</ref>.</p><p>The person named entities are candidates for dele- tion to create a cloze problem. For each per- son named entity we then identify a noun phrase in the automatic parse that is headed by that per- son. For example, if the question sentence is "Pres- ident Obama met yesterday with Apple Founder Steve Jobs" we identify the two person noun phrases "President Obama" and "Apple Founder Steve Jobs". When a person named entity is selected for deletion, the entire noun phrase is deleted. For example, when deleting the second named entity, we get "President Obama met yesterday with XXX" rather than "President Obama met yesterday with Apple founder XXX". This increases the difficulty of the problems because systems cannot rely on de- scriptors and other local contextual cues. About 700,000 question sentences are generated from Gi- gaword articles (8% of the total number of articles).</p><p>Once a cloze question has been formed we se- lect an appropriate article as a passage. The ar- ticle should be independent of the question arti- cle but should discuss the people and events men- tioned in the question sentence. To find a passage we search the Gigaword dataset using the Apache Lucene information retrieval system <ref type="bibr">(McCandless et al., 2010)</ref>, using the question sentence as the query. The named entity to be deleted is included in the query and required to be included in the returned article. We also restrict the search to articles pub- lished within two weeks of the date of the question article. Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed. We select the best matching article satisfying our constraints. If no such article can be found, we abort the process and move on to a new question.</p><p>Given a question and a passage we next form the list of choices. We collect all person named enti- ties in the passage except unblanked person named entities in the question. Choices that are subsets of longer choices are eliminated. For example the choice "Obama" would be eliminated if the list also contains "Barack Obama". We also discard ambigu- ous cases where a part of a blanked NE appears in multiple candidate answers, e.g., if a passage has "Bill Clinton" and "Hillary Clinton" and the blanked phrase is "Clinton". We found this simple corefer- ence rule to work well in practice since news arti- cles usually employ full names for initial mentions of persons. If the resulting choice list contains fewer than two or more than five choices, the process is aborted and we move on to a new question. <ref type="bibr">3</ref> After forming an initial set of problems we then remove "duplicated" problems. Duplication arises because Gigaword contains many copies of the same article or articles where one is clearly an edited ver- sion of another. Our duplication-removal process ensures that no two problems have very similar ques- tions. Here, similarity is defined as the ratio of the size of the bag of words intersection to the size of the smaller bag. In order to focus our dataset on the most interest- ing problems, we remove some problems to suppress the performance of the following simple baselines:</p><p>• First person in passage: Select the person that ap- pears first in the passage.</p><p>• Most frequent person: Select the most frequent person in the passage.</p><p>• n-gram: Select the most likely answer to fill the blank under a 5-gram language model trained on Gigaword minus articles which are too similar to one of the questions in word overlap and phrase matching.</p><p>• Unigram: Select the most frequent last name us- ing the unigram counts from the 5-gram model.</p><p>To minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e., max</p><formula xml:id="formula_0">α(C) C∈{0,1} |b| α(C)|T (C)|<label>(1)</label></formula><p>subject to</p><formula xml:id="formula_1">∀i C:C i =1 α(C)|T (C)| N ≤ k N = C∈{0,1} |b| α(C)|T (C)|<label>(2)</label></formula><p>where T (C) is the subset of the questions solved by the subset C of the suppressed baselines, α(C) is a keeping rate for question set T (C), C i = 1 indicates the i-th baseline is in the subset, |b| is the number of baselines, N is a total number of questions, and k is the upper bound for the baselines after suppres- sion. We choose k to yield random performance for the baselines. The performance of the baselines be- fore and after suppression is shown in    <ref type="table" target="#tab_3">Table 3</ref> shows statistics of our dataset after sup- pression. We split the final dataset into train, vali- dation, and test by taking the validation and test to be a random split of the most recent 20,000 prob- lems as measured by question article date. In this way there is very little overlap in semantic subject matter between the training set and either validation or test. We also provide a larger "relaxed" training set formed by applying less baseline suppression (a larger value of k in the optimization). The relaxed training set then has a slightly different distribution from the train, validation, and test sets which are all fully suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Benchmarks</head><p>We report the performance of several systems to characterize our dataset:</p><p>• Word overlap: Select the choice c inserted to the question q which is the most similar to any sentence s in the passage, i.e., CosSim(bag(c + q), bag(s)).</p><p>• Sliding window and Distance baselines (and their combination) from <ref type="bibr">Richardson et al. (2013)</ref>.</p><p>• Attentive Reader: LSTM with attention mecha- nism ( <ref type="bibr" target="#b5">Hermann et al., 2015</ref>).</p><p>• Stanford Reader: An attentive reader modified with a bilinear term ( <ref type="bibr" target="#b1">Chen et al., 2016</ref>).</p><p>• Attention Sum (AS) Reader: GRU with a point- attention mechanism ( <ref type="bibr" target="#b1">Kadlec et al., 2016</ref>).</p><p>• Gated-Attention (GA) Reader: Attention Sum Reader with gated layers ( ). <ref type="table">Table 4</ref> shows the performance of each system on the test data. For the Attention and Stanford Read- ers, we anonymized the Who-did-What data by re- placing named entities with entity IDs as in the CNN and Daily Mail datasets. We see consistent reductions in accuracy when moving from CNN to our dataset. The Attentive and Stanford Reader drop by up to 10% and the AS and GA readers drop by up to 17%. The ranking of the systems also changes. In contrast to the Atten- tive/Stanford readers, the AS/GA readers explicitly leverage the frequency of the answer in the passage, a heuristic which appears beneficial for the CNN and Daily Mail tasks. Our suppression of the most- frequent-person baseline appears to more strongly affect the performance of these latter systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a large-scale person-centered cloze dataset whose scalability and flexibility is suitable for neural methods. This dataset is different in a va- riety of ways from existing large-scale cloze datasets and provides a significant extension to the training and test data for machine comprehension.  <ref type="table">Table 4</ref>: System performance on test set. Human performance was computed by two annotators on a sample of 100 questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Result marked I is from ( <ref type="bibr" target="#b5">Hermann et al., 2015)</ref>, results marked</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Choices: ( 1 )</head><label>1</label><figDesc>Augusto Pinochet (2) Jack Straw (3) Ricardo Lagos Passage: Tottenham won 2-0 at Hapoel Tel Aviv in UEFA Cup action on Thursday night in a defensive display which impressed Spurs skipper Robbie Keane. ... Keane scored the first goal at the Bloomfield Stadium with Dimitar Berbatov, who insisted earlier on Thursday he was happy at the London club, heading a second. The 26-year-old Berbatov admitted the reports linking him with a move had affected his performances ... Spurs manager Juande Ramos has won the UEFA Cup in the last two seasons ... Question: Tottenham manager Juande Ramos has hinted he will allow XXX to leave if the Bulgaria striker makes it clear he is unhappy. Choices: (1) Robbie Keane (2) Dimitar Berbatov</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Sample reading comprehension problems from our dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of suppressed baselines.  *  Random per-

formance is computed as a deterministic function of the number 

of times each choice set size appears. Many questions have only 

two choices and there are about three choices on average. 

relaxed 
train 
valid 
test 
train 

# questions 
185,978 127,786 10,000 10,000 
avg. # choices 
3.5 
3.5 
3.4 
3.4 
avg. # tokens 
378 
365 
325 
326 
vocab. size 
347,406 
308,602 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Dataset statistics.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Available at tticnlp.github.io/who did what</note>

			<note place="foot" n="2"> The passages here only show certain salient portions of the passage. In the actual dataset, the entire article is given. The correct answers are (3) and (2).</note>

			<note place="foot" n="3"> The maximum of five helps to avoid sports articles containing structured lists of results.</note>

			<note place="foot">• Semantic features: NLP feature based system from Wang et al. (2015).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank NVIDIA Corporation for donating GPUs used in this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A thorough examination of II are from</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">and result marked IV is from</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>result marked III is from</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">the CNN/Daily Mail reading comprehension task</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2358" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated-attention readers for text comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhingra</surname></persName>
		</author>
		<idno>abs/1606.01549</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating nonlocal information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1684" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Goldilocks principle: Reading children&apos;s books with explicit memory representations</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>McCandless et al.2010] Michael McCandless, Erik Hatcher, and Otis Gospodnetic</editor>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw</addrLine></address></meeting>
		<imprint>
			<publisher>Manning Publications Co</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="193" to="203" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards AIcomplete question answering: A set of prerequisite toy tasks</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<editor>Weston et al.2016] Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov</editor>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="700" to="706" />
		</imprint>
	</monogr>
	<note>Proceedings of International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WIKIQA: A challenge dataset for opendomain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2013" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
