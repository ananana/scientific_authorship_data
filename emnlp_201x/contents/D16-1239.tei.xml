<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Anchoring and Agreement in Syntactic Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
							<email>berzak@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="laboratory" key="lab1">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="laboratory" key="lab2">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="laboratory" key="lab1">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="laboratory" key="lab2">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
							<email>andrei@0xab.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="laboratory" key="lab1">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="laboratory" key="lab2">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="laboratory" key="lab1">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="laboratory" key="lab2">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CSAIL MIT</orgName>
								<orgName type="department" key="dep2">CSAIL MIT</orgName>
								<orgName type="laboratory" key="lab1">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="laboratory" key="lab2">Language Technology Lab DTAL Cambridge University</orgName>
								<orgName type="institution">CSAIL MIT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Anchoring and Agreement in Syntactic Annotations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2215" to="2224"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making , where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal un-wanted consequences, including overestima-tion of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report systematically obtained inter-annotator agreement estimates for English dependency parsing. Our agreement results control for parser bias, and are consequential in that they are on par with state of the art parsing performance for En-glish newswire. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Research in NLP relies heavily on the availability of human annotations for various linguistic prediction tasks. Such resources are commonly treated as de facto "gold standards" and are used for both training <ref type="bibr">1</ref> The experimental data in this study will be made publicly available. and evaluation of algorithms for automatic annota- tion. At the same time, human agreement on these annotations provides an indicator for the difficulty of the task, and can be instrumental for estimating upper limits for the performance obtainable by com- putational methods.</p><p>Linguistic gold standards are often constructed using pre-existing annotations, generated by auto- matic tools. The output of such tools is then man- ually corrected by human annotators to produce the gold standard. The justification for this annotation methodology was first introduced in a set of exper- iments on POS tag annotation conducted as part of the Penn Treebank project <ref type="bibr" target="#b12">(Marcus et al., 1993)</ref>. In this study, the authors concluded that tagger-based annotations are not only much faster to obtain, but also more consistent and of higher quality compared to annotations from scratch. Following the Penn Treebank, syntactic annotation projects for various languages, including German ( <ref type="bibr" target="#b3">Brants et al., 2002</ref>), French ( <ref type="bibr" target="#b0">Abeillé et al., 2003)</ref>, <ref type="bibr">Arabic (Maamouri et al., 2004</ref>) and many others, were annotated us- ing automatic tools as a starting point. Despite the widespread use of this annotation pipeline, there is, to our knowledge, little prior work on syntactic an- notation quality and on the reliability of system eval- uations on such data.</p><p>In this work, we present a systematic study of the influence of automatic tool output on characteristics of annotations created for NLP purposes. Our in- vestigation is motivated by the hypothesis that anno- tations obtained using such methodologies may be subject to the problem of anchoring, a well estab- lished and robust cognitive bias in which human de- cisions are affected by pre-existing values <ref type="bibr" target="#b25">(Tversky and Kahneman, 1974)</ref>. In the presence of anchors, participants reason relative to the existing values, and as a result may provide different solutions from those they would have reported otherwise. Most commonly, anchoring is manifested as an alignment towards the given values.</p><p>Focusing on the key NLP tasks of POS tagging and dependency parsing, we demonstrate that the standard approach of obtaining annotations via hu- man correction of automatically generated POS tags and dependencies exhibits a clear anchoring effect - a phenomenon we refer to as parser bias. Given this evidence, we examine two potential adverse impli- cations of this effect on parser-based gold standards.</p><p>First, we show that parser bias entails substantial overestimation of parser performance. In particu- lar, we demonstrate that bias towards the output of a specific tagger-parser pair leads to over-estimation of the performance of these tools relative to other tools. Moreover, we observe general performance gains for automatic tools relative to their perfor- mance on human-based gold standards. Second, we study whether parser bias affects the quality of the resulting gold standards. Extending the experimen- tal setup of <ref type="bibr" target="#b12">Marcus et al. (1993)</ref>, we demonstrate that parser bias may lead to lower annotation qual- ity for parser-based annotations compared to human- based annotations.</p><p>Furthermore, we conduct an experiment on inter- annotator agreement for POS tagging and depen- dency parsing which controls for parser bias. Our experiment on a subset of section 23 of the WSJ Penn Treebank yields agreement rates of 95.65 for POS tagging and 94.17 for dependency parsing. This result is significant in light of the state of the art tagging and parsing performance for English newswire. With parsing reaching the level of human agreement, and tagging surpassing it, a more thor- ough examination of evaluation resources and eval- uation methodologies for these tasks is called for.</p><p>To summarize, we present the first study to mea- sure and analyze anchoring in the standard parser- based approach to creation of gold standards for POS tagging and dependency parsing in NLP. We conclude that gold standard annotations that are based on editing output of automatic tools can lead to inaccurate figures in system evaluations and lower annotation quality. Our human agreement experi- ment, which controls for parser bias, yields agree- ment rates that are comparable to state of the art automatic tagging and dependency parsing perfor- mance, highlighting the need for a more extensive investigation of tagger and parser evaluation in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotation Tasks</head><p>We examine two standard annotation tasks in NLP, POS tagging and dependency parsing. In the POS tagging task, each word in a sentence has to be cate- gorized with a Penn Treebank POS tag <ref type="bibr" target="#b20">(Santorini, 1990</ref>) (henceforth POS). The dependency parsing task consists of providing a sentence with a labeled dependency tree using the Universal Dependencies (UD) formalism ), accord- ing to version 1 of the UD English guidelines 2 . To perform this task, the annotator is required to specify the head word index (henceforth HIND) and relation label (henceforth REL) of each word in the sentence.</p><p>We distinguish between three variants of these tasks, annotation, reviewing and ranking. In the an- notation variant, participants are asked to conduct annotation from scratch. In the reviewing variant, they are asked to provide alternative annotations for all annotation tokens with which they disagree. The participants are not informed about the source of the given annotation, which, depending on the experi- mental condition can be either parser output or hu- man annotation. In the ranking task, the participants rank several annotation options with respect to their quality. Similarly to the review task, the participants are not given the sources of the different annotation options. Participants performing the annotation, re- viewing and ranking tasks are referred to as annota- tors, reviewers and judges, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Annotation Format</head><p>All annotation tasks are performed using a CoNLL style text-based template, in which each word ap- pears in a separate line. The first two columns of each line contain the word index and the word, re-spectively. The next three columns are designated for annotation of POS, HIND and REL.</p><p>In the annotation task, these values have to be specified by the annotator from scratch. In the review task, participants are required to edit pre- annotated values for a given sentence. The sixth col- umn in the review template contains an additional # sign, whose goal is to prevent reviewers from overlooking and passively approving existing anno- tations. Corrections are specified following this sign in a space separated format, where each of the exist- ing three annotation tokens is either corrected with an alternative annotation value or approved using a * sign. Approval of all three annotation tokens is marked by removing the # sign. The example be- low presents a fragment from a sentence used for the reviewing task, in which the reviewer approves the annotations of all the words, with the exception of "help", where the POS is corrected from VB to NN and the relation label xcomp is replaced with dobj.</p><p>. The format of the ranking task is exemplified be- low. The annotation options are presented to the par- ticipants in a random order. Participants specify the rank of each annotation token following the vertical bar. In this sentence, the label cop is preferred over aux for the word "be" and xcomp is preferred over advcl for the word "Common". The participants used basic validation scripts which checked for typos and proper formatting of the annotations, reviews and rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Metrics</head><p>We measure both parsing performance and inter- annotator agreement using tagging and parsing eval- uation metrics. This choice allows for a direct com- parison between parsing and agreement results. In this context, POS refers to tagging accuracy. We utilize the standard metrics Unlabeled Attachment Score (UAS) and Label Accuracy (LA) to measure accuracy of head attachment and dependency labels. We also utilize the standard parsing metric Labeled Attachment Score (LAS), which takes into account both dependency arcs and dependency labels. In all our parsing and agreement experiments, we exclude punctuation tokens from the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Corpora</head><p>We use sentences from two publicly available datasets, covering two different genres. The second corpus is the WSJ part of the Penn Treebank (WSJ PTB) <ref type="bibr" target="#b12">(Marcus et al., 1993)</ref>. Since its release, this dataset has been the most commonly used resource for training and evaluation of English parsers. Our experiment on inter-annotator agree- ment in section 5 uses a random subset of the sen- tences in section 23 of the WSJ PTB, which is tradi- tionally reserved for tagging and parsing evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Annotators</head><p>We recruited five students at MIT as annotators. Three of the students are linguistics majors and two are engineering majors with linguistics minors. Prior to participating in this study, the annotators completed two months of training. During training, the students attended tutorials, and learned the an- notation guidelines for PTB POS tags, UD guide- lines, as well as guidelines for annotating challeng- ing syntactic structures arising from grammatical er- rors. The students also annotated individually six practice batches of 20-30 sentences from the En- glish Web Treebank (EWT) (  and FCE corpora, and resolved annotation disagree- ments during group meetings.</p><p>Following the training period, the students anno- tated a treebank of learner English ( <ref type="bibr" target="#b2">Berzak et al., 2016</ref>) over a period of five months, three of which as a full time job. During this time, the students continued attending weekly meetings in which fur- ther annotation challenges were discussed and re- solved. The annotation was carried out for sentences from the FCE dataset, where both the original and error corrected versions of each sentence were an- notated and reviewed. In the course of the anno- tation project, each annotator completed approxi- mately 800 sentence annotations, and a similar num- ber of sentence reviews. The annotations and re- views were done in the same format used in this study. With respect to our experiments, the exten- sive experience of our participants and their prior work as a group strengthen our results, as these char- acteristics reduce the effect of anchoring biases and increase inter-annotator agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parser Bias</head><p>Our first experiment is designed to test whether ex- pert human annotators are biased towards POS tags and dependencies generated by automatic tools. We examine the common out-of-domain annotation sce- nario, where automatic tools are often trained on an existing treebank in one domain, and used to gener- ate initial annotations to speed-up the creation of a gold standard for a new domain. We use the EWT UD corpus as the existing gold standard, and a sam- ple of the FCE dataset as the new corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Procedure</head><p>Our experimental procedure, illustrated in <ref type="figure" target="#fig_1">figure  1</ref>(a) contains a set of 360 sentences (6,979 tokens) from the FCE, for which we generate three gold standards: one based on human annotations and two based on parser outputs. To this end, for each sen- tence, we assign at random four of the participants to the following annotation and review tasks. The fifth participant is left out to perform the quality ranking task described in section 4.</p><p>The first participant annotates the sentence from scratch, and a second participant reviews this an-  to one of three roles: annotation, review or quality assessment.</p><p>In the bias experiment, presented in section 3, every sentence is annotated by a human, Turbo parser (based on Turbo tag- ger output) and RBG parser (based on Stanford tagger output).</p><p>Each annotation is reviewed by a different human participant to produce three gold standards of each sentence: "Human Gold", "Turbo Gold" and "RBG Gold". The fifth annotator performs a quality assessment task described in section 4, which requires to rank the three gold standards in cases of disagreement.</p><p>notation. Assigning the reviews to the human annotations yields a human based gold standard for each sen- tence called "Human Gold". Assigning the reviews to the tagger and parser outputs yields two parser- based gold standards, "Turbo Gold" and "RBG Gold". We chose the Turbo-Turbo and Stanford- RBG tagger-parser pairs as these tools obtain com- parable performance on standard evaluation bench- Each parser performs better on the reviews of the other parser compared to its performance on Human Gold. The differences in <ref type="formula">(2)</ref> and <ref type="formula">(3)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parser Specific and Parser Shared Bias</head><p>In order to test for parser bias, in table 1 we compare the performance of the Turbo-Turbo and Stanford-RBG tagger-parser pairs on our three gold standards. First, we observe that while these tools perform equally well on Human Gold, each tagger- parser pair performs better than the other on its own reviews. These parser specific performance gaps are substantial, with an average of 1.15 POS, 2.63 UAS, 2.34 LA and 3.88 LAS between the two conditions. This result suggests the presence of a bias towards the output of specific tagger-parser combinations. The practical implication of this outcome is that a gold standard created by editing an output of a parser is likely to boost the performance of that parser in evaluations and over-estimate its performance rela- tive to other parsers.</p><p>Second, we note that the performance of each of the parsers on the gold standard of the other parser is still higher than its performance on the human gold standard. The average performance gap between these conditions is 1.08 POS, 1.66 UAS, 1.66 LA and 2.47 LAS. This difference suggests an annota- tion bias towards shared aspects in the predictions of taggers and parsers, which differ from the human based annotations. The consequence of this obser- vation is that irrespective of the specific tool that was used to pre-annotate the data, parser-based gold standards are likely to result in higher parsing per- formance relative to human-based gold standards.</p><p>Taken together, the parser specific and parser shared effects lead to a dramatic overall average er- ror reduction of 49.18% POS, 33.71% UAS, 34.9% LA and 35.61% LAS on the parser-based gold stan- dards compared to the human-based gold standard. To the best of our knowledge, these results are the first systematic demonstration of the tendency of the common approach of parser-based creation of gold standards to yield biased annotations and lead to overestimation of tagging and parsing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Annotation Quality</head><p>In this section we extend our investigation to ex- amine the impact of parser bias on the quality of parser-based gold standards. To this end, we per- form a manual comparison between human-based and parser-based gold standards.</p><p>Our quality assessment experiment, depicted schematically in <ref type="figure" target="#fig_1">figure 1(b)</ref>, is a ranking task. For each sentence, a randomly chosen judge, who did not annotate or review the given sentence, ranks dis- agreements between the three gold standards Human Gold, Turbo Gold and RBG Gold, generated in the parser bias experiment in section 3.  for the human-based gold standard over each of the two parser-based gold standards. In all three eval- uation categories, human judges tend to prefer the human-based gold standard over both parser-based gold standards. This result demonstrates that the ini- tial reduced quality of the parser outputs compared to human annotations indeed percolates via anchor- ing to the resulting gold standards. The analysis of the quality assessment experi- ment thus far did not distinguish between cases where the two parsers agree and where they dis- agree. In order to gain further insight into the rela- tion between parser bias and annotation quality, we break down the results reported in table 2 into two cases which relate directly to the parser specific and parser shared components of the tagging and pars- ing performance gaps observed in the parser bias re- sults reported in section 3. In the first case, called "parser specific approval", a reviewer approves a parser annotation which disagrees both with the out- put of the other parser and the Human Gold anno- tation. In the second case, called "parser shared ap- proval", a reviewer approves a parser output which is shared by both parsers but differs with respect to Human Gold. <ref type="table" target="#tab_9">Table 3</ref> presents the judge preference rates for the Human-Gold annotations in these two scenarios. We observe that cases in which the parsers disagree are of substantially worse quality compared to human- based annotations. However, in cases of agreement between the parsers, the resulting gold standards do not exhibit a clear disadvantage relative to the Hu- man Gold annotations.</p><p>This result highlights the crucial role of parser  specific approval in the overall preference of judges towards human-based annotations in table 2. Fur- thermore, it suggests that annotations on which mul- tiple state of the art parsers agree are of sufficiently high accuracy to be used to save annotation time without substantial impact on the quality of the re- sulting resource. In section 7 we propose an annota- tion scheme which leverages this insight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Inter-annotator Agreement</head><p>Agreement estimates in NLP are often obtained in annotation setups where both annotators edit the same automatically generated input. However, in such experimental conditions, anchoring can intro- duce cases of spurious disagreement as well as spu- rious agreement between annotators due to align- ment of one or both participants towards the given input. The initial quality of the provided annotations in combination with the parser bias effect observed in section 3 may influence the resulting agreement estimates. For example, in <ref type="bibr" target="#b12">Marcus et al. (1993)</ref> an- notators were shown to produce POS tagging agree- ment of 92.8 on annotation from scratch, compared to 96.5 on reviews of tagger output.</p><p>Our goal in this section is to obtain estimates for inter-annotator agreement on POS tagging and de- pendency parsing that control for parser bias, and as a result, reflect more accurately human agree- ment on these tasks. We thus introduce a novel pipeline based on human annotation only, which eliminates parser bias from the agreement measure- ments. Our experiment extends the human-based an- notation study of <ref type="bibr" target="#b12">Marcus et al. (1993)</ref> to include also syntactic trees. Importantly, we include an ad- ditional review step for the initial annotations, de- signed to increase the precision of the agreement measurements by reducing the number of errors in the original annotations.  The participants are assigned to the following tasks at random for each sentence. Two participants annotate the sentence from scratch, and the remaining two participants review one of these annotations each. Agreement is measured on the annotations ("scratch") as well after assigning the review edits ("scratch re- viewed").</p><p>For this experiment, we use 300 sentences (7,227 tokens) from section 23 of the PTB-WSJ, the stan- dard test set for English parsing in NLP. The exper- imental setup, depicted graphically in figure 2, in- cludes four participants randomly assigned for each sentence to annotation and review tasks. Two of the participants provide the sentence with annotations from scratch, while the remaining two participants provide reviews. Each reviewer edits one of the annotations independently, allowing for correction of annotation errors while maintaining the indepen- dence of the annotation sources. We measure agree- ment between the initial annotations ("scratch"), as well as the agreement between the reviewed versions of our sentences ("scratch reviewed").</p><p>The agreement results for the annotations and the reviews are presented in table 4. The initial agree- ment rate on POS annotation from scratch is higher than in <ref type="bibr" target="#b12">(Marcus et al., 1993)</ref>. This difference is likely to arise, at least in part, due to the fact that their experiment was conducted at the beginning of the annotation project, when the annotators had a more limited annotation experience compared to our participants. Overall, we note that the agree- ment rates from scratch are relatively low. The re- view round raises the agreement on all the evalua- tion categories due to elimination of annotation er- rors present the original annotations.  <ref type="table">Table 4</ref>: Inter-annotator agreement on 300 sentences (7,227 to- kens) from the PTB-WSJ section 23. "scratch" is agreement on independent annotations from scratch. "scratch reviewed" is agreement on the same sentences after an additional indepen- dent review round of the annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS</head><p>Our post-review agreement results are consequen- tial in light of the current state of the art performance on tagging and parsing in NLP. For more than a decade, POS taggers have been achieving over 97% accuracy with the PTB POS tag set on the PTB-WSJ test set. For example, the best model of the Stanford tagger reported in <ref type="bibr" target="#b24">Toutanova et al. (2003)</ref> produces an accuracy of 97.24 POS on sections 22-24 of the PTB-WSJ. These accuracies are above the human agreement in our experiment.</p><p>With respect to dependency parsing, recent parsers obtain results which are on par or higher than our inter-annotator agreement estimates. For exam- ple, <ref type="bibr" target="#b26">Weiss et al. (2015)</ref> report <ref type="bibr">94.26 UAS and Andor et al. (2016)</ref> report 94.61 UAS on section 23 of the PTB-WSJ using an automatic conversion of the PTB phrase structure trees to Stanford depen- dencies <ref type="bibr" target="#b5">(De Marneffe et al., 2006</ref>). These results are not fully comparable to ours due to differences in the utilized dependency formalism and the auto- matic conversion of the annotations. Nonetheless, we believe that the similarities in the tasks and eval- uation data are sufficiently strong to indicate that dependency parsing for standard English newswire may be reaching human agreement levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>The term "anchoring" was coined in a seminal paper by <ref type="bibr" target="#b25">Tversky and Kahneman (1974)</ref>, which demon- strated that numerical estimation can be biased by uninformative prior information. Subsequent work across various domains of decision making con- firmed the robustness of anchoring using both in- formative and uninformative anchors <ref type="bibr" target="#b8">(Furnham and Boo, 2011</ref>). Pertinent to our study, anchoring bi- ases were also demonstrated when the participants were domain experts, although to a lesser degree than in the early anchoring experiments ( <ref type="bibr" target="#b27">Wilson et al., 1996;</ref><ref type="bibr" target="#b14">Mussweiler and Strack, 2000</ref>).</p><p>Prior work in NLP examined the influence of pre-tagging <ref type="bibr" target="#b7">(Fort and Sagot, 2010)</ref> and pre-parsing (Skjaerholt, 2013) on human annotations. Our work introduces a systematic study of this topic using a novel experimental framework as well as substan- tially more sentences and annotators. Differently from these studies, our methodology enables charac- terizing annotation bias as anchoring and measuring its effect on tagger and parser evaluations.</p><p>Our study also extends the POS tagging exper- iments of <ref type="bibr" target="#b12">Marcus et al. (1993)</ref>, which compared inter-annotator agreement and annotation quality on manual POS tagging in annotation from scratch and tagger-based review conditions. The first result re- ported in that study was that tagger-based editing in- creases inter-annotator agreement compared to an- notation from scratch. Our work provides a novel agreement benchmark for POS tagging which re- duces annotation errors through a review process while controlling for tagger bias, and obtains agree- ment measurements for dependency parsing. The second result reported in <ref type="bibr" target="#b12">Marcus et al. (1993)</ref> was that tagger-based edits are of higher quality com- pared to annotations from scratch when evaluated against an additional independent annotation. We modify this experiment by introducing ranking as an alternative mechanism for quality assessment, and adding a review round for human annotations from scratch. Our experiment demonstrates that in this configuration, parser-based annotations are of lower quality compared to human-based annotations.</p><p>Several estimates of expert inter-annotator agree- ment for English parsing were previously reported. However, most such evaluations were conducted us- ing annotation setups that can be affected by an anchoring bias <ref type="bibr" target="#b4">(Carroll et al., 1999;</ref><ref type="bibr" target="#b17">Rambow et al., 2002;</ref>. A notable excep- tion is the study of <ref type="bibr" target="#b18">Sampson and Babarczy (2008)</ref> who measure agreement on annotation from scratch for English parsing in the SUSANNE framework <ref type="bibr" target="#b19">(Sampson, 1995)</ref>. The reported results, however, are not directly comparable to ours, due to the use of a substantially different syntactic representation, as well as a different agreement metric. Their study further suggests that despite the high expertise of the annotators, the main source of annotation disagree- ments was annotation errors. Our work alleviates this issue by using annotation reviews, which reduce the number of erroneous annotations while main- taining the independence of the annotation sources. Experiments on non-expert dependency annotation from scratch were previously reported for French, suggesting low agreement rates (79%) with an ex- pert annotation benchmark (Gerdes, 2013).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We present a systematic study of the impact of an- choring on POS and dependency annotations used in NLP, demonstrating that annotators exhibit an an- choring bias effect towards the output of automatic annotation tools. This bias leads to an artificial boost of performance figures for the parsers in question and results in lower annotation quality as compared with human-based annotations.</p><p>Our analysis demonstrates that despite the adverse effects of parser bias, predictions that are shared across different parsers do not significantly lower the quality of the annotations. This finding gives rise to the following hybrid annotation strategy as a po- tential future alternative to human-based as well as parser-based annotation pipelines. In a hybrid anno- tation setup, human annotators review annotations on which several parsers agree, and complete the re- maining annotations from scratch. Such a strategy would largely maintain the annotation speed-ups of parser-based annotation schemes. At the same time, it is expected to achieve annotation quality compa- rable to human-based annotation by avoiding parser specific bias, which plays a pivotal role in the re- duced quality of single-parser reviewing pipelines.</p><p>Further on, we obtain, to the best of our knowl-edge for the first time, syntactic inter-annotator agreement measurements on WSJ-PTB sentences. Our experimental procedure reduces annotation er- rors and controls for parser bias. Despite the de- tailed annotation guidelines, the extensive experi- ence of our annotators, and their prior work as a group, our experiment indicates rather low agree- ment rates, which are below state of the art tagging performance and on par with state of the art parsing results on this dataset. We note that our results do not necessarily reflect an upper bound on the achiev- able syntactic inter-annotator agreement for English newswire. Higher agreement rates could in princi- ple be obtained through further annotator training, refinement and revision of annotation guidelines, as well as additional automatic validation tests for the annotations. Nonetheless, we believe that our esti- mates reliably reflect a realistic scenario of expert syntactic annotation. The obtained agreement rates call for a more ex- tensive examination of annotator disagreements on parsing and tagging. Recent work in this area has already proposed an analysis of expert annotator dis- agreements for POS tagging in the absence of anno- tation guidelines <ref type="bibr" target="#b15">(Plank et al., 2014</ref>). Our annota- tions will enable conducting such studies for annota- tion with guidelines, and support extending this line of investigation to annotations of syntactic depen- dencies. As a first step towards this goal, we plan to carry out an in-depth analysis of disagreement in the collected data, characterize the main sources of inconsistent annotation and subsequently formu- late further strategies for improving annotation ac- curacy. We believe that better understanding of hu- man disagreements and their relation to disagree- ments between humans and parsers will also con- tribute to advancing evaluation methodologies for POS tagging and syntactic parsing in NLP, an im- portant topic that has received only limited attention thus far ( <ref type="bibr" target="#b21">Schwartz et al., 2011;</ref><ref type="bibr" target="#b16">Plank et al., 2015)</ref>.</p><p>Finally, since the release of the Penn Treebank in 1992, it has been serving as the standard benchmark for English parsing evaluation. Over the past few years, improvements in parsing performance on this dataset were obtained in small increments, and are commonly reported without a linguistic analysis of the improved predictions. As dependency parsing performance on English newswire may be reaching human expert agreement, not only new evaluation practices, but also more attention to noisier domains and other languages may be in place.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Experimental setup for parser bias (a) and annotation quality (b) on 360 sentences (6,979 tokens) from the FCE. For each sentence, five human annotators are assigned at random</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Experimental setup for the inter-annotator agreement experiment. 300 sentences (7,227 tokens) from section 23 of the PTB-WSJ are annotated and reviewed by four participants.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 presents the preference rates of judges</head><label>2</label><figDesc></figDesc><table>Human Gold Preference % POS 

HIND REL 
Turbo Gold 
64.32* 63.96* 61.5* 
# disagreements 
199 
444 
439 
RBG Gold 
56.72 
61.38* 57.73* 
# disagreements 
201 
435 
440 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 2 : Human preference rates for a human-based gold stan-</head><label>2</label><figDesc></figDesc><table>dard Human Gold over the two parser-based gold standards 

Turbo Gold and RBG Gold. # disagreements denotes the num-

ber of tokens that differ between Human Gold and the respec-

tive parser-based gold standard. Statistically significant values 

for a two-tailed Z test with p &lt; 0.01 are marked with *. Note 

that for both tagger-parser pairs, human judges tend to prefer 

human-based over parser-based annotations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Breakdown of the Human preference rates for the 

human-based gold standard over the parser-based gold stan-

dards in table 2, into cases of agreement and disagreement be-

tween the two parsers. Parser specific approval are cases in 

which a parser output approved by the reviewer differs from 

both the output of the other parser and the Human Gold anno-

tation. Parser shared approval denotes cases where an approved 

parser output is identical to the output of the other parser but dif-

fers from the Human Gold annotation. Statistically significant 

values for a two-tailed Z test with p &lt; 0.01 are marked with 

*. Note that parser specific approval is substantially more detri-

mental to the resulting annotation quality compared to parser 

shared approval. 

</table></figure>

			<note place="foot" n="2"> http://universaldependencies.org/#en</note>

			<note place="foot" n="3"> The annotation bias and quality results reported in sections 3 and 4 use the original learner sentences, which contain grammatical errors. These results were replicated on the error corrected versions of the sentences.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our terrific annotators Sebastian Garza, Jessica Kenney, Lucia Lam, Keiko Sophie Mori and Jing Xian Wang. We are also grateful to Karthik Narasimhan and the anonymous reviewers for valu-able feedback on this work. This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM) funded by NSF STC award CCF-1231216. This work was also supported by AFRL contract No. FA8750-15-C-0010 and by ERC Consolidator Grant LEXICAL (648909).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a treebank for french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Abeillé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Toussenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="165" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2442" to="2452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal dependencies for learner english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Kenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><surname>Spadine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing Xian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keiko</forename><forename type="middle">Sophie</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The tiger treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on treebanks and linguistic theories</title>
		<meeting>the workshop on treebanks and linguistic theories</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Corpus annotation for parser evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<idno>cs/9907013</idno>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katri</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4585" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Influence of preannotation on pos-tagged corpus development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karën</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth linguistic annotation workshop</title>
		<meeting>the fourth linguistic annotation workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A literature review of the anchoring effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Furnham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua Chu</forename><surname>Boo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of SocioEconomics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="42" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Collaborative dependency annotation. DepLing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Gerdes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">88</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-rank tensors for scoring dependency structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The penn arabic treebank: Building a large-scale annotated arabic corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Maamouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Bies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Buckwalter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wigdan</forename><surname>Mekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NEMLAR conference on Arabic language resources and tools</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="466" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Numeric judgments under uncertainty: The role of knowledge in anchoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mussweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fritz</forename><surname>Strack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Social Psychology</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="495" to="518" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Linguistically debatable or just plain wrong?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL: Short Papers</title>
		<meeting>ACL: Short Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="507" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do dependency parsing metrics correlate with human judgments?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Héctor Martínez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danijela</forename><surname>Zeljko Agi´cagi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Merkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Søgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dependency treebank for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandre</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harriet</forename><surname>Taber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Definitional and human constraints on structural annotation of english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Babarczy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="471" to="494" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">English for the computer: Susanne corpus and analytic scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Sampson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Part-of-speech tagging guidelines for the penn treebank project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>CIS</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical Reports</note>
	<note>3rd revision</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neutralizing linguistically problematic annotations in unsupervised dependency parsing evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="663" to="672" />
		</imprint>
	</monogr>
	<note>Roi Reichart, and Ari Rappoport</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A gold standard dependency corpus for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine De</forename><surname>Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2897" to="2904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Influence of preprocessing on dependency syntax annotation: speed and agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Skjaerholt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>LAW VII &amp; ID</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Judgment under uncertainty: Heuristics and biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kahneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">4157</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new look at anchoring effects: basic anchoring and its antecedents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">E</forename><surname>Timothy D Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Etling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brekke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">387</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new dataset and method for automatically grading ESOL texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Medlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
