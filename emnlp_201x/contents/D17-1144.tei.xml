<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying attack and support argumentative relations using deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Toni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying attack and support argumentative relations using deep learning</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1374" to="1379"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a deep learning architecture to capture argumentative relations of attack and support from one piece of text to another , of the kind that naturally occur in a debate. The architecture uses two (uni-directional or bidirectional) Long Short-Term Memory networks and (trained or non-trained) word embeddings, and allows to considerably improve upon existing techniques that use syntactic features and supervised classifiers for the same form of (relation-based) argument mining.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Argument Mining (AM) is a relatively new re- search area which involves, amongst others, the automatic detection in text of arguments, ar- gument components, and relations between ar- guments (see <ref type="bibr" target="#b13">(Lippi and Torroni, 2016</ref>) for an overview). We focus on a specific type of AM, referred to as Relation-based AM <ref type="bibr" target="#b3">(Carstens and Toni, 2015)</ref>, which has recently received atten- tion by several researchers (e.g. see ( <ref type="bibr" target="#b1">Bosc et al., 2016;</ref><ref type="bibr" target="#b4">Carstens and Toni, 2017)</ref>). This type of AM aims at identifying argumentative relations of at- tack and support between natural language argu- ments in text, by classifying pairs of pieces of text as belonging to attack, support or neither attack nor support relations. For example, consider the three texts taken from Carstens and Toni (2015):</p><p>t 1 : 'We should grant politicians immunity from prosecution'</p><p>t 2 : 'Giving politicians immunity allows them to focus on performing their duties' t 3 : 'The ability to prosecute politicians is the ul- timate protection against abuse of power'</p><p>Here t 2 supports t 1 , t 3 attacks t 1 , and t 2 and t 3 neither attack nor support one another.</p><p>Relation-based AM is useful, for example, to pave the way towards identifying accepted opin- ions ( <ref type="bibr" target="#b1">Bosc et al., 2016</ref>) or divisive issues ( <ref type="bibr" target="#b10">Konat et al., 2016</ref>) within debates.</p><p>We propose a deep learning architecture for Relation-based AM based on Long-Short Term Memory (LSTM) networks <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b19">Schuster and Paliwal, 1997)</ref>. Within the architecture, each input text is fed, as a (trained or non-trained) 100-dimensional GloVe embedding ( <ref type="bibr" target="#b16">Pennington et al., 2014</ref>), into a (unidi- rectional or bidirectional) LSTM which produces a vector representation of the text independently of the other text being analysed. The two vectors are then merged (using element-wise sum or concate- nation) and the resulting vector is fed to a softmax classifier which predicts whether the pair of input texts belongs to the attack, support or neither re- lations. The input texts may be at most 50 words long, but are not restricted to single sentences.</p><p>We experimented with several instances of the architecture and achieved 89.53% accuracy and 89.07% F 1 using unidirectional LSTMs and con- catenation as the merge layer, considerably outper- forming feature-based supervised classifiers used in the studies which presented the corpus we also use ( <ref type="bibr">Toni, 2015, 2017</ref>).</p><p>The remainder of the paper is organised as fol- lows. In Section 2 we discuss related work and the corpus we use. In Section 3 we describe our deep learning architecture and report experiments and results in Section 4. We conclude the paper and propose directions for future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent Neural Networks</head><p>Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b6">(Elman, 1990;</ref><ref type="bibr" target="#b15">Mikolov et al., 2010</ref>) are a type of neural networks in which the hidden layer is connected to itself so that the previous hidden state is used along with the input at the current step. RNNs tend to suffer from the vanishing gradients prob- lem ( <ref type="bibr" target="#b0">Bengio et al., 1994</ref>) while trying to capture long-term dependencies.</p><p>LSTM models <ref type="bibr" target="#b8">(Hochreiter and Schmidhuber, 1997</ref>), a type of RNNs, address this problem by introducing memory cells and gates into networks. LSTMs use memory cells to store contextual in- formation and three types of gates (input, forget, and output gates) that determine which informa- tion needs to be added or removed to learn long- term dependencies within a sequence.</p><p>One problem with RNNs/LSTMs in natu- ral language processing is that they do not make use of the information of future words. Bidirectional RNNs/LSTMs (BiRNNs/BiLSTMs) <ref type="bibr" target="#b19">(Schuster and Paliwal, 1997</ref>) solve this problem by using both previous and future words while pro- cessing the input sequence with two RNNs: one in the forward and one in the backward direction, resulting in two vectors for each input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related work</head><p>Identifying relations between texts has recently re- ceived a great deal of attention, e.g. in Argument Mining (AM) (see ( <ref type="bibr" target="#b13">Lippi and Torroni, 2016</ref>) for a recent overview). In particular, Relation-based AM <ref type="bibr" target="#b3">(Carstens and Toni, 2015)</ref> aims to automat- ically identify argumentative relations to create Bipolar Argumentation Frameworks (BAFs) <ref type="bibr">(Cayrol and Lagasquie-Schiex, 2005</ref>).</p><p>BAFs are triples AR, attacks, supports con- sisting of a set of arguments AR and two binary re- lations attacks and supports between arguments.</p><p>The example texts introduced in Section 1 form a BAF with AR = {t 1 , t 2 , t 3 } and attacks, supports given graphically (as -, + respectively) as follows:</p><formula xml:id="formula_0">t 2 t 3 t 1 + âˆ’</formula><p>Carstens and Toni (2017) obtained 61.8% accu- racy and 62.1% F 1 on a news articles using Sup- port Vector Machines (SVMs) and features such as distance measures, word overlap, sentence metrics and occurences of sentiment words. <ref type="bibr" target="#b1">Bosc et al. (2016)</ref> used a corpus consisting of tweets to determine attack and support relations between tweets. Using an encoder-decoder ar- chitecture and two LSTMs (the second LSTM initialised with the last hidden state of the first LSTM), they obtained negative results (0.2 F 1 for support and 0.16 F 1 for attack).</p><p>Other works in AM use deep learning models to determine relations between arguments, but of a different kind than in our work. Notably, <ref type="bibr" target="#b7">Habernal and Gurevych (2016)</ref> experimented with LSTM models extended with an attention mechanism and a convolution layer over the input pairs to deter- mine whether an input argument is more convinc- ing than the other input argument. Thus, their fo- cus is on determining a "more convincing than" relation, rather than attack and support argumen- tative relations, between arguments.</p><p>Several authors used neural network models for tasks related to the form of AM we consider. <ref type="bibr" target="#b21">Yin et al. (2016)</ref> proposed three attention mechanisms for Convolutional Neural Networks to model pairs of sentences in tasks such as textual entailment and answer selection, whereas dos <ref type="bibr" target="#b18">Santos et al. (2016)</ref> proposed a two-way attention mechanism to jointly learn the representation of two inputs in an answer selection setting. <ref type="bibr" target="#b2">Bowman et al. (2015)</ref> used stacked LSTMs to determine entailment, neutral and contradic- tion relations amongst sentence pairs using the SNLI (Stanford Natural Language Inference) cor- pus, with the bottom layer taking as input the concatenation of the input sentences. Recognis- ing textual entailment between two sentences was also addressed in <ref type="bibr" target="#b17">(RocktÃ¤schel et al., 2015)</ref> which used LSTMs and a word-by-word neural attention mechanism on the SNLI corpus. <ref type="bibr" target="#b14">Liu et al. (2016)</ref> proposed two models captur- ing the interdependencies between the two paral- lel LSTMs encoding two input sentences for the tasks of recognising textual entailment and match- ing questions and answers. Further, <ref type="bibr" target="#b11">Koreeda et al. (2016)</ref> used a BiRNN with a word-embedding- based attention model to determine whether a piece of an evidence supports a claim that a phrase promotes or suppresses a value, using a dataset of 1000 pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>Determining relations between any texts can be seen as a three-class problem, with labels L = {attack, support, neither}. We used a dataset covering various topics such as movies, technol- ogy and politics 1 , where attack relations repre- sent 31% of the dataset, support relations repre- sent 32% of the dataset and neither relations rep- resent 37% of the dataset.</p><p>We have also explored the use of other corpora (e.g. the SNLI corpus <ref type="bibr" target="#b2">(Bowman et al., 2015)</ref> and Araucaria in AIFdb 2 ) that we ultimately decided not to include due to their structure not being di- rectly amenable to our analysis. <ref type="figure">Figure 1</ref> summarises the deep learning architec- ture that we use for predicting which relation from L = {attack, support, neither} holds between the first and the second texts in any input pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>We do not limit input texts to be single sen- tences, but limit them to 50 words (as this is the average text length in our corpus): inputs whose size is smaller than this threshold are padded with zeros at the end to give sequences of exactly 50 words. The input texts are (separately) embed- ded as 100-dimensional GloVe vectors <ref type="bibr" target="#b16">(Pennington et al., 2014</ref>), with the words that do not ap- pear in the vectors being treated as unknown. As we will see in Section 4, we experimented with the pre-trained word representations (freezing the weights during learning) as well as learning the weights.</p><p>The architecture relies upon two parallel LSTMs to model the two texts separately. We ex- perimented with both unidirectional and bidirec- tional LSTMs (see Section 4). In both cases, we set the LSTM dimension to 32, as this proved to be the best, amongst alternatives (64, 100, 128), for mitigating overfitting. In addition, our LSTMs use a Rectified Linear Unit (ReLU) activation, each re- turning a vector of dimension 32.</p><p>Each LSTM network produces a vector repre- sentation of the input text, independently of the other text being analysed. The two vectors are then merged and the resulting vector fed to a softmax classifier which predicts the label for the relation between the first and the second input texts. As we will see in Section 4, we experimented with  two types of merge layer: sum, which performs element-wise sum, and concat, which performs tensor concatenation.</p><p>After the merge layer, our architecture incorpo- rates an optional dense feedforward layer. Our ex- periments (see Section 4) included testing whether the inclusion of this layer has an impact on the re- sults. Again, we chose the dimension (32) as it proved better for mitigating overfitting than alter- natives that we tried (64).</p><p>The values for the hyper-parameters used in our experiments (see Section 4) are summarised in Ta- ble 1. We used a mini-batch size of 128 and cross-entropy loss. To avoid overfitting, we ap- plied dropout before the merge layer with prob- ability 0.2, but not on the recurrent units. The hyper-parameters were optimised using the Adam method <ref type="bibr" target="#b9">(Kingma and Ba, 2014</ref>) with learning rate 0.001, which turned out to give better perfor- mances than alternative optimisers we tried (Ada- grad, Adadelta and RMSprop).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We trained for 50 epochs or until the performance on the development set stopped improving, in or- der to avoid overfitting. The development set was 20% of the training dataset in the 10-fold cross- validation setup. In more detail, we run 10 strati- fied fold cross-validation for 5 times (so that each fold is a good representative of the whole). We report the average results of the 5x10 fold cross- validation in <ref type="table">Table 2</ref>. As baseline, we used Logis- tic Regression (LR) and unigrams obtained from concatenating the two input texts.</p><p>We experimented with using BiLSTMs and uni- directional LSTMs with the two types of merge layers and using non-trained embeddings, namely pre-trained word representations (freezing the weights during learning), or trained embeddings, learning the weights during training.</p><p>We achieved 89.53% accuracy and 89.07% F 1 by concatenating the output of the two sepa- rate LSTMs. Unexpectedly, BiLSTMs performed worse than LSTMs <ref type="table">(Table 2</ref> only includes the best performing BiLSTM instance of the architecture, using concatenation and the feedforward layer). We believe this is because of the size of the dataset and that this effect could be diminished by acquir- ing more data. For the LSTM model with trained embeddings, the accuracy varied between 84.84% and 90.02%. Concatenating the LSTMs' output vectors yields better performance than performing element-wise sum of the vectors. We believe this is because this allows the system to encode more features, allowing the network to use more infor- mation.</p><p>Using the default, pre-trained word embeddings yields worse results compared to the baseline. We believe this is because the quality of word embed- dings is dependent on the training corpora. <ref type="bibr">3</ref> Train- ing the word embeddings results in better perfor- mance compared to the baseline with improve- ments of up to 12% in accuracy and up to 11.5% in F 1 .</p><p>In all cases, training the word embeddings re- sults in dramatic improvements compared to freez- ing the embedding weights during learning, vary- ing from 9.9% to 21.3% increase in accuracy and up to 25% in F 1 . We also report the standard de- viation of our models with trained embeddings. This shows that our best models (LSTMs with a concatenation layer) are stable and perform con- sistently on the task considered. Using One-Way ANOVA, the result is significant at p &lt; 0.05 (the f-ratio value is 145.45159, the p-value is &lt; 0.00001).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a deep learning architecture based on Long Short-Term Memory (LSTM) networks to capture the argumentative relation of attack and support between any two texts. Our architecture uses two (unidirectional or bidirectional) LSTMs to analyse separately two 100-dimensional (non- trained or trained) GloVe vectors representing the two input texts. The outputs of the two LSTMs are then concatenated and fed to a softmax classifier to predict the relation between the input texts.</p><p>Our unidirectional LSTM model with trained embeddings and a concatenation layer achieved 89.53% accuracy and 89.07% F 1 . The results indi- cate that LSTMs may be better suited for Relation- based Argument Mining at least for non-micro texts ( <ref type="bibr" target="#b1">Bosc et al., 2016</ref>) than standard classifiers as used in e.g. <ref type="bibr" target="#b4">Carstens and Toni (2017)</ref>, as LSTMs are better at capturing long-term dependencies be- tween words and they operate over sequences, as found in text.</p><p>In future work, we plan to test our model on corpora such as the Language of Opposition from AIFdb 4 (by converting the finer-grained relation types used in this corpus to argumentative rela- tions of the kind we considered), on datasets pro- posed for different tasks (e.g. identifying tex- tual entailment could be seen as identifying sup- port) and thus possibly use the corpus proposed by <ref type="bibr" target="#b2">Bowman et al. (2015)</ref>, as well as the twitter dataset of <ref type="bibr" target="#b1">Bosc et al. (2016)</ref> once it becomes publicly available. Also, attack and support relations of the kind we have considered in this paper may be seen as special types of discourse relations ( <ref type="bibr" target="#b20">Teufel et al., 1999;</ref><ref type="bibr" target="#b12">Lin et al., 2009)</ref>. It would be inter- esting to see whether any corpora for identifying discourse relations could be useful for furthering our experimentation. Finally, we plan to incorpo- rate an attention-based mechanism as well as ad- ditional features (e.g. extracted through sentiment analysis) to determine which parts of the texts are most relevant in identifying attack and support.  <ref type="table">Table 2</ref>: 5x10 fold cross-validation results, using c(oncat) or s(um) for merging the output of the two (Bi)LSTMs, with (non-)trained embeddings; T (True)/F (False) represent inclusion/omission, respec- tively, of the Dense 32 ReLU layer. std represents standard deviation of 5x10 fold cross-validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>Figure 1: Our architecture: two (unidirectional or bidirectional) LSTMs are run with one text each. The dashed layer (Dense 32 ReLU) is optional.</figDesc></figure>

			<note place="foot" n="3"> Pennington et al. (2014) computed the 100-dimensional GloVe embeddings on a a dump of English Wikipedia pages from 2014 consisting of 400k words.</note>

			<note place="foot" n="4"> https://corpora.aifdb.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tweeties squabbling: Positive and negative results in applying argument mining on social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bosc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Argument COMMA</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards relation based argumentation mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Carstens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Argumentation Mining</title>
		<meeting>the 2nd Workshop on Argumentation Mining</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using argumentation to improve classification in natural language problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Carstens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems. Forthcoming</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the acceptability of arguments in bipolar argumentation frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudette</forename><surname>Cayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Christine</forename><surname>Lagasquieschiex</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symbolic and Quantitative Approaches to Reasoning with Uncertainty: 8th European Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="378" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in web argumentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1214" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A corpus of argument networks: Using graph properties to analyse divisive issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Konat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joonsuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katarzyna</forename><surname>Budzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural attention model for classification of sentences that support promoting/suppressing relationship</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Koreeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohsuke</forename><surname>Yanai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misa</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiki</forename><surname>Niwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Argument Mining</title>
		<meeting>the Third Workshop on Argument Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing implicit discourse relations in the penn discourse treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Argumentation mining: State of the art and emerging trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Internet Technology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modelling interaction of sentence pair with coupled-lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1703" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>KarafiÃ¡t</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">LukÃ¡s</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>CernockÂ´ycernockÂ´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Reasoning about entailment with neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">TomÃ¡s</forename><surname>KociskÂ´ykociskÂ´y</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>abs/1509.06664</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attentive pooling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">CÃ­cero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.03609</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Kuldip</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An annotation scheme for discourse-level argumentation in research articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Carletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL 9th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ABCNN: attention-based convolutional neural network for modeling sentence pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Wenpeng Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>SchÃ¼tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
