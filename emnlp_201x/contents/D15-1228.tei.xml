<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:23+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extractive Summarization by Maximizing Semantic Volume</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<email>dyogatama@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
							<email>feiliu@cs.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<postCode>98195</postCode>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extractive Summarization by Maximizing Semantic Volume</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The most successful approaches to extrac-tive text summarization seek to maximize bigram coverage subject to a budget constraint. In this work, we propose instead to maximize semantic volume. We embed each sentence in a semantic space and construct a summary by choosing a subset of sentences whose convex hull maximizes volume in that space. We provide a greedy algorithm based on the Gram-Schmidt process to efficiently perform volume maximization. Our method out-performs the state-of-the-art summariza-tion approaches on benchmark datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In artificial intelligence, changes in representation sometimes suggest new algorithms. For example, increased attention to distributed meaning repre- sentations suggests that existing combinatorial al- gorithms for NLP might be supplanted by alterna- tives designed specifically for embeddings. In this work, we consider summarization.</p><p>Classical approaches to extractive summariza- tion represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy <ref type="bibr" target="#b4">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b19">McDonald, 2007)</ref>, or (b) maximize bigram coverage ( <ref type="bibr" target="#b24">Yih et al., 2007;</ref><ref type="bibr" target="#b6">Gillick et al., 2008</ref>). The sentence representa- tion is fundamentally discrete, and a range of greedy <ref type="bibr" target="#b4">(Carbonell and Goldstein, 1998)</ref>, approx- imate ( <ref type="bibr" target="#b0">Almeida and Martins, 2013)</ref>, and exact op- timization algorithms <ref type="bibr" target="#b19">(McDonald, 2007;</ref><ref type="bibr" target="#b18">Martins and Smith, 2009;</ref><ref type="bibr" target="#b3">Berg-Kirkpatrick et al., 2011</ref>) have been proposed.</p><p>Recent studies have explored continuous sen- tence representations, including the paragraph vector ( <ref type="bibr" target="#b13">Le and Mikolov, 2014</ref>), a convolutional neural network architecture ( <ref type="bibr" target="#b9">Kalchbrenner et al., 2014)</ref>, and a dictionary learning approach <ref type="bibr" target="#b8">(Jenatton et al., 2011</ref>). If sentences are represented as low-dimensional embeddings in a distributed se- mantic space, then we begin to imagine a geomet- ric relationship between a summary and a doc- ument. We propose that the volume of a sum- mary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume ( §2), and we provide a fast greedy algorithm that can be used to maximize it ( §3). We show that our method outperforms competing extractive base- lines under similar experimental conditions on benchmark summarization datasets ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Extractive Summarization Models</head><p>Assume we are given a set of N sentences: D = {s 1 , s 2 , . . . , s N } from one or many documents, and the goal is to produce a summary by choos- ing a subset S of M sentences, where S ⊆ D and M ≤ N , and the length of the summary is less than or equal to L words. In this work, we as- sume no summaries are available as training data. Denote a binary indicator vector y ∈ R N , where sentence i is included if and only if y i = 1 and 0 otherwise. Extractive summarization can be writ- ten as an optimization problem:</p><formula xml:id="formula_0">max score(S) = score(D, y)</formula><p>with respect to S equivalently y subject to length(S) ≤ L with a scoring function score <ref type="bibr">(D, y)</ref>. A good scor- ing function should assign higher scores to bet- ter summaries. In the following, we describe two commonly used scoring functions and our pro- posed scoring function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Maximal Marginal Relevance</head><p>The Maximal Marginal Relevance (MMR) method <ref type="bibr" target="#b4">(Carbonell and Goldstein, 1998</ref>) considers the fol- lowing scoring function:</p><formula xml:id="formula_1">score(D, y) = N i=1 y i Rel(s i ) − N i,j=1 y i y j Sim(s i , s j )</formula><p>where Rel(s i ) measures the relevancy of sentence i and Sim(s i , s j ) measures the (e.g., cosine) simi- larity between sentence i and sentence j. The in- tuition is to choose sentences that are highly rel- evant to the document(s) and avoid redundancy.</p><p>The above maximization problem has been shown to be NP-hard, solvable exactly using ILP <ref type="bibr" target="#b19">(McDonald, 2007)</ref>. A greedy algorithm that approxi- mates the global solution by adding one sentence at a time to maximize the overall score ( <ref type="bibr" target="#b15">Lin and Bilmes, 2010</ref>) is often used in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Coverage-Based Summarization</head><p>Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. <ref type="bibr">Gillick</ref>  </p><formula xml:id="formula_2">score(D, y, z) = B j=1</formula><p>b j z j and the two additional constraints are:</p><formula xml:id="formula_3">∀j ∈ [B], ∀i ∈ [N ] y i m i,j ≤ z j ∀j ∈ [B] N i=1 y i m i,j ≥ z j</formula><p>where we use <ref type="bibr">[B]</ref> as a shorthand for {1, 2, . . . , B}.</p><p>The first constraint makes sure that selecting a sen- tence implies selecting all its bigrams, whereas the Consider the case when the maximum summary length is four sentences. Our scoring function is optimized by chooseing the four sentences in red as the summary, since they maximize the volume (area in two dimensions).</p><p>second constraint makes sure that selecting a bi- gram implies selecting at least one of the sentences that contains it. In this formulation, there is no ex- plicit penalty on redundancy. However, insofar as redundant sentences cover fewer bigrams, they are implicitly discouraged. Although the above scor- ing function also results in an NP-hard problem, an off-the-shelf ILP solver ( <ref type="bibr" target="#b6">Gillick et al., 2008)</ref> or a dual decomposition algorithm <ref type="bibr" target="#b0">(Almeida and Martins, 2013)</ref> can be used to solve it in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Semantic Volume</head><p>We introduce a new scoring function for summa- rization. The main idea is based on the notion of coverage, but in a distributed semantic space: a good summary should have broad semantic cover- age with respect to document contents. For every sentence s i , i ∈ [N ], we denote its continuous se- mantic representation in a K-dimensional seman- tic space by Ω(s i ) = u i ∈ R K , where Ω is a func- tion that takes a sentence and returns its semantic vector representation. We denote embeddings of all sentences in D with the function Ω by Ω(D).</p><p>We will return to the choice of Ω later. We propose to use a scoring function that maximizes the vol- ume of selected sentences in this semantic space:</p><formula xml:id="formula_4">score(D, y) = Volume(Ω(D), y) = Volume(Ω(S))</formula><p>In the case when K = 2, this scoring function maximizes the area of a polytope, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In the example, there exists a maximum number of sentences that can be selected such that adding more sentences does not increase the score, i.e., the set of selected sentences forms a convex hull of the set of all sentences. The sentences forming a convex hull may together be longer than L words, so we seek to maximize the volume of the summary under this constraint. There are many choices of Ω that we can use to produce sentence embeddings. As an exploratory study, we construct a vector of bigrams for each sentence, that is, s i ∈ R B , ∀i ∈ <ref type="bibr">[N ]</ref>. If bigram b is present in s i , we let s i,b be the number of doc- uments in the corpus that contain bigram b, and zero otherwise. We stack these vectors in columns to produce a matrix S ∈ R N ×B , where N is the number</p><note type="other">of sentences in the corpus and B is the number of bigrams. We then perform singular value decomposition (SVD) on S = UΣV . We use U K ∈ R N ×K as the sentence representations, where K is a parameter that specifies the number of latent dimensions. Instead of performing SVD, we can also take s i ∈ R B as our sentence repre- sentation, which makes our method resemble the bigram coverage-based summarization approach. However, this makes s i a very sparse vector. Pro- jecting to a lower dimensional space makes sense to allow the representation to incorporate informa- tion from (bigram) cooccurrences and share infor- mation across bigrams.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Volume Maximization</head><p>Given the semantic coverage scoring function in §2.3, our optimization problem is:</p><formula xml:id="formula_5">max score(S) = Volume(Ω(S))</formula><p>with respect to S subject to length(S) ≤ L For computational considerations, we propose to use a greedy algorithm that approximates the so- lution by iteratively adding a sentence that max- imizes the current semantic coverage, given that the length constraint is still satisfied. The main steps in our algorithm are as follows. We first find the sentence that is farthest from the cluster centroid and add it to S. Next, we find the sen- tence that is farthest from the first sentence and add it to S. Given a set of already selected sen- tences, we choose the next one by finding the sen- tence farthest from the subspace spanned by sen- tences already in the set. We repeat this process until we have gone through all sentences, break- ing ties arbitrarily and checking whether adding a sentence to S will result in a violation of the length constraint. This method is summarized in Algorithm 1. We note that related variants of our method for maximizing volume have appeared in Algorithm 1 Greedy algorithm for approximately maximizing the semantic volume given a budget constraint. Input: Budget constraint L, sentence representa- tions R = {u 1 , u 2 , . . . , u N } S = {}, B = {} Compute the cluster centroid c: 1</p><formula xml:id="formula_6">N N i=1 u i . p ← index of sentence that is farthest from c. S = S ∪ {s p }.</formula><p>add first sentence q ← index of sentence that is farthest from s p . S = S ∪ {s q }. Computing Distance to a Subspace Our algo- rithm involves finding a point farthest from a sub- space (except for the first and second sentences, which can be selected by computing pointwise dis- tances). In order for this algorithm to be efficient, we need this operation to be fast, since it is ex- ecuted frequently. There are several established methods to compute the distance between a point to a subspace spanned by sentences in S. For com- pleteness, we describe one method based on the Gram-Schmidt process (Laplace, 1812) here.</p><p>We maintain a set of basis vectors, denoted by B. Our first basis vector consists of one element: b 0 = uq uq , where q is the second sentence chosen above. Next, we project each candidate sentence i to this basis vector:</p><formula xml:id="formula_7">Proj b 0 (u i ) = (u i b 0 )b 0 ,</formula><p>and find the distance by computing Distance(u i , B) = u i − Proj b 0 (u i ). Once we find the farthest sentence r, we add a new basis vector B = B ∪ {b r }, where b r = ur ur and repeat this process. When there are more than one basis vectors, we find the distance by computing:</p><formula xml:id="formula_8">Distance(u i , B) = u i − b j ∈B Proj b j (u i ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluate our proposed method on the non- update portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document sum- marization problems, respectively. Each problem has 10 news articles as input; each is to be sum- marized in a maximum of L = 100 words. There are 4 human reference summaries for each prob- lem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR, §2.1) and the coverage-based summariza- tion method (CBS, §2.2). ROUGE <ref type="bibr" target="#b17">(Lin, 2004</ref>) is used to evaluate the summarization results. For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each sum- marization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix ( §2.3). Note that this also means we only need to do one SVD for each dataset. <ref type="table">Table 1</ref> shows results on the TAC-2008 and TAC- 2009 datasets. We report results for our method with K = 500 (Volume 500), and K = 600 (Vol- ume 600). We also include results for an oracle model that has access to the human reference sum- maries and extracts sentences that maximize bi- gram recall as an upper bound. Similar to previous findings, CBS is generally better than MMR. Our method outperforms other competing methods, al- though the optimal value of K is different in each dataset. The improvements with our proposed ap- proach are small in terms of R-2. This is likely because the R-2 score computes bigram overlaps, and the CBS method that directly maximizes bi- gram coverage is already a resonable approach to optimizing this metric (although still worse than the best of our methods).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Runtime comparisons In terms of inference running time, all methods perform reasonably fast. MMR is the slowest, on average it takes 0.38 sec- onds per problem, followed by our method at 0.17 seconds per problem, and CBS at 0.15 seconds per problem. However, our implementations of MMR and Algorithm 1 are in Python, whereas we use an optimzed solver from Gurobi for our CBS baseline. For preprocessing, our method is the slowest, since we need to compute sentence em- beddings using SVD. There are about 10,000 sen- tences and 3,000 bigrams for each dataset. SVD takes approximately 2.5 minutes (150 seconds) us- ing Matlab on our 12-core machine with 24GB RAM. Our method introduces another hyperpa- rameter, the number of latent dimensions K for sentence embeddings. We observe that the optimal value depends on the dataset, although a value in the range of 400 to 800 seems best. <ref type="figure" target="#fig_2">Figure 2</ref> shows R-SU4 scores on the TAC-2008 dataset as we vary K.</p><p>Other sentence projection methods We use SVD in this study for computing sentence embed- dings. As mentioned previously, our summariza-tion approach can benefit from advances in neural- network-based sentence representations <ref type="bibr" target="#b8">(Jenatton et al., 2011;</ref><ref type="bibr" target="#b13">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b9">Kalchbrenner et al., 2014</ref>). These models can also produce vec- tor representations of sentences, so Algorithm 1 can be readily applied to the learned representa- tions. Our work opens up a possibility to make summarization a future benchmark task for evalu- ating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; <ref type="bibr" target="#b5">Gillenwater et al., 2012;</ref>) in that they both seek to maxi- mize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tai- lored to encode quality in terms of representative- ness directly. In contrast, we rely on sentence em- bedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a sim- ple singular value decomposition embedding of sentences-one that is not especially tuned for this task-produces reasonably good results. We leave exploration of other sentence embedding methods to future work.</p><p>Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This re- sembles the summarization methods that jointly extracts and compresses <ref type="bibr" target="#b3">(Berg-Kirkpatrick et al., 2011;</ref><ref type="bibr" target="#b23">Woodsend and Lapata, 2012;</ref><ref type="bibr" target="#b0">Almeida and Martins, 2013)</ref>. Another alternative is a pipeline approach, where extractive summarization is fol- lowed or preceded by a sentence compression module, which can be built and tuned indepen- dent of our proposed extractive method <ref type="bibr" target="#b10">(Knight and Marcu, 2000;</ref><ref type="bibr" target="#b16">Lin, 2003;</ref><ref type="bibr" target="#b25">Zajic et al., 2007;</ref><ref type="bibr" target="#b22">Wang et al., 2013;</ref><ref type="bibr" target="#b14">Li et al., 2013)</ref>.</p><p>We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redun- dancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity.</p><p>Finally, an interesting future direction is find- ing an exact tractable solution to the volume max- imization problem (or demonstrating that one does not exist).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a summarization approach based on maximizing volume in a semantic vector space. We showed an algorithm to efficiently perform volume maximization in this semantic space. We demonstrated that our method outperforms exist- ing state-of-the-art extractive methods on bench- mark summarization datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A toy example of seven sentences projected into a two-dimensional semantic space. Consider the case when the maximum summary length is four sentences. Our scoring function is optimized by chooseing the four sentences in red as the summary, since they maximize the volume (area in two dimensions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>add second sentence b 0 = uq uq , B = B ∪ {u 0 } total length = length(s p ) + length(s q ) for i = 1, . . . , N − 2 do r ← index of sentence that is farthest from the subspace of Span(B). see text if total length + length(s r ) ≤ L then S = S ∪ {s r }. b r = ur ur , B = B ∪ {b r }. total length = total length + length(s r ) end if end for other applications, such as remote sensing (Nasci- mento and Dias, 2005; Gomez et al., 2007) and topic modeling (Arora et al., 2012; Arora et al., 2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: R-SU4 scores as we vary the number of dimensions (K) on the TAC-2008 datasets.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank anonymous reviewers for helpful sug-gestions. This work was supported by the Defense Advanced Research Projects Agency through grant FA87501420244 and by NSF grant SaTC-1330596. This work was completed while the au-thors were at CMU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast and robust compressive summarization with dual decomposition and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computing a nonnegative matrix factorization-provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of STOC</title>
		<meeting>of STOC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A practical algorithm for topic modeling with provable guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoni</forename><surname>Halpren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jointly learning to extract and compress</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGIR</title>
		<meeting>of SIGIR</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering diverse and salient threads in document collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP-CoNLL</title>
		<meeting>of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ICSI summarization system at TAC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of TAC</title>
		<meeting>of TAC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">N-findr method versus independent component analysis for lithological identification in hyperspectral imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le Borgne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Allemand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Delacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ledru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="5315" to="5338" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Proximal methods for hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodolphe</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gullaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2297" to="2334" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statisticsbased summarization-step one: Sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Determinantal point processes for machine learning. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="123" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Theorie analytique des probabilites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Simon</forename><surname>Laplace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1812" />
			<publisher>Courcier</publisher>
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Document summarization via guided sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuliang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving summarization performance by sentence compression-A pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop on Information Retrieval with Asian Language</title>
		<meeting>of Workshop on Information Retrieval with Asian Language</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rouge: a package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL Workshop on Text Summarization Branches Out</title>
		<meeting>of the ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Summarization with a joint model for sentence extraction and compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACL Workshop on Integer Linear Programming for Natural Language Processing</title>
		<meeting>of the ACL Workshop on Integer Linear Programming for Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECIR</title>
		<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vertex component analysis: A fast algorithm to unmix hyperspectral data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M Bioucas</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Transaction on Geoscience and Remote Sensing</title>
		<meeting>of IEEE Transaction on Geoscience and Remote Sensing</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="898" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A sentence compression based framework to query-focused multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Hema Raghavan Vittorio Castelli Radu Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiple aspect summarization using integer linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-document summarization by maximizing informative content-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-candidate reduction: Sentence compression as a tool for document summarization tasks. Information Processing and Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1549" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
