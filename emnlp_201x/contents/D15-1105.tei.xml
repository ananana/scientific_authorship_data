<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Much Information Does a Human Translator Add to the Original?</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute Department of Computer Science</orgName>
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Much Information Does a Human Translator Add to the Original?</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We ask how much information a human translator adds to an original text, and we provide a bound. We address this question in the context of bilingual text compression: given a source text, how many bits of additional information are required to specify the target text produced by a human translator? We develop new compression algorithms and establish a benchmark task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text compression exploits redundancy in human language to store documents compactly, and trans- mit them quickly. It is natural to think about com- pressing bilingual texts, which have even more re- dundancy:</p><p>"From an information theoretic point of view, accurately translated copies of the original text would be expected to con- tain almost no extra information if the original text is available, so in princi- ple it should be possible to store and transmit these texts with very little ex- tra cost." <ref type="bibr" target="#b25">(Nevill and Bell, 1992)</ref> Of course, if we look at actual translation data <ref type="figure" target="#fig_1">(Figure 1</ref>), we see that there is quite a bit of unpre- dictability. But the intuition is sound. If there were a million equally-likely translations of a short sen- tence, it would only take us log 2 (1m) = 20 bits to specify which one.</p><p>By finding and exploiting patterns in bilingual data, we want to provide an upper bound for this question: How much information does a human translator add to the original? We do this in the context of building a practical compressor for bilingual text.  We adopt the same scheme used in mono- lingual text compression benchmark evaluations, such as the Hutter Prize <ref type="bibr" target="#b13">(Hutter, 2006</ref>), a com- petition to compress a 100m-word extract of En- glish Wikipedia. A valid entry is an executable, or self-extracting archive, that prints out Wikipedia, byte-for-byte. Decompression code, dictionaries, and/or other resources must be embedded in the executable-we cannot assume that the recipient of the compressed file has access to those re- sources. This view of compression goes by the name of algorithmic information theory (or Kol- mogorov complexity).</p><p>Any executable is permitted. For example, if our job were to compress the first million digits of π, then we might submit a very short piece of code that prints those digits. The brevity of that compression would demonstrate our understand- ing of the sequence. Of course, in our application, we will find it useful to develop generic algorithms that can compress any text.</p><p>Our approach will be as follows. Given a bilin- gual text (file1 and file2), we develop this com- pression interface:</p><p>% compress file1 &gt; file1.exe % bicompress file2 file1 &gt; file2.exe</p><p>The second command compresses file2 while looking at file1. We take the size of file1.exe <ref type="table">English  Uncompressed size  324.9 Mb  294.5 Mb  Word tokens  57,068,133 54,364,566  Vocabulary size  195,314  140,340  Distinct word cooc  93,184,127  Segment pairs  1,965,</ref>  as the amount of information in the original text. We bound how much information the translator adds to the original by:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spanish</head><p>|file2.exe| / |file1.exe| We can say that bilingual compression is more ef- fective that monolingual compression if: |file2.exe| &lt; |file3.exe|, where</p><formula xml:id="formula_0">% compress file2 &gt; file3.exe</formula><p>Our decompression interface is:</p><formula xml:id="formula_1">% file1.exe &gt; file1 % file2.exe file1 &gt; file2</formula><p>The second command decompresses file2 while looking at (uncompressed) file1. The contributions of this paper are:</p><p>1. We provide a new quantitative bound for how much information a translator adds to an orig- inal text. 2. We present practical software to compress bilingual text with compression rates that ex- ceed the previous state-of-the-art. 3. We set up a public benchmark bilingual text compression challenge to stimulate new re- searchers to find and exploit patterns in bilin- gual text. Ultimately, we want to feed those ideas into practical machine translation sys- tems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>We propose the widely accessible Spanish/English Europarl corpus v7 ( <ref type="bibr" target="#b16">Koehn, 2005</ref>) as a benchmark for bilingual text compression ( <ref type="figure" target="#fig_2">Figure 2</ref>). Por- tions of this large corpus have been used in pre- vious compression work ( <ref type="bibr" target="#b29">Sánchez-Martínez et al., 2012</ref>). The Spanish side is in UTF-8. For En- glish, we have removed accent marks and further eliminated all but the 95 printable ASCII charac- ters ( <ref type="bibr" target="#b5">Brown et al., 1992)</ref>, plus newline. Our task is to compress the data "as is": un- <ref type="table">Spanish  English  Uncompressed size  32.3 Mb  29.3 Mb  Word tokens  5,682,667 5,426,131  Vocabulary size  73,726  45,423  Distinct word cooc  21,231,874  Segment pairs</ref> 196,573 Ave. segment length 28.9 27.6 (word tokens) Longest segment 733 682 (word tokens) <ref type="figure">Figure 3</ref>: Small Europarl Spanish/English corpus.</p><p>tokenized, but already segment aligned. We also include a tokenized version with 334 manually word-aligned segment pairs ( <ref type="bibr" target="#b17">Lambert et al., 2005</ref>) distributed throughout the corpus.</p><p>For rapid development and testing, we have ar- ranged a smaller corpus that is 10% the size of the full corpus ( <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Monolingual compression</head><p>Compression captures patterns in data. Language modeling also captures patterns, but at first blush, these two areas seem distinct. In compression, we seek a small executable that prints out a text, while in language modeling, we seek an executable that assigns low perplexity to held-out test data. 1 Ac- tually, the two areas have much more in common, as a review of compression algorithms reveals.</p><p>Huffman coding. A well-known compression technique is to create a binary Huffman tree whose leaves are characters in the text, 2 and whose edges are labeled 0 or 1 (Huffman and others, 1952). The tree is arranged so that frequent characters have short binary codes (edge sequences). It is very im- portant that the Huffman tree for a particular text be included at the beginning of the compressed file, so that decompression knows how to process the compressed bit string.</p><p>Adaptive Huffman. Actually, we can avoid shipping the Huffman tree inside the compressed file, by building the tree adaptively, as the com- pressor processes the input text. If we start with a uniform distribution, the first few characters may not compress very well, but soon we will converge onto a good tree and good compression. It is very important that the decompressor exactly recapitu- late the same sequence of Huffman trees that the compressor made. It can do this by counting char- acters as it outputs them, just as the compressor counted characters as it consumed them.</p><p>Adaptive compression can also nicely accom- modate shifting topics in text, if we give higher counts to recent events. By its single-pass nature, it is also good for streaming data.</p><p>Arithmetic coding. Huffman coding exploits a predictive unigram distribution over the next character. If we use more context, we can make sharper distributions. An n-gram table is one way to map contexts onto predictions.</p><p>How do we convert good predictions into good compression? The solution is called arithmetic coding ( <ref type="bibr" target="#b28">Rissanen and Langdon Jr., 1981;</ref><ref type="bibr" target="#b33">Witten et al., 1987)</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> sketches the technique. We produce context-dependent probability inter- vals, and each time we observe a character, we move to its interval. Our working interval be- comes smaller and smaller, but the better our pre- dictions, the wider it stays. A document's com- pression is the shortest bit string that fits inside the final interval. In practice, we do the bit-coding as we navigate probability intervals.</p><p>Arithmetic coding separates modeling and com- pression, making our job similar to language mod- eling, where we use try to use context to predict the next symbol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PPM</head><p>PPM is the most well-known adaptive, predic- tive compression technique <ref type="bibr" target="#b7">(Cleary and Witten, 1984)</ref>. PPM updates character n-gram tables (usu- ally n=1..5) as it compresses. In a given context, an n-gram table may predict only a subset of char- acters, so PPM reserves some probability mass for an escape (ESC), after which it executes a hard backoff to the (n-1)-gram table. In PPMA, P(ESC) is 1/(1+D), where D is the number of times the context has been seen. PPMB uses q/D, where q is the number of distinct character types seen in the context. PPMC uses q/(q+D), aka Witten-Bell. PPMD uses q/2D. PPM* uses the shortest previously-seen deter- ministic context, which may be quite long. If there is no deterministic context, PPM* goes to the longest matching context and starts PPMD. In- stead of the longest context, PPMZ rates all con- texts between lengths 0 and 12 according to each context's most probable character. PPMZ also im- plements an adaptive P(ESC) that combines con- text length, number of previous ESC in the con- text, etc.</p><p>We use our own C++ implementation of PPMC for monolingual compression experiments in this paper. When we pass over a set of characters in favor of ESC, we remove those characters from the hard backoff.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PAQ</head><p>PAQ ( <ref type="bibr" target="#b22">Mahoney, 2005</ref>) is a family of state-of-the- art compression algorithms and a perennial Hutter Prize winner. PAQ combines hundreds of mod- els with a logistic unit when making a prediction. This is most efficient when predictions are at the bit-level instead of the character-level. The unit's model weights are adaptively updated by:</p><formula xml:id="formula_2">w i ← w i + ηx i (correct − P(1))</formula><p>, where <ref type="formula">(1)</ref>) η = fixed learning rate P i (1) = ith model's prediction PAQ models include a character n-gram model that adapts to recent text, a unigram word model (where word is defined as a subsequence of char- acters with ASCII &gt; 32), a bigram model, and a skip-bigram model.</p><formula xml:id="formula_3">x i = ln(P i (1)/(1 − P i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bilingual Compression: Prior Work</head><p>Nevill and Bell (1992) introduce the concept but actually carry out experiments on paraphrase cor- pora, such as different English versions of the Bible.</p><p>Conley and <ref type="bibr" target="#b8">Klein (2008)</ref> and <ref type="bibr" target="#b9">Conley and Klein (2013)</ref> compress a target text that has been word- aligned to a source text, to which they add a lem- matizer and bilingual glossary. They obtain a 1%- 6% improvement over monolingual compression, without counting the cost of auxiliary files needed for decompression. , , <ref type="bibr" target="#b1">Adiego et al. (2010)</ref> rewrite bilingual text by first interleaving source words with their trans- lations, then compressing this sequence of bi- words. <ref type="bibr" target="#b29">Sánchez-Martínez et al. (2012)</ref> improve the interleaving scheme and include offsets to enable decompression to reconstruct the original word order. They also compare several character- based and word-based compression schemes for biword sequences. On Spanish-English Europarl data, they reach an 18.7% compression rate on word-interleaved text, compared to 20.1% for con- catenated texts, a 7.2% improvement.</p><p>Al-Onaizan et al. (1999) study the perplexity of learned translation models, i.e., the probabil- ity assigned to the target corpus given the source corpus. They observed iterative training to im- prove training-set perplexity (as guaranteed) but degrade test-set perplexity. They hypothesized that an increasingly tight, unsmoothed translation dictionary might exclude word translations needed to explain test-set data. Subsequently, research moved to extrinsic evaluation of translation mod- els, in the context of end-to-end machine transla- tion. <ref type="bibr" target="#b10">Foster et al. (2002)</ref> and others have used predic- tion to propose auto-completions to speed up hu- man translation. As we have seen, prediction and compression are highly related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Predictive Bilingual Compression</head><p>Our algorithm compresses target-language file2 while looking at source-language file1: % bicompress file2 file1 &gt; file2.exe</p><p>To make use of arithmetic coding, we consider the task of predicting the next target character, given the source sentence and target string so far: <ref type="bibr">3</ref> P(e j |f 1 . . . f l , e 1 . . . e j−1 ) If we are able to accurately predict what a human translator will type next, then we should be able to build a good machine translator. Here is an exam- ple of the task:</p><p>Spanish  <ref type="figure">Figure 5</ref>: Compressing a file of (unidirectional) automatic Viterbi word alignments computed from our large Spanish/English corpus (sentences less than 50 words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word alignment</head><p>Let us first work at the word level instead of the character level. If we are predicting the jth English word, and we know that it translates f i ("aligns to f i "), and if f i has only a handful of translations, then we may be able to specify e j with just a few bits. We may therefore suppose that a set of Viterbi word alignments may be useful for compression <ref type="bibr" target="#b8">(Conley and Klein, 2008;</ref><ref type="bibr">SánchezMartínez et al., 2012</ref>).</p><p>We consider unidirectional alignments that link each target position j to a single source position i (including the null word at i = 0). Such align- ments can be computed automatically using EM ( <ref type="bibr" target="#b6">Brown et al., 1993)</ref>, and stored in one of two for- mats:</p><p>Absolute: 1 2 5 5 7 0 3 6 . . . Relative: +1 +1 +3 0 +2 null -4 +3 . . . In order to interpret the bits produced by the compressor, our decompressor must also have ac- cess to the same Viterbi alignments. Therefore, we must include those alignments at the beginning of the compressed file. So let's compress them too.</p><p>How compressible are alignment sequences? <ref type="figure">Figure 5</ref> gives results for Viterbi alignments de- rived from our large parallel Spanish/English cor- pus. First, some interesting facts:</p><p>• Huffman works better on relative offsets, be- cause the common "+1" gets a short bit code. • PPMC's use of context makes it impressively insensitive to alignment format.</p><p>• PPMC beats Huffman on relative offsets.</p><p>This would not happen if relative offset inte- gers were independent of one another, as as- sumed by <ref type="bibr" target="#b6">(Brown et al., 1993)</ref> and ( <ref type="bibr" target="#b32">Vogel et al., 1996)</ref>. Bigram statistics bear this out: P(+1 | -2) = 0.20 P(+1 | +1) = 0.59 P(+1 | -1) = 0.20 P(+1 | +2) = 0.49 P(+1 | 0) = 0.52 So this small compression experiment already suggests that translation aligners might want to model more context than just P(offset).</p><p>However, the main point of <ref type="figure">Figure 5</ref> is that the compressed alignment file requires 12.4 Mb! This is too large for us to prepend to our compressed file, for the sake of enabling decompression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Translation dictionary</head><p>Another approach is to forget Viterbi alignments and instead exploit a probabilistic translation dic- tionary table t(e|f ). To predict the next target word e j , we admit the possibility that e j might be translating any of the source tokens. IBM Model 2 ( <ref type="bibr" target="#b6">Brown et al., 1993</ref>) tells us how to do this:</p><p>Given f 1 . . . f l :</p><p>1. Choose English length m (m|l)</p><p>2. For j = 1..m, choose alignment aj a(a j |j, l) 3. For j = 1..m, choose translation ej t(e j |f a j ) which, via the "IBM trick" implies:</p><formula xml:id="formula_4">P(e 1 . . . e m |f 1 . . . f l ) = (m|l) m j=1 l i=0 a(i|j, l)t(e j |f i )</formula><p>In compression, we must predict English words in- crementally, before seeing the whole string. Fur- thermore, we must predict P(ST OP ) to end the English sentence. We can adapt IBM Model 2 to make incremental predictions:</p><formula xml:id="formula_5">P(ST OP |f 1 . . . f l , e 1 . . . e j−1 ) ∼ P(ST OP |j, l) = (j − 1|l)/ max k=j−1 (k|l) P(e j |f 1 . . . f l , e 1 . . . e j−1 ) ∼ P(e j |f 1 . . . f l ) = [1 − P(ST OP |j, l)] l i=0</formula><p>a(i|j, l)t(e j |f i ) We can train t, a, and on our bilingual text us- ing EM ( <ref type="bibr" target="#b6">Brown et al., 1993)</ref>. However, the t-table is still too large to prepend to the compressed En- glish file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adaptive translation modeling</head><p>Instead, inspired by PPM, we build up transla- tion tables in RAM, during a single pass of our compressor. Our decompressor then rebuilds these same tables, in the same way, in order to interpret the compressed bit string.</p><p>Neal and Hinton (1998) describe online EM, which updates probability tables after each train- ing example. <ref type="bibr" target="#b19">Liang and Klein (2009)</ref> and <ref type="bibr" target="#b18">Levenberg et al. (2010)</ref> apply online EM to a number of language tasks, including word alignment. Here we concentrate on the single-pass case.</p><p>We initialize a uniform translation model, use it to collect fractional counts from the first segment pair, normalize those counts to probabilities, use those new probabilities to collect fractional counts from the second segment pair, and so on. Because we pass through the data only once, we hope to converge quickly to high-quality tables for com- pressing the bulk of the text.</p><p>Unlike in batch EM, we need not keep sepa- rate count and probability tables. We only need count tables, including summary counts for nor- malization groups, so memory savings are signif- icant. Whenever we need a probability, we com- pute it on the fly. To avoid zeroes being immedi- ately locked in, we invoke add-λ smoothing every time we compute a probability from counts: <ref type="bibr">4</ref> t(e|f ) = count(e,f )+λt</p><formula xml:id="formula_6">count(f )+λt|V E | a(i|j, l) = count(i,j,l)+λa count(j,l)+λa(l+1)</formula><p>where |V E | is the size of the English vocabulary. We determine |V E | via a quick initial pass through the data, then include it at the top of our com- pressed file.</p><p>In batch EM, we usually run IBM Model 1 for a few iterations before Model 2, gripped by an atavistic fear that the a probabilities will enforce rigid alignments before word co-occurrences have a chance to settle in. It turns out this fear is jus- tified in online EM! Because the a table initially learns to align most words to null, we smooth it more heavily (λ a = 10 2 , λ t = 10 −4 ).</p><p>We also implement a single-pass HMM align- ment model ( <ref type="bibr" target="#b32">Vogel et al., 1996</ref>). In the IBM mod- els, we can either collect fractional counts after we have compressed a whole sentence, or we can do it word-by-word. In the HMM model, alignment choices are no longer independent of one another:</p><p>Given f 1 . . . f l :</p><p>1. Choose English length m w/prob (m|l)</p><p>2. For j = 1..m:</p><p>2a. set aj to null w/prob p1, or</p><formula xml:id="formula_7">2b. choose non-null aj w/prob (1 − p1)o(aj − a k )</formula><p>3. For j = 1..m, choose translation ej w/prob t(ej|fa j )</p><p>In the expression o(a j − a k ), k is the maximum English index (k &lt; j) such that a k = 0. The relative offset o-table learns to encourage adjacent English words to align to adjacent Spanish words. Batch HMM performs poorly under uniform initialization, with two causes of failure. First, EM training sets o(0) too high, leading to absolute alignments like "1 2 2 2 5 5 5 5 . . . ". We avoid Against silver standard (Batch EM) Against gold standard <ref type="table">(Human)  IBM1 IBM2  HMM IBM1 IBM2  HMM  Batch EM  100.0</ref>   <ref type="figure">Figure 6</ref>: Word alignment f-scores. Batch EM for IBM 1 is run for 5 iterations; Batch IBM2 adds 5 further iterations of IBM2; Batch HMM adds a further 5 iterations of HMM. Online EM is single- pass. Against the silver standard, alignments are unidirectional; against gold, they are bidirectional and symmetrized with grow-dial-final ( <ref type="bibr" target="#b14">Koehn et al., 2003</ref>). First and last 50% report on different portions of the corpus. Reordered is on segment pairs ordered short to long. All runs exclude segment pairs with segments longer than 50 words.</p><p>this with a standard schedule of 5 IBM1 iterations, 5 IBM2 iterations, then 5 HMM iterations. How- ever, HMM still learns a very high value for p 1 , aligning most tokens to null, so we fix p 1 = 0.1 for the duration of training.</p><p>Single-pass, online HMM suffers the same two problems, both solved when we smooth differen- tially (λ o = 10 2 , λ t = 10 −4 ) and fix p 1 = 0.1.</p><p>Two quick asides before we examine the effec- tiveness of our online methods:</p><p>• Translation researchers often drop long seg- ment pairs that slow down HMM model pro- cessing. In compression, we cannot drop any of the text. Therefore, if the source segment contains more than 50 words, we use only monolingual PPMC to compress the target. This affects 26.5% of our word tokens.</p><p>• We might assist an online aligner by permut- ing our n segment pairs to place shorter, less ambiguous ones at the top. However, we would have to communicate the permutation to the decompressor, at a prohibitive cost of log 2 (n!)/(8 · 10 6 ) = 4.8 Mb. We next look at alignment accuracy (f-score) on our large Spanish/English corpus ( <ref type="figure">Figure 6</ref>). We evaluate against both a silver standard (Batch EM Viterbi alignments 5 ) and a gold standard of 334 human-aligned segment pairs distributed through- out the corpus. We see that online methods gener- ate competitive translation dictionaries. Because single-pass alignment is significantly faster than traditional multi-pass, we also investigate its im- pact on an overall Moses pipeline for phrase-based <ref type="bibr">5</ref> We confirm that our Batch HMM implementation gives f-scores (f=70.2, p=80.4, r=62.3) similar to GIZA++ (f=71.2, p=85.5, r=61.0), and its differently parameterized HMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alignment</head><p>Test  machine translation ( <ref type="bibr" target="#b15">Koehn et al., 2007)</ref>. <ref type="figure" target="#fig_4">Figure 7</ref> shows that we can achieve competitive translation accuracy using fast, single-pass alignment, speed- ing up the system development cycle. For this use case, we can get an additional +0.3 alignment f- score (just as fast) if we print Viterbi alignments in a second pass instead of during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Word tokenization</head><p>We now want our continuously-improving trans- lation model (TM) to predict target text, and to combine its predictions with PPM's. For that to happen, our TM will need to predict the exact text, including spurious double-spaces, how parenthe- ses combine with quotation marks, and so on. We devise a tokenization scheme that records spacing information in the word tokens, which al- lows us to recover the original text uniquely. First, we identify word tokens as subsequences of [a-zA- Z]*, [0-9]*, and [other]*, appending to each token the number of spaces following it (e.g., "...@2"). Next, we remove all "@1", which leaves unique recoverability intact. Finally, we move any suffix on an alpha-numeric word i to become a prefix on a non-alpha-numeric word i + 1. This reduces the vocabulary size for TM learning. An example:</p><p>"String-theory?" he asked. &lt;=&gt; S@0 "@0 String@0 -@0 theory@0 ?@0 "@1 he@2 asked@0 .@0 &lt;=&gt; S@0 "@0 String@0 -@0 theory@0 ?@0 " he@2 asked@0 .@0 &lt;=&gt; S@0 "@0 String @0-@0 theory @0?@0 " he@2 asked @0.@0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Predicting target words</head><p>Under this tokenization scheme, we now ask our TM to give us a probability distribution over pos- sible next words. The TM knows the entire source word sequence f 1 ...f l and the target words e 1 ...e j−1 seen so far. As candidates, we consider target words that can be produced, via the current t-table, from any (non-NULL) source words with probability greater than 10 −4 .</p><p>For HMM, we compute a prediction lattice that gives a distribution over possible source alignment positions for the current word we are predicting. Intuitively, the prediction lattice tells us "where we currently are" in translating the source string, and it prefers translations of source words in that vicin- ity. We efficiently reuse the lattice as we make predictions for each subsequent target word.</p><p>To make the TM's prediction more accurate, we weight its prediction for each word with a smoothed, adapted English bigram word language model (LM). This discourages the TM from trying to predict the first character of a word by simply using the most frequent source words. We found that exponentiating the LM's score by 0.2 before weighting keeps it from overpowering the HMM predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Predicting target characters</head><p>To convert word predictions into character predic- tions, we combine scores for words that share the next character. For example, if the TM predicts "monkey 0.4, car 0.3, cat 0.2, dog 0.1", then we have "P(c) 0.5, P(m) 0.4, P(d) 0.1". Addition- ally, we restrict ourselves to words prefixed by the portion of e j already observed. The TM predicts the space character when a predicted word fully matches the observed prefix.</p><p>We also adjust PPM to produce a full distribu- tion over the 96 possible next characters. PPM  normally computes a distribution over only char- acters previously seen in the current context (plus ESC). We now back off to the lowest context for every prediction.</p><p>We interpolate PPM and TM probabilities:</p><formula xml:id="formula_8">P(e k |f 1 . . . f l , e 1 . . . e k−1 ) = µ P P P M (e k |e 1 . . . e k−1 )+ (1 − µ) P T M (e k |f 1 . . . f l , e 1 .</formula><p>. . e k−1 ) We adjust µ dynamically based on the relative con- fidence of the models: µ = max(P P M ) 2.5 max(P P M ) 2.5 +max(HM M ) 2.5</p><p>Here, max(model) refers to the highest probabil- ity assigned to any character in the current context by the model. This yields better compression rates than simply setting µ to a constant. When the TM is unable to extend a word, we set µ = 1. <ref type="figure">Figure 8</ref> shows that monolingual PPM compresses the Spanish side of our corpus to 15.8% of the original. <ref type="figure" target="#fig_5">Figure 9</ref> (Main results) shows results for the English side of the corpus. Monolingual PPM compresses to 16.5%, while our HMM-based bilingual compression compresses to 11.9%. <ref type="bibr">6</ref> We can say that a human translation is charac- terized by an additional 0.95 bits per byte on top of the original, rather than the 1.32 bits per byte we   would need if the English were independent text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Assuming our Spanish compression is good, we can also say that the human translator produces at most 68.1% (35.0/51.4) of the information that the original Spanish author produced. Intuitively, we feel this bound is high and should be reduced with better translation modeling. <ref type="figure" target="#fig_5">Figure 9</ref> also reports our Shannon game exper- iments in which bilingual humans guessed subse- quent characters of the English text. As suggested by Shannon, we upper-bound bpb as the cross- entropy of a unigram model over a human guess sequence (e.g., 1 1 2 5 17 1 1 . . . ), which records how many guesses it took to identify each subse- quent English character, given context. For a 502- character English sequence, a team of four bilin- guals working together gave us an upper-bound bpb of 0.51. This team had access to the original Spanish, plus a Google translation. Monolinguals guessing on the same data (minus the Spanish and Google translation) yielded an upper-bound bpb of 1.61. These human-level models indicate that hu- man translators are actually only adding ∼ 32% more information on top of the original, and that our current translation models are only capturing some fraction of this redundancy. 7 <ref type="figure" target="#fig_1">Figure 10</ref> shows compression of the entire bilingual corpus, allowing us to compare with the previous state-of-the-art ( <ref type="bibr" target="#b29">Sánchez-Martínez et al., 2012)</ref>, which compresses a single, word- interleaved bilingual corpus. It shows how PPMC does on a concatenated Spanish/English file.</p><p>Uncompressed English (294.5 Mb) is 90.6% the size of uncompressed Spanish (324.9 Mb). Huff- man narrows this gap to 93.0%, and PPM nar- rows it further to 94.4%, consistent with <ref type="bibr" target="#b3">Behr et al. (2003)</ref> and <ref type="bibr" target="#b20">Liberman (2008)</ref>. Spanish redun- dancies like adjective-noun agreement and bal- anced question marks ("¿ . . . ?") may remain un- exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have created a bilingual text compression chal- lenge web site. 8 This web site contains standard bilingual data, specifies what a valid compression is, and maintains benchmark results.</p><p>There are many future directions to pursue. First, we would like to develop and exploit better predictive translation modeling. We have so far adapted machine translation technology circa only 1996. For example, the HMM alignment model cannot "cross off" a source word and stop trying to translate it. Also possible are phrase-based trans- lation, neural nets, or as-yet-unanticipated pattern- finding algorithms. We only require an executable that prints the bilingual text.</p><p>Our current method requires segment-aligned input. To work with real-life bilingual corpora, the compressor should take care of segment align- ment, in a way that allows decompression back to the original text. Similarly, we are currently re- stricted to texts written in the Latin alphabet, per our definition of "word."</p><p>More broadly, we would also like to import more compression ideas into NLP. Compression has so far appeared sporadically in NLP tasks like native language ID <ref type="bibr" target="#b4">(Bobicev, 2013</ref>), text in- put methods <ref type="bibr" target="#b27">(Powers and Huang, 2004</ref>), word segmentation ( <ref type="bibr" target="#b31">Teahan et al., 2000;</ref><ref type="bibr" target="#b30">Sornil and Chaiwanarom, 2004;</ref><ref type="bibr" target="#b12">Hutchens and Alder, 1998</ref>), alignment ( <ref type="bibr" target="#b21">Liu et al., 2014)</ref>, and text categoriza- tion <ref type="bibr">(Caruana &amp; Lang, unpub. 1995)</ref>.</p><p>Translation researchers may also view bilingual compression as an alternate, reference-free evalu- ation metric for translation models. We anticipate that future ideas from bilingual compression can be brought back into translation. Like <ref type="bibr" target="#b5">Brown et al. (1992)</ref>, with their gauntlet thrown down and fury of competitive energy, we hope that cross- fertilizing compression and translation will bring fresh ideas to both areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>上</head><label></label><figDesc>个 星 期 的 战 斗 至 少 夺 取12个 人 的 生 命 。 At least 12 people were killed in the battle last week. Last week's fight took at least 12 lives. The fighting last week killed at least 12. The battle of last week killed at least 12 persons. At least 12 people lost their lives in last week's fighting. At least 12 persons died in the fighting last week. At least 12 died in the battle last week. At least 12 people were killed in the fighting last week. During last week's fighting, at least 12 people died. Last week at least twelve people died in the fighting. Last week's fighting took the lives of twelve people.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Eleven human translations of the same source sentence (LDC2002T01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Large Europarl Spanish/English corpus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Arithmetic coding.</figDesc><graphic url="image-1.png" coords="3,72.00,62.81,216.01,141.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Fast, single-pass HMM alignment yields competitive Spanish-English Moses phrasebased translation accuracy, as measured by Bleu (Papineni et al., 2002). In-domain (Europarl) and out-of-domain (SMAT-07 News Commentary) tune/test sets each consist of approximately 1000 sentences, all longer than 50 words to avoid overlap with training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Main results. Compression of the English side of the bilingual corpus. Bilingual compression improves results. For Shannon game studies, bpb are estimated as cross-entropies of ngram models fitted to human guess sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>File</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Compression of Spanish plus English. All methods are run on a single file of Spanish concatenated with English, except for "Bilingual (this paper)," which records the sum of (1) Spanish compression and (2) English-given-Spanish compression. Comparative numbers copied from Sánchez-Martínez et al (2012) are for a different subset of Europarl data.</figDesc></figure>

			<note place="foot" n="1"> File size has advantages, as perplexity computations are often buggy, and they usually gloss over how probability is apportioned to out-of-vocabulary items. 2 Or other symbols, such as words, bytes, or unicode sequences.</note>

			<note place="foot" n="3"> We predict e from f in this paper, reversed from Brown et al. (1993), who predict f from e.</note>

			<note place="foot" n="4"> In their online EM Model 1 aligner, Liang and Klein (p.c.) skirt the smoothing issue by running an epoch of batch EM to initialize a full set of probabilities before starting.</note>

			<note place="foot" n="6"> For this result, we divide the English corpus into two pieces and compress them in parallel, and we further increase the sentence length threshold from 50 to 60, incurring a speed penalty. Our fictional Weissman score is 0.676.</note>

			<note place="foot" n="7"> Machine models can also generate guess sequences, and we see that entropy of a 30m-character PPMC guess sequence (1.43) upper-bounds actual PPMC bpb (1.28).</note>

			<note place="foot" n="8"> www.isi.edu/natural-language/compression</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by a USC Provost Fel-lowship and ARO grant W911NF-10-1-0533.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A two-level structure for compressing aligned bitexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquín</forename><surname>Adiego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><forename type="middle">R</forename><surname>Brisaboa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Martínez-Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">String Processing and Information Retrieval</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modelling parallel texts for boosting compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquín</forename><surname>Adiego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Martínez-Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><forename type="middle">E</forename><surname>Hoyos-Torío</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conference (DCC)</title>
		<meeting>Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Curin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jahr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Melamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz-Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Purdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
		<ptr target="http://bit.ly/1u9jJsx" />
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating and comparing entropies across written natural languages using PPM compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Fossum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conference (DCC)</title>
		<meeting>Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Native language identification with PPM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><surname>Bobicev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An estimate of an upper bound for the entropy of English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J Della</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data compression using adaptive coding and partial string matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cleary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="396" to="402" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using alignment for multilingual text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><forename type="middle">S</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><forename type="middle">T</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Foundations of Computer Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="89" to="101" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved alignment-based algorithm for multilingual text compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><forename type="middle">S</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><forename type="middle">T</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics in Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="153" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">User-friendly text prediction for translators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lapalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A method for the construction of minimum redundancy codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IRE</title>
		<meeting>IRE</meeting>
		<imprint>
			<date type="published" when="1952" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1098" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding structure via compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">L</forename><surname>Hutchens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael D Alder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning</title>
		<meeting>Joint Conferences on New Methods in Language essing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">50,000 Euro prize for compressing human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="http://prize.hutter1.net.Accessed" />
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2015" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL Poster and Demo Session</title>
		<meeting>ACL Poster and Demo Session</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MT Summit X</title>
		<meeting>MT Summit X</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Guidelines for word alignment evaluation and manual alignment. Language Resources and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adrì A De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mariño</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stream-based translation models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="394" to="402" />
		</imprint>
	</monogr>
	<note>Proc. NAACL</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online EM for unsupervised models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Is English more efficient than Chinese after all?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
		<ptr target="http://languagelog.ldc.upenn.edu/nll/?p=93.Accessed" />
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2015" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experiments with a PPM compressionbased method for English-Chinese bilingual sentence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Teahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adaptive weighting of context models for lossless data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Florida Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report CS-2005-16</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the use of word alignments to enhance bitext compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Martínez-Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adiego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sánchezmartínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De La Fuente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Data Compression Conference (DCC)</title>
		<meeting>Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Compression of parallel texts. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Nevill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="781" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive compression-based approach for chinese pinyin input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Martin Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Hu</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third SIGHAN Workshop on Chinese Language Learning</title>
		<meeting>Third SIGHAN Workshop on Chinese Language Learning</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal modeling and coding. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorma</forename><surname>Rissanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glen G Langdon</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized biwords for bitext compression and translation spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><surname>Sánchez-Martínez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><forename type="middle">C</forename><surname>Carrasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Martínez-Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Adiego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="389" to="418" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Combining prediction by partial matching and logistic regression for thai word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohm</forename><surname>Sornil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paweena</forename><surname>Chaiwanarom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING, page 1208. Association for Computational Linguistics</title>
		<meeting>COLING, page 1208. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A compression-based algorithm for Chinese word segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>William John Teahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodger</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Mcnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="393" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">HMM-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
