<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Recombination for Efficient Decoding of Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Information and Communications Technology (NICT)</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
							<email>zhaohai@cs.sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Recombination for Efficient Decoding of Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4785" to="4790"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4785</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recom-bination in NMT decoding based on the concept of the &quot;equivalence&quot; of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, end-to-end Neural Machine Translation (NMT) models <ref type="bibr" target="#b15">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> have achieved notable success. A remarkable characteristic of NMT is that the de- coder, which is typically implemented using Re- current Neural Network (RNN), can capture the features of the entire decoding history. This model * Zhisong Zhang was a graduate student at SJTU and a re- search intern at NICT when conducting this work. This work is partially supported by the program "Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technol- ogy" of MIC, Japan. Hai Zhao was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foun- dation of <ref type="bibr">China (No. 61672343 and No. 61733011)</ref> cities have established ...</p><p>according to some sources , the workers of  search. The hidden layers of the partial hypotheses ending with "cities" correspond to the nodes box ed in <ref type="figure" target="#fig_1">Figure 1</ref> (only three hypotheses are listed for brevity). The negative log probabilities calculated by the model for the words predicted after "cities" are given in parentheses. does not depend on any independence assump- tions and treats sequences with different prefixes as totally different hypotheses. However, many of the NMT output sequences are quite similar and they typically contain only local differences that do not influence future decoding significantly. <ref type="table" target="#tab_1">Table 1</ref> and <ref type="figure" target="#fig_1">Figure 1</ref> present an example of such pattern of local differences in NMT decoding. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the three partial hypotheses that Algorithm 1 Merging for Beam Search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Require: list of sorted candidates C; beam size k;</head><p>equivalence function Eq. Ensure: list of candidates surviving in the beam: C .</p><p>1: C = [ ] 2: # Scan according to the sorted order.</p><p>3: for c in C: 4: merge flag = False 5:</p><p># Check previous surviving states for merging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>for s in C : 7:</p><p># Check with candidate merger states. 8:</p><p>for s in sequence(s): 9:</p><p>if Eq(c, s ) and score(c)&lt;score(s ): 10: merge flag = True 11:</p><p># Pruning by the merger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>if not merge flag: 13:</p><p>C .append(c) 14:</p><p># Pruning by the beam size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>if len(C ) &gt;= k: 16: break 17: return C end with "cities" share similar patterns. Firstly, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>, their hidden layer features are close in the latent space. Moreover, for future pre- dictions, the model predicts identical sequences and gives similar scores for them. Although go- ing through different paths, these partial hypothe- ses appear to be similar or likely equivalent.</p><p>Intuitively, for efficiency, we do not need to ex- pand all of these partial hypotheses (states) since they have similar future predictions. In fact, this corresponds to the idea of hypothesis recombi- nation (also known as state merging, which will be used interchangeably) from traditional Phrase- Based Statistical Machine Translation (PBSMT) ( <ref type="bibr" target="#b5">Koehn et al., 2003)</ref>. Given a method to find mergeable states, we can employ recombination in NMT decoding as well.</p><p>In this paper, we adopt the mechanism of recombination in NMT decoding based on the definition of "equivalence" of partial hypothe- ses. Heuristically, we try a simple n-gram suf- fix based equivalence function and apply it to beam search without adding any neural computa- tion cost. Through experiments on two large-scale translation tasks, we show that it can help to make the decoding more efficient.</p><p>Most recent NMT studies have focused on model improvement ( <ref type="bibr" target="#b7">Luong et al., 2015;</ref><ref type="bibr" target="#b17">Tu et al., 2016b;</ref><ref type="bibr" target="#b3">Gehring et al., 2017;</ref><ref type="bibr" target="#b18">Vaswani et al., 2017)</ref>, and only a few have studied the search problem di- rectly. For example, <ref type="bibr" target="#b4">Khayrallah et al. (2017)</ref> and <ref type="bibr" target="#b14">Stahlberg et al. (2016)</ref> explored searching on lat- tices generated by traditional Statistical Machine Translation (SMT). In addition, Freitag and Al- Onaizan (2017) investigated different beam search pruning strategies; however, they primarily fo- cused on pruning candidates locally. ( <ref type="bibr" target="#b10">Niehues et al., 2017)</ref> analyzed the effects of modeling and searching, but focused on re-ranking analy- sis. Rather than considering candidates from other model's k-best lists, we focus on the own explo- ration space of a single NMT model and provide a method for more efficient searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>For state merging, "equivalence" should be de- fined from the aspect of future predictions: states with the same predictions in the future decoding process can be regarded as equivalent. We use an equivalence function Eq(s 1 , s 2 ) to denote that the two states s 1 and s 2 can be regarded as equivalent.</p><p>With the concept of equivalence, we can build the method of recombination over it. There are mainly two problems to solve: 1. How to merge states given function Eq? ( §2.1) 2. How to obtain this equivalence function? ( §2.2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Search with Merging</head><p>To adopt an equivalence function Eq(s 1 , s 2 ) to merge states in a search process, we need to spec- ify the logic of the merging mechanism. Here, without loss of generality, we specifically focus on the typical beam search.</p><p>We adopt merging in NMT beam search with a simple method: retaining the word-level search process and adding a state merger when pruning the beam at each time step. Algorithm 1 shows the proposed merging-enhanced pruning method.</p><p>Ordinary beam search only prunes candidates based on beam size (Lines 15-16), while the pro- posed method adds a merger to prune extra equiv- alent states (Lines 6-10). To manage the merging process, candidate list C are ordered 1 by model score and considered in turn. When checking equivalence for one candidate state c, we con- sider all current-step surviving states and their previous-step antecedences. We include previous- step states, because equivalent states may have dif- ferent sequence lengths and thus not be in the same beam-search step. In Line 8, we define "sequence" as a function of obtaining the possible states that can merge the current candidate c. If a candidate state c is not merged with any higher-ranked state, it is added to the surviving list C (Line 13) and can possibly merge the lower-ranked ones later.</p><p>When deciding whether to merge, we also con- sider a criterion on model scores: we only merge state c when its score is lower than s . Since we also consider previous-step states with different sequence lengths, a length reward λ is added for this comparison of partial hypotheses: score(s) = y∈s λ + log p(y). We also attempted length nor- malization, but found it performed slightly worse.</p><p>The merged partial hypotheses can be stored, and by assuming that their future predictions will be the same as their mergers, a lattice-like trans- lation graph can be obtained. We can further ex- tract k-best list from this structure using another beam-search on the lattice (also with length re- ward when comparing partial hypotheses). Note that this beam search process can be fast, since we reuse the model scores from previous search and no extra neural computations will be included.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Equivalence Function</head><p>Finding an exact equivalence function for NMT is difficult, because future predictions relies on the features from the entire previous sequence and any different sequences are not the same according to the NMT model. Here, we consider a n-gram suf- fix based heuristic approximation for this problem.</p><p>We adopt an approximate equivalence function:</p><formula xml:id="formula_0">Eq (s 1 , s 2 ) ≡ s 1 .suf f ix(n) = s 2 .suf f ix(n) ∧ |s 1 .length − s 2 .length| &lt; r</formula><p>Here, suf f ix(n) represents the n-gram suffix of the sequence of a state, and r is the threshold for the length different of the two states. This definition of equivalence only considers a subset of state features, which are inspired by PB- SMT. In PBSMT, different sequences could lead to states with identical features based on n-gram suffix, and these states are exactly equivalent. Al- though this is not the case for NMT, the subset may encodes important and relevant features.</p><p>Although this function is simple and brings ex- tra approximation, it has the merit of efficiency. In Algorithm 1, we can store the n-gram features of the surviving states in a hash-map and replace the for-loop checking (Line 6-10) with hashing, making the extra time-complexity O(1) for each state. During experiments, we found the extra cost brought by feature matching is far less than the cost of original neural computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Analysis</head><p>The proposed method was evaluated on two trans- lation tasks: NIST Chinese-English (Zh-En) and WMT English-German (En-De). For Zh-En, the training set comprised 1.4M sentences pairs from LDC corpora. NIST 02 was selected as the devel- opment set and NIST 03 to 06 were used for test- ing. For En-De, 4.5M WMT training data were utilized, the concatenation of newstest 2012 and 2013 was adopted as the development set, and newstest 2014 to 2016 were adopted as the test set.</p><p>We implemented 2 an attentional RNN-based NMT model and its decoder in Python with the DyNet toolkit ( <ref type="bibr" target="#b9">Neubig et al., 2017</ref>). All the ex- periments were carried out on one P100 GPU. For Zh-En, we set the vocabulary size of both sides to 30K, and for En-De, we adopted 50K BPE opera- tions ( <ref type="bibr" target="#b12">Sennrich et al., 2016</ref>). The evaluation met- ric was tokenized BLEU ( <ref type="bibr" target="#b11">Papineni et al., 2002</ref>) calculated by multi-bleu.perl. Detailed set- tings can be found in the supplementary material.</p><p>We added a local threshold pruner to exclude unlikely words whose probabilities were less than 10% of the highest and adopted length normaliza- tion for final hypotheses ranking. For comparing partial hypotheses, the length reward λ was set to 1.0 and 0.4 for Zh-En and En-De, respectively. For the equivalence function, we utilized a suffix of 4- gram and a length difference threshold r of 2.</p><p>These hyper-parameters were set by prelim- inary experiments. For the length difference threshold r, we found that relatively small r like 1 or 2 was better than larger ones, which is rea- sonable since if the merged hypotheses differs too much in length, there are higher chances that they covered different information. For n-gram suffix, we found smaller n-grams made more bad merges and 4-gram is a reasonably good choice, slightly larger ones gave slightly worse results and also less chances of recombination. <ref type="figure" target="#fig_2">Figure 2</ref> show the results of various beam sizes on the concatenation of all test sets. Separate results are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Results</head><p>As shown by the speed curves, merging adds little extra cost (less than 10%) to decoding at the same beam size. Moreover, since bringing no extra neural computations, the proposed merging mechanism is transparent to neural architectures and easy to adopt. In our experiments, we used batched decoding on GPU and merging did not in- fluence the efficiency of this implementation.</p><p>For translation quality, the results indicate that the proposed methods can yield improvements at various beam sizes for Zh-En and small beam sizes for En-De. Moreover, in some way, merging can make the search more efficient. For example, in both datasets, merge-enhanced searchers with beam-size 6 can obtain comparable or better re- sults compared to those of ordinary searchers with beam-size 12 (on BLEU, 37.17 vs. 37.11 for Zh- En, 24.64 vs. 24.67 for En-De). As for decoding speed, the one of beam-size 6 can be more than twice of the one of beam-size 12 (over 200 to- kens/second vs. around 100 tokens/second). That is to say, with merging, we can achieve similar translation quality with a smaller beam size, which leads to higher decoding speed.</p><p>The results show that for large beam sizes, ex- panding explored search space by increasing beam size or adopting merging helps more in Zh-En than En-De. A possible explanation for this is that in NIST Zh-En dataset, each source sentences has four references for evaluation, which encour- ages the diversity brought by expanding reached search space. In <ref type="table">Table 2</ref>, we compare the BLEU scores with multiple and single references on sev- eral beam sizes, and the single-reference results does not always increase along the beam size like the multiple ones. The En-De dataset also has only one reference and is similar to this case.</p><p>The results also show that expanding explored search space does not always bring improvements. This concerns more on modeling than searching and corresponds with previous findings on the re- lations between NMT searching and modeling ( <ref type="bibr" target="#b16">Tu et al., 2016a;</ref><ref type="bibr" target="#b10">Niehues et al., 2017;</ref><ref type="bibr" target="#b6">Li et al., 2018</ref>  <ref type="table">Table 2</ref>: Comparisons of multi-and single-reference BLEU scores of NIST 03-06 with "w/o merging". "Multi-Ref" uses all four references, and "Single-Ref" only uses the second one, whose evaluations disagree most with "Multi-ref".</p><p>The potential of the proposed method might be better realized with improved NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>We further analyzed the merge-enhanced search process. For these analyses, we mainly checked decoding with a beam size of 10 on Zh-En dataset.</p><p>Frequency of Merging First, we investigated how often recombination occurs and how much it expands the explored output space. For a beam size of 10, with influences from the local pruner and the proposed merger, the average expanding size is 7.60 for each step, and the average num- ber of merger-pruned partial hypotheses is 0.61 per step (22.5 per sentence). This indicates that a partial hypothesis is recombined in every two steps. The output translation graph can hold much more output space than the original k-best list, and we found that on average the possible output se- quences were averagely 200 times the beam size. <ref type="figure">Figure 3</ref> shows an example of the output transla- tion graph.</p><p>Merging  <ref type="table">Table 3</ref>: Comparisons of prediction model scores between different searching settings and a basic setting, which is "Beam=10, w/o merge". The pattern "a% / b%" means that compared with the basic setting, a% of the sentences get higher model scores and b% get lower ones. For the rest (1- a%-b%), they give identical predictions.</p><p>threshold) in the equivalence function, however, we found that this does not bring obvious addi- tional benefits.</p><p>Effects of Merging We further conducted com- parisons between the predictions of ordinary and merge-enhanced beam search. First, we investi- gated the model scores of their predictions. As shown in <ref type="table">Table 3</ref>, we selected "Beam=10, no merge" as the basic setting, and compared the pre- dictions of other settings with it. Overall, the merge-enhanced searcher can obtain higher model score predictions, which suggests its stronger search ability, because the goal of searching is to return hypotheses with higher model scores. Moreover, we tried a re-ranking experiment on 100-best lists with 4-checkpoint-model-ensemble, and only found similar slight improvements for plain and merge-enhanced search. Nevertheless, since merge-enhanced search can obtain a output translation graph, we expect that the graph can contain more diverse hypotheses.</p><p>To verify this, we compared the oracle BLEU scores within the reached space. To extract or- acle hypotheses from the translation graphs, we simply adopted approximate Partial BLEU Ora- cle ( <ref type="bibr" target="#b1">Dreyer et al., 2007;</ref><ref type="bibr" target="#b13">Sokolov et al., 2012)</ref>. Merge-based searcher could obtain an oracle score of 47.83, while ordinary beam searcher could only get 42.57. Only by increasing the beam size up to 100 could the ordinary beam searcher achieve a better result of 48.74. This indicates that recombi- nation helps to touch more output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion and Discussion</head><p>In this work, 1) we show that decoding with heuristic recombination can obtain similar trans- lation qualities with smaller beam sizes, thus in- creasing efficiency, and, 2) we empirically explore the decoding process and analyze the influences of recombination from various aspects.</p><p>Although the improvements brought by recom- bination depend on careful refinements of the model, this concerns more on modeling, since the goal of decoding is to find hypotheses with higher model scores. The potential of recombination may be further realized by improving how the output sequences are modeled. Another interesting topic will be the combination with SMT or extra larger language models ( <ref type="bibr" target="#b19">Wang et al., 2013</ref><ref type="bibr" target="#b20">Wang et al., , 2014</ref>.</p><p>For the equivalence function, there can also be extensions. For example, a model-based equiva- lence function can be trained by using the neural features (hidden layers in RNN). However, model- based equivalence functions may bring extra neu- ral computation cost and be harder to efficiently implemented. In this work, we focus on the merg- ing mechanism and leave the study of equivalence function for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:</head><label></label><figDesc>::: these ::: two :::: cities have(−0.075) already(−0.331) set(−0.536) up(−0.001) ... Output according to some sources , workers of :::: these ::: two :::: cities have(−0.073) already(−0.248) set(−0.783) up(−0.001) ... it has been reported that the workers of :::: these ::: two :::: cities have(−0.058) already(−0.414) set(−0.608) up(−0.001) ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: t-SNE visualization (Maaten and Hinton, 2008) of the recurrent hidden layer vectors for partial hypotheses for the example in Table 1. Reference and prediction hypotheses are presented as red and blue nodes, respectively. The nodes inside the box represent the hidden features of partial hypotheses ending with "cities".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Translation quality and speed of Zh-En and En-De test sets (5453 sentences by concatenating NIST 03 to 06 and 8171 sentences by concatenating newstest 2014 to 2016, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Example of similar partial hypotheses in beam</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) .</head><label>.</label><figDesc></figDesc><table>Ref \ Beam 
10 
16 
30 
50 

Multi-Ref 
37.07 37.17 37.19 37.32 
Single-Ref 
18.23 18.29 18.23 18.26 

</table></figure>

			<note place="foot" n="1"> In plain beam search, the candidates may not need to be sorted. We use a local selector to make the sorting efficient: a local k-best selector is first applied on each previous-step candidate states, making the size of the candidate list at most k * k rather than k * |V |, where |V | is the vocabulary size.</note>

			<note place="foot" n="2"> https://github.com/zzsfornlp/znmt-merge</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparing reordering constraints for smt using efficient bleu oracle computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation</title>
		<meeting>SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation<address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beam search strategies for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A convolutional encoder model for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="123" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural lattice search for domain adaptation in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP</title>
		<meeting>IJCNLP<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple and effective approach to coverage-aware neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="292" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clothiaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.03980</idno>
	</analytic>
	<monogr>
		<title level="m">The dynamic neural network toolkit</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing neural mt search and model performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Philadelphia, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing lattice bleu oracle scores for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Yvon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Syntactically guided neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="299" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling coverage for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="76" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Converting continuous-space language models into n-gram language models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural network based bilingual language model growing for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
