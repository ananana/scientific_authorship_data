<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Dynamic Programming Algorithm for Computing N-gram Posteriors from Lattices</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do˘</forename><surname>Gan Can</surname></persName>
							<email>dogancan@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California Los Angeles</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Dynamic Programming Algorithm for Computing N-gram Posteriors from Lattices</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Efficient computation of n-gram posterior probabilities from lattices has applications in lattice-based minimum Bayes-risk decoding in statistical machine translation and the estimation of expected document frequencies from spoken corpora. In this paper, we present an algorithm for computing the posterior probabilities of all n-grams in a lattice and constructing a minimal deterministic weighted finite-state automaton associating each n-gram with its posterior for efficient storage and retrieval. Our algorithm builds upon the best known algorithm in literature for computing n-gram posteriors from lattices and leverages the following observations to significantly improve the time and space requirements: i) the n-grams for which the posteriors will be computed typically comprises all n-grams in the lattice up to a certain length, ii) posterior is equivalent to expected count for an n-gram that do not repeat on any path, iii) there are efficient algorithms for computing n-gram expected counts from lattices. We present experimental results comparing our algorithm with the best known algorithm in literature as well as a baseline algorithm based on weighted finite-state automata operations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many complex speech and natural language processing (NLP) pipelines such as Automatic Speech Recognition (ASR) and Statistical Ma- chine Translation (SMT) systems store alternative hypotheses produced at various stages of process- ing as weighted acyclic automata, also known as lattices. Each lattice stores a large number of hypotheses along with the raw system scores as- signed to them. While single-best hypothesis is typically what is desired at the end of the pro- cessing, it is often beneficial to consider a large number of weighted hypotheses at earlier stages of the pipeline to hedge against errors introduced by various subcomponents. Standard ASR and SMT techniques like discriminative training, rescoring with complex models and Minimum Bayes-Risk (MBR) decoding rely on lattices to represent in- termediate system hypotheses that will be fur- ther processed to improve models or system out- put. For instance, lattice based MBR decoding has been shown to give moderate yet consistent gains in performance over conventional MAP decoding in a number of speech and NLP applications in- cluding ASR <ref type="bibr" target="#b9">(Goel and Byrne, 2000</ref>) and SMT ( <ref type="bibr" target="#b15">Tromble et al., 2008;</ref><ref type="bibr" target="#b4">Blackwood et al., 2010;</ref><ref type="bibr" target="#b7">de Gispert et al., 2013</ref>).</p><p>Most lattice-based techniques employed by speech and NLP systems make use of posterior quantities computed from probabilistic lattices. In this paper, we are interested in two such posterior quantities: i) n-gram expected count, the expected number of occurrences of a particular n-gram in a lattice, and ii) n-gram posterior probability, the total probability of accepting paths that include a particular n-gram. Expected counts have applica- tions in the estimation of language model statis- tics from probabilistic input such as ASR lattices <ref type="bibr" target="#b0">(Allauzen et al., 2003</ref>) and the estimation term frequencies from spoken corpora while posterior probabilities come up in MBR decoding of SMT lattices ( <ref type="bibr" target="#b15">Tromble et al., 2008)</ref>, relevance ranking of spoken utterances and the estimation of docu- ment frequencies from spoken corpora <ref type="bibr" target="#b10">(Karakos et al., 2011;</ref><ref type="bibr" target="#b5">Can and Narayanan, 2013)</ref>.</p><p>The expected count c(x|A) of n-gram x given lattice A is defined as</p><formula xml:id="formula_0">c(x|A) = y∈Σ * # y (x)p(y|A)<label>(1)</label></formula><p>where # y (x) is the number of occurrences of n-gram x in hypothesis y and p(y|A) is the posterior probability of hypothesis y given lattice A. Simi- larly, the posterior probability p(x|A) of n-gram x given lattice A is defined as</p><formula xml:id="formula_1">p(x|A) = y∈Σ * 1 y (x)p(y|A)<label>(2)</label></formula><p>where 1 y (x) is an indicator function taking the value 1 when hypothesis y includes n-gram x and 0 otherwise. While it is straightforward to com- pute these posterior quantities from weighted n- best lists by examining each hypothesis separately and keeping a separate accumulator for each ob- served n-gram type, it is infeasible to do the same with lattices due to the sheer number of hypothe- ses stored. There are efficient algorithms in lit- erature ( <ref type="bibr" target="#b0">Allauzen et al., 2003;</ref><ref type="bibr" target="#b1">Allauzen et al., 2004</ref>) for computing n-gram expected counts from weighted automata that rely on weighted finite state transducer operations to reduce the compu- tation to a sum over n-gram occurrences elimi- nating the need for an explicit sum over accept- ing paths. The rather innocent looking difference between Equations 1 and 2, # y (x) vs. 1 y (x), makes it hard to develop similar algorithms for computing n-gram posteriors from weighted au- tomata since the summation of probabilities has to be carried out over paths rather than n-gram oc- currences ( <ref type="bibr" target="#b4">Blackwood et al., 2010;</ref><ref type="bibr" target="#b7">de Gispert et al., 2013</ref>). The problem of computing n-gram posteriors from lattices has been addressed by a number of recent works <ref type="bibr" target="#b15">(Tromble et al., 2008;</ref><ref type="bibr" target="#b3">Allauzen et al., 2010;</ref><ref type="bibr" target="#b4">Blackwood et al., 2010;</ref><ref type="bibr" target="#b7">de Gispert et al., 2013</ref>) in the context of lattice-based MBR for SMT. In these works, it has been reported that the time required for lattice MBR decoding is domi- nated by the time required for computing n-gram posteriors. Our interest in computing n-gram pos- teriors from lattices stems from its potential appli- cations in spoken content retrieval ( <ref type="bibr" target="#b6">Chelba et al., 2008;</ref><ref type="bibr" target="#b10">Karakos et al., 2011;</ref><ref type="bibr" target="#b5">Can and Narayanan, 2013)</ref>. Computation of document frequency statis- tics from spoken corpora relies on estimating n- gram posteriors from ASR lattices. In this con- text, a spoken document is simply a collection of ASR lattices. The n-grams of interest can be word, syllable, morph or phoneme sequences. Unlike in the case of lattice-based MBR for SMT where the n-grams of interest are relatively short -typically up to 4-grams -, the n-grams we are interested in are in many instances relatively long sequences of subword units.</p><p>In this paper, we present an efficient algorithm for computing the posterior probabilities of all n- grams in a lattice and constructing a minimal de- terministic weighted finite-state automaton asso- ciating each n-gram with its posterior for efficient storage and retrieval. Our n-gram posterior com- putation algorithm builds upon the custom forward procedure described in <ref type="bibr" target="#b7">(de Gispert et al., 2013)</ref> and introduces a number of refinements to signifi- cantly improve the time and space requirements:</p><p>• The custom forward procedure described in <ref type="bibr" target="#b7">(de Gispert et al., 2013</ref>) computes unigram posteriors from an input lattice. Higher or- der n-gram posteriors are computed by first transducing the input lattice to an n-gram lat- tice using an order mapping transducer and then running the custom forward procedure on this higher order lattice. We reformulate the custom forward procedure as a dynamic programming algorithm that computes pos- teriors for successively longer n-grams and reuses the forward scores computed for the previous order. This reformulation subsumes the transduction of input lattices to n-gram lattices and obviates the need for construct- ing and applying order mapping transducers.</p><p>• Comparing Eq. 1 with Eq. 2, we can observe that posterior probability and expected count are equivalent for an n-gram that do not re- peat on any path of the input lattice. The key idea behind our algorithm is to limit the costly posterior computation to only those n- grams that can potentially repeat on some path of the input lattice. We keep track of repeating n-grams of order n and use a sim- ple impossibility argument to significantly re- duce the number of n-grams of order n + 1 for which posterior computation will be per- formed. The posteriors for the remaining n-grams are replaced with expected counts. This filtering of n-grams introduces a slight bookkeeping overhead but in return dramat- ically reduces the runtime and memory re- quirements for long n-grams.</p><p>• We store the posteriors for n-grams that can potentially repeat on some path of the input lattice in a weighted prefix tree that we con- struct on the fly. Once that is done, we com- </p><formula xml:id="formula_2">SEMIRING SET ⊕ ⊗ 0 1 Boolean {0, 1} ∨ ∧ 0 1 Probability R + ∪ {+∞} + × 0 1 Log R ∪ {−∞, +∞} ⊕ log + +∞ 0 Tropical R ∪ {−∞, +∞} min + +∞ 0 a ⊕ log b = − log(e −a + e −b )</formula><p>pute the expected counts for all n-grams in the input lattice and represent them as a min- imal deterministic weighted finite-state au- tomaton, known as a factor automaton ( <ref type="bibr" target="#b1">Allauzen et al., 2004;</ref>, us- ing the approach described in <ref type="bibr" target="#b1">(Allauzen et al., 2004</ref>). Finally we use general weighted au- tomata algorithms to merge the weighted fac- tor automaton representing expected counts with the weighted prefix tree representing posteriors to obtain a weighted factor au- tomaton representing posteriors that can be used for efficient storage and retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>This section introduces the definitions and nota- tion related to weighted finite state automata and transducers <ref type="bibr" target="#b12">(Mohri, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Semirings</head><formula xml:id="formula_3">Definition 1 A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) where (K, ⊕, 0) is a commutative monoid, (K, ⊗, 1)</formula><p>is a monoid, ⊗ distributes over ⊕ and 0 is an annihilator for ⊗. <ref type="table" target="#tab_0">Table 1</ref> lists common semirings. In speech and language processing, two semirings are of particu- lar importance. The log semiring is isomorphic to the probability semiring via the negative-log mor- phism and can be used to combine probabilities in the log domain. The tropical semiring, provides the algebraic structure necessary for shortest-path algorithms and can be derived from the log semir- ing using the Viterbi approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weighted Finite-State Automata</head><p>Definition 2 A weighted finite-state automaton (WFSA) A over a semiring (K, ⊕, ⊗, 0, 1) is a 7- tuple A = (Σ, Q, I, F, E, λ, ρ) where: Σ is the finite input alphabet; Q is a finite set of states; I, F ⊆ Q are respectively the set of initial and </p><formula xml:id="formula_4">final states; E ⊆ Q × (Σ ∪ {ε}) × K × Q is a finite set of arcs; λ : I → K, ρ : F → K</formula><formula xml:id="formula_5">i[π] = i[e 1 ] . . . i[e k ] and w[π] = w[e 1 ] ⊗ . . . ⊗ w[e k ].</formula><p>We denote by Π(q, q ) the set of paths from q to q and by Π(q, x, q ) the set of paths from q to q with input string x ∈ Σ * . These definitions can also be extended to subsets S, S ⊆ Q, e.g.</p><formula xml:id="formula_6">Π(S, x, S ) = q∈S,q ∈S Π(q, x, q ).</formula><p>An accepting path in an automaton A is a path in Π(I, F ). A string x is accepted by A if there exists an accepting path π labeled with x. A is determin- istic if it has at most one initial state and at any state no two outgoing transitions share the same input label. The weight associated by an automa- ton A to a string x ∈ Σ * is given by</p><formula xml:id="formula_7">A(x) = π∈Π(I,x,F ) λ(s[π]) ⊗ w[π] ⊗ ρ(t[π]) and A(x) 0 when Π(I, x, F ) = ∅.</formula><p>A weighted automaton A defined over the prob- ability semiring (R + , +, ×, 0, 1) is said to be probabilistic if for any state q ∈ Q, the sum of the weights of all cycles at q, ⊕ π∈Π(q,q) w[π], is well-defined and in R + and x∈Σ * A(x) = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">N-gram Mapping Transducer</head><p>We denote by Φ n the n-gram mapping transducer ( <ref type="bibr" target="#b4">Blackwood et al., 2010;</ref><ref type="bibr" target="#b7">de Gispert et al., 2013)</ref> of order n. This transducer maps label sequences to n-gram sequences of order n. Φ n is similar in form to the weighted finite-state transducer rep- resentation of a backoff n-gram language model ( <ref type="bibr" target="#b0">Allauzen et al., 2003)</ref>. We denote by A n the n- gram lattice of order n obtained by composing lat- tice A with Φ n , projecting the resulting transducer onto its output labels, i.e. n-grams, to obtain an automaton, removing ε-transitions, determinizing and minimizing <ref type="bibr" target="#b12">(Mohri, 2009)</ref>. A n is a compact lattice of n-gram sequences of order n consistent with the labels and scores of lattice A. A n typi- cally has more states than A due to the association of distinct n-gram histories with states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Factor Automata</head><p>Definition 3 Given two strings x, y ∈ Σ * , x is a factor (substring) of y if y = uxv for some u, v ∈ Σ * . More generally, x is a factor of a language L ⊆ Σ * if x is a factor of some string y ∈ L. The factor automaton S(y) of a string y is the minimal deterministic finite-state automa- ton recognizing exactly the set of factors of y. The factor automaton S(A) of an automaton A is the minimal deterministic finite-state automaton rec- ognizing exactly the set of factors of A, that is the set of factors of the strings accepted by A.</p><p>Factor automaton ( ) is an ef- ficient and compact data structure for representing a full index of a set of strings, i.e. an automaton. It can be used to determine if a string x is a factor in time linear in its length O(|x|). By associating a weight with each factor, we can generalize the fac- tor automaton structure to weighted automata and use it for efficient storage and retrieval of n-gram posteriors and expected counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Computation of N-gram Posteriors</head><p>In this section we present an efficient algorithm based on the n-gram posterior computation algo- rithm described in (de <ref type="bibr" target="#b7">Gispert et al., 2013</ref>) for computing the posterior probabilities of all n- grams in a lattice and constructing a weighted fac- tor automaton for efficient storage and retrieval of these posteriors. We assume that the input lattice is an ε-free acyclic probabilistic automaton. If that is not the case, we can use general weighted au- tomata ε-removal and weight-pushing algorithms <ref type="bibr" target="#b12">(Mohri, 2009)</ref> to preprocess the input automaton.</p><p>Algorithm 1 reproduces the original algo- rithm of (de <ref type="bibr" target="#b7">Gispert et al., 2013</ref>) in our no- tation. Each iteration of the outermost loop starting at line 1 computes posterior probabili- ties of all unigrams in the n-gram lattice A n = (Σ n , Q n , I n , F n , E n , λ n , ρ n ), or equivalently all n-grams of order n in the lattice A. The inner loop starting at line 6 is essentially a custom for- ward procedure computing not only the standard forward probabilities α[q], the marginal probabil- ity of paths that lead to state q,</p><formula xml:id="formula_8">α[q] = π ∈ Π(I,q) λ(s[π]) ⊗ w[π]<label>(3)</label></formula><formula xml:id="formula_9">= e ∈ E t[e] = q α[s[e]] ⊗ w[e]<label>(4)</label></formula><p>but also the label specific forward probabilities˜α probabilities˜ probabilities˜α[q] <ref type="bibr">[x]</ref>, the marginal probability of paths that lead to state q and include label x.</p><formula xml:id="formula_10">˜ α[q][x] = π ∈ Π(I,q) ∃ u,v ∈ Σ * : i[π] = uxv λ(s[π]) ⊗ w[π] (5) = e ∈ E t[e] = q i[e] = x α[s[e]] ⊗ w[e] ⊕ e ∈ E t[e] = q i[e] = x ˜ α[s[e]][x] ⊗ w[e]<label>(6)</label></formula><p>Just like in the case of the standard forward al- gorithm, visiting states in topological order en- sures that forward probabilities associated with a state has already been computed when that state is visited. At each state s, the algorithm examines each arc e = (s, x, w, q) and updates the forward probabilities for state q in accordance with the re- cursions in Equations 4 and 6 by propagating the forward probabilities computed for s (lines 8-12). The conditional on line 11 ensures that the label specific forward probability˜αprobability˜ probability˜α <ref type="bibr">[s]</ref>[y] is propagated to state q only if label y is different from label x, the label on the current arc. In other words, if a label y repeats on some path π leading to state q, then π contributes tõ α <ref type="bibr">[q]</ref>[y] only once. This is exactly what is required by the indicator func- tion in Equation 2 when computing unigram pos- teriors. Whenever a final state is processed, the posterior probability accumulator for each label observed on paths reaching that state is updated by multiplying the label specific forward probabil- ity and the final weight associated with that state Algorithm 1 Compute N-gram Posteriors</p><formula xml:id="formula_11">1 for n ← 1, . . . , N do 2 A n ← Min(Det(RmEps(ProjOut(A • Φ n )))) 3 α[q] ← λ n (q), ∀ state q ∈ Q n 4 ˜ α[q][x] ← 0, ∀ state q ∈ Q n , ∀ label x ∈ Σ n 5 p(x|A) ← 0, ∀ label x ∈ Σ n 6 for each state s ∈ Q n do</formula><p>In topological order <ref type="bibr">7</ref> for each arc (s, x, w, q) ∈ E n do</p><formula xml:id="formula_12">8 α[q] ← α[q] ⊕ α[s] ⊗ w 9 ˜ α[q][x] ← ˜ α[q][x] ⊕ α[s] ⊗ w 10 for each label y ∈ ˜ α[s] do 11 if y = x then 12˜α 12˜ 12˜α[q][y] ← ˜ α[q][y] ⊕ ˜ α[s][y] ⊗ w 13 if s ∈ F n then 14 for each label x ∈ ˜ α[s] do 15 p(x|A) ← p(x|A) ⊕ ˜ α[s][x] ⊗ ρ n (s) 16 P ← Min(ConstructPrefixTree(p))</formula><p>and adding the resulting value to the accumulator (lines 13-15). It should be noted that this algo- rithm is a form of marginalization (de Gispert et al., 2013), rather than a counting procedure, due to the conditional on line 11. If that conditional were to be removed, this algorithm would com- pute n-gram expected counts instead of posterior probabilities.</p><p>The key idea behind our algorithm is to re- strict the computation of posteriors to only those n-grams that may potentially repeat on some path of the input lattice and exploit the equivalence of expected counts and posterior probabilities for the remaining n-grams. It is possible to extend Algo- rithm 1 to implement this restriction by keeping track of repeating n-grams of order n and replac- ing the output labels of appropriate arcs in Φ n+1 with ε labels. Alternatively we can reformulate Algorithm 1 as in Algorithm 2. In this formulation we compute n-gram posteriors directly on the in- put lattice A without constructing the n-gram lat- tice A n . We explicitly associate states in the orig- inal lattice with distinct n-gram histories which is implicitly done in Algorithm 1 by constructing the n-gram lattice A n . This explicit association lets us reuse forward probabilities computed at order n while computing the forward probabilities at or- der n + 1. Further, we can directly restrict the n-grams for which posterior computation will be performed.</p><formula xml:id="formula_13">In Algorithm 2, ´ α[n][q][h]</formula><p>represents the his- tory specific forward probability of state q, the marginal probability of paths that lead to state q and include length n string h as a suffix.  <ref type="bibr">[x]</ref> represents the history and n-gram specific forward probability of state q, the marginal probability of paths that lead to state q, include length n − 1 string h as a suffix and Algorithm 2 Compute N-gram Posteriors (Reformulation) <ref type="bibr">1</ref> </p><formula xml:id="formula_14">´ α[n][q][h] = π ∈ Π(I,q) ∃ z ∈ Σ * : i[π] = zh λ(s[π]) ⊗ w[π] (7) = e ∈ E t[e] = q g ∈ ´ α[n−1][s[e]] gi[e] = h ´ α[n − 1][s[e]][g] ⊗ w[e]<label>(8)</label></formula><formula xml:id="formula_15">´ α[n][q][h]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R[0] ← {ε}</head><formula xml:id="formula_16">2 ´ α[0][q][ε] ← α[q], ∀ state q ∈ Q 3 for n ← 1, . . . , N do 4 R[n] ← ∅ 5 ´ α[n][q][x] ← 0, ∀ state q ∈ Q, ∀ ngram x ∈ Σ n 6 ˆ α[q][h][x] ← 0, ∀ state q ∈ Q, ∀ history h ∈ Σ n−1 , ∀ ngram x ∈ Σ n 7 p(x|A) ← 0, ∀ ngram x ∈ Σ n 8 for each state s ∈ Q do</formula><p>In topological order h</p><formula xml:id="formula_17">← x[1 : n] Drop first label 13 if h ∈ R[n − 1] then 14´α 14´ 14´α[n][q][x] ← ´ α[n][q][x] ⊕ ´ α[n − 1][s][g] ⊗ w 15ˆα 15ˆ 15ˆα[q][h][x] ← ˆ α[q][h][x] ⊕ ´ α[n − 1][s][g] ⊗ w 16 for each ngram y ∈ ˆ α[s][g] do 17 if y = x then 18ˆα 18ˆ 18ˆα[q][h][y] ← ˆ α[q][h][y] ⊕ ˆ α[s][g][y] ⊗ w 19 else 20 R[n] ← R[n] ∪ {y} 21 if s ∈ F then 22 for each history g ∈ ˆ α[s] do 23 for each ngram x ∈ ˆ α[s][g] do 24 p(x|A) ← p(x|A) ⊕ ˆ α[s][g][x] ⊗ ρ(s) 25 P ← ConstructPrefixTree(p) 26 C ← ComputeExpectedCounts(A, N ) 27 P ← Min(Det(RmEps((C − RmWeight(P )) ⊕ P )))</formula><p>include n-gram x as a substring. <ref type="bibr">[x]</ref> is the analogue of˜αof˜ of˜α[q] <ref type="bibr">[x]</ref> in Algorithm 1. R[n] represents the set of n-grams of order n that repeat on some path of A. We start by defin- ing R[0] {ε}, i.e. the only repeating n-gram of order 0 is the empty string ε, and computing´α computing´ computing´α <ref type="bibr">[0]</ref>[q][ε] ≡ α[q] using the standard forward algo- rithm. Each iteration of the outermost loop start- ing at line 3 computes posterior probabilities of all n-grams of order n directly on the lattice A. At iteration n, we visit the states in topological order and examine each length n−1 history g associated with s, the state we are in. For each history g, we go over the set of arcs leaving state s, construct the current n-gram x by concatenating g with the cur- rent arc label i (line 11), construct the length n − 1 history h of the target state q (line 12), and update the forward probabilities for the target state his- tory pair (q, h) in accordance with the recursions in Equations 8 and 10 by propagating the forward probabilities computed for the state history pair (s, g) (lines 14-18). Whenever a final state is pro- cessed, the posterior probability accumulator for each n-gram of order n observed on paths reach- ing that state is updated by multiplying the n-gram specific forward probability and the final weight associated with that state and adding the resulting value to the accumulator (lines <ref type="bibr">[21]</ref><ref type="bibr">[22]</ref><ref type="bibr">[23]</ref><ref type="bibr">[24]</ref>.</p><formula xml:id="formula_18">ˆ α[q][h][x] = π ∈ Π(I,q) ∃ z ∈ Σ * : i[π] = zh ∃ u,v ∈ Σ * : i[π] = uxv λ(s[π]) ⊗ w[π] (9) = e ∈ E t[e] = q g ∈ ´ α[|h|][s[e]] gi[e] = x ´ α[|h|][s[e]][g] ⊗ w[e] ⊕ e ∈ E t[e] = q g ∈ ˆ α[s[e]] gi[e] = x ˆ α[s[e]][g][x] ⊗ w[e] (10) ˆ α[q][h]</formula><p>We track repeating n-grams of order n to re- strict the costly posterior computation operation to only those n-grams of order n + 1 that can poten- tially repeat on some path of the input lattice. The conditional on line 17 checks if any of the n-grams observed on paths reaching state history pair (s, g) is the same as the current n-gram x, and if so adds it to the set of repeating n-grams. At each iteration n, we check if the current length n − 1 history g of the state we are in is in R[n − 1], the set of re- peating n-grams of order n − 1 (line 9). If it is not, then no n-gram x = gi can repeat on some path of A since that would require g to repeat as well. If g is in R[n − 1], then for each arc e = (s, i, w, q) we check if the length n − 1 history h = g[1 : n − 1]i of the next state q is in R[n − 1] (line 13). If it is not, then the n-gram x = g <ref type="bibr">[0]</ref>h can not repeat either.</p><p>We keep the posteriors p(x|A) for n-grams that can potentially repeat on some path of the input lattice in a deterministic WFSA P that we con- struct on the fly. P is a prefix tree where each path π corresponds to an n-gram posterior, i.e.</p><formula xml:id="formula_19">i[π] = x =⇒ w[π] = ρ(t[π]) = p(x|A).</formula><p>Once the computation of posteriors for possibly repeating n-grams is finished, we use the algo- rithm described in <ref type="bibr" target="#b1">(Allauzen et al., 2004</ref>) to con- struct a weighted factor automaton C mapping all n-grams observed in A to their expected counts,</p><formula xml:id="formula_20">i.e. ∀π in C, i[π] = x =⇒ w[π] = c(x|A).</formula><p>We use P and C to construct another weighted factor automaton P mapping all n-grams observed in A to their posterior probabilities, i.e. ∀π in P ,</p><formula xml:id="formula_21">i[π] = x =⇒ w[π] = p(x|A).</formula><p>First we remove the n-grams accepted by P from C using the dif- ference operation <ref type="bibr" target="#b12">(Mohri, 2009)</ref>,</p><formula xml:id="formula_22">C = C − RmWeight(P )</formula><p>then take the union of the remaining automaton C and P , and finally optimize the result by remov- ing ε-transitions, determinizing and minimizing P = Min(Det(RmEps(C ⊕ P ))).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Discussion</head><p>In this section we provide experiments comparing the performance of Algorithm 2 with Algorithm 1 as well as a baseline algorithm based on the ap- proach of <ref type="bibr" target="#b15">(Tromble et al., 2008)</ref>. All algorithms were implemented in C++ using the OpenFst Li- brary ( <ref type="bibr" target="#b2">Allauzen et al., 2007)</ref>. Algorithm 1 imple- mentation is a thin wrapper around the reference implementation. All experiments were conducted on the 88K ASR lattices (total size: #states + #arcs = 33M, disk size: 481MB) generated from the training subset of the IARPA Babel Turkish lan- guage pack, which includes 80 hours of conversa- tional telephone speech. Lattices were generated with a speaker dependent DNN ASR system that was trained on the same data set using IBM's At- tila toolkit ( <ref type="bibr" target="#b14">Soltau et al., 2010</ref>). All lattices were pruned to a logarithmic beam width of 5. <ref type="figure" target="#fig_2">Figure 1</ref> gives a scatter plot of the posterior probability computation time vs. the number of lattice n-grams (up to 5-grams) where each point represents one of the 88K lattices in our data set. Similarly, <ref type="figure">Figure 2</ref> gives a scatter plot of the max- imum memory used by the program (maximum resident set size) during the computation of pos- teriors vs. the number of lattice n-grams (up to 5-grams). Algorithm 2 requires significantly less resources, particularly in the case of larger lattices with a large number of unique n-grams.</p><p>To better understand the runtime characteris- tics of Algorithms 1 and 2, we conducted a small experiment where we randomly selected 100 lat- tices (total size: #states + #arcs = 81K, disk size: 1.2MB) from our data set and analyzed the re- lation between the runtime and the maximum n- gram length N . <ref type="table" target="#tab_2">Table 2</ref> gives a runtime compari- son between the baseline posterior computation al- gorithm described in ( <ref type="bibr" target="#b15">Tromble et al., 2008</ref>), Algo- rithm 1, Algorithm 2 and the expected count com- putation algorithm of <ref type="bibr" target="#b1">(Allauzen et al., 2004</ref>). The baseline method computes posteriors separately for each n-gram by intersecting the lattice with an automaton accepting only the paths including that n-gram and computing the total weight of the re- sulting automaton in log semiring. Runtime com- plexities of the baseline method and Algorithm 1 are exponential in N due to the explicit enu- meration of n-grams and we can clearly see this trend in the 3rd and 4th rows of <ref type="table" target="#tab_2">Table 2</ref>. Algo- rithm 2 (5th row) takes advantage of the WFSA based expected count computation algorithm (6th row) to do most of the work for long n-grams, hence does not suffer from the same exponential growth. Notice the drops in the runtimes of Algo- rithm 2 and the WFSA based expected count com- putation algorithm when all n-grams are included into the computation regardless of their length. These drops are due to the expected count compu- tation algorithm that processes all n-grams simul- taneously using WFSA operations. Limiting the maximum n-gram length requires pruning long n- grams, which in general can increase the sizes of intermediate WFSAs used in computation and re- sult in longer runtimes as well as larger outputs.</p><p>When there is no limit on the maximum n-gram length, the output of Algorithm 2 is a weighted factor automaton mapping each factor to its pos- terior. <ref type="table" target="#tab_3">Table 3</ref> compares the construction and storage requirements for posterior factor automata with similar factor automata structures. We use the approach described in <ref type="bibr" target="#b1">(Allauzen et al., 2004</ref>) for constructing both the unweighted and the ex- pected count factor automata. We construct the unweighted factor automata by first removing the weights on the input lattices and then applying the determinization operation on the tropical semir- ing so that path weights are not added together. The storage requirements of the posterior factor automata produced by Algorithm 2 is similar to those of the expected count factor automata. Un- weighted factor automata, on the other hand, are significantly more compact than their weighted counterparts even though they accept the same set of strings. This difference in size is due to ac- commodating path weights which in general can significantly impact the effectiveness of automata determinization and minimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Efficient computation of n-gram expected counts from weighted automata was first addressed in ( <ref type="bibr" target="#b0">Allauzen et al., 2003</ref>) in the context of estimating n-gram language model statistics from ASR lat- tices. Expected counts for all n-grams of interest observed in the input automaton are computed by composing the input with a simple counting trans- ducer, projecting on the output side, and remov- ing ε-transitions. The weight associated by the re- sulting WFSA to each n-gram it accepts is simply the expected count of that n-gram in the input au- tomaton. Construction of such an automaton for all substrings (factors) of the input automaton was later explored in <ref type="bibr" target="#b1">(Allauzen et al., 2004</ref>) in the con- text of building an index for spoken utterance re- trieval (SUR) ( <ref type="bibr" target="#b13">Saraclar and Sproat, 2004</ref>). This is the approach used for constructing the weighted factor automaton C in Algorithm 2. While ex- pected count works well in practice for ranking spoken utterances containing a query term, poste- rior probability is in theory a better metric for this task. The weighted factor automaton P produced by Algorithm 2 can be used to construct an SUR index weighted with posterior probabilities.</p><p>The problem of computing n-gram posteriors from lattices was first addressed in <ref type="bibr" target="#b15">(Tromble et al., 2008</ref>) in the context of lattice-based MBR for SMT. This is the baseline approach used in our experiments and it consists of building a separate FSA for each n-gram of interest and intersecting this automaton with the input lattice to discard those paths that do not include that n-gram and summing up the weights of remaining paths. The fundamental shortcoming of this approach is that it requires separate intersection and shortest distance computations for each n-gram. This shortcoming was first tackled in <ref type="bibr" target="#b3">(Allauzen et al., 2010)</ref> by in- troducing a counting transducer for simultaneous computation of posteriors for all n-grams of order n in a lattice. This transducer works well for un- igrams since there is a relatively small number of unique unigrams in a lattice. However, it is less efficient for n-grams of higher orders. This inef- ficiency was later addressed in ( <ref type="bibr" target="#b4">Blackwood et al., 2010</ref>) by employing n-gram mapping transducers to transduce the input lattices to n-gram lattices of order n and computing unigram posteriors on the higher order lattices. Algorithm 1 was described in <ref type="bibr" target="#b7">(de Gispert et al., 2013</ref>) as a fast alternative to counting transducers. It is a lattice specialization of a more general algorithm for computing n-gram posteriors from a hypergraph in a single inside pass ( <ref type="bibr" target="#b8">DeNero et al., 2010)</ref>. While this algorithm works really well for relatively short n-grams, its time and space requirements scale exponentially with the maximum n-gram length. Algorithm 2 builds upon this algorithm by exploiting the equiv- alence of expected counts and posteriors for non- repeating n-grams and eliminating the costly pos- terior computation operation for most n-grams in the input lattice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have described an efficient algorithm for com- puting n-gram posteriors from an input lattice and constructing an efficient and compact data struc- ture for storing and retrieving them. The runtime and memory requirements of the proposed algo- rithm grow linearly with the length of the n-grams as opposed to the exponential growth observed with the original algorithm we are building upon. This is achieved by limiting the posterior compu- tation to only those n-grams that may repeat on some path of the input lattice and using the rela- tively cheaper expected count computation algo- rithm for the rest. This filtering of n-grams in- troduces a slight bookkeeping overhead over the baseline algorithm but in return dramatically re- duces the runtime and memory requirements for long n-grams.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>are respectively the initial and final weight functions. Given an arc e ∈ E, we denote by i[e] its in- put label, w[e] its weight, s[e] its source or origin state and t[e] its target or destination state. A path π = e 1 · · · e k is an element of E * with consecutive arcs satisfying t[e i−1 ] = s[e i ], i = 2, . . . , k. We extend t and s to paths by setting t[π] = s[e k ] and s[π] = t[e 1 ]. The labeling and the weight func- tions can also be extended to paths by defining</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>9</head><label>9</label><figDesc>for each history g ∈ ´ α[n − 1][s] where g ∈ R[n − 1] do 10 for each arc (s, i, w, q) ∈ E do 11 x ← gi Concatenate history and label 12</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Runtime comparison</figDesc><graphic url="image-1.png" coords="7,294.56,62.81,250.83,188.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Common semirings.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Runtime Comparison 

Max n-gram length 
1 
2 
3 
4 
5 
6 
10 
all 

log 10 (#n-grams) 
3.0 3.8 4.2 4.5 4.8 5.1 
6.3 11.2 

Baseline (sec) 
5 15 32 69 147 311 5413 
-
Algorithm 1 (sec) 
0.5 0.6 0.9 1.6 3.9 
16 
997 
-
Algorithm 2 (sec) 
0.7 0.8 0.9 1.1 1.2 1.3 
1.7 
1.0 
Expected Count (sec) 0.3 0.4 0.5 0.6 0.7 0.8 
1.0 
0.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Factor Automata Comparison</head><label>3</label><figDesc></figDesc><table>FA Type 
Unweighted Expected Count Posterior 

#states + #arcs (M) 
16 
20 
21 
On disk size (MB) 
219 
545 
546 

Runtime (min) 
5.5 
11 
22 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Cyril Allauzen and Graeme W. Blackwood for helpful discus-sions. This work uses IARPA-babel105b-v0.4 Turkish full language pack from the IARPA Babel Program language collection and is supported by the Intelligence Advanced Research Projects Ac-tivity (IARPA) via Department of Defense U.S. Army Research Laboratory (DoD/ARL) contract number W911NF-12-C-0012. The U.S. Govern-ment is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized algorithms for constructing statistical language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="47" />
		</imprint>
	</monogr>
	<note>ACL &apos;03. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">General indexation of weighted automata: Application to spoken utterance retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLTNAACL Workshop on Interdisciplinary Approaches to Speech Indexing and Retrieval</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<ptr target="http://www.openfst.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Implementation and Application of Automata</title>
		<meeting>the Ninth International Conference on Implementation and Application of Automata</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">4783</biblScope>
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Expected sequence similarity maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="957" to="965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient path counting transducers for minimum bayes-risk decoding of statistical machine translation lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010-07" />
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the computation of document frequency statistics from spoken corpora using factor automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dogan</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Retrieval and browsing of spoken content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="39" to="49" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">N-gram posterior probability confidence measures for statistical machine translation: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graeme</forename><surname>Adrì A De Gispert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalo</forename><surname>Blackwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="114" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model combination for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT &apos;10</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="975" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Minimum bayes-risk automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="135" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating document frequencies in a speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damianos</forename><surname>Karakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><forename type="middle">Ward</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<editor>David Nahamoo and Michael Picheny</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="407" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factor automata of automata and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="168" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weighted automata algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Weighted Automata</title>
		<editor>Manfred Droste, Werner Kuich, and Heiko Vogler</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="213" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latticebased search for spoken utterance retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Saraclar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The ibm attila speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lattice minimum bayes-risk decoding for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Roy W Tromble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="620" to="629" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
