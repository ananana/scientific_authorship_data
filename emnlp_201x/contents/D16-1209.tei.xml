<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gated Word-Character Recurrent Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="1992">1992-1997. November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasumasa</forename><surname>Miyamoto</surname></persName>
							<email>yasumasa.miyamoto@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences &amp; Centre for Data Science</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
							<email>kyunghyun.cho@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Data Science</orgName>
								<orgName type="department" key="dep2">Courant Institute of Mathematical Sciences &amp; Centre for Data Science</orgName>
								<orgName type="institution" key="instit1">New York University</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gated Word-Character Recurrent Language Model</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<date type="published" when="1992">1992-1997. November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirec-tional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) achieve state-of- the-art performance on fundamental tasks of natural language processing (NLP) such as language model- ing (RNN-LM) ( <ref type="bibr">Józefowicz et al., 2016;</ref><ref type="bibr">Zoph et al., 2016)</ref>. RNN-LMs are usually based on the word- level information or subword-level information such as characters ( <ref type="bibr" target="#b7">Mikolov et al., 2012)</ref>, and predictions are made at either word level or subword level re- spectively.</p><p>In word-level LMs, the probability distribution over the vocabulary conditioned on preceding words is computed at the output layer using a softmax func- tion. <ref type="bibr">1</ref> Word-level LMs require a predefined vocab- ulary size since the computational complexity of a softmax function grows with respect to the vocab- ulary size. This closed vocabulary approach tends to ignore rare words and typos, as the words do not appear in the vocabulary are replaced with an out- of-vocabulary (OOV) token. The words appearing in vocabulary are indexed and associated with high- dimensional vectors. This process is done through a word lookup table.</p><p>Although this approach brings a high degree of freedom in learning expressions of words, infor- mation about morphemes such as prefix, root, and suffix is lost when the word is converted into an index. Also, word-level language models require some heuristics to differentiate between the OOV words, otherwise it assigns the exactly same vector to all the OOV words. These are the major limita- tions of word-level LMs.</p><p>In order to alleviate these issues, we introduce an RNN-LM that utilizes both character-level and word-level inputs. In particular, our model has a gate that adaptively choose between two distinct ways to represent each word: a word vector derived from the character-level information and a word vector stored in the word lookup table. This gate is trained to make this decision based on the input word.</p><p>According to the experiments, our model with the gate outperforms other models on the Penn Treebank (PTB), BBC, and IMDB Movie Review datasets. Also, the trained gating values show that the gating mechanism effectively utilizes the character-level information when it encounters rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Character-level language models that make word-level prediction have recently been pro- posed. <ref type="bibr" target="#b5">Ling et al. (2015a)</ref> introduce the compo- sitional character-to-word (C2W) model that takes as input character-level representation of a word and generates vector representation of the word us- ing a bidirectional LSTM ( <ref type="bibr" target="#b1">Graves and Schmidhuber, 2005</ref>). <ref type="bibr" target="#b5">Kim et al. (2015)</ref> propose a convolu- tional neural network (CNN) based character-level language model and achieve the state-of-the-art per- plexity on the PTB dataset with a significantly fewer parameters.</p><p>Moreover, word-character hybrid models have been studied on different NLP tasks. <ref type="bibr" target="#b4">Kang et al. (2011)</ref> apply a word-character hybrid language model on Chinese using a neural network language model ( <ref type="bibr" target="#b0">Bengio et al., 2003)</ref>. <ref type="bibr">Santos and Zadrozny (2014)</ref> produce high performance part-of-speech taggers using a deep neural network that learns character-level representation of words and asso- ciates them with usual word representations. <ref type="bibr" target="#b0">Bojanowski et al. (2015)</ref> investigate RNN models that predict characters based on the character and word level inputs. <ref type="bibr" target="#b5">Luong and Manning (2016)</ref> present word-character hybrid neural machine translation systems that consult the character-level information for rare words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>The model architecture of the proposed word- character hybrid language model is shown in <ref type="figure">Fig. 1</ref>.</p><p>Word Embedding At each time step t, both the word lookup table and a bidirectional LSTM take the same word w t as an input. The word-level input is projected into a high-dimensional space by a word lookup table E ∈ R |V |×d , where |V | is the vocabu- lary size and d is the dimension of a word vector:</p><formula xml:id="formula_0">x word wt = E w wt ,<label>(1)</label></formula><p>where w wt ∈ R |V | is a one-hot vector whose i-th el- ement is 1, and other elements are 0. The character- level input is converted into a word vector by us- ing a bidirectional LSTM. The last hidden states of forward and reverse recurrent networks are linearly <ref type="figure">Figure 1</ref>: The model architecture of the gated word-character recurrent language model. wt is an input word at t. x word w t is a word vector stored in the word lookup table. x char w t is a word vector derived from the character-level input. gw t is a gating value of a word wt. ˆ wt+1 is a prediction made at t.</p><p>combined:</p><formula xml:id="formula_1">x char wt = W f h f wt + W r h r wt + b,<label>(2)</label></formula><p>where h f wt , h r wt ∈ R d are the last states of the forward and the reverse LSTM respectively. W f , W r ∈ R d×d and b ∈ R d are trainable param- eters, and x char wt ∈ R d is the vector representation of the word w t using a character input. The generated vectors x word wt and x char wt are mixed by a gate g wt as</p><formula xml:id="formula_2">g wt = σ v g x word wt + b g x wt = (1 − g wt ) x word wt + g wt x char wt ,<label>(3)</label></formula><p>where v g ∈ R d is a weight vector, b g ∈ R is a bias scalar, σ(·) is a sigmoid function. This gate value is independent of a time step. Even if a word appears in different contexts, the same gate value is applied. Hashimoto and Tsuruoka (2016) apply a very similar approach to compositional and non- compositional phrase embeddings and achieve state- of-the-art results on compositionality detection and verb disambiguation tasks.</p><p>Language Modeling The output vector x wt is used as an input to a LSTM language model. Since the word embedding part is independent from the lan- guage modeling part, our model retains the flexibil- ity to change the architecture of the language model- ing part. We use the architecture similar to the non- regularized LSTM model by <ref type="bibr">Zaremba et al. (2014)</ref>.   One step of LSTM computation corresponds to</p><note type="other">95.44 77.94 78.29 Word &amp; Character (Pre-train) 122.31 118.85 84.27 91.24 80.60 81.01 Non-regularized LSTM (Zaremba, 2014) 120.7 114.5 - - - -</note><formula xml:id="formula_3">f t = σ (W f x wt + U f h t−1 + b f ) i t = σ (W i x wt + U i h t−1 + b i ) ˜ c t = tanh (W ˜ c x wt + U ˜ c h t−1 + b ˜ c ) o t = σ (W o x wt + U o h t−1 + b o ) c t = f t c t−1 + i t ˜ c t h t = o t tanh (c t ) ,<label>(4)</label></formula><p>where W s , U s ∈ R d×d and b s ∈ R d for s ∈ {f, i, ˜ c, o} are parameters of LSTM cells. σ(·) is an element-wise sigmoid function, tanh(·) is an element-wise hyperbolic tangent function, and is an element-wise multiplication.</p><p>The hidden state h t is affine-transformed fol- lowed by a softmax function:</p><formula xml:id="formula_4">Pr (w t+1 = k|w &lt;t+1 ) = exp v k h t + b k k exp v k h t + b k ,<label>(5)</label></formula><p>where v k is the k-th column of a parameter matrix V ∈ R d×|V | and b k is the k-th element of a bias vector b ∈ R d . In the training phase, we minimizes the negative log-likelihood with stochastic gradient descent. Stochastic gradient decent (SGD) with mini-batch size of 32 is used to train the models. In the first k epochs, the learning rate is 1. After the k-th epoch, the learning rate is divided by l each epoch. k man- ages learning rate decay schedule, and l controls speed of decay. k and l are tuned for each model based on the validation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Settings</head><p>As the standard metric for language modeling, perplexity (PPL) is used to evaluate the model per- formance. Perplexity over the test set is computed as PPL = exp</p><formula xml:id="formula_5">− 1 N N i=1 log p (w i |w &lt;i )</formula><p>, where N is the number of words in the test set, and p (w i |w &lt;i ) is the conditional probability of a word w i given all the preceding words in a sentence. We use Theano (2016) to implement all the models. The code for the models is available from https://github.com/ nyu-dl/gated_word_char_rlm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Variations</head><p>Word Only (baseline) This is a traditional word- level language model and is a baseline model for our experiments.</p><p>Character Only This is a language model where each input word is represented as a character se-  Word &amp; Character This model simply concate- nates the vector representations of a word con- structed from the character input x char wt and the word input x word wt to get the final representation of a word x wt , i.e.,</p><formula xml:id="formula_6">x wt = x char wt ; x word wt .<label>(6)</label></formula><p>Before being concatenated, the dimensions of x char wt and x word wt are reduced by half to keep the size of x wt comparably to other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Word &amp; Character, Fixed Value</head><p>This model uses a globally constant gating value to com- bine vector representations of a word constructed from the character input x char wt and the word input x word wt as</p><formula xml:id="formula_7">x wt = (1 − g) x word wt + gx char wt ,<label>(7)</label></formula><p>where g is some number between 0 and 1. We choose g = {0.25, 0.5, 0.75}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Word &amp; Character, Adaptive</head><p>This model uses adaptive gating values to combine vector repre- sentations of a word constructed from the character input x char wt and the word input x word wt as the Eq (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>Penn Treebank We use the Penn Treebank Corpus ( <ref type="bibr" target="#b6">Marcus et al., 1993</ref>) preprocessed by <ref type="bibr" target="#b6">Mikolov et al. (2010)</ref>. We use 10k most frequent words and 51 characters. In the training phase, we use only sentences with less than 50 words. BBC We use the BBC corpus prepared by <ref type="bibr">Greene &amp; Cunningham (2006)</ref>. We use 10k most frequent words and 62 characters. In the training phase, we use sentences with less than 50 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMDB Movie Reviews</head><p>We use the IMDB Move Review Corpus prepared by <ref type="bibr" target="#b5">Maas et al. (2011)</ref>. We use 30k most frequent words and 74 characters. In the training phase, we use sentences with less than 50 words. In the validation and test phases, we use sentences with less than 500 characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pre-training</head><p>For the word-character hybrid models, we applied a pre-training procedure to encourage the model to use both representations. The entire model is trained only using the word-level input for the first m epochs and only using the character-level input in the next m epochs. In the first m epochs, a learn- ing rate is fixed at 1, and a smaller learning rate 0.1 is used in the next m epochs. After the 2m-th epoch, both the character-level and the word-level inputs are used. We use m = 2 for PTB and BBC, m = 1 for IMDB. <ref type="bibr" target="#b5">Lample et al. (2016)</ref> report that a pre-trained word lookup table improves performance of their word &amp; character hybrid model on named entity recognition (NER). In their method, word embed- dings are first trained using skip-n-gram ( <ref type="bibr" target="#b5">Ling et al., 2015b)</ref>, and then the word embeddings are fine- tuned in the main training phase.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Perplexity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Values of Word-Character Gate</head><p>The BBC and IMDB datasets retain out-of- vocabulary (OOV) words while the OOV words have been replaced by &lt;unk&gt; in the Penn Treebank dataset. On the BBC and IMDB datasets, our model assigns a significantly high gating value on the un- known word token UNK compared to the other words.</p><p>We observe that pre-training results the different distributions of gating values. As can be seen in <ref type="figure" target="#fig_3">Fig. 2 (a)</ref>, the gating value trained in the gated word &amp; character model without pre-training is in general higher for less frequent words, implying that the re- current language model has learned to exploit the spelling of a word when its word vector could not have been estimated properly. <ref type="figure" target="#fig_3">Fig. 2 (b)</ref> shows that the gating value trained in the gated word &amp; charac- ter model with pre-training is less correlated with the frequency ranks than the one without pre-training. The pre-training step initializes a word lookup table using the training corpus and includes its informa- tion into the initial values. We hypothesize that the recurrent language model tends to be word-input- oriented if the informativeness of word inputs and character inputs are not balanced especially in the early stage of training.</p><p>Although the recurrent language model with or without pre-training derives different gating values, the results are still similar. We conjecture that the flexibility of modulating between word-level and character-level representations resulted in a better language model in multiple ways.</p><p>Overall, the gating values are small. However, this does not mean the model does not utilize the character-level inputs. We observed that the word vectors constructed from the character-level inputs usually have a larger L2 norm than the word vec- tors constructed from the word-level inputs do. For instance, the mean values of L2 norm of the 1000 most frequent words in the IMDB training set are 52.77 and 6.27 respectively. The small gate values compensate for this difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced a recurrent neural network language model with LSTM units and a word-character gate. Our model was empirically found to utilize the character-level input especially when the model en- counters rare words. The experimental results sug- gest the gate can be efficiently trained so that the model can find a good balance between the word- level and character-level inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>PTB</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>We test five different model architectures on the three English corpora. Each model has a unique word embedding method, but all models share the same LSTM language modeling architecture, that has 2 LSTM layers with 200 hidden units, d = 200. Except for the character only model, weights in the language modeling part are initialized with uniform random variables between -0.1 and 0.1. Weights of a bidirectional LSTM in the word embedding part are initialized with Xavier initialization (Glorot and Bengio, 2010). All biases are initialized to zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>a) Gated word &amp; character. (b) Gated word &amp; character with pre-taining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A log-log plot of frequency ranks and gating values trained in the gated word &amp; character models with/without pretraining.</figDesc><graphic url="image-2.png" coords="5,90.67,62.81,196.56,135.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Validation and test perplexities on Penn Treebank (PTB), BBC, IMDB Movie Reviews datasets.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The size of each dataset. 

quence similar to the C2W model in (Ling et al., 
2015a). The bidirectional LSTMs have 200 hidden 
units, and their weights are initialized with Xavier 
initialization. In addition, the weights of the forget, 
input, and output gates are scaled by a factor of 4. 
The weights in the LSTM language model are also 
initialized with Xavier initialization. All biases are 
initialized to zero. A learning rate is fixed at 0.2. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 1 compares</head><label>1</label><figDesc>the models on each dataset. On the PTB and IMDB Movie Review dataset, the gated word &amp; character model with a fixed gating value, g const = 0.25, and pre-training achieves the lowest perplexity . On the BBC datasets, the gated word &amp; character model without pre-training achieves the lowest perplexity. Even though the model with fixed gating value performs well, choosing the gating value is not clear and might depend on characteristics of datasets such as size. The model with adaptive gating values does not require tuning it and achieves similar perplexity.</figDesc><table></table></figure>

			<note place="foot" n="1"> softmax function is defined as f (xi) = exp x i  k exp x k .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is done as a part of the course DS-GA 1010-001 Independent Study in Data Science at the Center for Data Science, New York Univer-sity. KC thanks the support by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of <ref type="bibr">Excellence 2015</ref><ref type="bibr">Excellence-2016</ref>. YM thanks Ken-taro Hanaki, Israel Malkin, and Tian Wang for their helpful feedback. KC and YM thanks the anony-mous reviewers for their insightful comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alternative structures for character-level rnns. CoRR, abs/1511.06303. [dos Santos and Zadrozny2014] Cícero Nogueira dos Santos and Bianca Zadrozny</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Glorot and Bengio2010] Xavier Glorot and Yoshua Bengio</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Beijing, China; Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>Chia Laguna Resort</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical solutions to the problem of diagonal dominance in kernel document clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Padraig</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006)</title>
		<meeting><address><addrLine>Pittsburgh, Pennsylvania, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06-25" />
			<biblScope unit="page" from="377" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive joint learning of compositional and non-compositional phrase embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno>abs/1602.02410</idno>
	</analytic>
	<monogr>
		<title level="j">Rafal Józefowicz</title>
		<editor>Mike Schuster, Noam Shazeer, and Yonghui Wu</editor>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Exploring the limits of language modeling</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mandarin word-character hybridinput neural network language model</title>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2011, 12th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-08-27" />
			<biblScope unit="page" from="625" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding function in form: Compositional character models for open vocabulary word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Kim</surname></persName>
		</author>
		<idno>abs/1604.00788</idno>
	</analytic>
	<monogr>
		<title level="m">The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference</title>
		<meeting><address><addrLine>Lisbon; Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2011-06-24" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
<note type="report_type">Portugal</note>
	<note>Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>Tomas Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan Kombrink</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network regularization. CoRR, abs/1409.2329</title>
	</analytic>
	<monogr>
		<title level="m">Theano: A Python framework for fast computation of mathematical expressions. arXiv eprints, abs/1605.02688</title>
		<meeting><address><addrLine>Jonathan May, and Kevin Knight</addrLine></address></meeting>
		<imprint>
			<publisher>Ashish Vaswani</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Zoph et al.2016. Simple, fast noise-contrastive estimation for large rnn vocabularies. NAACL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
