<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WordRank: Learning Word Embeddings via Robust Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Ji</surname></persName>
							<email>shihao.ji@intel.com</email>
							<affiliation key="aff0">
								<orgName type="department">Santa Cruz</orgName>
								<orgName type="laboratory">Parallel Computing Lab</orgName>
								<orgName type="institution" key="instit1">Intel</orgName>
								<orgName type="institution" key="instit2">Purdue University</orgName>
								<orgName type="institution" key="instit3">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Santa Cruz</orgName>
								<orgName type="laboratory">Parallel Computing Lab</orgName>
								<orgName type="institution" key="instit1">Intel</orgName>
								<orgName type="institution" key="instit2">Purdue University</orgName>
								<orgName type="institution" key="instit3">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Santa Cruz</orgName>
								<orgName type="laboratory">Parallel Computing Lab</orgName>
								<orgName type="institution" key="instit1">Intel</orgName>
								<orgName type="institution" key="instit2">Purdue University</orgName>
								<orgName type="institution" key="instit3">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Matsushima</surname></persName>
							<email>shin matsushima@mist. i.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Santa Cruz</orgName>
								<orgName type="laboratory">Parallel Computing Lab</orgName>
								<orgName type="institution" key="instit1">Intel</orgName>
								<orgName type="institution" key="instit2">Purdue University</orgName>
								<orgName type="institution" key="instit3">University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Santa Cruz</orgName>
								<orgName type="laboratory">Parallel Computing Lab</orgName>
								<orgName type="institution" key="instit1">Intel</orgName>
								<orgName type="institution" key="instit2">Purdue University</orgName>
								<orgName type="institution" key="instit3">University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">WordRank: Learning Word Embeddings via Robust Ranking</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="658" to="668"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Embedding words in a vector space has gained a lot of attention in recent years. While state-of-the-art methods provide efficient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight , we propose a novel framework Wor-dRank that efficiently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the-arts on large corpora, while outperforms them by a significant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Embedding words into a vector space, such that se- mantic and syntactic regularities between words are preserved, is an important sub-task for many appli- cations of natural language processing. <ref type="bibr" target="#b22">Mikolov et al. (2013a)</ref> generated considerable excitement in the machine learning and natural language processing communities by introducing a neural network based model, which they call word2vec. It was shown that word2vec produces state-of-the-art performance on both word similarity as well as word analogy tasks. The word similarity task is to retrieve words that are similar to a given word. On the other hand, word analogy requires answering queries of the form a:b;c:?, where a, b, and c are words from the vocab- ulary, and the answer to the query must be semanti- cally related to c in the same way as b is related to a. This is best illustrated with a concrete example: Given the query king:queen;man:? we expect the model to output woman.</p><p>The impressive performance of word2vec led to a flurry of papers, which tried to explain and im- prove the performance of word2vec both theoreti- cally ( <ref type="bibr" target="#b1">Arora et al., 2015</ref>) and empirically ( <ref type="bibr" target="#b16">Levy and Goldberg, 2014)</ref>. One interpretation of word2vec is that it is approximately maximizing the positive pointwise mutual information (PMI), and <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> showed that directly optimizing this gives good results. On the other hand, <ref type="bibr" target="#b25">Pennington et al. (2014)</ref> showed performance comparable to word2vec by using a modified matrix factorization model, which optimizes a log loss. Somewhat surprisingly, <ref type="bibr" target="#b17">Levy et al. (2015)</ref> showed that much of the performance gains of these new word embedding methods are due to certain hyper- parameter optimizations and system-design choices. In other words, if one sets up careful experiments, then existing word embedding models more or less perform comparably to each other. We conjecture that this is because, at a high level, all these methods are based on the following template: From a large text corpus eliminate infrequent words, and compute a |W| × |C| word-context co-occurrence count ma- trix; a context is a word which appears less than d distance away from a given word in the text, where d is a tunable parameter. Let w ∈ W be a word and c ∈ C be a context, and let X w,c be the (poten- tially normalized) co-occurrence count. One learns a function f (w, c) which approximates a transformed version of X w,c . Different methods differ essentially in the transformation function they use and the para- metric form of f ( <ref type="bibr" target="#b17">Levy et al., 2015</ref>). For exam- ple, <ref type="bibr">GloVe (Pennington et al., 2014</ref>) uses f (w, c) = u w , v c where u w and v c are k dimensional vec- tors, ·, ·· denotes the Euclidean dot product, and one approximates f (w, c) ≈ log X w,c . On the other hand, as <ref type="bibr" target="#b16">Levy and Goldberg (2014)</ref> show, word2vec can be seen as using the same f (w, c) as GloVe but trying to approximate f (w, c) ≈ PMI(X w,c ) − log n, where PMI(·) is the pairwise mutual informa- tion <ref type="bibr" target="#b8">(Cover and Thomas, 1991)</ref> and n is the number of negative samples.</p><p>In this paper, we approach the word embedding task from a different perspective by formulating it as a ranking problem. That is, given a word w, we aim to output an ordered list (c 1 , c 2 , · · · ) of context words from C such that words that co- occur with w appear at the top of the list. If rank(w, c) denotes the rank of c in the list, then typ- ical ranking losses optimize the following objective:</p><p>(w,c)∈Ω ρ (rank(w, c)), where Ω ⊂ W × C is the set of word-context pairs that co-occur in the corpus, and ρ(·) is a ranking loss function that is monotoni- cally increasing and concave (see Sec. 2 for a justi- fication).</p><p>Casting word embedding as ranking has two dis- tinctive advantages. First, our method is discrimina- tive rather than generative; in other words, instead of modeling (a transformation of) X w,c directly, we only aim to model the relative order of X w,· val- ues in each row. This formulation fits naturally to popular word embedding tasks such as word simi- larity/analogy since instead of the likelihood of each word, we are interested in finding the most relevant words in a given context 1 . Second, casting word 1 Roughly speaking, this difference in viewpoint is analo- gous to the difference between pointwise loss function vs list- embedding as a ranking problem enables us to de- sign models robust to noise <ref type="bibr" target="#b32">(Yun et al., 2014</ref>) and focusing more on differentiating top relevant words, a kind of attention mechanism that has been proved very useful in deep learning <ref type="bibr" target="#b14">(Larochelle and Hinton, 2010;</ref><ref type="bibr" target="#b24">Mnih et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. Both issues are very critical in the domain of word embedding since (1) the co-occurrence matrix might be noisy due to grammatical errors or unconven- tional use of language, i.e., certain words might co- occur purely by chance, a phenomenon more acute in smaller document corpora collected from diverse sources; and (2) it's very challenging to sort out a few most relevant words from a very large vocabu- lary, thus some kind of attention mechanism that can trade off the resolution on most relevant words with the resolution on less relevant words is needed. We will show in the experiments that our method can mitigate some of these issues; with 17 million to- kens our method performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Word Embedding via Ranking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>We use w to denote a word and c to denote a con- text. The set of all words, that is, the vocabulary is denoted as W and the set of all context words is denoted C. We will use Ω ⊂ W × C to denote the set of all word-context pairs that were observed in the data, Ω w to denote the set of contexts that co- occured with a given word w, and similarly Ω c to denote the words that co-occurred with a given con- text c. The size of a set is denoted as |·|. The inner product between vectors is denoted as ·, ··.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking Model</head><p>Let u w denote the k-dimensional embedding of a word w, and v c denote that of a context c. For convenience, we collect embedding parameters for words and contexts as U := {u w } w∈W , and V := {v c } c∈C .</p><p>We aim to capture the relevance of context c for word w by the inner product between their embed- ding vectors, u w , v c ; the more relevant a context is, the larger we want their inner product to be.</p><p>wise loss function used in ranking ( <ref type="bibr" target="#b15">Lee and Lin, 2013</ref>).</p><p>We achieve this by learning a ranking model that is parametrized by U and V. If we sort the set of con- texts C for a given word w in terms of each context's inner product score with the word, the rank of a spe- cific context c in this list can be written as <ref type="bibr" target="#b28">(Usunier et al., 2009)</ref>:</p><formula xml:id="formula_0">rank (w, c) = c ∈C\{c} I (u w , v c − u w , v c ≤ 0) = c ∈C\{c} I (u w , v c − v c ≤ 0) , (1)</formula><p>where I(x ≤ 0) is a 0-1 loss function which is 1 if x ≤ 0 and 0 otherwise. Since I(x ≤ 0) is a dis- continuous function, we follow the popular strategy in machine learning which replaces the 0-1 loss by its convex upper bound (·), where (·) can be any popular loss function for binary classification such as the hinge loss (x) = max (0, 1 − x) or the lo- gistic loss (x) = log 2 (1 + 2 −x ) ( <ref type="bibr">Bartlett et al., 2006</ref>). This enables us to construct the following convex upper bound on the rank:</p><formula xml:id="formula_1">rank (w, c) ≤ rank (w, c)= c ∈C\{c} (u w , v c −v c ) (2)</formula><p>It is certainly desirable that the ranking model po- sitions relevant contexts at the top of the list; this motivates us to write the objective function to mini- mize as:</p><formula xml:id="formula_2">J (U, V):= w∈W c∈Ωw r w,c ·ρ rank (w, c)+β α<label>(3)</label></formula><p>where r w,c is the weight between word w and con- text c quantifying the association between them, ρ(·) is a monotonically increasing and concave ranking loss function that measures goodness of a rank, and α &gt; 0, β &gt; 0 are the hyperparameters of the model whose role will be discussed later. Following Pen- nington et al. <ref type="formula" target="#formula_3">(2014)</ref>, we use</p><formula xml:id="formula_3">r w,c = (X w,c /x max ) if X w,c &lt; x max 1 otherwise,<label>(4)</label></formula><p>where we set x max = 100 and = 0.75 in our ex- periments. That is, we assign larger weights (with a saturation) to contexts that appear more often with the word of interest, and vice-versa. For the ranking loss function ρ(·), on the other hand, we consider the class of monotonically increasing and concave functions. While monotonicity is a natural require- ment, we argue that concavity is also important so that the derivative of ρ is always non-increasing; this implies that the ranking loss to be the most sensitive at the top of the list (where the rank is small) and becomes less sensitive at the lower end of the list (where the rank is high). Intuitively this is desir- able, because we are interested in a small number of relevant contexts which frequently co-occur with a given word, and thus are willing to tolerate errors on infrequent contexts 2 . Meanwhile, this insensitivity at the bottom of the list makes the model robust to noise in the data either due to grammatical errors or unconventional use of language. Therefore, a sin- gle ranking loss function ρ(·) serves two different purposes at two ends of the curve (see the example plots of ρ in <ref type="figure" target="#fig_0">Figure 1</ref>); while the left hand side of the curve encourages "high resolution" on most rel- evant words, the right hand side becomes less sen- sitive (with "low resolution") to infrequent and pos- sibly noisy words <ref type="bibr">3</ref> . As we will demonstrate in our experiments, this is a fundamental attribute (in addi- tion to the ranking nature) of our method that con- tributes its superior performance as compared to the state-of-the-arts when the training set is limited (i.e., sparse and noisy). What are interesting loss functions that can be used for ρ (·)? Here are four possible alternatives, all of which have a natural interpretation (see the plots of all four ρ functions in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and the related work in Sec. 3 for a discussion). We will explore the performance of each of these variants in our experiments. For now, we turn our attention to efficient stochastic optimization of the objective function (3).</p><formula xml:id="formula_4">ρ 0 (x) := x (identity) (5) ρ 1 (x) := log 2 (1 + x) (logarithm) (6) ρ 2 (x) := 1− 1 log 2 (2 + x) (negative DCG) (7) ρ 3 (x) := x 1−t − 1 1 − t (log t with t = 1) (8) 0 0.5 1 1.5 2 −1 −0.5 0 0.5 1 1.5 2 x ρ(x) ρ 0 (x) ρ 1 (x) ρ 2 (x)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Stochastic Optimization</head><p>Plugging <ref type="formula">(2)</ref> into <ref type="formula" target="#formula_2">(3)</ref>, and replacing w∈W c∈Ωw by (w,c)∈Ω , the objective function becomes:</p><formula xml:id="formula_5">J (U, V) = (w,c)∈Ω r w,c · ρ c ∈C\{c} (u w , v c −v c )+β α .<label>(9)</label></formula><p>This function contains summations over Ω and C, both of which are expensive to compute for a large corpus. Although stochastic gradient descent (SGD) (Bottou and Bousquet, 2011) can be used to re- place the summation over Ω by random sampling, the summation over C cannot be avoided unless ρ(·) is a linear function. To work around this problem, we propose to optimize a linearized upper bound of the objective function obtained through a first-order Taylor approximation. Observe that due to the con- cavity of ρ(·), we have</p><formula xml:id="formula_6">ρ(x) ≤ ρ ξ −1 + ρ ξ −1 · x − ξ −1<label>(10)</label></formula><p>for any x and ξ = 0. Moreover, the bound is tight when ξ = x −1 . This motivates us to introduce a set of auxiliary parameters Ξ := {ξ w,c } (w,c)∈Ω and define the following upper bound of J (U, V):</p><formula xml:id="formula_7">J (U, V, Ξ) := (w,c)∈Ω r w,c · ρ(ξ −1 wc ) + ρ (ξ −1 wc ) · α −1 β +α −1 c ∈C\{c} (u w , v c −v c )−ξ −1 w,c .<label>(11)</label></formula><p>Note that J (U, V) ≤ J (U, V, Ξ) for any Ξ, due to (10) <ref type="bibr">4</ref> . Also, minimizing (11) yields the same U and V as minimizing <ref type="formula" target="#formula_5">(9)</ref>. To see this, supposê U := {û w } w∈W andˆVandˆ andˆV := {ˆv{ˆv c } c∈C minimizes (9). Then, by lettingˆΞlettingˆ lettingˆΞ := ˆ ξ w,c</p><formula xml:id="formula_8">(w,c)∈Ω wherê wherê ξ w,c = α c ∈C\{c} (ˆ u w , ˆ v c − ˆ v c ) + β ,<label>(12)</label></formula><formula xml:id="formula_9">we have J ˆ U, ˆ V, ˆ Ξ = J ˆ U, ˆ V</formula><p>. Therefore, it suffices to optimize (11). However, unlike (9), (11) admits an efficient SGD algorithm. To see this, rewrite (11) as</p><formula xml:id="formula_10">J(U,V, Ξ) = (w,c,c ) r w,c · ρ(ξ −1 w,c )+ρ (ξ −1 w,c )·(α −1 β−ξ −1 w,c ) |C| − 1 + 1 α ρ (ξ −1 w,c ) · (u w , v c − v c ) ,<label>(13)</label></formula><p>where (w, c, c ) ∈ Ω × (C \ {c}). Then, it can be seen that if we sample uniformly from (w, c) ∈ Ω and c ∈ C \ {c}, then j(w, c, c ) :=</p><formula xml:id="formula_11">|Ω|·(|C|−1) · r w,c · ρ(ξ −1 w,c )+ρ (ξ −1 w,c )·(α −1 β −ξ −1 w,c ) |C| − 1 + 1 α ρ (ξ −1 w,c )· (u w , v c − v c ) ,<label>(14)</label></formula><p>which does not contain any expensive summa- tions and is an unbiased estimator of <ref type="formula" target="#formula_2">(13)</ref>, i.e., E [j(w, c, c )] = J (U, V, Ξ). On the other hand, one can optimize ξ w,c exactly by using (12). Putting everything together yields a stochastic optimiza- tion algorithm WordRank, which can be special- ized to a variety of ranking loss functions ρ(·) with weights r w,c (e.g., DCG (Discounted Cumulative Gain) ( <ref type="bibr" target="#b21">Manning et al., 2008</ref>) is one of many pos- sible instantiations). Algorithm 1 contains detailed pseudo-code. It can be seen that the algorithm is di- vided into two stages: a stage that updates (U, V) and another that updates Ξ. Note that the time com- plexity of the first stage is O(|Ω|) since the cost of each update in Lines 8-10 is independent of the size of the corpus. On the other hand, the time complex- ity of updating Ξ in Line 15 is O(|Ω| |C|), which can be expensive. To amortize this cost, we em- ploy two tricks: we only update Ξ after a few it- erations of U and V update, and we exploit the fact that the most computationally expensive operation in (12) involves a matrix and matrix multiplication which can be calculated efficiently via the SGEMM routine in BLAS ( <ref type="bibr" target="#b10">Dongarra et al., 1990</ref>).</p><p>Algorithm 1 WordRank algorithm.  Sample (w, c) uniformly from Ω</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Sample c uniformly from C \ {c}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>// following three updates are executed simultaneously 8:</p><formula xml:id="formula_12">u w ← u w − η · r w,c · ρ (ξ −1 w,c ) · (u w , v c −v c ) · (v c −v c ) 9: v c ← v c − η · r w,c · ρ (ξ −1 w,c ) · (u w , v c −v c ) · u w 10: v c ← v c + η · r w,c · ρ (ξ −1 w,c ) · (u w , v c −v c ) · u w 11:</formula><p>until U and V are converged end for 18: until U, V and Ξ are converged</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Parallelization</head><p>The updates in Lines 8-10 have one remarkable property: To update u w , v c and v c , we only need to read the variables u w , v c , v c and ξ w,c . What this means is that updates to another triplet of variables u ˆ w , v ˆ c and v ˆ c can be performed independently. This observation is the key to developing a parallel optimization strategy, by distributing the computa- tion of the updates among multiple processors. Due to lack of space, details including pseudo-code are relegated to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Interpreting of α and β</head><p>The update (12) indicates that ξ −1 w,c is proportional to rank (w, c). On the other hand, one can observe that the loss function (·) in <ref type="formula" target="#formula_3">(14)</ref> is weighted by a ρ ξ −1</p><formula xml:id="formula_13">w,c</formula><p>term. Since ρ (·) is concave, its gradient ρ (·) is monotonically non-increasing <ref type="bibr" target="#b27">(Rockafellar, 1970)</ref>. Consequently, when rank (w, c) and hence</p><formula xml:id="formula_14">ξ −1 w,c is large, ρ ξ −1 w,c</formula><p>is small. In other words, the loss function "gives up" on contexts with high ranks in order to focus its attention on top of the list. The rate at which the algorithm gives up is determined by the hyperparameters α and β. For the illustration of this effect, see the example plots of ρ 1 with dif- ferent α and β in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Intuitively, α can be viewed as a scale parameter while β can be viewed as an offset parameter. An equivalent interpretation is that by choosing different values of α and β one can modify the behavior of the ranking loss ρ (·) in a problem dependent fashion. In our experiments, we found that a common setting of α = 1 and β = 0 of- ten yields uncompetitive performance, while setting α = 100 and β = 99 generally gives good results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Our work sits at the intersection of word embed- ding and ranking optimization. As we discussed in Sec. 2.2 and Sec. 2.5, it's also related to the atten- tion mechanism widely used in deep learning. We therefore review the related work along these three axes.</p><p>Word Embedding. We already discussed some related work (word2vec and GloVe) on word em- bedding in the introduction. Essentially, word2vec and GloVe derive word representations by modeling a transformation (PMI or log) of X w,c directly, while</p><p>WordRank learns word representations via robust ranking. Besides these state-of-the-art techniques, a few ranking-based approaches have been proposed for word embedding recently, e.g., <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b29">Vilnis and McCallum, 2015;</ref><ref type="bibr" target="#b18">Liu et al., 2015)</ref>. However, all of them adopt a pair-wise binary classification approach with a linear rank- ing loss ρ 0 . For example, <ref type="bibr" target="#b7">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b29">Vilnis and McCallum, 2015</ref>) employ a hinge loss on positive/negative word pairs to learn word representations and ρ 0 is used implicitly to evaluate ranking losses. As we discussed in Sec. 2.2, ρ 0 has no benefit of the attention mechanism and robust- ness to noise since its linearity treats all the rank- ing errors uniformly; empirically, sub-optimal per- formances are often observed with ρ 0 in our exper- iments. More recently, by extending the Skip-Gram model of word2vec, <ref type="bibr" target="#b18">Liu et al. (2015)</ref> incorporates additional pair-wise constraints induced from 3rd- party knowledge bases, such as WordNet, and learns word representations jointly. In contrast, WordRank is a fully ranking-based approach without using any additional data source for training.</p><p>Robust Ranking. The second line of work that is very relevant to WordRank is that of ranking objec- tive (3). The use of score functions u w , v c for ranking is inspired by the latent collaborative re- trieval framework of <ref type="bibr" target="#b31">Weston et al. (2012)</ref>. Writing the rank as a sum of indicator functions (1), and upper bounding it via a convex loss (2) is due to <ref type="bibr" target="#b28">Usunier et al. (2009)</ref>. Using ρ 0 (·) (5) corresponds to the well-known pairwise ranking loss (see e.g., <ref type="bibr" target="#b15">(Lee and Lin, 2013)</ref>). On the other hand, <ref type="bibr" target="#b32">Yun et al. (2014)</ref> observed that if they set ρ = ρ 2 as in <ref type="formula">(7)</ref>, then −J (U, V) corresponds to the DCG (Dis- counted Cumulative Gain), one of the most popular ranking metrics used in web search ranking <ref type="bibr" target="#b21">(Manning et al., 2008</ref>). In their RobiRank algorithm they proposed the use of ρ = ρ 1 (6), which they consid- ered to be a special function for which one can de- rive an efficient stochastic optimization procedure. However, as we showed in this paper, the general class of monotonically increasing concave functions can be handled efficiently. Another important differ- ence of our approach is the hyperparameters α and β, which we use to modify the behavior of ρ, and which we find are critical to achieve good empirical results. <ref type="bibr" target="#b9">Ding and Vishwanathan (2010)</ref> proposed the use of ρ = log t in the context of robust binary classi- fication, while here we are concerned with ranking, and our formulation is very general and applies to a variety of ranking losses ρ (·) with weights r w,c . Op- timizing over U and V by distributing the computa- tion across processors is inspired by work on dis- tributed stochastic gradient for matrix factorization <ref type="bibr" target="#b12">(Gemulla et al., 2011</ref>).</p><p>Attention. Attention is one of the most impor- tant advancements in deep learning in recent years ( <ref type="bibr" target="#b14">Larochelle and Hinton, 2010)</ref>, and is now widely used in state-of-the-art image recognition and ma- chine translation systems ( <ref type="bibr" target="#b24">Mnih et al., 2014;</ref><ref type="bibr" target="#b2">Bahdanau et al., 2015)</ref>. Recently, attention has also been applied to the domain of word embedding. For ex- ample, under the intuition that not all contexts are created equal, <ref type="bibr" target="#b30">Wang et al. (2015)</ref> assign an impor- tance weight to each word type at each context po- sition and learn an attention-based Continuous Bag- Of-Words (CBOW) model. Similarly, within a rank- ing framework, WordRank expresses the context im- portance by introducing the auxiliary variable ξ w,c , which "gives up" on contexts with high ranks in or- der to focus its attention on top of the list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In our experiments, we first evaluate the impact of the weight r w,c and the ranking loss function ρ(·) on the test performance using a small dataset. We then pick the best performing model and compare it against word2vec ( <ref type="bibr" target="#b23">Mikolov et al., 2013b</ref>) and GloVe ( <ref type="bibr" target="#b25">Pennington et al., 2014</ref>). We closely follow the framework of <ref type="bibr" target="#b17">Levy et al. (2015)</ref> to set up a careful and fair comparison of the three methods. Our code is publicly available at https://bitbucket. org/shihaoji/wordrank.</p><p>Training Corpus Models are trained on a com- bined corpus of 7.2 billion tokens, which consists of the 2015 Wikipedia dump with 1.6 billion tokens, the WMT14 News Crawl 5 with 1.7 billion tokens, the "One Billion Word Language Modeling Bench- mark" 6 with almost 1 billion tokens, and UMBC Corpus Size 17M * 32M 64M 128M 256M 512M 1.0B 1.6B 7.2B Vocabulary Size <ref type="table" target="#tab_0">|W| 71K 100K 100K 200K 200K 300K 300K 400K 620K  Window Size win  15  15  15  10  10  10  10  10  10  Dimension k  100  100  100  200  200  300  300  300  300</ref> * This is the Text8 dataset from http://mattmahoney.net/dc/text8.zip, which is widely used for word embedding demo.  webbase corpus 7 with around 3 billion tokens. The pre-processing pipeline breaks the paragraphs into sentences, tokenizes and lowercases each corpus with the Stanford tokenizer. We further clean up the dataset by removing non-ASCII characters and punctuation, and discard sentences that are shorter than 3 tokens or longer than 500 tokens. In the end, we obtain a dataset of 7.2 billion tokens, with the first 1.6 billion tokens from Wikipedia. When we want to experiment with a smaller corpus, we ex- tract a subset which contains the specified number of tokens.</p><p>Co-occurrence matrix construction We use the GloVe code to construct the co-occurrence matrix X, and the same matrix is used to train GloVe and WordRank models. When constructing X, we must choose the size of the vocabulary, the context win- dow and whether to distinguish left context from right context. We follow the findings and design choices of GloVe and use a symmetric window of size win with a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count. Specifically, when the corpus is small (e.g., 17M, 32M, 64M) we let win = 15 and for larger corpora we let win = 10. The larger win- dow size alleviates the data sparsity issue for small corpus at the expense of adding more noise to X. The parameter settings used in our experiments are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Using the trained model It has been shown by <ref type="bibr" target="#b25">Pennington et al. (2014)</ref> that combining the u w and v c vectors with equal weights gives a small boost 7 http://ebiquity.umbc.edu/resource/html/ id/351 in performance. This vector combination was origi- nally motivated as an ensemble method ( <ref type="bibr" target="#b25">Pennington et al., 2014</ref>), and later <ref type="bibr" target="#b17">Levy et al. (2015)</ref> provided a different interpretation of its effect on the cosine similarity function, and show that adding context vectors effectively adds first-order similarity terms to the second-order similarity function. In our ex- periments, we find that vector combination boosts the performance in word analogy task when training set is small, but when dataset is large enough (e.g., 7.2 billion tokens), vector combination doesn't help anymore. More interestingly, for the word similarity task, we find that vector combination is detrimen- tal in all the cases, sometimes even substantially 8 . Therefore, we will always use u w on word similarity task, and use u w + v c on word analogy task unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation</head><p>Word Similarity We use six datasets to evaluate word similarity: WS-353 ( <ref type="bibr" target="#b11">Finkelstein et al., 2002</ref> Word Analogies For this task, we use the Google analogy dataset ( <ref type="bibr" target="#b22">Mikolov et al., 2013a)</ref>. It contains 19544 word analogy questions, partitioned into 8869 semantic and 10675 syntactic questions. A question is correctly answered only if the algorithm selects the word that is exactly the same as the correct word in the question: synonyms are thus counted as mis- takes. There are two ways to answer these questions, namely, by using 3CosAdd or 3CosMul (see <ref type="bibr" target="#b16">(Levy and Goldberg, 2014</ref>) for details). We will report scores by using 3CosAdd by default, and indicate when 3CosMul gives better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The impact of r w,c and ρ(·)</head><p>In Sec. 2.2 we argued the need for adding weight r w,c to ranking objective (3), and we also presented our framework which can deal with a variety of ranking loss functions ρ. We now study the utility of these two ideas. We report results on the 17 mil- lion token dataset in Table 2. For the similarity task, we use the WS-353 test set and for the analogy task we use the Google analogy test set. The best scores for each task are underlined. We set t = 1.5 for ρ 3 . "Off" means that we used uniform weight r w,c = 1, and "on" means that r w,c was set as in (4). For com- parison, we also include the results using RobiRank ( <ref type="bibr" target="#b32">Yun et al., 2014)</ref>  <ref type="bibr">9</ref> .</p><p>It can be seen from <ref type="table" target="#tab_1">Table 2</ref> that adding the weight r w,c improves performance in all the cases, espe- cially on the word analogy task. Among the four ρ functions, ρ 0 performs the best on the word simi- larity task but suffers notably on the analogy task, while ρ 1 = log performs the best overall. Given these observations, which are consistent with the re- sults on large scale datasets, in the experiments that follow we only report WordRank with the best con- figuration, i.e., using ρ 1 with the weight r w,c as de- fined in (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to state-of-the-arts</head><p>In this section we compare the performance of Wor- dRank with word2vec 10 and GloVe 11 , by using the code provided by the respective authors. For a fair comparison, GloVe and WordRank are given as in- put the same co-occurrence matrix X; this elimi- nates differences in performance due to window size and other such artifacts, and the same parameters are used to word2vec. Moreover, the embedding di- mensions used for each of the three methods is the same (see <ref type="table" target="#tab_0">Table 1</ref>). With word2vec, we train the Skip-Gram with Negative Sampling (SGNS) model since it produces state-of-the-art performance, and is widely used in the NLP community <ref type="bibr" target="#b23">(Mikolov et al., 2013b</ref>). For GloVe, we use the default parame- ters as suggested by <ref type="bibr" target="#b25">(Pennington et al., 2014</ref>). The results are provided in <ref type="figure">Figure 2</ref> (also see <ref type="table">Table 4</ref> in the supplementary material for additional details).</p><p>As can be seen, when the size of corpus increases, in general all three algorithms improve their predic- tion accuracy on both tasks. This is to be expected since a larger corpus typically produces better statis- tics and less noise in the co-occurrence matrix X. When the corpus size is small (e.g., 17M, 32M, 64M, 128M), WordRank yields the best performance with significant margins among three, followed by word2vec and GloVe; when the size of corpus in- creases further, on the word analogy task word2vec and GloVe become very competitive to WordRank, and eventually perform neck-to-neck to each other <ref type="figure">(Figure 2(b)</ref>). This is consistent with the findings of ( <ref type="bibr" target="#b17">Levy et al., 2015)</ref> indicating that when the number of tokens is large even simple algorithms can per- form well. On the other hand, WordRank is dom- inant on the word similarity task for all the cases <ref type="figure">(Figure 2(a)</ref>) since it optimizes a ranking loss explic- itly, which aligns more naturally with the objective of word similarity than the other methods; with 17 million tokens our method performs almost as well as existing methods using 7.2 billion tokens on the word similarity benchmark.</p><p>To further evaluate the model performance on the word similarity/analogy tasks, we use the best per- forming models trained on the 7.2-billion-token cor- pus to predict on the six word similarity datasets de- scribed in Sec. 4.1. Moreover, we breakdown the performance of the models on the Google word anal- ogy dataset into the semantic and syntactic subtasks. Results are listed in <ref type="table" target="#tab_3">Table 3</ref>. As can be seen, Wor- dRank outperforms word2vec and GloVe on 5 of 6 similarity tasks, and 1 of 2 Google analogy subtasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Visualizing the results</head><p>To understand whether WordRank produces syntat- ically and semantically meaningful vector space, we did the following experiment: we use the best performing model produced using 7.2 billion to- kens, and compute the nearest neighbors of the word "cat". We then visualize the words in two dimen- sions by using t-SNE <ref type="bibr" target="#b20">(Maaten and Hinton, 2008)</ref>. As can be seen in <ref type="figure" target="#fig_5">Figure 3</ref>, our ranking-based model is indeed capable of capturing both semantic (e.g., cat, feline, kitten, tabby) and syntactic (e.g., leash, leashes, leashed) regularities of the English lan- guage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed WordRank, a ranking-based approach, to learn word representations from large scale tex- tual corpora. The most prominent difference be- tween our method and the state-of-the-art tech- niques, such as word2vec and GloVe, is that Wor- dRank learns word representations via a robust rank- ing model, while word2vec and GloVe typically model a transformation of co-occurrence count X w,c directly. Moreover, by a ranking loss function ρ(·), WordRank achieves its attention mechanism and ro- bustness to noise naturally, which are usually lack- ing in other ranking-based approaches. These at- tributes significantly boost the performance of Wor- dRank in the cases where training data are sparse and noisy. Our multi-node distributed implementation of WordRank is publicly available for general usage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Visualizing different ranking loss functions ρ(x) as defined in Eqs. (5-8); the lower part of ρ3(x) is truncated in order to visualize the other functions better. (b) Visualizing ρ1((x + β)/α) with different α and β; ρ0 is included to illustrate the dramatic scale differences between ρ0 and ρ1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for w ∈ W do 14: for c ∈ C do 15: ξ w,c = α/ c ∈C\{c} (u w , v c −v c )+β 16: end for 17:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) partitioned into two subsets: WordSim Similarity and WordSim Relatedness (Agirre et al., 2009); MEN (Bruni et al., 2012); Mechanical Turk (Radin- sky et al., 2011); Rare words (Luong et al., 2013); and SimLex-999 (Hill et al., 2014). They contain word pairs together with human-assigned similarity judgments. The word representations are evaluated by ranking the pairs according to their cosine simi- larities, and measuring the Spearman's rank correla- tion coefficient with the human judgments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Nearest neighbors of "cat" found by projecting a 300d word embedding learned from WordRank onto a 2d space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Parameter settings used in the experiments.</head><label>1</label><figDesc></figDesc><table>Task 
Robi 
ρ 0 
ρ 1 
ρ 2 
ρ 3 
off 
on 
off 
on 
off 
on 
off 
on 
Similarity 41.2 69.0 71.0 66.7 70.4 66.8 70.8 68.1 68.0 
Analogy 
22.7 24.9 31.9 34.3 44.5 32.3 40.4 33.6 42.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance of different ρ functions on Text8 dataset with 17M tokens. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of the best word2vec, GloVe and WordRank models, learned from 7.2 billion tokens, on six similarity tasks 

and Google semantic and syntactic subtasks. 

</table></figure>

			<note place="foot" n="2"> This is similar to the attention mechanism found in human visual system that is able to focus on a certain region of an image with &quot;high resolution&quot; while perceiving the surrounding image in &quot;low resolution&quot; (Larochelle and Hinton, 2010; Mnih et al., 2014). 3 Due to the linearity of ρ0(x) = x, this ranking loss doesn&apos;t have the benefit of attention mechanism and robustness to noise since it treats all ranking errors uniformly.</note>

			<note place="foot" n="4"> When ρ = ρ0, one can simply set the auxiliary variables ξw,c = 1 because ρ0 is already a linear function.</note>

			<note place="foot" n="5"> http://www.statmt.org/wmt14/ translation-task.html 6 http://www.statmt.org/lm-benchmark</note>

			<note place="foot" n="8"> This is possible since we optimize a ranking loss: the absolute scores don&apos;t matter as long as they yield an ordered list correctly. Thus, WordRank&apos;s uw and vc are less comparable to each other than those generated by GloVe, which employs a point-wise L2 loss.</note>

			<note place="foot" n="9"> We used the code provided by the authors at https:// bitbucket.org/d_ijk_stra/robirank. Although related to RobiRank, we attribute the superior performance of WordRank to the use of weight rw,c (4), introduction of hyperparameters α and β, and many implementation details. 10 https://code.google.com/p/word2vec/ 11 http://nlp.stanford.edu/projects/glove</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We'd like to thank Omer Levy for sharing his script for preprocessing the corpora used in the paper. We also thank the anonymous reviewers for their valu-able comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1502.03520.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convexity, classification, and risk bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">473</biblScope>
			<biblScope unit="page" from="138" to="156" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The tradeoffs of large-scale learning. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">351</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam Khanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">t-logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<editor>Richard Zemel, John Shawe-Taylor, John Lafferty, Chris Williams, and Alan Culota</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A set of level 3 basic linear algebra subprograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du Croz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hammarling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale matrix factorization with distributed stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sismanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="69" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 23</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale linear ranksvm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation. To Appear</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Max Welling, Zoubin Ghahramani, Corinna Cortes, Neil Lawrence, and Kilian Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning semantic word embeddings based on ordinal knowledge constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1501" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Visualizing high-dimensional data using t-sne. jmlr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>Chris Burges, Leon Bottou, Max Welling, Zoubin Ghahramani, and Kilian Weinberger</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS) 27</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A word at a time: Computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on World Wide Web</title>
		<meeting>the 20th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Convex Analysis</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ranking with ordered weighted pairwise classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Word representations via gaussian embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Not all contexts are created equal: Better word representations with variable attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Trancoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Berenzweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4603</idno>
		<title level="m">Latent collaborative retrieval</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ranking via robust binary classification and parallel parameter estimation in large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyokun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parameswaran</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In nips</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
