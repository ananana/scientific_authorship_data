<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to refine text based recommendations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youyang</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to refine text based recommendations</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2103" to="2108"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a text-based recommendation engine that utilizes recurrent neural networks to flexibly map textual input into continuous vector representations tailored to the recommendation task. Here, the text objects are documents such as Wikipedia articles or question and answer pairs. As neural models require substantial training time, we introduce a sequential component so as to quickly adjust the learned metric over objects as additional evidence accrues. We evaluate the approach on recommending Wikipedia descriptions of ingredients to their associated product categories. We also exemplify the sequential metric adjustment on retrieving similar Stack Exchange AskUbuntu questions. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern recommender problems involve complex objects, often described in textual form. In order to learn to predict how disparate objects may go to- gether, it is helpful to first map them into a common representation where they are easily compared, re- gardless of their origin. Neural models are partic- ularly well-suited for this task as continuous vec- tor representations of objects can be tailored in a flexible way to the desired task. While these mod- els have been shown to be effective across NLP tasks ( <ref type="bibr" target="#b14">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Andreas et al., 2016;</ref><ref type="bibr" target="#b4">Hermann et al., 2015)</ref>, they take considerable time to learn and are therefore ill-suited to be adjusted rapidly as additional evidence accumulates.</p><p>We cast our text-to-text recommendation problem in two phases. In the first phase, flexible neural text- to-vector mappings are learned from currently avail- able data. Such mappings are optimized to function well in a collaborative filtering setting. For exam- ple, in the context of recommending food product categories for ingredients based on their Wikipedia pages, the continuous vectors are adjusted so that their inner product directly reflects the degree of as- sociation between the objects. Once learned, the mapping can be applied to any previously unseen text to yield the corresponding vector representation, and therefore also used for predicting associations. In the second phase, we no longer adjust text-to- vector mappings but rather parameterize and learn how the vectors are compared. For example, we can optimize the metric separately for each new ingredi- ent based on a few category observations for that in- gredient. The goal of this second phase is to specif- ically boost the accuracy when the neural baseline (unaware of the new evidence) would otherwise not perform well.</p><p>Our approach builds on the recent work on recur- rent convolutional models to obtain text-to-vector mappings ( <ref type="bibr" target="#b9">Lei et al., 2016)</ref>. This architecture is particularly well suited for noisy Wikipedia pages as it can learn to omit and high- light different parts of the text, as needed. The ad- ditional sequential component is a regularized logis- tic regression model (for ingredient-product predic- tion) or a ranking model (for question retrieval). We demonstrate the accuracy of the baseline neural rec- ommender and the gains from the second sequential phase in both of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A great deal of recent effort has gone into devel- oping flexible neural models for text and their use across variety of NLP tasks. This includes build- ing vector representations for sentences and docu- ments ( <ref type="bibr" target="#b7">Le and Mikolov, 2014)</ref>, convolutional neu- ral network models of text <ref type="bibr" target="#b2">(Collobert and Weston, 2008;</ref><ref type="bibr" target="#b15">Zhang and LeCun, 2015)</ref>, non-consecutive variants of CNNs ( , and compo- sitional architectures <ref type="bibr" target="#b11">(Socher et al., 2013</ref>), among many others. Our work is most closely related to the use of such models for question retrieval ( <ref type="bibr" target="#b9">Lei et al., 2016</ref>) but differs, in particular, in terms of our two-phase collaborative filtering formulation and the ingredient mapping task from Wikipedia pages <ref type="bibr">(cf.(Sutskever et al., 2011;</ref><ref type="bibr" target="#b12">Song and Roth, 2015)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recommender Problems</head><p>We explore two recommender problems in this work. In the first problem, we are given a food in- gredient, and our goal is to predict which product categories it could appear in. Both ingredients and product categories are provided in terms of natural language descriptions via their associated Wikipedia pages. For example, if given "tomato", we would predict "canned foods" as one likely category for the ingredient. A small number of categories appear as targets for each ingredient.</p><p>We also consider the task of predicting questions that are similar to the one provided as a query. The purpose is to facilitate effective question answer- ing by retrieving related past questions (and the as- sociated answers that are available). For this we use Stack Exchange's AskUbuntu question retrieval dataset used in recent work (dos <ref type="bibr" target="#b3">Santos et al., 2015;</ref><ref type="bibr" target="#b9">Lei et al., 2016)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approach</head><p>We explain our approach in terms of the first task: predicting product categories from ingredients. Col- laborative predictions are made by mapping each in- gredient into a vector representation and comparing that representation with an analogous one for prod- uct categories. We train these vectors in an end-to- end manner to function well as part of the collab- orative task. The vector representations are based on Wikipedia pages that are available for most in- gredients and categories in our problem. Rather than derive the vector from the entire article (which can be long), we only use the top summary section. For the AskUbuntu question-answering dataset, we make use of both the title and the question body.</p><p>We use a recurrent neural network (RNN) model to map each text description into a vector represen- tation. Our model builds on the recurrent convo- lutional neural network model of ( <ref type="bibr" target="#b9">Lei et al., 2016)</ref> used to train the AskUbuntu question representa- tions. We describe below a modified version used for ingredient-product category prediction.</p><p>Let v ✓ (x) 2 R d be the parameterized RNN map- ping of text x into a vector representation, where d is the dimension of the hidden representation. Let x i and z p be the Wikipedia pages for ingredient i 2 I and product category p 2 P , respectively. We use the same parameters ✓ to generate the representa- tions for both ingredients and product categories due to their overall similarity. Thus v ✓ (x i ) is the vector representation for ingredient i and v ✓ (z p ) is the vec- tor representation for product category p for an RNN model with parameters ✓. We train the RNN model to predict each association Y ip = 1 as a binary pre- diction task, i.e.,</p><formula xml:id="formula_0">P (Y ip = 1|✓) = (v ✓ (z p ) · v ✓ (x i )),<label>(1)</label></formula><p>where is the sigmoid function (t) = (1 + exp(t)) 1 . The formulation is akin to a binary collaborative filtering task where user/item feature vectors are produced by the RNN. The parameters ✓ can be learned by back-propagating log-likelihood of the binary 0/1 predictions back to ✓.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sequential learning</head><p>Our RNN model, once trained, will be able to map any new ingredient and product category (their text descriptions) into vectors, and make a binary predic- tion of whether the two go together. However, train- ing the model takes considerable time and cannot be easily adjusted in the face of new evidence, e.g., a few positive and negative categories for a previously unseen ingredient. Since RNN features are global (affecting the mapping from text to features for all ingredients/products), it is not clear how the adjust- ments made in light of additional information about a specific new ingredient will impact predictions for other ingredients. We propose a sequential approach that is instead local, tailored to the new ingredient. In order to sequentially adjust the model predic- tions with new evidence, we introduce parameters w = [w 1 , . . . , w d ], w j 2 R + that modify the com- parison of ingredient and category vectors. Specifi- cally, the association is predicted by</p><formula xml:id="formula_1">P (Y ip = 1|✓, w) = {v ✓ (z p ) T diag(w)v ✓ (x i )},<label>(2)</label></formula><p>where diag(w) is a diagonal matrix with the entries specified by w. We assume that, at this stage, the RNN parameters ✓ and therefore the vector repre- sentations v ✓ (z p ) and v ✓ (x i ) are nonadjustable. We will only update weights w in response to each new observation, separately for each ingredient. The ob- servations can both be positive (Y = 1) and negative (Y = 0).</p><p>Because we expect a new input may only have a small number of observations, it is important to properly regularize the weights as to avoid over- fitting. We append the log-likelihood objective with a regularizer</p><formula xml:id="formula_2">reg(w) = 2 d X j=1 (w j 1) 2<label>(3)</label></formula><p>where is the overall regularization parameter. Note that for large values of , the regularizer keeps the parameters at the default values w j = 1 correspond- ing to the baseline RNN collaborative predictions, unmodified by the new evidence. In the context of predicting similar questions, we use a modified binary formulation where the goal is to classify each triplet of questions (x, z 1 , z 2 ) in terms of whether z 1 is closer to the query than z 2 . In this ranking model, the probability that z 1 is closer is given by</p><formula xml:id="formula_3">⇣ (v ✓ (z 1 ) v ✓ (z 2 )) T diag(w)v ✓ (x) ⌘ ,<label>(4)</label></formula><p>The parameters w are again trained from ob- served additional triplet relations in the AskUbuntu dataset. The parameters w are regularized as in the ingredient-product category setup. The sequential part can therefore be viewed as a content recommendation task which is tailored to the specific query (e.g., ingredient) using features from previously trained RNNs. It assumes addi- tional feedback in order to adjust the feature com- parison using the introduced weights w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup and Results</head><p>Ingredients: We use the FoodEssentials LabelAPI 2 and Rapid Alert System for Food and Feed (RASFF) 3 databases to extract 5439 ingredients and the prod- uct categories they appear in. On average, each in- gredient appears in 16.3 product categories (out of 131 categories). We leverage Mechanical Turk to link each ingredient to the appropriate Wikipedia ar- ticle. From the 5439 ingredients, there are 1680 unique Wikipedia articles. Each ingredient sum- mary description has a median of 169 tokens. AskUbuntu: The dataset consists of 167k questions and 16k user-marked similar question pairs taking from a 2014 dump of AskUbuntu website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training, development, and test sets</head><p>Ingredients: We take the set of unique Wikipedia articles and randomly split them into training, de- velopment, and test sets (60/20/20). We then assign the ingredients to the appropriate data set based on their Wikipedia articles. This is to ensure that the ar- ticles of the ingredients used in the development and test sets are not seen in training. AskUbuntu: We take 8000 human annotated ques- tion pairs as our development and test sets. There are 200 query questions in each set. Each query ques- tion is paired with 20 candidate questions which are annotated as similar or non-similar. We evaluate by ranking these candidate questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sequential scenario</head><p>Ingredients: Let n be the total number of labeled positive categories for the ingredient. We provide min(20, n/2) positive categories for the sequential model to train. We also include k negative cat- egories, where k is selected using the validation set. We evaluate the performance on the remaining n min(20, n/2) positive categories as well as on the negative categories not included in training.   <ref type="table" target="#tab_3">Table 1</ref>: The three most likely food product category predictions generated by the baseline RNN model on eight unseen ingredients.</p><p>The number in parenthesis represents the probability provided by the model.</p><p>AskUbuntu: We use the difference vectors in Equa- tion 4 to compute the loss and sequentially update the feature weights w. Let n be the total number of labeled positive examples (similar questions). We select up to n/2 positive and negative examples. From the n 2 /4 possible pairs, we select the 20 most informative pairs for training.</p><p>While we use the loss function commonly used for binary classification during training, we ulti- mately want to frame our question as a ranking prob- lem. Therefore, after iterating through the initial observations, we compute the mean average preci- sion (MAP) over the remaining (unseen) ingredi- ents/questions and compare it to the MAP of the baseline RNN model on the same unseen examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN:</head><p>We use Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2015)</ref> as the optimization method with the default setting sug- gested by the authors. We use a hidden dimension of d = 50 for the ingredients and d = 400 for the AskUbuntu questions. Additional parameters such as dropout ( <ref type="bibr" target="#b5">Hinton et al., 2012)</ref>, hidden layers, regu- larization, stopping criteria, batch size, and learning rate is tuned on the development set. Word Vectors: For the ingredient/product pre- diction task, we used the GloVe pre-trained vec- tors (Common Crawl, 42 billion tokens, 300- dimensional) ( <ref type="bibr" target="#b10">Pennington et al., 2014</ref>). The word vectors for the AskUbuntu vectors are pre-trained using the AskUbuntu and Wikipedia corpora. Sequential: We utilize the bounded limited- memory BFGS algorithm (L-BFGS-B) ( <ref type="bibr" target="#b1">Byrd et al., 1995)</ref> to solve for the optimal feature weights with bounds w j 2 [0. <ref type="bibr">01,</ref><ref type="bibr">2]</ref>. We tuned the the constraint bounds and the regularization parameter on the de- velopment set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ing / Dev</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ing / Test</head><p>AskUbuntu / Dev AskUbuntu / Test Mean MAP gain (percent) 0.0525 (30.9%) 0.0492 (26.5%) 0.0246 (8.2%) 0.0224 (7.5%) Mean # positive observations 8.6 9.1 3.2 2.9     <ref type="table" target="#tab_1">Table 2</ref>. We are able to generate consistent im- provements in the MAP after seeing half of the ob- servations. Box plots of the test set MAP improve- ments can be seen in <ref type="figure" target="#fig_1">Figure 1</ref>. For the ingredi- ents prediction task, the sequential model offers the greatest improvements when the baseline RNN has low MAP. In the AskUbuntu questions, on the other hand, the positive effect is greatest when the base- line MAP is around 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>There are three possible reasons for the difference in performance between the two tasks:</p><p>• The mean number of positive observations in the AskUbuntu task is 2.9, compared to 9.1 observations in the ingredients task <ref type="table" target="#tab_1">(Table 2)</ref>. This is a key factor in determining the sequen- tial model's ability to tune for the optimal pa- rameters. Having access to more annotated data would likely result in an increase in per- formance.</p><p>• Owing to the complexity of information en- coded, the vectors for the AskUbuntu task are of dimension of 400 as opposed to 50 in the in- gredients task. As a result, the sequential model would require more feedback to find near opti- mal weights w.</p><p>• We hypothesize that the sequential model leads to the most increased performance when the baseline model is mediocre. This is espe- cially highlighted in the AskUbuntu task, as ex- tremely poor performance indicate a complete mismatch of questions, while an exceptional performance leaves little room for additional improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We demonstrated a text-based neural recommender approach to predict likely food products from a given ingredient as well as other similar questions from a given AskUbuntu question. We then ex- tended this model to an online stream of new data, which improves over the off-line trained version for both of the two tasks tested. This sequential process improves model performance while requiring mini- mal additional training time and resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Ingredient</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Box plot of the mean absolute mean average precision (MAP) improvement of the sequential model on the ingredients dataset (top) and AskUbuntu questions (bottom). They are divided into five quintiles based on the baseline RNN MAP score. The model shows gains in cases where the baseline RNN model's performance is poor or mediocre. The number of data points in each of the five quintiles of the ingredients dataset are, respectively: 131, 210, 240, 135, 191. For the AskUbuntu dataset, they are: 15, 26, 32, 40, 41.</figDesc><graphic url="image-2.png" coords="4,313.20,367.26,226.79,170.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>averaged across 5 runs. The two metrics shown are the mean average precision (MAP) and precision at N (P@N), where N is the total number of positive examples. The random model generates a random ranking of food categories for each ingredi- ent. The baseline model uses the mean occurrence distribution of the food categories for all ingredients to rank the predictions. The multilayer perceptron model (MLP) is a three-layer neural network trained on the hierarchical properties of the input in- gredients (extracted from the UMLS Metathesaurus). The RNN model outperforms all other baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : We show the mean absolute improvement in the mean average precision (MAP) over the unobserved data points for each ingredient/question. The percent improvement shown is an average percent improvement across the ingredients/questions. They</head><label>2</label><figDesc></figDesc><table>are the average of 100 runs per ingredient and 20 runs per AskUbuntu question. 

Model 
Validation set 
Test set 
Random 0.150 / 0.120 0.158 / 0.129 
Baseline 0.320 / 0.291 0.331 / 0.300 
MLP 
0.432 / 0.390 0.459 / 0.416 
RNN 
0.476 / 0.422 0.478 / 0.426 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Results of the RNN model on the ingredient dataset,</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 and</head><label>1</label><figDesc></figDesc><table>3 shows our results from using RNN 
to predict likely food product categories from 
Wikipedia text descriptions of ingredients. 
We show the gains of the sequential update model 
in </table></figure>

			<note place="foot" n="1"> The code/data is available at https://github.com/ youyanggu/rcnn.</note>

			<note place="foot" n="2"> http://developer.foodessentials.com/ 3 http://ec.europa.eu/food/safety/rasff/index en.htm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank the MIT NLP group and the reviewers for their helpful comments. The work was partially sup-ported by the U.S. Food &amp; Drug Administration, and by Google Faculty Award (Barzilay and Jaakkola). Any opinions, findings, conclusions, or recommen-dations expressed in the paper are those of the au-thors alone, and do not necessarily reflect the views of the funding organizations.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A limited memory algorithm for bound constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">H</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciyou</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific and Statistical Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1190" to="1208" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep neural networks with multitask learning. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning hybrid representations to retrieve semantically equivalent questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasha</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="694" to="699" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grenfenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Molding cnns for text: non-linear, non-consecutive convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised question retrieval with recurrent convolutions. Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishikesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Tymoshenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Alessandro Moschitti, and Lluis Marquez</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised sparse vector densification for short text similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01710</idno>
		<title level="m">Text understanding from scratch</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
