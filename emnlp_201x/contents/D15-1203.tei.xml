<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches , statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolu-tional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem , distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive base-line methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples. One common technique for coping with this difficulty is distant supervision ( <ref type="bibr" target="#b14">Mintz et al., 2009</ref>) which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of the auto- matic labeling of data through distant supervision.</p><p>In this example, Apple and Steve Jobs are two re- lated entities in Freebase <ref type="bibr">1</ref> . All sentences that con- tain these two entities are selected as training in- stances. The distant supervision strategy is an ef- fective method of automatically labeling training data. However, it has two major shortcomings when used for relation extraction. First, the distant supervision assumption is too strong and causes the wrong label problem. A sen- tence that mentions two entities does not necessar- ily express their relation in a knowledge base. It is possible that these two entities may simply share the same topic. For instance, the upper sentence indeed expresses the "company/founders" relation in <ref type="figure" target="#fig_0">Figure 1</ref>. The lower sentence, however, does not express this relation but is still selected as a train- ing instance. This will hinder the performance of a model trained on such noisy data.</p><p>Second, previous methods ( <ref type="bibr" target="#b14">Mintz et al., 2009;</ref><ref type="bibr" target="#b18">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2011</ref>) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision. These features are often derived from preexisting Natural Language Processing (NLP) tools. Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation. Dis- tant supervised relation extraction generally ad- dresses corpora from the Web, including many informal texts. <ref type="figure" target="#fig_1">Figure 2</ref> shows the sentence length distribution of a benchmark distant super- vision dataset that was developed by <ref type="bibr" target="#b18">Riedel et al. (2010)</ref>. Approximately half of the sentences are longer than 40 words. <ref type="bibr" target="#b12">McDonald and Nivre (2007)</ref> showed that the accuracy of syntactic pars- ing decreases significantly with increasing sen- tence length. Therefore, when using traditional features, the problem of error propagation or ac- cumulation will not only exist, it will grow more serious.</p><p>In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PC- NNs) with multi-instance learning to address the two problems described above. To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to pre- vious studies ( <ref type="bibr" target="#b18">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012</ref>). In multi-instance problem, the training set consists of many bags, and each contains many instances. The labels of the bags are known; however, the labels of the in- stances in the bags are unknown. We design an objective function at the bag level. In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem.</p><p>To address the second problem, we adopt con- volutional architecture to automatically learn rel- evant features without complicated NLP prepro- cessing inspired by <ref type="bibr" target="#b24">Zeng et al. (2014)</ref>. Our pro- posal is an extension of <ref type="bibr" target="#b24">Zeng et al. (2014)</ref>, in which a single max pooling operation is utilized to determine the most significant features. Al- though this operation has been shown to be effec- tive for textual feature representation <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b11">Kim, 2014)</ref>, it reduces the size of the hidden layers too rapidly and cannot capture the structural information between two entities <ref type="bibr" target="#b6">(Graham, 2014</ref>). For example, to identify the relation between Steve Jobs and Apple in <ref type="figure" target="#fig_0">Figure 1</ref>, we need to specify the entities and extract the structural features between them. Several approaches have employed manually crafted features that attempt to model such structural information. These ap- proaches usually consider both internal and exter- nal contexts. A sentence is inherently divided into three segments according to the two given entities. The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities ( ). Clearly, single max pooling is not suf- ficient to capture such structural information. To capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer. The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence. Thus, it is expected to exhibit superior performance compared with tra- ditional methods.</p><p>The contributions of this paper can be summa- rized as follows.</p><p>• We explore the feasibility of performing dis- tant supervised relation extraction without hand-designed features. PCNNS are pro- posed to automatically learn features without complicated NLP preprocessing.</p><p>• To address the wrong label problem, we de- velop innovative solutions that incorporate multi-instance learning into the PCNNS for distant supervised relation extraction.</p><p>• In the proposed network, we devise a piece- wise max pooling layer, which aims to cap- ture structural information between two enti- ties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Relation extraction is one of the most important topics in NLP. Many approaches to relation ex- traction have been developed, such as bootstrap- ping, unsupervised relation discovery and super- vised classification. Supervised approaches are the most commonly used methods for relation extraction and yield relatively high performance ( <ref type="bibr" target="#b2">Bunescu and Mooney, 2006;</ref><ref type="bibr" target="#b23">Zelenko et al., 2003;</ref><ref type="bibr" target="#b27">Zhou et al., 2005</ref>). In the supervised paradigm, re- lation extraction is considered to be a multi-class classification problem and may suffer from a lack of labeled data for training. To address this prob- lem, <ref type="bibr" target="#b14">Mintz et al. (2009)</ref> adopted Freebase to per- form distant supervision. As described in Sec- tion 1, the algorithm for training data generation is sometimes faced with the wrong label problem.</p><p>To address this shortcoming, ( <ref type="bibr" target="#b18">Riedel et al., 2010;</ref><ref type="bibr" target="#b8">Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012</ref>) de- veloped the relaxed distant supervision assump- tion for multi-instance learning. The term 'multi- instance learning was coined by ( <ref type="bibr" target="#b4">Dietterich et al., 1997</ref>) while investigating the problem of predict- ing drug activity. In multi-instance learning, the uncertainty of instance labels can be taken into ac- count. The focus of multi-instance learning is to discriminate among the bags.</p><p>These methods have been shown to be effec- tive for relation extraction. However, their per- formance depends strongly on the quality of the designed features. Most existing studies have con- centrated on extracting features to identify the relations between two entities. Previous meth- ods can be generally categorized into two types: feature-based methods and kernel-based methods. In feature-based methods, a diverse set of strate- gies is exploited to convert classification clues (e.g., sequences, parse trees) into feature vec- tors <ref type="bibr" target="#b10">(Kambhatla, 2004;</ref><ref type="bibr" target="#b20">Suchanek et al., 2006</ref>). Feature-based methods suffer from the necessity of selecting a suitable feature set when convert- ing structured representations into feature vectors. Kernel-based methods provide a natural alterna- tive to exploit rich representations of input classifi- cation clues, such as syntactic parse trees. Kernel- based methods enable the use of a large set of fea- tures without needing to extract them explicitly. Several kernels have been proposed, such as the convolution tree kernel ( <ref type="bibr" target="#b17">Qian et al., 2008)</ref>, the sub- sequence kernel ( <ref type="bibr" target="#b2">Bunescu and Mooney, 2006</ref>) and the dependency tree kernel ( <ref type="bibr" target="#b1">Bunescu and Mooney, 2005</ref>).</p><p>Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using ex- isting NLP tools. With the recent revival of in- terest in neural networks, many researchers have investigated the possibility of using neural net- works to automatically learn features <ref type="bibr" target="#b19">(Socher et al., 2012;</ref><ref type="bibr" target="#b24">Zeng et al., 2014</ref>). Inspired by <ref type="bibr" target="#b24">Zeng et al. (2014)</ref>, we propose the use of PCNNs with multi-instance learning to automatically learn fea- tures for distant supervised relation extraction. <ref type="bibr" target="#b4">Dietterich et al. (1997)</ref> suggested that the design of multi-instance modifications for neural net- works is a particularly interesting topic. Zhang and Zhou (2006) successfully incorporated multi- instance learning into traditional Backpropagation (BP) and Radial Basis Function (RBF) networks and optimized these networks by minimizing a sum-of-squares error function. In contrast to their method, we define the objective function based on the cross-entropy principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Distant supervised relation extraction is formu- lated as multi-instance problem. In this section, we present innovative solutions that incorporate multi-instance learning into a convolutional neu- ral network to fulfill this task. PCNNs are pro- posed for the automatic learning of features with- out complicated NLP preprocessing. <ref type="figure" target="#fig_2">Figure 3</ref> shows our neural network architecture for distant supervised relation extraction. It illustrates the procedure that handles one instance of a bag. This procedure includes four main parts: Vector Rep- resentation, Convolution, Piecewise Max Pooling and Softmax Output. We describe these parts in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vector Representation</head><p>The inputs of our network are raw word tokens. When using neural networks, we typically trans- form word tokens into low-dimensional vectors. In our method, each input word token is trans- formed into a vector by looking up pre-trained word embeddings. Moreover, we use position fea- tures (PFs) to specify entity pairs, which are also transformed into vectors by looking up position embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Word Embeddings</head><p>Word embeddings are distributed representations of words that map each word in a text to a 'k'- dimensional real-valued vector. They have re- cently been shown to capture both semantic and syntactic information about words very well, set- ting performance records in several word similar- ity tasks ( <ref type="bibr" target="#b13">Mikolov et al., 2013;</ref><ref type="bibr" target="#b16">Pennington et al., 2014</ref> word position enhancing many other NLP tasks ( <ref type="bibr" target="#b15">Parikh et al., 2014;</ref><ref type="bibr" target="#b9">Huang et al., 2014)</ref>. A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm. Recent research ( <ref type="bibr" target="#b5">Erhan et al., 2010)</ref> has shown that neural networks can converge to better local minima when they are initialized with word em- beddings. Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text. Researchers have proposed several methods of training word embeddings ( <ref type="bibr" target="#b0">Bengio et al., 2003;</ref><ref type="bibr" target="#b3">Collobert et al., 2011;</ref><ref type="bibr" target="#b13">Mikolov et al., 2013)</ref>. In this paper, we use the Skip-gram model <ref type="bibr" target="#b13">(Mikolov et al., 2013</ref>) to train word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Position Embeddings</head><p>In relation extraction, we focus on assigning labels to entity pairs. Similar to <ref type="bibr" target="#b24">Zeng et al. (2014)</ref>, we use PFs to specify entity pairs. A PF is defined as the combination of the relative distances from the current word to e 1 and e 2 . For instance, in the following example, the relative distances from son to e 1 (Kojo Annan) and e 2 (Kofi Annan) are 3 and -2, respectively.</p><p>... hired Kojo Annan , the son of Kofi Annan , in ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">- 2</head><p>Two position embedding matrixes (PF 1 and PF 2 ) are randomly initialized. We then transform the relative distances into real valued vectors by looking up the position embedding matrixes. In the example shown in <ref type="figure" target="#fig_2">Figure 3</ref>, it is assumed that the size of the word embedding is d w = 4 and that the size of the position embedding is d p = 1. In combined word embeddings and position embed- dings, the vector representation part transforms an instance into a matrix S ∈ R s×d , where s is the sentence length and</p><formula xml:id="formula_0">d = d w + d p * 2.</formula><p>The matrix S is subsequently fed into the convolution part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Convolution</head><p>In relation extraction, an input sentence that is marked as containing the target entities corre- sponds only to a relation type; it does not predict labels for each word. Thus, it might be necessary to utilize all local features and perform this predic- tion globally. When using a neural network, the convolution approach is a natural means of merg- ing all these features <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>.</p><p>Convolution is an operation between a vector of weights, w, and a vector of inputs that is treated as a sequence q. The weights matrix w is regarded as the filter for the convolution. In the example shown in <ref type="figure" target="#fig_2">Figure 3</ref>, we assume that the length of the filter is w (w = 3); thus, w ∈ R m (m = w * d).</p><p>We consider S to be a sequence {q 1 , q 2 , · · · , q s }, where q i ∈ R d . In general, let q i:j refer to the concatenation of q i to q j . The convolution op- eration involves taking the dot product of w with each w-gram in the sequence q to obtain another sequence c ∈ R s+w−1 :</p><formula xml:id="formula_1">c j = wq j−w+1:j<label>(1)</label></formula><p>where the index j ranges from 1 to s + w − 1. Out- of-range input values q i , where i &lt; 1 or i &gt; s, are taken to be zero. The ability to capture different features typi- cally requires the use of multiple filters (or feature maps) in the convolution. Under the assumption that we use n filters (W = {w 1 , w 2 , · · · , w n }), the convolution operation can be expressed as fol- lows:</p><formula xml:id="formula_2">c ij = w i q j−w+1:j 1 ≤ i ≤ n<label>(2)</label></formula><p>The convolution result is a matrix C = {c 1 , c 2 , · · · , c n } ∈ R n×(s+w−1) . <ref type="figure" target="#fig_2">Figure 3</ref> shows an example in which we use 3 different filters in the convolution procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Piecewise Max Pooling</head><p>The size of the convolution output matrix C ∈ R n×(s+w−1) depends on the number of tokens s in the sentence that is fed into the network. To apply subsequent layers, the features that are ex- tracted by the convolution layer must be com- bined such that they are independent of the sen- tence length. In traditional Convolution Neural Networks (CNNs), max pooling operations are of- ten applied for this purpose <ref type="bibr" target="#b3">(Collobert et al., 2011;</ref><ref type="bibr" target="#b24">Zeng et al., 2014</ref>). This type of pooling scheme naturally addresses variable sentence lengths. The idea is to capture the most significant features (with the highest values) in each feature map. However, despite the widespread use of single max pooling, this approach is insufficient for rela- tion extraction. As described in the first section, single max pooling reduces the size of the hidden layers too rapidly and is too coarse to capture fine- grained features for relation extraction. In addi- tion, single max pooling is not sufficient to cap- ture the structural information between two enti- ties. In relation extraction, an input sentence can be divided into three segments based on the two selected entities. Therefore, we propose a piece- wise max pooling procedure that returns the max- imum value in each segment instead of a single maximum value. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the output of each convolutional filter c i is divided into three segments {c i1 , c i2 , c i3 } by Kojo Annan and Kofi Annan. The piecewise max pooling procedure can be expressed as follows:</p><formula xml:id="formula_3">p ij = max(c ij ) 1 ≤ i ≤ n, 1 ≤ j ≤ 3 (3)</formula><p>For the output of each convolutional filter, we can obtain a 3-dimensional vector p i = {p i1 , p i2 , p i3 }. We then concatenate all vectors p 1:n and apply a non-linear function, such as the hyperbolic tangent. Finally, the piecewise max pooling procedure outputs a vector:</p><formula xml:id="formula_4">g = tanh(p 1:n )<label>(4)</label></formula><p>where g ∈ R 3n . The size of g is fixed and is no longer related to the sentence length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Softmax Output</head><p>To compute the confidence of each relation, the feature vector g is fed into a softmax classifier.</p><formula xml:id="formula_5">o = W 1 g + b<label>(5)</label></formula><p>W 1 ∈ R n 1 ×3n is the transformation matrix, and o ∈ R n 1 is the final output of the network, where n 1 is equal to the number of possible relation types for the relation extraction system. We employ dropout ( <ref type="bibr" target="#b7">Hinton et al., 2012</ref>) on the penultimate layer for regularization. Dropout pre- vents the co-adaptation of hidden units by ran- domly dropping out a proportion p of the hidden units during forward computing. We first apply a "masking" operation (g•r) on g, where r is a vec- tor of Bernoulli random variables with probability p of being 1. Eq. <ref type="formula" target="#formula_5">(5)</ref> becomes:</p><formula xml:id="formula_6">o = W 1 (g • r) + b<label>(6)</label></formula><p>Each output can then be interpreted as the con- fidence score of the corresponding relation. This score can be interpreted as a conditional probabil- ity by applying a softmax operation (see Section 3.5). In the test procedure, the learned weight vec- tors are scaled by p such thatˆWthatˆ thatˆW 1 = pW 1 and are used (without dropout) to score unseen instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Multi-instance Learning</head><p>In order to alleviate the wrong label problem, we use multi-instance learning for PCNNs. The PCNNs-based relation extraction can be stated as a quintuple θ = (E, PF 1 , PF 2 , W, W 1 ) 2 . The in- put to the network is a bag. Suppose that there are T bags {M 1 , M 2 , · · · , M T } and that the i-th bag contains q i instances M i = {m 1 i , m 2 i , · · · , m q i i }. The objective of multi-instance learning is to pre- dict the labels of the unseen bags. In this paper, all instances in a bag are considered independently. Given an input instance m j i , the network with the parameter θ outputs a vector o, where the r-th component o r corresponds to the score associated Algorithm 1 Multi-instance learning 1: Initialize θ. Partition the bags into mini- batches of size b s . 2: Randomly choose a mini-batch, and feed the bags into the network one by one. 3: Find the j-th instance m j i (1 ≤ i ≤ b s ) in each bag according to Eq. (9). 4: Update θ based on the gradients of m j i (1 ≤ i ≤ b s ) via Adadelta. 5: Repeat steps 2-4 until either convergence or the maximum number of epochs is reached.</p><p>with relation r. To obtain the conditional probabil- ity p(r|m, θ), we apply a softmax operation over all relation types:</p><formula xml:id="formula_7">p(r|m j i ; θ) = e or n 1 ∑ k=1 e o k<label>(7)</label></formula><p>The objective of multi-instance learning is to dis- criminate bags rather than instances. To do so, we must define the objective function on the bags. Given all (T ) training bags (M i , y i ), we can define the objective function using cross-entropy at the bag level as follows:</p><formula xml:id="formula_8">J (θ) = T ∑ i=1 log p(y i |m j i ; θ)<label>(8)</label></formula><p>where j is constrained as follows:</p><formula xml:id="formula_9">j * = arg max j p(y i |m j i ; θ) 1 ≤ j ≤ q i (9)</formula><p>Using this defined objective function, we max- imize J(θ) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule. The entire training pro- cedure is described in Algorithm 1. From the introduction presented above, we know that the traditional backpropagation algo- rithm modifies a network in accordance with all training instances, whereas backpropagation with multi-instance learning modifies a network based on bags. Thus, our method captures the nature of distant supervised relation extraction, in which some training instances will inevitably be incor- rectly labeled. When a trained PCNN is used for prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is assigned a positive label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments are intended to provide evidence that supports the following hypothesis: automat- ically learning features using PCNNs with multi- instance learning can lead to an increase in perfor- mance. To this end, we first introduce the dataset and evaluation metrics used. Next, we test several variants via cross-validation to determine the pa- rameters to be used in our experiments. We then compare the performance of our method to those of several traditional methods. Finally, we evalu- ate the effects of piecewise max pooling and multi- instance learning 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>We evaluate our method on a widely used dataset 4 that was developed by ( <ref type="bibr" target="#b18">Riedel et al., 2010)</ref> and has also been used by <ref type="bibr" target="#b8">(Hoffmann et al., 2011;</ref><ref type="bibr" target="#b21">Surdeanu et al., 2012)</ref>. This dataset was generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as the training corpus and sentences from 2007 used as the testing corpus.</p><p>Following previous work <ref type="bibr" target="#b14">(Mintz et al., 2009</ref>), we evaluate our method in two ways: the held-out evaluation and the manual evaluation. The held- out evaluation only compares the extracted rela- tion instances against Freebase relation data and reports the precision/recall curves of the experi- ments. In the manual evaluation, we manually check the newly discovered relation instances that are not in Freebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pre-trained Word Embeddings</head><p>In this paper, we use the Skip-gram model (word2vec) <ref type="bibr">5</ref> to train the word embeddings on the NYT corpus. Word2vec first constructs a vocab- ulary from the training text data and then learns vector representations of the words. To obtain the embeddings of the entities, we concatenate the to- kens of a entity using the ## operator when the entity has multiple word tokens. Since a compar- ison of the word embeddings is beyond the scope <ref type="table">Window  size  Feature  maps  Word  dimension  Position  dimension  Batch  size</ref> Adadelta parameter Dropout probability w = 3 n = 230 d w = 50 d p = 5 b s = 50 ρ = 0.95, ε = 1e −6 p = 0.5 <ref type="table">Table 1</ref>: Parameters used in our experiments.</p><p>of this paper, our experiments directly utilize 50- dimensional vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Parameter Settings</head><p>In this section, we experimentally study the ef- fects of two parameters on our models: the win- dow size, w, and the number of feature maps, n. Following ( <ref type="bibr" target="#b21">Surdeanu et al., 2012)</ref>, we tune all of the models using three-fold validation on the train- ing set. We use a grid search to determine the op- timal parameters and manually specify subsets of the parameter spaces: w ∈ {1, 2, 3, · · · , 7} and n ∈ {50, 60, · · · , 300}. <ref type="table">Table 1</ref> shows all parame- ters used in the experiments. Because the position dimension has little effect on the result, we heuris- tically choose d p = 5. The batch size is fixed to 50. We use Adadelta <ref type="bibr" target="#b22">(Zeiler, 2012</ref>) in the up- date procedure; it relies on two main parameters, ρ and ε, which do not significantly affect the per- formance <ref type="bibr" target="#b22">(Zeiler, 2012)</ref>. Following <ref type="bibr" target="#b22">(Zeiler, 2012)</ref>, we choose 0.95 and 1e −6 , respectively, as the val- ues of these parameters. In the dropout operation, we randomly set the hidden unit activities to zero with a probability of 0.5 during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with Traditional Approaches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Held-out Evaluation</head><p>The held-out evaluation provides an approximate measure of precision without requiring costly hu- man evaluation. Half of the Freebase relations are used for testing. The relation instances discovered from the test articles are automatically compared with those in Freebase.</p><p>To evaluate the proposed method, we select the following three traditional methods for com- parison. Mintz represents a traditional distant- supervision-based model that was proposed by <ref type="bibr" target="#b14">(Mintz et al., 2009)</ref>. MultiR is a multi-instance learning method that was proposed by <ref type="bibr" target="#b8">(Hoffmann et al., 2011</ref>). MIML is a multi-instance multi- label model that was proposed by ( <ref type="bibr" target="#b21">Surdeanu et al., 2012</ref>). <ref type="figure" target="#fig_3">Figure 4</ref> shows the precision-recall curves for each method, where PCNNs+MIL denotes our method, and demonstrates that PCNNs+MIL achieves higher precision over the entire range of recall. PCNNs+MIL enhances the recall to ap-   <ref type="table">Table 2</ref>: Precision values for the top 100, top 200, and top 500 extracted relation instances upon man- ual evaluation.</p><p>proximately 34% without any loss of precision. In terms of both precision and recall, PCNNs+MIL outperforms all other evaluated approaches. No- tably, the results of the methods evaluated for com- parison were obtained using manually crafted fea- tures. By contrast, our result is obtained by au- tomatically learning features from original words.</p><p>The results demonstrate that the proposed method is an effective technique for distant supervised re- lation extraction. Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction. Incorpo- rating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Manual Evaluation</head><p>It is worth emphasizing that there is a sharp de- cline in the held-out precision-recall curves of PC- NNs+MIL at very low recall <ref type="figure" target="#fig_3">(Figure 4)</ref>. A manual check of the misclassified examples that were pro- duced with high confidence reveals that the ma-jorities of these examples are false negatives and are actually true relation instances that were mis- classified due to the incomplete nature of Free- base. Thus, the held-out evaluation suffers from false negatives in Freebase. We perform a manual eval- uation to eliminate these problems. For the manual evaluation, we choose the entity pairs for which at least one participating entity is not present in Freebase as a candidate. This means that there is no overlap between the held-out and manual can- didates. Because the number of relation instances that are expressed in the test data is unknown, we cannot calculate the recall in this case. Instead, we calculate the precision of the top N extracted rela- tion instances. <ref type="table">Table 2</ref> presents the manually eval- uated precisions for the top 100, top 200, and top 500 extracted instances. The results show that PC- NNs+MIL achieves the best performance; more- over, the precision is higher than in the held-out evaluation. This finding indicates that many of the false negatives that we predict are, in fact, true re- lational facts. The sharp decline observed in the held-out precision-recall curves is therefore rea- sonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Piecewise Max Pooling and</head><p>Multi-instance Learning</p><p>In this paper, we develop a method of piecewise max pooling and incorporate multi-instance learn- ing into convolutional neural networks for distant supervised relation extraction. To demonstrate the effects of these two techniques, we empirically study the performance of systems in which these techniques are not implemented through held-out evaluations ( <ref type="figure">Figure 5</ref>). CNNs represents convolu- tional neural networks to which single max pool- ing is applied. <ref type="figure">Figure 5</ref> shows that when piecewise max pooling is used (PCNNs), better results are produced than those achieved using CNNs. More- over, compared with CNNs+MIL, PCNNs achieve slightly higher precision when the recall is greater than 0.08. Since the parameters for all the model are determined by grid search, we can observe that CNNs cannot achieve competitive results com- pared to PCNNs when increasing the size of the hidden layer of convolutional neural networks. It means that we cannot capture more useful infor- mation by simply increasing the network param- eter. These results demonstrate that the proposed piecewise max pooling technique is beneficial and <ref type="figure">Figure 5</ref>: Effect of piecewise max pooling and multi-instance learning.</p><p>can effectively capture structural information for relation extraction. A similar phenomenon is also observed when multi-instance learning is added to the network. Both CNNs+MIL and PCNNs+MIL outperform their counterparts CNNs and PCNNs, respectively, thereby demonstrating that incorpo- ration of multi-instance learning into our neural network was successful in solving the wrong label problem. As expected, PCNNs+MIL obtains the best results because the advantages of both tech- niques are achieved simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we exploit Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning for distant supervised relation extraction. In our method, features are automatically learned without complicated NLP preprocessing. We also successfully devise a piecewise max pooling layer in the proposed network to capture structural in- formation and incorporate multi-instance learning to address the wrong label problem. Experimental results show that the proposed approach offers sig- nificant improvements over comparable methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training instances generated through distant supervision. Upper sentence: correct labeling; lower sentence: incorrect labeling.</figDesc><graphic url="image-1.png" coords="1,310.92,209.58,212.56,64.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The sentence length distribution of Riedel's dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of PCNNs (better viewed in color) used for distant supervised relation extraction, illustrating the procedure for handling one instance of a bag and predicting the relation between Kojo Annan and Kofi Annan.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance comparison of the proposed method with traditional approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). Using word embeddings that have been trained a priori has become common practice for</figDesc><table>... 

hired 

, 
the 
son 
of 

, 
in 

... 

... 

m 
a 
x 
( 
c 1 

1 

) 

V 
e 
c 
t 
o 
r 
r 
e 
p 
r 
e 
s 
e 
n 
t 
a 
t 
i 
o 
n 
C 
o 
n 
v 
o 
l 
u 
t 
i 
o 
n 
P 
i 
e 
c 
e 
w 
i 
s 
e 
m 
a 
x 
p 
o 
o 
l 
i 
n 
g 
S 
o 
f 
t 
m 
a 
x 
c 
l 
a 
s 
s 
i 
f 
i 
e 
r 

m 
a 
x 
( 
c 1 

2 

) 

m 
a 
x 
( 
c 

1 
3 

) 

</table></figure>

			<note place="foot" n="1"> http://www.freebase.com/</note>

			<note place="foot" n="2"> E represents the word embeddings.</note>

			<note place="foot" n="3"> With regard to the position feature, our experiments yield the same positive results described in Zeng et al. (2014). Because the position feature is not the main contribution of this paper, we do not present the results without the position feature. 4 http://iesl.cs.umass.edu/riedel/ecml/ 5 https://code.google.com/p/word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was sponsored by the National Basic Research Program of China (no. 2014CB340503) and the National Natural Science Foundation of China (no. 61272332 and no. 61202329). We thank the anonymous reviewers for their insight-ful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A shortest path dependency kernel for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT/EMNLP</title>
		<meeting>HLT/EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Subsequence kernels for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas G Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomás</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Why does unsupervised pretraining help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Fractional max-pooling. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing coadaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-based weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning representations for weakly supervised natural language processing tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="120" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLdemo</title>
		<meeting>ACLdemo</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Characterizing the errors of data-driven dependency parsing models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLAFNLP</title>
		<meeting>ACLAFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
	<note>Rion Snow, and Dan Jurafsky</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral unsupervised parsing with additive tree metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Shay B Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1062" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2014</title>
		<meeting>EMNLP 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting constituent dependencies for tree kernel-based semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="697" to="704" />
		</imprint>
	</monogr>
	<note>Qiaoming Zhu, and Peide Qian</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECML PKDD</title>
		<meeting>ECML PKDD</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Combining linguistic and statistical analysis to extract relations from web documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Ifrim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="712" to="717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL</title>
		<meeting>EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel methods for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Zelenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chinatsu</forename><surname>Aone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Richardella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1083" to="1106" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting rbf neural networks to multi-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A composite kernel to extract relations between entities with both flat and structured features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="825" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploring various knowledge in relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
