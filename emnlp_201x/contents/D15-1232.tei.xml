<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Summarization Based on Embedding Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="1984">1984-1989. September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kobayashi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>9-7-1 Akasaka, Minato-ku</addrLine>
									<postCode>107-6211</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Noguchi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>9-7-1 Akasaka, Minato-ku</addrLine>
									<postCode>107-6211</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taichi</forename><surname>Yatsuka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yahoo Japan Corporation</orgName>
								<address>
									<addrLine>9-7-1 Akasaka, Minato-ku</addrLine>
									<postCode>107-6211</postCode>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Summarization Based on Embedding Distributions</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="1984">1984-1989. September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this study, we consider a summariza-tion method using the document level similarity based on embeddings, or distributed representations of words, where we assume that an embedding of each word can represent its &quot;meaning.&quot; We formalize our task as the problem of maximizing a sub-modular function defined by the negative summation of the nearest neighbors&apos; distances on embedding distributions, each of which represents a set of word embed-dings in a document. We proved the sub-modularity of our objective function and that our problem is asymptotically related to the KL-divergence between the probability density functions that correspond to a document and its summary in a continuous space. An experiment using a real dataset demonstrated that our method performed better than the existing method based on sentence-level similarity.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document summarization aims to rephrase a doc- ument in a short form called a summary while keeping its "meaning." In the present study, we aim to characterize the meaning of a document us- ing embeddings or distributed representations of words in the document, where an embedding of each word is represented as a real valued vector in a Euclidean space that corresponds to the word ( <ref type="bibr" target="#b13">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b14">Mikolov et al., 2013b</ref>).</p><p>Many previous studies have investigated sum- marization ( <ref type="bibr" target="#b8">Lin and Bilmes, 2010;</ref><ref type="bibr" target="#b9">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b10">Lin and Bilmes, 2012;</ref><ref type="bibr" target="#b21">Sipos et al., 2012;</ref><ref type="bibr" target="#b15">Morita et al., 2013</ref>), but to the best of our knowledge, only one <ref type="bibr">(Kågebäck et al., 2014</ref>) con- sidered a direct summarization method using em- beddings, where the summarization problem was formalized as maximizing a submodular function defined by the summation of cosine similarities based on sentence embeddings. Essentially, this method assumes linear meanings since the objec- tive function is characterized by the summation of sentence-level similarities. However, this assump- tion is not always valid in real documents, and thus there may be a better combination of two other sentences than the best and second best sentences in terms of similarity in a document.</p><p>In this study, we consider a summarization method based on document-level similarity, where we assume the non-linearity of meanings. First, we examine an objective function defined by a co- sine similarity based on document embeddings in- stead of sentence embeddings. Unfortunately, in contrast to our intuition, this similarity is not sub- modular, which we disprove later. Thus, we pro- pose a valid submodular function based on em- bedding distributions, each of which represents a set of word embeddings in a document, as the document-level similarity. Our objective func- tion is calculated based on the nearest neighbors' distances on embedding distributions, which can be proved to be asymptotically related to KL- divergence in a continuous space. Several stud- ies ( <ref type="bibr" target="#b7">Lerman and McDonald, 2009;</ref><ref type="bibr" target="#b3">Haghighi and Vanderwende, 2009)</ref> have addressed summariza- tion using KL-divergence, but they calculated KL- divergence based on word distributions in a dis- crete space. In other words, our study is the first attempt to summarize by asymptotically estimat- ing KL-divergence based on embedding distribu- tions in a continuous space. In addition, they in- volved the inference of complex models, whereas our method is quite simple but still powerful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We treat a document as a bag-of-sentences and a sentence as a bag-of-words. Formally, let D be a document, and we refer to an element s ∈ D of a sentence and w ∈ s of a word. We denote the size of a set S by |S|. Note that D and s are defined as multisets. For example, we can define a document such as D := {s 1 , s 2 } with s 1 := {just, do, it} and s 2 := {never, say, never}, which cor- respond to two sentences "Just do it" and "Never say never," respectively. From the definition, we have |s 1 | = 3 and |s 2 | = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Submodularity</head><p>Submodularity is a property of set functions, which is similar to the convexity or concavity of continuous functions.</p><p>We formally define submodularity as follows.</p><p>Definition 1 (Submodularity). Given a set X, a set function f : 2 X → R is called submodular if for any two sets S 1 and S 2 such that S 1 ⊂ S 2 ⊂ X and element x ∈ X \ S 2 ,</p><formula xml:id="formula_0">f (S 1 ∪ {x}) − f (S 1 ) ≥ f (S 2 ∪ {x}) − f (S 2 ).</formula><p>For simplicity, we define f S (x) := f (S ∪ {x}) − f (S), which is called the marginal value of x with respect to S. A set function f is called monotone if f S (x) ≥ 0 for any set S ⊂ X and element x ∈ X \ S.</p><p>If a set function f is monotone submodular, we can approximate the optimal solution efficiently by a simple greedy algorithm, which iteratively selects x * = argmax x∈X\S i f S i (x) where ties are broken arbitrarily, and we substitute S i+1 = S i ∪ {x * } in the i-th iteration beginning with S 0 = ∅. This algorithm is quite simple but it is guaranteed to find a near optimal solution within 1 − 1/e ≈ 0.63 ( <ref type="bibr" target="#b1">Calinescu et al., 2007</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Embedding</head><p>An embedding or distributed representation of a word is a real valued vector in an m-dimensional Euclidean space R m , which expresses the "mean- ing" of the word. We denote an embedding of a word w by w ∈ R m . If for any two words w 1 and w 2 , the meaning of w 1 is similar to that of w 2 , then w 1 is expected to be near to w 2 . A recent study <ref type="bibr" target="#b13">(Mikolov et al., 2013a)</ref> showed that a simple log-bilinear model can learn high quality embeddings to obtain a better result than recurrent neural networks, where the concept of embeddings was originally proposed in studies of neural language models ( <ref type="bibr" target="#b0">Bengio et al., 2003</ref>). In the present study, we use the CW Vector 1 and W2V Vector 2 which are also used in the previous study <ref type="bibr">(Kågebäck et al., 2014</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this study, we focus on a summarization task as sentence selection in a document. The opti- mization framework in our task is the same as in the previous study and formalized in Algorithm 1, where w s represents the pre-defined weight or cost of a sentence s, e.g., sentence length, and r is its scaling factor. This algorithm, called modified greedy, was proposed in ( <ref type="bibr" target="#b8">Lin and Bilmes, 2010)</ref> and interestingly performed better than the state- of-the-art abstractive approach as shown in ( <ref type="bibr" target="#b9">Lin and Bilmes, 2011)</ref>. Note that we have omitted the notation of D from f for simplicity because D is fixed in an optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Modified greedy algorithm.</head><p>Data: Document D, objective function f , and summary size .</p><formula xml:id="formula_1">Result: Summary C ⊂ D. 1 C ← ∅; U ← D; 2 while U = ∅ do 3 s * ← argmax s∈U f C (s)/(w s ) r ; 4 if s∈C w s + w s * ≤ then C ← C ∪ {s * }; 5 U ← U \ {s * }; 6 s * ← argmax s∈D:ws≤ f ({s}); 7 return C ← argmax C ∈{C,{s * }} f (C );</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Based on Document Embeddings</head><p>First, we examine an objective function f Cos de- fined by a cosine similarity based on document embeddings. An embedding of a document D is defined as v D := s∈D w∈s w. We formalize the objective function f Cos as follows.</p><formula xml:id="formula_2">f Cos (C) := v C · v D v C v D .</formula><p>Note that the optimal solution does not change, if we use an average embedding v D / s∈D |s| in- stead of v D . The next theorem shows that a solu- tion of f Cos by Algorithm 1 is not guaranteed to be near optimal.</p><formula xml:id="formula_3">Theorem 1. f Cos is not submodular.</formula><p>Proof. A simple counterexample is sufficient to prove the theorem. Let us consider D := {s 1 := {w 1 }, s 2 := {w 2 }, s 3 := {w 3 }, s 4 := {w 4 }} with corresponding vectors w 1 := (1, 1), w 2 := (1, 2), w 3 := (1, −1), and w 4 := (1, −2), re- spectively. In this case, the document embedding v D is (4, 0). We set C 1 := {s 1 } and C 2 := {s 1 , s 2 }. Clearly, C 1 ⊂ C 2 . However, we ob- tain f Cos</p><formula xml:id="formula_4">C 1 (s 4 ) = f Cos ({s 1 , s 4 }) − f Cos ({s 1 }) ≈ 0.187 and f Cos C 2 (s 4 ) = f Cos ({s 1 , s 2 , s 4 }) − f Cos ({s 1 , s 2 }) ≈ 0.394. Therefore, we have f Cos C 2 (s 4 ) &gt; f Cos C 1 (s 4 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Based on Embedding Distributions</head><p>We propose a valid submodular objective function f NN based on embedding distributions. The key observation is that for any two embedding distri- butions A and B, when A is similar to B, each em- bedding in A should be near to some embedding in B. In order to formalize this idea, we define the nearest neighbor of a word w in a summary C as n(w, C) := argmin v∈s:s∈C, w = v d( w, v), where d is the Euclidian distance in the embedding space, i.e., d( w, v) := w − v. We denote the dis- tance of w to its nearest neighbor n := n(w, C) by N (w, C) := d( w, n). Finally, we define f NN as follows:</p><formula xml:id="formula_5">f NN (C) := − s∈D w∈s g(N (w, C)),</formula><p>where g is a non-decreasing scaling function. The function f NN represents the negative value −δ of dissimilarity δ between a document and summary based on embedding distributions. Note that we can use sentence embeddings instead of word em- beddings as embedding distributions, although we focus on word embeddings in this section.</p><p>The next theorem shows the monotone submod- ularity of our objective function, which means that a solution of f NN by Algorithm 1 is guaranteed to be near optimal. </p><formula xml:id="formula_6">∈ D \ C, we have f NN C (s) = f NN (C s ) − f NN (C) = D w (g(N (w, C)) − g(N (w, C s ))). Since C ⊂ C s , obviously N (w, C) ≥ N (w, C s ) holds.</formula><p>Therefore, we obtain f NN C (s) ≥ 0 from the non- decreasing property of g.</p><p>(Submodularity) Next, we prove the submodu- larity. For any two sets C 1 and C 2 of sentences such that C 1 ⊂ C 2 ⊂ D, and sentence s ∈ D \C 2 , we have f NN</p><formula xml:id="formula_7">C 1 (s) − f NN C 2 (s) = f NN (C s 1 ) − f NN (C 1 ) − (f NN (C s 2 ) − f NN (C 2 )) = D w (g(N (w, C 1 )) − g(N (w, C s 1 )) − g(N (w, C 2 )) + g(N (w, C s 2 ))). Let α := g(N (w, C 1 )) − g(N (w, C s 1 )) − g(N (w, C 2 )) + g(N (w, C s 2 )). If n(w, C s 2 ) ∈ s, then n(w, C s 1 ) ∈ s holds, since C s 1 ⊂ C s 2 . This means that N (w, C s 2 ) = N (w, C s 1 ) = N (w, {s}). Clearly, N (w, C 1 ) ≥ N (w, C 2 ), since C 1 ⊂ C 2 .</formula><p>Therefore, we obtain α ≥ 0 from the non-decreasing property of g.</p><p>If n(w, C s 2 ) / ∈ s and n(w, C s 1 ) / ∈ s, we have N (w, C s 1 ) = N (w, C 1 ) and N (w, C s 2 ) = N (w, C 2 ). This indicates that α = 0.</p><p>If n(w, C s 2 ) / ∈ s and n(w, C s 1 ) ∈ s, so sim- ilarly N (w, C s 1 ) ≤ N (w, C 1 ) and N (w, C s 2 ) = N (w, C 2 ) hold. Therefore, we obtain α ≥ 0.</p><p>The objective function f NN is simply heuristic for small documents, but the next theorem shows that f NN is asymptotically related to an approxima- tion of KL-divergence in a continuous space, if g is a logarithmic function. This result implies that we can use mathematical techniques of a contin- uous space for different NLP tasks, by mapping a document into a continuous space based on word embeddings.</p><p>Theorem 3. Suppose that we have a document D and two summaries C 1 and C 2 such that |C 1 | = |C 2 |, which are samples drawn from some proba- bility density functions p, q, and r, i.e., D ∼ p, C 1 ∼ q, and C 2 ∼ r, respectively. If the scal- ing function g of f NN is a logarithmic function, the order relation of the expectations of f NN (C 1 ) and f NN (C 2 ) is asymptotically the same as that of the KL-divergences D KL (p || r) and</p><formula xml:id="formula_8">D KL (p || q), i.e., E[f NN (C 1 )] − E[f NN (C 2 )] &gt; 0 ⇔ D KL (p || r) − D KL (p || q) &gt; 0, as |D| → ∞, |C 1 | → ∞, and |C 2 | → ∞.</formula><p>Proof. Let m be the dimension on embeddings. Using a divergence estimator based on nearest neighbor distances in <ref type="bibr" target="#b16">(Pérez-Cruz, 2009;</ref><ref type="bibr" target="#b22">Wang et al., 2009)</ref>, we can approximate</p><formula xml:id="formula_9">D KL (p || q) by D KL (D, C 1 ) := m |D| D w ln N (w,C 1 ) N (w,D) + ln |C 1 | |D|−1 . Therefore, we obtain D KL (D, C 2 ) − D KL (D, C 1 ) ∝ D w ln N (w, C 2 ) − D w ln N (w, C 1 ). Since g(x) = ln(x), we have f NN (C 1 ) − f NN (C 2 ) &gt; 0 if and only if D KL (D, C 2 ) − D KL (D, C 1 ) &gt; 0 holds. The fact that E[ D KL (D, C 1 )] → D KL (p || q)</formula><p>as |C 1 | → ∞ and |D| → ∞ concludes the theo- rem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compared our two proposed methods DocEmb and EmbDist with two state-of-the-art methods SenEmb and TfIdf. The first two methods DocEmb and EmbDist represent Algorithm 1 with our proposed objective functions f Cos and f NN , respectively. TfIdf represents Algorithm 1 with an objective function based on the sum of co- sine similarities of tf-idf vectors that correspond to sentences, which was proposed in ( <ref type="bibr" target="#b9">Lin and Bilmes, 2011</ref>). SenEmb uses a cosine similar- ity measure based on embeddings instead of tf-idf vectors in the same framework as TfIdf, which was proposed in <ref type="bibr">(Kågebäck et al., 2014</ref>).</p><p>We conducted an experiment with almost the same setting as in the previous study, where they used the Opinosis dataset ( <ref type="bibr" target="#b2">Ganesan et al., 2010)</ref>. This dataset is a collection of user reviews in 51 different topics such as hotels, cars, and products; thus, it is more appropriate for evaluating sum- marization of user-generated content than well- known DUC datasets, which consist of formal news articles. Each topic in the collection com- prises 50-575 sentences and includes four and five gold standard summaries created by human au- thors, each of which comprises 1-3 sentences.</p><p>We ran an optimization process to choose sen- tences within 100 words 3 by setting the summary size and weights as = 100 and w s = |s| for any sentence s, respectively. As for TfIdf and SenEmb, we set a cluster size of k-means as k = |D|/5 and chose the best value for a threshold coefficient α, trade-off coefficient λ, and the scal- ing factor r, as in ( <ref type="bibr" target="#b9">Lin and Bilmes, 2011)</ref>. Note that our functions DocEmb and EmbDist have only one parameter r, and we similarly chose the best value of r. Regarding DocEmb, EmbDist, and SenEmb, we used the best embeddings from the CW Vector and W2V Vector for each method, and created document and sentence embeddings by averaging word embeddings with tf-idf weights since it performed better in this experiment. In the case of EmbDist, we used a variant of f NN based R-1 R-2 R-3 R-4 on distributions of sentence embeddings. In ad- dition, we examined three scaling functions: log- arithmic, linear, and exponential functions, i.e., ln x, x, e x , respectively. We calculated the ROUGE-N metric <ref type="bibr" target="#b12">(Lin, 2004)</ref>  <ref type="bibr">4</ref> , which is a widely-used evaluation met- ric for summarization methods. ROUGE-N is based on the co-occurrence statistics of N-grams, and especially ROUGE-1 has been shown to have the highest correlation with human summaries ( <ref type="bibr" target="#b11">Lin and Hovy, 2003)</ref>. ROUGE-N is similar to the BLEU metric for machine translation, but ROUGE-N is a recall-based metric while BLEU is a precision-based metric. <ref type="table">Table 1</ref> shows the results obtained for ROUGE- N (N ≤ 4) using DocEmb, EmbDist, SenEmb, and TfIdf. ApxOpt represents the approxima- tion results of the optimal solution in our prob- lem, where we optimized ROUGE-1 with the gold standard summaries by Algorithm 1. The ob- tained results indicate that our proposed method EmbDist with exponential scaling performed the best for ROUGE-1, which is the best metric in terms of correlation with human summaries. The W2V Vector was the best choice for EmbDist. Furthermore, the other proposed method DocEmb performed better than the state-of-the-art methods SenEmb and TfIdf, although DocEmb is not theoretically guaranteed to obtain a near optimal solution. These results imply that our methods based on the document-level similarity can capture more complex meanings than the sentence-level similarity. On the other hand, TfIdf with tf-idf vectors performed the worst for ROUGE-1. A pos- sible reason is that a wide variety of expressions by users made it difficult to calculate similarities. This also suggests that embedding-based methods naturally have robustness for user-generated con- tent.</p><p>In the case of N ≥ 2, TfIdf performed the best for ROUGE-2 and ROUGE-3, while EmbDist with logarithmic scaling is better than TfIdf for ROUGE-4. According to <ref type="bibr" target="#b11">(Lin and Hovy, 2003)</ref>, the higher order ROUGE-N is worse than ROUGE-1 since it tends to score grammatical- ity rather than content. Conversely, <ref type="bibr" target="#b17">Rankel et al. (2013)</ref> reports that there is a dataset where the higher order ROUGE-N is correlated with human summaries well. We may need to conduct human judgments to decide which metric is the best in this dataset for more accurate comparison. How- ever, it is still important that our simple objective functions can obtain good results competing with the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, we proposed simple but power- ful summarization methods using the document- level similarity based on embeddings, or dis- tributed representations of words. Our experimen- tal results demonstrated that the proposed meth- ods performed better than the existing state-of-the- art methods based on the sentence-level similar- ity. This implies that the document-level similar- ity can capture more complex meanings than the sentence-level similarity.</p><p>Recently, <ref type="bibr" target="#b6">Kusner et al. (2015)</ref> independently discovered a similar definition to our objective function f NN through a different approach. They constructed a dissimilarity measure based on a framework using Earth Mover's Distance (EMD) developed in the image processing field <ref type="bibr" target="#b19">(Rubner et al., 1998;</ref><ref type="bibr" target="#b20">Rubner et al., 2000</ref>). EMD is a con- sistent measure of distance between two distribu- tions of points. Interestingly, their heuristic lower bound of EMD is exactly the same as −f NN with a linear scaling function, i.e., g(x) = x. Moreover, they showed that this bound appears to be tight in real datasets. This suggests that our intuitive framework can theoretically connect the two well- known measures, KL-divergence and EMD, based on the scaling of distance. Note that, to the best of our knowledge, there is currently no known study that considers such a theoretical relationship.</p><p>In future research, we will explore other scal- ing functions suitable for our problem or different problems. A promising direction is to consider a relative scaling function to extract a biased sum- mary of a document. This direction should be use- ful for query-focused summarization tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 2 .</head><label>2</label><figDesc>f NN is monotone submodular. Proof. (Monotonicity) First, we prove the mono- tonicity. For simplicity, we use the follow- ing two abbreviations: C s := C ∪ {s} and D w := s∈D w∈s . For any set C ⊂ D of sentences and sentence s</figDesc></figure>

			<note place="foot" n="1"> http://metaoptimize.com/projects/ wordreprs 2 https://code.google.com/p/word2vec</note>

			<note place="foot" n="3"> The previous work used a sentence-based constraint as = 2 and ws = 1, but we changed the setting since the variation in length has a noticeable impact on ROUGE scores as suggested in (Hong et al., 2014).</note>

			<note place="foot" n="4"> We used their software ROUGE version 1.5.5 with the parameters:-n 4-m-a-l 100-x-c 95-r 1000-f A-p 0.5-t 0.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank the reviewers for their helpful comments, especially about Earth Mover's Distance.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1532" to="4435" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximizing a Submodular Set Function Subject to a Matroid Constraint (Extended Abstract)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gruia</forename><surname>Calinescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Chekuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Pál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Integer Programming and Combinatorial Optimization (IPCO 2007)</title>
		<meeting>the 12th International Conference on Integer Programming and Combinatorial Optimization (IPCO 2007)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="182" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Opinosis: A Graph-based Approach to Abstractive Summarization of Highly Redundant Opinions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010)</title>
		<meeting>the 23rd International Conference on Computational Linguistics (COLING 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="340" to="348" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring Content Models for Multi-document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC 2014)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC 2014)</meeting>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extractive Summarization using Continuous Vector Space Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olof</forename><surname>Mogren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Tahmasebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devdatt</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2014)</title>
		<meeting>the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC 2014)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="31" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From Word Embeddings To Document Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">I</forename><surname>Kolkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML 2015)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML 2015)</meeting>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contrastive Summarization: An Experiment with Consumer Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-document Summarization via Budgeted Maximization of Submodular Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT 2010)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Class of Submodular Functions for Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning Mixtures of Submodular Shells with Application to Document Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012)</title>
		<meeting>the 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012)</meeting>
		<imprint>
			<publisher>AUAI</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="479" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Evaluation of Summaries Using N-gram CoOccurrence Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003). Association for Computational Linguistics</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL-HLT 2003). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ROUGE: A Package for Automatic Evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Text Summarization Branches Out</title>
		<meeting>the Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26 (NIPS 2013)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic Regularities in Continuous Space Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Subtree Extractive Summarization via Submodular Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1023" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation of Information Theoretic Measures for Continuous Random Variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pérez-Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS 2009)</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Rankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoa</forename><forename type="middle">Trang</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st</title>
		<meeting>the 51st</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL 2013)</title>
		<imprint>
			<biblScope unit="page" from="131" to="136" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Metric for Distributions with Applications to Image Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Computer Vision (ICCV 1998)</title>
		<meeting>the Sixth International Conference on Computer Vision (ICCV 1998)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s Distance as a Metric for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal Corpus Summarization Using Submodular Word Coverage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Sipos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pannaga</forename><surname>Shivaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012)</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Divergence Estimation for Multidimensional Densities Via k-Nearest-Neighbor Distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2392" to="2405" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
