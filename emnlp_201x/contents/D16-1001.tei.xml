<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of EECS</orgName>
								<orgName type="institution">Oregon State University</orgName>
								<address>
									<settlement>Corvallis</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="1" to="11"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however , neural models have not surpassed state-of-the-art approaches in constituency parsing. To remedy this, we introduce a new shift-reduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n 3) oracles for standard dependency parsing. Training with this oracle, we achieve the best F 1 scores on both English and French of any parser that does not use reranking or external data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parsing is an important problem in natural language processing which has been studied extensively for decades. Between the two basic paradigms of pars- ing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.</p><p>There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing ( <ref type="bibr" target="#b1">Andor et al., 2016)</ref>. In constituency parsing, however, neural approaches are still behind the state-of-the-art ( <ref type="bibr" target="#b5">Carreras et al., 2008;</ref><ref type="bibr" target="#b27">Shindo et al., 2012;</ref><ref type="bibr" target="#b31">Thang et al., 2015)</ref>; see more details in Section 5.</p><p>To remedy this, we design a new parsing frame- work that is more suitable for constituency parsing, and that can be accurately modeled by neural net- works. Observing that constituency parsing is pri- marily focused on sentence spans (rather than indi- vidual words, as is dependency parsing), we propose a novel adaptation of the shift-reduce system which reflects this focus. In this system, the stack consists of sentence spans rather than partial trees. It is also factored into two types of parser actions, structural and label actions, which alternate during a parse. The structural actions are a simplified analogue of shift-reduce actions, omitting the directionality of reduce actions, while the label actions directly as- sign nonterminal symbols to sentence spans.</p><p>Our neural model processes the sentence once for each parse with a recurrent network. We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions). Extending <ref type="bibr" target="#b33">Wang and Chang (2016)</ref>, each span is represented as the difference of recurrent out- put from multiple layers in each direction. No pre- trained embeddings are required.</p><p>We also extend the idea of dynamic oracles from dependency to constituency parsing. The latter is significantly more difficult than the former due to F 1 being a combination of precision and recall <ref type="bibr" target="#b18">(Huang, 2008)</ref>, and yet we propose a simple and extremely efficient oracle (amortized O(1) time). This oracle is proved optimal for F 1 as well as both of its compo- nents, precision and recall. Trained with this oracle, our parser achieves what we believe to be the best results for any parser without reranking which was trained only on the Penn Treebank and the French Treebank, despite the fact that it is not only linear- time, but also strictly greedy.</p><p>We make the following main contributions:</p><p>• A novel factored transition parsing system where the stack elements are sentence spans rather than partial trees (Section 2).</p><p>• A neural model where sentence spans are rep- resented as differences of output from a multi- layer bi-directional LSTM (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsing System</head><p>We present a new transition-based system for con- stituency parsing whose fundamental unit of com- putation is the sentence span. It uses a stack in a similar manner to other transition systems, except that the stack contains sentence spans with no re- quirement that each one correspond to a partial tree structure during a parse. The parser alternates between two types of ac- tions, structural and label, where the structural ac- tions follow a path to make the stack spans corre- spond to sentence phrases in a bottom-up manner, while the label actions optionally create tree brack- ets for the top span on the stack. There are only two structural actions: shift is the same as other transi- tion systems, while combine merges the top two sen- tence spans. The latter is analogous to a reduce ac- tion, but it does not immediately create a tree struc- ture and is non-directional. Label actions do create a partial tree on top of the stack by assigning one or more non-terminals to the topmost span.</p><p>Except for the use of spans, this factored approach is similar to the odd-even parser from <ref type="bibr" target="#b23">Mi and Huang (2015)</ref>. The fact that stack elements do not have to be tree-structured, however, means that we can cre- ate productions with arbitrary arity, and no binariza- tion is required either for training or parsing. This also allows us to remove the directionality inherent in the shift-reduce system, which is at best an im- perfect fit for constituency parsing. We do follow the practice in that system of labeling unary chains of non-terminals with a single action, which means our parser uses a fixed number of steps, (4n − 2) for a sentence of n words. <ref type="figure" target="#fig_0">Figure 1</ref> shows the formal deductive system for this parser. The stack σ is modeled as a list of strictly increasing integers whose first element is always 1 code: https://github.com/jhcross/span-parser input: zero. These numbers are word boundaries which de- fine the spans on the stack. In a slight abuse of no- tation, however, we sometimes think of it as a list of pairs (i, j), which are the actual sentence spans, i.e., every consecutive pair of indices on the stack, ini- tially empty. We represent stack spans by trapezoids ( i Some text and the symbol or scaled j ) in the figures to emphasize that they may or not have tree stucture.</p><formula xml:id="formula_0">w 0 . . . w n−1 axiom: 0, [0], ∅∅ goal: 2(2n − 1), [0, n], t sh z, σ | j, t z + 1, σ | j | j +1, t j &lt; n, even z comb z, σ | i | k | j, t z + 1, σ | i | j, t even z label-X z, σ | i | j, t z + 1, σ | i | j, t ∪ { i X j }} odd z nolabel z, σ | i | j, t z + 1, σ | i | j, t z &lt; (4n−1), odd z</formula><p>The parser alternates between structural actions and label actions according to the parity of the parser step z. In even steps, it takes a structural action, ei- ther combining the top two stack spans, which re- quires at least two spans on the stack, or introducing a new span of unit length, as long as the entire sen- tence is not already represented on the stack In odd steps, the parser takes a label action. One possibility is labeling the top span on the stack, (i, j) with either a nonterminal label or an ordered unary chain (since the parser has only one opportunity to label any given span). Taking no action, designated nolabel, is also a possibility. This is essentially a null operation except that it returns the parser to an even step, and this action reflects the decision that (i, j) is not a (complete) labeled phrase in the tree. In the final step, (4n − 2), nolabel is not allowed   since the parser must produce a tree. <ref type="figure" target="#fig_2">Figure 2</ref> shows a complete example of applying this parsing system to a very short sentence ("I do like eating fish") that we will use throughout this section and the next. The action in step 2 is label- NP because "I" is a one-word noun phrase (parts of speech are taken as input to our parser, though it could easily be adapted to include POS tagging in label actions). If a single word is not a complete phrase (e.g., "do"), then the action after a shift is nolabel.</p><p>The ternary branch in this tree (VP → MD VBP S) is produced by our parser in a straightforward man- ner: after the phrase "do like" is combined in step 7, no label is assigned in step 8, successfully delay- ing the creation of a bracket until the verb phrase is fully formed on the stack. Note also that the unary production in the tree is created with a single action, label-S-VP, in step 14.</p><p>The static oracle to train this parser simply con- sists of taking actions to generate the gold tree with a "short-stack" heuristic, meaning combine first whenever combine and shift are both possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LSTM Span Features</head><p>Long short-term memory networks (LSTM) are a type of recurrent neural network model proposed by <ref type="bibr" target="#b16">Hochreiter and Schmidhuber (1997)</ref> which are very effective for modeling sequences. They are able to capture and generalize from interactions among their sequential inputs even when separated by a long distance, and thus are a natural fit for analyz- ing natural language. LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling <ref type="bibr" target="#b29">(Sundermeyer et al., 2012</ref>) and translation <ref type="bibr" target="#b30">(Sutskever et al., 2014</ref>).</p><p>LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an en- tire sentence ( <ref type="bibr" target="#b32">Vinyals et al., 2015)</ref>, separately mod- eling the stack, buffer, and action history , to encode words based on their character forms ( , and as an element in a recursive structure to combine dependency sub- trees with their left and right children <ref type="bibr" target="#b19">(Kiperwasser and Goldberg, 2016a</ref>). For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence. We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different time steps, which correspond to word boundaries. If the sequential output of the recurrent network for the sentence is f 0 , ..., f n in the forward direction and b n , ..., b 0 in the backward direction then the span (i, j) would be represented as the concatenation of the vector differences (f j − f i ) and</p><formula xml:id="formula_1">(b i − b j ).</formula><p>The spans are represented using output from both backward and forward LSTM components, as can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>. This is essentially the LSTM- Minus feature representation described by <ref type="bibr" target="#b33">Wang and Chang (2016)</ref> extended to the bi-directional case. In initial experiments, we found that there was essen- tially no difference in performance between using the difference features and concatenating all end- point vectors, but our approach is almost twice as fast.</p><formula xml:id="formula_2">s I do like eating fish /s 0 f 0 b 0 1 f 1 b 1 2 f 2 b 2 3 f 3 b 3 4 f 4 b</formula><p>This model allows a sentence to be processed once, and then the same recurrent outputs can be used to compute span features throughout the parse. Intuitively, this allows the span differences to learn to represent the sentence spans in the context of the rest of the sentence, not in isolation (especially true for LSTM given the extra hidden recurrent connec- tion, typically described as a "memory cell"). In practice, we use a two-layer bi-directional LSTM, where the input to the second layer combines the forward and backward outputs from the first layer at that time step. For each direction, the components from the first and second layers are concatenated to form the vectors which go into the span features. See <ref type="bibr" target="#b8">Cross and Huang (2016)</ref> for more details on this ap- proach.</p><p>For the particular case of our transition con- stituency parser, we use only four span features to determine a structural action, and three to determine a label action, in each case partitioning the sentence exactly. The reason for this is straightforward: when considering a structural action, the top two spans on the stack must be considered to determine whether they should be combined, while for a label action, only the top span on the stack is important, since that is the candidate for labeling. In both cases the re- maining sentence prefix and suffix are also included. These features are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>The input to the recurrent network at each time step consists of vector embeddings for each word</p><formula xml:id="formula_3">Action Stack LSTM Span Features Structural σ | i | k | j 0 2 i</formula><p>Some text and the symbol or scaled 1 k Some text and the symbol or scaled</p><formula xml:id="formula_4">1 j 2 n Label σ | i | j 0 2 i</formula><p>Some text and the symbol or scaled 1 j 2 n and its part-of-speech tag. Parts of speech are pre- dicted beforehand and taken as input to the parser, as in much recent work in parsing. In our experi- ments, the embeddings are randomly initialized and learned from scratch together with all other network weights, and we would expect further performance improvement from incorporating embeddings pre- trained from a large external corpus. The network structure after the the span features consists of a separate multilayer perceptron for each type of action (structural and label). For each ac- tion we use a single hidden layer with rectified linear (ReLU) activation. The model is trained on a per- action basis using a single correct action for each parser state, with a negative log softmax loss func- tion, as in Chen and Manning (2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dynamic Oracle</head><p>The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and com- bine). This method suffers from a well-documeted problem, however, namely that it only "prepares" the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice. To alleviate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best possible action(s) at any arbitrary configuration.</p><p>In this section, we introduce an easy-to-compute optimal dynamic oracle for our constituency parser. We will first define some concepts upon which the dynamic oracle is built and then show how optimal actions can be very efficiently computed using this framework. In broad strokes, in any arbitrary parser configuration c there is a set of brackets t * (c) from the gold tree which it is still possible to reach. By following dynamic oracle actions, all of those brack- ets and only those brackets will be predicted.</p><p>Even though proving the optimality of our dy- namic oracle (Sec. 4.3) is involved, computing the oracle actions is extremely simple (Secs. 4.2) and efficient (Sec. 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries and Notations</head><p>Before describing the computation of our dynamic oracle, we first need to rigorously establish the de- sired optimality of dynamic oracle. The structure of this framework follows <ref type="bibr" target="#b13">Goldberg et al. (2014)</ref>. Definition 1. We denote c τ c iff. c is the result of action τ on configuration c, also denoted func- tionally as c = τ (c). We denote to be the union of τ for all actions τ , and * to be the reflexive and transitive closure of . Definition 2 (descendant/reachable trees). We de- note D(c) to be the set of final descendant trees derivable from c, i.e., D(c) = {t | c * z, σ, t}. This set is also called "reachable trees" from c.</p><p>Definition 3 (F 1 ). We define the standard F 1 metric of a tree t with respect to gold tree t G as</p><formula xml:id="formula_5">F 1 (t) = 2rp r+p , where r = |t∩t G | |t G | , p = |t∩t G | |t| .</formula><p>The following two definitions are similar to those for dependency parsing by <ref type="bibr" target="#b13">Goldberg et al. (2014)</ref>. Definition 4. We extend the F 1 function to config- urations to define the maximum possible F 1 from a given configuration: F 1 (c) = max t 1 ∈D(c) F 1 (t 1 ).</p><p>Definition 5 (oracle). We can now define the desired dynamic oracle of a configuration c to be the set of actions that retrain the optimal F 1 :</p><formula xml:id="formula_6">oracle(c) = {τ | F 1 (τ (c)) = F 1 (c)}.</formula><p>This abstract oracle is implemented by dyna(·) in Sec. 4.2, which we prove to be correct in Sec. 4.3.</p><p>Definition 6 (span encompassing). We say span (i, j) is encompassed by span (p, q), notated (i, j) (p, q), iff. p ≤ i &lt; j ≤ q.</p><p>Definition 7 (strict encompassing). We say span (i, j) is strictly encompassed by span (p, q), notated (i, j) (p, q), iff. (i, j) (p, q) and (i, j) = (p, q). We then extend this relation from spans to brackets, and notate i X j p Y q iff. (i, j) (p, q).  We next define a central concept, reachable brackets, which is made up of two parts, the left ones left(c) which encompass (i, j) without crossing any stack spans, and the right ones right(c) which are completely on the queue. See <ref type="figure" target="#fig_5">Fig. 4</ref> for examples. Definition 8 (reachable brackets). For any configu- ration c = z, σ | i | j, t, we define the set of reach- able gold brackets (with respect to gold tree t G ) as</p><formula xml:id="formula_7">reach(c) = left(c) ∪ right(c)</formula><p>where the left-and right-reachable brackets are</p><formula xml:id="formula_8">left(c) = { p X q ∈ t G | (i, j) (p, q), p ∈ σ | i} right(c) = { p X q ∈ t G | p ≥ j}</formula><p>for even z, with the replaced by for odd z.</p><p>Special case (initial):</p><formula xml:id="formula_9">reach(0, [0], ∅∅) = t G .</formula><p>The notation p ∈ σ | i simply means (p, q) does not "cross" any bracket on the stack. Remember our stack is just a list of span boundaries, so if p coin- cides with one of them, (p, q)'s left boundary is not crossing and its right boundary q is not crossing ei- ther since q ≥ j due to (i, j) (p, q).</p><p>Also note that reach(c) is strictly disjoint from t, i.e., reach(c) ∩ t = ∅ and reach(c) ⊆ t G − t. See <ref type="figure" target="#fig_7">Figure 6</ref> for an illustration. Definition 9 (next bracket). For any configuration c = z, σ | i | j, t, the next reachable gold bracket (with respect to gold tree t G ) is the smallest reach- able bracket (strictly) encompassing (i, j):</p><p>next(c) = min left(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Structural and Label Oracles</head><p>For an even-step configuration c = z, σ | i | j, t, we denote the next reachable gold bracket next(c) to be p X q , and define the dynamic oracle to be:</p><formula xml:id="formula_10">dyna(c) =     </formula><p>{sh} if p = i and q &gt; j {comb} if p &lt; i and q = j {sh, comb} if p &lt; i and q &gt; j</p><p>As a special case dyna(0, [0], ∅∅) = {sh}. <ref type="figure">Figure 5</ref> shows examples of this policy. The key insight is, if you follow this policy, you will not miss the next reachable bracket, but if you do not follow it, you certainly will. We formalize this fact below (with proof omitted due to space constraints) which will be used to prove the central results later. Lemma 1. For any configuration c, for any τ ∈ dyna(c), we have reach(τ (c)) = reach(c); for any τ / ∈ dyna(c), we have reach(τ (c)) reach(c).</p><p>The label oracles are much easier than struc- tural ones. For an odd-step configuration c = z, σ | i | j, t, we simply check if (i, j) is a valid span in the gold tree t G and if so, label it accord- ingly, otherwise no label. More formally,</p><formula xml:id="formula_12">dyna(c) = {label-X} if some i X j ∈ t G {nolabel} otherwise (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Correctness</head><p>To show the optimality of our dynamic oracle, we begin by defining a special tree t * (c) and show that it is optimal among all trees reachable from config- uration c. We then show that following our dynamic oracle (Eqs. 1-2) from c will lead to t * (c).</p><p>Definition 10 (t * (c)). For any configuration c = z, σ, t, we define the optimal tree t * (c) to include all reachable gold brackets and nothing else. More formally, t * (c) = t ∪ reach(c). Some text and the symbol or scaled <ref type="figure">Figure 5</ref>: Dynamic oracle with respect to the gold parse in We can show by induction that t * (c) is attainable:</p><p>Lemma 2. For any configuration c, the optimal tree is a descendant of c, i.e., t * (c) ∈ D(c).</p><p>The following Theorem shows that t * (c) is indeed the best possible tree:</p><formula xml:id="formula_13">Theorem 1 (optimality of t * ). For any configura- tion c, F 1 (t * (c)) = F 1 (c).</formula><p>Proof. (SKETCH) Since t * (c) adds all possible addi- tional gold brackets (the brackets in reach(c)), it is not possible to get higher recall. Since it adds no in- correct brackets, it is not possible to get higher pre-  cision. Since F 1 is the harmonic mean of precision and recall, it also leads to the best possible F 1 .</p><formula xml:id="formula_14">t t G reach(c) t * (c) = t ∪ reach(c)</formula><p>Corollary 1. For any c = z, σ, t, for any t ∈ D(c) and t = t * (c), we have F 1 (t ) &lt; F 1 (c).</p><p>We now need a final lemma about the policy dyna(·) (Eqs. 1-2) before proving the main result. Lemma 3. From any c = z, σ, t, for any action τ ∈ dyna(c), we have t * (τ (c)) = t * (c). For any action τ / ∈ dyna(c), we have t * (τ (c)) = t * (c).</p><p>Proof. (SKETCH) By case analysis on even/odd z.</p><p>We are now able to state and prove the main the- oretical result of this paper (using Lemma 3, Theo- rem 1 and Corollary 1):</p><p>Theorem 2. The function dyna(·) in Eqs. (1-2) sat- isfies the requirement of a dynamic oracle (Def. 5):</p><p>dyna(c) = oracle(c) for any configuration c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation and Complexity</head><p>For any configuration, our dynamic oracle can be computed in amortized constant time since there are only O(n) gold brackets and thus bounding |reach(c)| and the choice of next(c). After each action, next(c) either remains unchanged, or in the case of being crossed by a structural action or mislabeled by a label action, needs to be updated. This update is simply tracing the parent link to the next smallest gold bracket repeatedly until the new bracket encompasses span (i, j). Since there are at most O(n) choices of next(c) and there are O(n) steps, the per-step cost is amortized constant time. Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard depen- dency parsing in <ref type="bibr" target="#b13">Goldberg et al. (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Neural networks have been used for constituency parsing in a number of previous instances. For example, <ref type="bibr" target="#b28">Socher et al. (2013)</ref> learn a recursive network that combines vectors representing partial trees, <ref type="bibr" target="#b32">Vinyals et al. (2015)</ref> adapt a sequence-to- sequence model to produce parse trees, <ref type="bibr" target="#b34">Watanabe and Sumita (2015)</ref> use a recursive model applying a shift-reduce system to constituency parsing with beam search, and  adapt the Stack- LSTM dependency parsing approach to this task. <ref type="bibr" target="#b9">Durrett and Klein (2015)</ref> combine both neural and sparse features for a CKY parsing system. Our own previous work <ref type="bibr" target="#b8">(Cross and Huang, 2016</ref>) use a recur- rent sentence representation in a head-driven tran- sition system which allows for greedy parsing but does not achieve state-of-the-art results. The concept of "oracles" for constituency parsing (as the tree that is most similar to t G among all pos- sible trees) was first defined and solved by <ref type="bibr" target="#b18">Huang (2008)</ref> in bottom-up parsing. In transition-based parsing, the dynamic oracle for shift-reduce depen- dency parsing costs worst-case O(n 3 ) time <ref type="bibr" target="#b13">(Goldberg et al., 2014</ref>). On the other hand, after the sub- mission of our paper we became aware of a paral- lel work <ref type="bibr" target="#b7">(Coavoux and Crabbé, 2016</ref>) that also pro- posed a dynamic oracle for their own incremental constituency parser. However, it is not optimal due to dummy non-terminals from binarization.</p><note type="other">Network architecture Word embeddings 50 Tag embeddings 20 Morphological embeddings † 10 LSTM layers 2 LSTM units 200 / direction ReLU hidden units 200 / action type Training settings Embedding intialization random Training epochs 10 Minibatch size 10 sentences Dropout (on LSTM output) p = 0.5 ADADELTA parameters ρ = 0.99, = 1 × 10 −7</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We present experiments on both the Penn English Treebank ( <ref type="bibr" target="#b21">Marcus et al., 1993</ref>) and the French Tree- bank ( <ref type="bibr" target="#b0">Abeillé et al., 2003</ref>). In both cases, all state- action training pairs for a given sentence are used at the same time, greatly increasing training speed since all examples for the same sentence share the same forward and backward pass through the recur- rent part of the network. Updates are performed in minibatches of 10 sentences, and we shuffle the training sentences before each epoch. The results we report are trained for 10 epochs.</p><p>The only regularization which we employ during training is dropout ( <ref type="bibr" target="#b15">Hinton et al., 2012)</ref>, which is applied with probability 0.5 to the recurrent outputs. It is applied separately to the input to the second LSTM layer for each sentence, and to the input to the ReLU hidden layer (span features) for each state- action pair. We use the ADADELTA method <ref type="bibr" target="#b36">(Zeiler, 2012</ref>) to schedule learning rates for all weights. All of these design choices are summarized in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>In order to account for unknown words during training, we also adopt the strategy described by <ref type="bibr" target="#b20">Kiperwasser and Goldberg (2016b)</ref>, where words in the training set are replaced with the unknown- word symbol UNK with probability</p><formula xml:id="formula_15">p unk = z z+f (w)</formula><p>where f (w) is the number of times the word ap- pears in the training corpus. We choose the pa- rameter z so that the training and validation cor- pora have approximately the same proportion of un- known words. For the Penn Treebank, for exam- ple, we used z = 0.8375 so that both the validation set and the (rest of the) training set contain approx- imately 2.76% unknown words. This approach was helpful but not critical, improving F 1 (on dev) by about 0.1 over training without any unknown words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Training with Dynamic Oracle</head><p>The most straightforward use of dynamic oracles to train a neural network model, where we collect all action examples for a given sentence before updat- ing, is "training with exploration" as proposed by <ref type="bibr" target="#b12">Goldberg and Nivre (2013)</ref>. This involves parsing each sentence according to the current model and us- ing the oracle to determine correct actions for train- ing. We saw very little improvement on the Penn treebank validation set using this method, however. Based on the parsing accuracy on the training sen- tences, this appears to be due to the model overfitting the training data early during training, thus negating the benefit of training on erroneous paths.</p><p>Accordingly, we also used a method recently pro- posed by , which specifi- cally addresses this problem. This method intro- duces stochasticity into the training data parses by randomly taking actions according to the softmax distribution over action scores. This introduces re- alistic mistakes into the training parses, which we found was also very effective in our case, leading to higher F 1 scores, though it noticeably sacrifices recall in favor of precision.</p><p>This technique can also take a parameter α to flat- ten or sharpen the raw softmax distribution. The re- sults on the Penn treebank development set for var- ious values of α are presented in <ref type="table" target="#tab_5">Table 3</ref>. We were surprised that flattening the distribution seemed to be the least effective, as training accuracy (taking into account sampled actions) lagged somewhat be- hind validation accuracy. Ultimately, the best results were for α = 1, which we used for final testing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Penn Treebank</head><p>Following the literature, we used the Wall Street Journal portion of the Penn Treebank, with stan- dard splits for training (secs 2-21), development (sec 22), and test sets (sec 23). Because our pars- ing system seamlessly handles non-binary produc- tions, minimal data preprocessing was required. For the part-of-speech tags which are a required input to our parser, we used the Stanford tagger with 10-way jackknifing. <ref type="table" target="#tab_7">Table 4</ref> compares test our results on PTB to a range of other leading constituency parsers. De- spite being a greedy parser, when trained using dy- namic oracles with exploration, it achieves the best F 1 score of any closed-set single-model parser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">French Treebank</head><p>We also report results on the French treebank, with one small change to network structure. Specifically, we also included morphological features for each word as input to the recurrent network, using a small embedding for each such feature, to demonstrate that our parsing model is able to exploit such ad- ditional features.</p><p>We used the predicted morphological features, part-of-speech tags, and lemmas (used in place of word surface forms) supplied with the SPMRL 2014 <ref type="formula">(2006)</ref> 88.1 87.8 87.9 <ref type="bibr" target="#b24">Petrov and Klein (2007)</ref> 90.1 90.3 90.2 <ref type="bibr" target="#b5">Carreras et al. (2008)</ref> 90.7 91.4 91.1 <ref type="bibr" target="#b27">Shindo et al. (2012)</ref> 91.1 †Socher et al. <ref type="formula" target="#formula_11">(2013)</ref> 90.4 <ref type="bibr" target="#b37">Zhu et al. (2013)</ref> 90.2 90.7 90.4 <ref type="bibr" target="#b23">Mi and Huang (2015)</ref> 90.7 90.9 90.8 †Watanabe and Sumita <ref type="formula" target="#formula_11">(2015)</ref> 90.7 <ref type="bibr" target="#b31">Thang et al. (2015)</ref>  <ref type="table">(A*)</ref> 90.9 91.2 91.1 †* <ref type="bibr">Dyer et al. (2016) (discrim.)</ref> 89.8 †* <ref type="bibr" target="#b8">Cross and Huang (2016)</ref> 90.0 †*static oracle 90.7 91.4 91.0 †*dynamic/exploration 90.5 92.1 91.3 External/Reranking/Combo †Henderson <ref type="formula">(2004)</ref>    data set ( <ref type="bibr" target="#b26">Seddah et al., 2014)</ref>. It is thus possible that results could be improved further using an integrated or more accurate predictor for those features. Our parsing and evaluation also includes predicting POS tags for multi-word expressions as is the standard practice for the French treebank, though our results are similar whether or not this aspect is included.</p><note type="other">Closed Training &amp; Single Model LR LP F 1 Sagae and Lavie</note><p>We compare our parser with other recent work in <ref type="table" target="#tab_8">Table 5</ref>. We achieve state-of-the-art results even in comparison to <ref type="bibr" target="#b4">Björkelund et al. (2014)</ref>, which uti- lized both external data and reranking in achieving the best results in the SPMRL 2014 shared task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Notes on Experiments</head><p>For these experiments, we performed very little hy- perparameter tuning, due to time and resource con- traints. We have every reason to believe that per- formance could be improved still further with such techniques as random restarts, larger hidden lay- ers, external embeddings, and hyperparameter grid search, as demonstrated by <ref type="bibr" target="#b35">Weiss et al. (2015)</ref>.</p><p>We also note that while our parser is very accu- rate even with greedy decoding, the model is eas- ily adaptable for beam search, particularly since the parsing system already uses a fixed number of ac- tions. Beam search could also be made considerably more efficient by caching post-hidden-layer feature components for sentence spans, essentially using the precomputation trick described by <ref type="bibr" target="#b6">Chen and Manning (2014)</ref>, but on a per-sentence basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>We have developed a new transition-based con- stituency parser which is built around sentence spans. It uses a factored system alternating between structural and label actions. We also describe a fast dynamic oracle for this parser which can determine the optimal set of actions with respect to a gold training tree in an arbitrary state. Using an LSTM model and only a few sentence spans as features, we achieve state-of-the-art accuracy on the Penn Tree- bank for all parsers without reranking, despite using strictly greedy inference.</p><p>In the future, we hope to achieve still better re- sults using beam search, which is relatively straight- forward given that the parsing system already uses a fixed number of actions. Dynamic programming <ref type="bibr" target="#b17">(Huang and Sagae, 2010</ref>) could be especially pow- erful in this context given the very simple feature representation used by our parser, as noted also by <ref type="bibr" target="#b20">Kiperwasser and Goldberg (2016b)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deductive system for the Structure/Label transition parser. The stack σ is represented as a list of integers where the span defined by each consecutive pair of elements is a sentence segment on the stack. Each X is a nonterminal symbol or an ordered unary chain. The set t contains labeled spans of the form iX j , which at the end of a parse, fully define a parse tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The running example. It contains one ternary branch and one unary chain (S-VP), and NP-PRP-I and NP-NN-fish are not unary chains in our system. Each stack is just a list of numbers but is visualized with spans here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Word spans are modeled by differences in LSTM output. Here the span 3 eating fish 5 is represented by the vector differences (f5 − f3) and (b3 − b5). The forward difference corresponds to LSTM-Minus (Wang and Chang, 2016).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Reachable brackets (w.r.t. gold tree in Fig. 1) for c = 10, [0, 1, 2, 4], {0NP1}} which mistakenly combines "like eating". Trapezoids indicate stack spans (the top one in red), and solid triangles denote reachable brackets, with left(c) in blue and right(c) in cyan. The next reachable bracket, next(c) = 1VP5, is in bold. Brackets 3VP5 and 3S5 (in dotted triangle) cross the top span (thus unreachable), and 0NP1 is already recognized (thus not in reach(c) either).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The last three examples are off the gold path with strike out indicating structural or label mistakes. Trapezoids denote stack spans (top one in red) and the blue triangle denotes the next reachable bracket next(c) which is 1VP5 in all cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The optimal tree t * (c) adds all reachable brackets and nothing else. Note that reach(c) and t are disjoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>6</head><label>6</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Features used for the parser. No label or tree-structure features are required.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Hyperparameters. † French only.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Comparison of performance on PTB development set using different oracle training approaches.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of performance of different parsers on 

PTB test set.  †Neural. *Greedy.  ‡ External embeddings. 

Parser 
LR 
LP 
F 1 
Björkelund et al. (2014)  * , ‡ 
82.53 
Durrett and Klein (2015)  ‡ 
81.25 
Coavoux and Crabbé (2016) 
80.56 
static oracle 
83.50 82.87 83.18 
dynamic/exploration 
81.90 84.77 83.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Results on French Treebank.  *  reranking,  ‡ external. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the three anonymous reviewers for com-ments, Kai Zhao, Lemao Liu, Yoav Goldberg, and Slav Petrov for suggestions, Juneki Hong for proof-reading, and Maximin Coavoux for sharing their manuscript. This project was supported in part by NSF IIS-1656051, DARPA FA8750-13-2-0041 (DEFT), and a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building a treebank for french</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Abeillé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Toussenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="165" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally normalized transition-based neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Presta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL<address><addrLine>Slav Petrov, and Michael Collins</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improved transition-based parsing by modeling characters instead of words with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.00657</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training with exploration improves a greedy stack-lstm parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03793</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing the imswrocław-szeged-cis entry at the spmrl 2014 shared task: Reranking and morpho-syntax meet unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Björkelund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozlem</forename><surname>Cetinoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Falenska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richárd</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Seeker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Szántó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Computational Natural Language Learning</title>
		<meeting>the Twelfth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural greedy constituent parsing with dynamic oracles. Proceedings of ACL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoˆıtbenoˆıt</forename><surname>Crabbé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incremental parsing with minimal features using bi-directional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Neural crf parsing. Proceedings of ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A tabular method for dynamic oracles in transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Sartorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Satta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. of ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative training of a neural network statistical parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic programming for linear-time incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL: HLT</title>
		<meeting>the ACL: HLT<address><addrLine>Columbus, OH</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Easyfirst dependency parsing with hierarchical tree lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00375</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Simple and accurate dependency parsing using bidirectional LSTM feature representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>abs/1603.04351</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reranking and self-training for parser adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="337" to="344" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shift-reduce constituency parsing with dynamic programming and pos tag lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved inference for unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL</title>
		<meeting>HLTNAACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A best-first probabilistic shift-reduce parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="691" to="698" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introducing the spmrl 2014 shared task on parsing morphologically-rich languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</title>
		<meeting>the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian symbol-refined tree substitution grammars for syntactic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="194" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimal shift-reduce constituent parsing with structured perceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Le Quang Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miyao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transitionbased neural constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACLIJCNLP</title>
		<meeting>ACLIJCNLP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured training for neural network transition-based parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<idno>abs/1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast and accurate shift-reduce constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
