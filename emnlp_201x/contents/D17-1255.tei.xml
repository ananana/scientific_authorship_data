<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadi</forename><surname>Amiri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Boston Children&apos;s Hospital Informatics Program</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Boston Children&apos;s Hospital Informatics Program</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Boston Children&apos;s Hospital Informatics Program</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2401" to="2410"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel approach for training artificial neural networks. Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition). We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models. The core part of our algorithm is a cognitively-motivated sched-uler according to which training instances and their &quot;reviews&quot; are spaced over time. Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural models are known to be computa- tionally expensive to train even with fast hard- ware ( <ref type="bibr" target="#b18">Sutskever et al., 2014;</ref><ref type="bibr" target="#b21">Wu et al., 2016)</ref>. For example, it takes three weeks to train a deep neu- ral machine translation system on 100 Graphics Processing Units (GPUs) ( <ref type="bibr" target="#b21">Wu et al., 2016)</ref>. Fur- thermore, a large amount of data is usually required to train effective neural models ( <ref type="bibr" target="#b8">Goodfellow et al., 2016;</ref><ref type="bibr" target="#b9">Hirschberg and Manning, 2015)</ref>. <ref type="bibr" target="#b2">Bengio et al. (2009)</ref> and <ref type="bibr" target="#b13">Kumar et al. (2010)</ref> de- veloped training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier con- cepts and gradually proceeds with more difficult concepts. Since these approaches are motivated by a "starting small" strategy they are called curricu- lum or self-paced learning.</p><p>In this paper, we present a novel training paradigm which is inspired by the broad evidence in psychology that shows human ability to retain information improves with repeated exposure and exponentially decays with delay since last expo- sure ( <ref type="bibr" target="#b3">Cepeda et al., 2006;</ref><ref type="bibr" target="#b0">Averell and Heathcote, 2011</ref>). Spaced repetition was presented in psychol- ogy <ref type="bibr" target="#b5">(Dempster, 1989)</ref> and forms the building block of many educational devices, including flashcards, in which small pieces of information are repeatedly presented to a learner on a schedule determined by a spaced repetition algorithm. Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materi- als <ref type="bibr" target="#b5">(Dempster, 1989;</ref><ref type="bibr" target="#b15">Novikoff et al., 2012)</ref>.</p><p>We investigate the analogy between training neu- ral models and findings in psychology about human memory model and develop a spaced repetition al- gorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models. The core part of our algorithm is a scheduler that ensures a given neural network spends more time working on difficult training instances and less time on easier ones. Our scheduler is inspired by fac- tors that affect human memory retention, namely, difficulty of learning materials, delay since their last review, and strength of memory. The scheduler uses these factors to lengthen or shorten review intervals with respect to individual learners and training instances. We evaluate schedulers based on their scheduling accuracy, i.e., accuracy in es- timating network memory retention with respect to previously-seen instances, as well as their effect on the efficiency and effectiveness of downstream neural networks. <ref type="bibr">2</ref> The contributions of this paper are: (1) we show that memory retention in neural networks is af- fected by the same (known) factors that affect mem- ory retention in humans, (2) we present a novel training paradigm for neural networks based on spaced repetition, and (3) our approach can be ap- plied without modification to any neural network.</p><p>Our best RbF algorithm uses 34-50% of train- ing data per epoch while producing similar results to state-of-the-art systems on three tasks, namely sentiment classification, image categorization, and arithmetic addition. 3 It also runs 2.9-4.8 times faster than standard training, and outperforms com- peting state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Neural and Brain Memory Models</head><p>Research in psychology describes the following memory model for human learning: the probability that a human recalls a previously-seen item (e.g., the Korean translation of a given English word) de- pends on the difficulty of the item, delay since last review of the item, and the strength of the human memory. The relation between these indicators and memory retention has the following functional form ( <ref type="bibr" target="#b16">Reddy et al., 2016;</ref><ref type="bibr" target="#b6">Ebbinghaus, 1913)</ref>:</p><formula xml:id="formula_0">Pr(recall) = exp(− dif f iculty × delay strength ). (1)</formula><p>An accurate memory model enables estimating the time by which an item might be forgotten by a learner so that a review can be scheduled for the learner before that time.</p><p>We investigate the analogy between the above memory model and memory model of artificial neural networks. Our intuition is that if the proba- bility that a network recalls an item (e.g., correctly predicts its category) depends on the same factors (difficulty of the item, delay since last review of the item, or strength of the network), then we can develop spaced repetition algorithms to efficiently and effectively train neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recall Indicators</head><p>We design a set of preliminarily experiments to directly evaluate the effect of the aforementioned factors (recall indicators) on memory retention in neural networks. For this purpose, we use a set of training instances that are partially made avail- able to the network during training. This scheme <ref type="bibr">3</ref> We obtained similar results on QA tasks ( <ref type="bibr" target="#b19">Weston et al., 2016</ref>) but they are excluded due to space limit.  <ref type="figure">Figure 1</ref>: Effect of recall indicators on network retention. Training data is uniformly at random divided into three disjoint sets A, B, and C that respectively contain 80%, 10%, and 10% of the data. Network retention is computed against set B instances at recall point.</p><formula xml:id="formula_1">epoch = 0 {A ∪ B } {A ∪ C } {A ∪ C }</formula><p>will allow us to intrinsically examine the effect of recall indicators on memory retention in isolation from external effects such as size of training data, number of training epochs, etc.</p><p>We first define the following concepts to ease understanding the experiments (see <ref type="figure">Figure 1</ref>):</p><p>• First and Last review points (fRev and lRev) of a training instance are the first and last epochs in which the instance is used to train the network respectively,</p><p>• Recall point (Rec) is the epoch in which net- work retention is computed against some train- ing instances; network retention is the prob- ability that a neural network recalls (i.e. cor- rectly classifies) a previously-seen training in- stance, and</p><p>• Delay since last review of a training instance is the difference between the recall point and the last review point of the training instance.</p><p>Given training data and a neural network, we uni- formly at random divide the data into three disjoint sets: a base set A, a review set B, and a replace- ment set C that respectively contain 80%, 10%, and 10% of the data. As depicted in <ref type="figure">Figure 1</ref>, instances of A are used for training at every epoch, while those in B and C are partially used for training. The network initially starts to train with {A ∪ C} instances. Then, starting from the first review point, we inject the review set B and remove C, training with {A ∪ B} instances at every epoch until the last review point. The network will then continue training with {A ∪ C} instances until the recall point. At this point, network retention is computed against set B instances, with delay defined as the number of epochs since last review point. The intu- ition behind using review and replacement sets, B and C respectively, is to avoid external effects (e.g.  size of data or network generalization and learning capability) for our intrinsic evaluation purpose.</p><p>To conduct these experiments, we identify dif- ferent neural models designed for different tasks. <ref type="bibr">4</ref> For each network, we fix the recall point to either the epoch in which the network is fully trained (i.e., obtains its best performance based on standard or "rote" training in which all instances are used for training at every iteration), or partially trained (i.e., obtains half of its best performance based on rote training). We report average results across these networks for each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Delay since Last Review</head><p>As aforementioned, delay since last review of a training instance is the difference between the re- call point (Rec) and the last review point (lRev) of the training instance. We evaluate the effect of delay on network retention (against set B instances) by keeping the recall point fixed while moving the sliding window in <ref type="figure">Figure 1</ref>. Figures 2(a) and 2(b) show average network retention across networks for the fully and partially trained recall points re- spectively. The results show an inverse relationship between network retention and delay since last re- view in neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Item Difficulty</head><p>We define difficulty of training instances by the loss values generated by a network for the instances. <ref type="figure" target="#fig_1">Figure 2</ref>(c) shows the difficulty of set B instances at the last review point against average network reten- tion on these instances at recall point. We normal- ize loss values to unit vectors (to make them com- <ref type="bibr">4</ref> See section 4, we use Addition and CIFAR10 datasets and their corresponding neural networks for these experiments. parable across networks) and then average them across networks for both fully and partially trained recall points. As the results show, network reten- tion decreases as item difficulty increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Network Strength</head><p>We define strength of a network by its performance on validation data. To understand the effect of network strength on its retention, we use the same experimental setup as before except that we keep the delay (difference between recall point and last review point) fixed while gradually increasing the recall point; this will make the networks stronger by training them for more epochs. Then, at every recall point, we record network retention on set B instances and network accuracy on validation data. Average results across networks for two sets of 10 consecutive recall points (before fully and partially trained recall points) are shown in <ref type="figure" target="#fig_1">Figure 2</ref>(d). As the results show, network retention increases as memory strength increases.</p><p>The above experiments show that memory re- tention in neural networks is affected by the same factors that affect memory retention in humans: (a) neural networks forget training examples after a certain period of intervening training data (b): the period of recall is shorter for more difficult exam- ples, and (c): recall improves as networks achieve better overall performance. We conclude that de- lay since last review, item difficulty (loss values of training instances), and memory strength (network performance on validation data) are key indicators that affect network retention and propose to design spaced repetition algorithms that take such indica- tors into account in training neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1. Leitner System</head><p>Input: H : training data, V : validation data, k : num- ber of iterations, n : number of queues Output: trained model</p><formula xml:id="formula_2">0 Q = [q0, q1, . . . , qn−1] 1 q0 = [H], qi = [] for i in [1, n − 1] 2 For epoch = 1 to k: 3 current batch = [] 4</formula><p>For i = 0 to n − 1: 5</p><p>If epoch%2 i == 0:</p><formula xml:id="formula_3">6 current batch = current batch + qi 7</formula><p>End For 8</p><p>pmos, dmos, model = train(current batch, V) 9</p><p>update queue(Q, pmos, dmos) 10 End for 11 return model q0 epochs = {1, 2, 3, 4, 5, . . .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spaced Repetition</head><p>We present two spaced repetition-based algorithms: a modified version of the Leitner system developed in ( <ref type="bibr" target="#b16">Reddy et al., 2016</ref>) and our Repeat before For- getting (RbF) model respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Leitner System</head><p>Suppose we have n queues {q 0 , q 1 , . . . , q n−1 }. The Leitner system initially places all training instances in the first queue, q 0 . As Algorithm 1 shows, at each training iteration, the Leitner scheduler chooses some queues to train a downstream neu- ral network. Only instances in the selected queues will be used for training the network. During train- ing, if an instance from q i is recalled (e.g. correctly classified) by the network, the instance will be "pro- moted" to q i+1 , otherwise it will be "demoted" to the first queue, q 0 . <ref type="bibr">5</ref> The Leitner scheduler reviews instances of q i at every 2 i iterations. Therefore, instance in lower queues (difficult/forgotten instances) are re- viewed more frequently than those in higher queues (easy/recalled ones). <ref type="figure" target="#fig_3">Figure 3</ref> (bottom) provides examples of queues and their processing epochs. Note that the overhead imposed on training by the Leitner system is O(|current batch|) at every epoch for moving instances between queues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">RbF Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">RbF Memory Models</head><p>The challenge in developing memory models is to estimate the time by which a training instance should be reviewed before it is forgotten by the network. Accurate estimation of the review time leads to efficient and effective training. However, a heuristic scheduler such as Leitner system is sub- optimal as its hard review schedules (i.e. only 2 i - iteration delays) may lead to early or late reviews.</p><p>We develop flexible schedulers that take recall in- dicators into account in the scheduling process. Our schedulers lengthen or shorten inter-repetition in- tervals with respect to individual training instances. In particular, we propose using density kernel func- tions to estimate the latest epoch in which a given training instance can be recalled. We aim to investi- gate how much improvement (in terms of efficiency and effectiveness) can be achieved using more flex- ible schedulers that utilize the recall indicators.</p><p>We propose considering density kernels as sched- ulers that favor (i.e., more confidently delay) less difficult training instances in stronger networks. As a kernel we can use any non-increasing function of the following quantity:</p><formula xml:id="formula_4">x i = d i × t i s e ,<label>(2)</label></formula><p>where d i indicates the loss of network for a training instance h i ∈ H, t i indicates the number of epochs to next review of h i , and s e indicates the perfor- mance of network-on validation data-at epoch e. We investigate the Gaussian, Laplace, Linear, Cosine, Quadratic, and Secant kernels as described below respectively:  where τ is a learning parameter. <ref type="figure" target="#fig_4">Figure 4</ref> depicts these kernels with τ = 1. As we will discuss in the next section, we use these kernels to optimize delay with respect to item difficulty and network strength for each training instance.</p><formula xml:id="formula_5">f gau (x, τ ) = exp(−τ x 2 ),<label>(3)</label></formula><formula xml:id="formula_6">f lap (x, τ ) = exp(−τ x),<label>(4)</label></formula><formula xml:id="formula_7">f lin (x, τ ) = 1 − τ x x &lt; 1 τ 0 otherwise ,<label>(5)</label></formula><formula xml:id="formula_8">f cos (x, τ ) = 1 2 cos(τ πx) + 1 x &lt; 1 τ 0 otherwise ,<label>(6)</label></formula><formula xml:id="formula_9">f qua (x, τ ) = 1 − τ x 2 x 2 &lt; 1 τ 0 otherwise ,<label>(7)</label></formula><formula xml:id="formula_10">f sec (x, τ ) = 2 exp(−τ x 2 ) + exp(τ x 2 ) ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">RbF Algorithm</head><p>Our Repeat before Forgetting (RbF) model is a spaced repetition algorithm that takes into account the previously validated recall indicators to train neural networks, see Algorithm 2. RbF divides training instances into current and delayed batches based on their delay values at each iteration. In- stances in the current batch are those that RbF is less confident about their recall and therefore are reviewed (used to re-train the network) at current iteration. On the other hand, instances in the de- layed batch are those that are likely to be recalled by the network in the future and therefore are not re- viewed at current epoch. At each iteration, the RbF scheduler estimates the optimum delay (number of epochs to next review) for each training instance in the current batch. RbF makes such item-specific estimations as follows: Given the difficulty of a training instance d i , the memory strength of the neural network at epoch e, s e , and an RbF memory model f (see section 3.2.1), RbF scheduler estimates the maximum delayˆtdelayˆ delayˆt i for the instance such that it can be recalled with a con- fidence greater than the given threshold η ∈ (0, 1) at time e + ˆ t i . As described before, d i and s e can be represented by the current loss of the network for the instance and the current performance of the network on validation data respectively. Therefore, the maximum delay between the current (epoch e) and next reviews of the instance can be estimated as follows:</p><formula xml:id="formula_11">ˆ t i = arg min t i f (x i , ˆ τ ) − η 2 ,<label>(9)</label></formula><formula xml:id="formula_12">s.t 1 ≤ t i ≤ k − e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2. RbF Training Model</head><p>Input: H : training data, V : validation data, k : num- ber of iterations, f : RbF kernel, η: recall confidence Output: trained model 0 ti = 1 for hi ∈ H 1 For epoch = 1 to k:</p><formula xml:id="formula_13">2 current batch = {hi : ti &lt;= 1} 3 delayed batch = {hi : ti &gt; 1} 4 s epoch , model = train(current batch, V) 5 ˆ τ = arg minτ f (xi, τ ) − ai 2 ∀hi ∈ V, ai ≥ η 6 ˆ ti = arg mint i f (xi, ˆ τ )−η 2 ∀hi ∈ current batch 7</formula><p>ti = ti − 1 ∀hi ∈ delayed bach 8 End for 9 return model <ref type="figure">Figure 5</ref>: RbF training model. The train(.) func- tion at line 5 trains the network for one epoch using instances in the current batch. Note that at each iteration epoch, x i is computed using Equation <ref type="formula" target="#formula_4">(2)</ref> and strength of the current model, s epoch .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>wherê</head><p>τ is the optimum value for the learning pa- rameter obtained from validation data, see Equa- tion (10). In principle, reviewing instances could be delayed for any number of epochs; in practice however, delay is bounded both below and above (e.g., by queues in the Leitner system). Thus, we assume that, at each epoch e, instances could be delayed for at least one iteration and at most k − e iterations where k is the total number of training epochs. We also note that t i is a lower bound of the maximum delay as s e is expected to increase and d i is expected to decrease as the network trains in next iterations.</p><p>Algorithm 2 shows the outline of the proposed RbF model. We estimate the optimum value of τ (line 5 of Algorithm 2) for RbF memory models us- ing validation data. In particular, RbF uses the loss values of validation instances and strength of the network obtained at the previous epoch to estimate network retention for validation instances at the current epoch (therefore t i = 1 for every validation instance). The parameter τ for each memory model is computed as follows:</p><formula xml:id="formula_14">ˆ τ = arg min τ f (x j , τ ) − a j 2 , ∀h j ∈ V, a j ≥ η,<label>(10)</label></formula><p>where a j ∈ (0, 1) is the current accuracy of the model for the validation instance h j . RbF then predicts the delay for current batch instances and reduces the delay for those in the delayed batch by one epoch. The overhead of RbF is O(|H|) to compute delays and O(|V|) to computê τ . Note that (9) and (10) have closed form solutions.   <ref type="table">Table 1</ref>: Datasets, models, and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Table 1 describes the tasks, datasets, and models that we consider in our experiments. It also reports the training epochs for which the models produce their best performance on validation data (based on rote training). We note that the Addition dataset is randomly generated and contains numbers with at most 4 digits. <ref type="bibr">6</ref> We consider three schedulers as baselines: a slightly modified version of the Leitner scheduler (Lit) developed in <ref type="bibr" target="#b16">Reddy et al. (2016)</ref> for human learners (see Footnote 5), curriculum learning (CL) in which training instances are scheduled with re- spect to their easiness ( <ref type="bibr" target="#b11">Jiang et al., 2015)</ref>, and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch. For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best perfor- mance of this scheduler across all datasets.</p><p>Curriculum learning starts training with easy instances and gradually introduces more com- plex instances for training. Since easiness infor- mation is not readily available in most datasets, previous approaches have used heuristic tech- niques ( <ref type="bibr" target="#b17">Spitkovsky et al., 2010;</ref><ref type="bibr" target="#b1">Basu and Christensen, 2013</ref>) or optimization algorithms ( <ref type="bibr" target="#b11">Jiang et al., 2015</ref><ref type="bibr" target="#b10">Jiang et al., , 2014</ref>) to quantify easiness of training instances. These approaches consider an instance as easy if its loss is smaller than a threshold (λ). We adopt this technique as follows: at each itera- tion e, we divide the entire training data into easy and hard sets using iteration-specific λ e and the loss values of instances, obtained from the current partially-trained network. All easy instances in con- junction with α e ∈ [0, 1] fraction of easiest hard instances (those with smallest loss values greater than λ e ) are used for training at iteration e. We set  <ref type="figure">Figure 6</ref>: Accuracy of schedulers in predicting network retention. For these experiments recall confidence is set to its default value, η = 0.5.</p><p>each λ e to the average loss of training instances that are correctly classified by the current partially- trained network. Furthermore, at each iteration e, we set α e = e/k to gradually introduce complex instances at every new iteration. <ref type="bibr">7</ref> Note that we treat all instances as easy at e = 0.</p><p>Performance values reported in experiments are averaged over 10 runs of systems and the confi- dence parameter η is always set to 0.5 unless other- wise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation of Memory Models</head><p>In these experiments, we evaluate memory sched- ulers with respect to their accuracy in predicting network retention for delayed instances. Since cur- riculum learning does not estimate delay for train- ing instances, we only consider Leitner and RbF schedulers in these experiments.</p><p>For this evaluation, if a scheduler predicts a delay t for a training instance h at epoch e, we evaluate network retention with respect to h at epoch e + t. If the network recalls (correctly classifies) the in- stance at epoch e + t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction. We use this binary outcome to evaluate the accuracy of each sched- uler. Note that the performance of schedulers on instances that have not been delayed is not a ma- jor concern. Although failing to delay an item inversely affects efficiency, it makes the network stronger by providing more instances to train from. Therefore, we consider a good scheduler as the one that accurately delays more items. <ref type="figure">Figure 6</ref> depicts the average accuracy of sched- ulers in predicting networks' retention versus the average fraction of training instances that they de- layed per epoch. As the results show, all schedulers delay substantial amount of instances per epoch. In particular, Cos and Qua outperform Lit in both pre- dicting network retention and delaying items, de- laying around 50% of training instances per epoch. This is while Gau and Sec show comparable ac- curacy to Lit but delay more instances. On the other hand, Lap, which has been found effective in Psychology, and Lin are less accurate in predicting network retention. This is because of the trade- off between delaying more instances and creating stronger networks. Since these schedulers are more flexible in delaying greater amount of instances, they might not provide networks with enough data to fully train. <ref type="figure" target="#fig_6">Figure 7</ref> shows the performance of RbF sched- ulers with respect to the recall confidence parame- ter η, see Equation (9). As the results show, sched- ulers have poor performance with smaller values of η. This is because smaller values of η make sched- ulers very flexible in delaying instances. However, the performance of schedulers are not dramatically low even with very small ηs. Our further analyses on the delay patterns show that although a smaller η leads to more delayed instances, the delays are significantly shorter. Therefore, most delayed in- stances will be "reviewed" shortly in next epochs. These bulk reviews make the network stronger and help it to recall most delayed instance in future iterations.</p><p>On the other hand, greater ηs lead to more ac- curate schedulers at the cost of using more train- ing data. In fact, we found that larger ηs do not delay most training instances in the first few itera- tions. However, once the network obtains a reason- ably high performance, schedulers start delaying instances for longer durations. We will further study this effect in the next section.  improvement over rote training.</p><note type="other">0.866 0.44 3.09 2.33 Cos η = 0.9 0.880 0.76 2.36 1.42 Rote 0.887 1.00 1.00 1.00</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiency and Effectiveness</head><p>We compare RbF against Leitner and curriculum learning in terms of efficiency of training and effec- tiveness of trained models. We define effectiveness as the accuracy of a trained network on balanced test data, and efficiency as (a): fraction of instances used for training per epoch, and (b): required time for training the networks. For RbF schedulers, we set η to 0.5 and consider the best performing kernel Cosine with η = 0.9 based on results in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>The results in <ref type="table" target="#tab_5">Table 2</ref> show that all training paradigms have comparable effectiveness (Accu- racy) to that of rote training (Rote). Our RbF sched- ulers use less data per epoch (34-50% of data) and run considerably faster than Rote (2.90-4.78 times faster for η = 0.5). The results also show that Lit is slightly less accurate but runs 2.87 time faster than Rote; note that, as a scheduler, Lit is less accurate than RbF models, see <ref type="bibr">Figures 6 and 7.</ref> In addition, CL leads to comparable performance to RbF but is considerably slower than other sched- ulers. This is because this scheduler has to identify easier instances and sort the harder ones to sample training data at each iteration. Overall, the perfor- mance of Lit, CL, Cos η = .5 and Cos η = .9 are only 2.76, 1.90, 1.88, and 0.67 absolute values lower than that of Rote respectively. Considering the achieved efficiency, these differences are negli- gible (see the overall gain in <ref type="table" target="#tab_5">Table 2</ref>). <ref type="figure" target="#fig_7">Figure 8</ref> reports detailed efficiency and effective- ness results across datasets and networks. For clear illustration, we report accuracy at iterations 2 i ∀i in which Lit is trained on the entire data, and consider Cos η = .5 as RbF scheduler. In terms of efficiency (first row of <ref type="figure" target="#fig_7">Figure 8</ref>), CL starts with (small set of)  easier instances and gradually increases the amount of training data by adding slightly harder instances into its training set. On the other hand, Lit and RbF start big and gradually delay reviewing (easy) instances that the networks have learned. The dif- ference between these two training paradigms is apparent in <ref type="figure" target="#fig_7">Figures 8(a)</ref>-8(c).</p><p>The results also show that the efficiency of a training paradigm depends on the initial effective- ness of the downstream neural network. For CL to be efficient, the neural network need to initially have low performance (accuracy) so that the sched- uler works on smaller set of easy instances. For example, in case of Addition, Figures 8(b) and 8(e), the initial network accuracy is only 35%, therefore most instances are expected to be initially treated as hard instances and don't be used for training. On the other hand, CL shows a considerably lower efficiency for networks with slightly high initial accuracy, e.g. in case of IMDb or CIFAR10 where the initial network accuracy is above 56%, see In contrast to CL, Lit and RbF are more efficient when the network has a relatively higher initial per- formance. A higher initial performance helps the schedulers to more confidently delay "reviewing" most instances and therefore train with a much smaller set of instances. For example, since the initial network accuracy in IMDb or CIFAR10 is above 56%, Lit and RbF are considerably more efficient from the beginning of the training pro- cess. However, in case of low initial performance, Lit and RbF tend to avoid delaying instances at lower iterations which leads to poor efficiency at the beginning. This is the case for the Addition dataset in which instances are gradually delayed by these two schedulers even at epoch 8 when the performance of the network reaches above 65%, see Figures 8(e) and 8(b). However, Lit gains its true efficiency after iteration 12, see <ref type="figure" target="#fig_7">Figure 8</ref>(b), while RbF still gradually improves the efficiency. This might be because of the lower bound delays that RbF estimates, see Equation (9). Furthermore, the effectiveness results in <ref type="figure" target="#fig_7">Figure 8</ref> (bottom) show that all schedulers produce compa- rable accuracy to the Rote scheduler throughout the training process, not just at specific iterations. This indicates that these training paradigms can much faster achieve the same generalizability as standard training, see Figures 8(b) and 8(e).  <ref type="figure">Figure 9</ref>: Robustness against overtraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness against Overtraining</head><p>We investigate the effect of spaced repetition on overtraining. The optimal number of training epochs required to train fastText on the IMDb dataset is 8 epochs (see <ref type="table">Table 1</ref>). In this experiment, we run fastText on IMDb for greater number of iterations to investigate the robustness of differ- ent schedulers against overtraining. The results in <ref type="figure">Figure 9</ref> show that Lit and RbF (Cos η = 0.5) are more robust against overtraining. In fact, the perfor- mance of Lit and RbF further improve at epoch 16 while CL and Rote overfit at epoch 16 (note that CL and Rote also require considerably more amount of time to reach to higher iterations). We attribute the robustness of Lit and RbF to the scheduling mechanism which helps the networks to avoid re- training with easy instances. On the other hand, overtraining affects Lit and RbF at higher training iterations, compare performance of each scheduler at epochs 8 and 32. This might be because these training paradigms overfit the network by paying too much training attention to very hard instances which might introduce noise to the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Ebbinghaus <ref type="bibr">(1913,</ref><ref type="bibr">2013)</ref>, and recently <ref type="bibr" target="#b14">Murre and Dros (2015)</ref>, studied the hypothesis of the expo- nential nature of forgetting, i.e. how information is lost over time when there is no attempt to re- tain it. Previous research identified three critical indicators that affect the probability of recall: re- peated exposure to learning materials, elapsed time since their last review <ref type="bibr" target="#b6">(Ebbinghaus, 1913;</ref><ref type="bibr" target="#b20">Wixted, 1990;</ref><ref type="bibr" target="#b5">Dempster, 1989)</ref>, and more recently item difficulty ( <ref type="bibr" target="#b16">Reddy et al., 2016)</ref>. We based our inves- tigation on these findings and validated that these indicators indeed affect memory retention in neural networks. We then developed training paradigms that utilize the above indicators to train networks. <ref type="bibr" target="#b2">Bengio et al. (2009)</ref> and <ref type="bibr" target="#b13">Kumar et al. (2010)</ref> also developed cognitively-motivated training paradigms which are inspired by the principle that learning can be more effective when training starts with easier concepts and gradually proceeds with more difficult ones. Our idea is motivated by the spaced repetition principle which indicates learn- ing improves with repeated exposure and decays with delay since last exposure <ref type="bibr" target="#b6">(Ebbinghaus, 1913;</ref><ref type="bibr" target="#b5">Dempster, 1989)</ref>. Based on this principle, we devel- oped schedulers that space the reviews of training instances over time for efficient and effective train- ing of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We developed a cognitively-motivated training paradigm (scheduler) that space instances over time for efficient and effective training of neural net- works. Our scheduler only uses a small fraction of training data per epoch but still effectively train neural networks. It achieves this by estimating the time (number of epochs) by which training could be delayed for each instance. Our work was in- spired by three recall indicators that affect memory retention in humans, namely difficulty of learning materials, delay since their last review, and mem- ory strength of the learner, which we validated in the context of neural networks.</p><p>There are several avenues for future work in- cluding the extent to which our RbF model and its kernels could be combined with curriculum learn- ing or Leitner system to either predict easiness of novel training instances to inform curriculum learn- ing or incorporate Leitner's queueing mechanism to the RbF model. Other directions include extend- ing RbF to dynamically learn the recall confidence parameter with respect to network behavior, or de- veloping more flexible delay functions with theo- retical analysis on their lower and upper bounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Delay since last review vs. average network retention (accuracy) on set B instances at recall point. Recall point is fixed and set to the epoch in which networks obtain their best performance based on rote training. (b) The same as (a) except that recall point is set to the epoch in which networks obtain half of their best performance based on rote training. (c) Item Difficulty (normalized loss at last review point) vs. average network retention at recall point on set B instances. (d) Network strength (network accuracy on validation data at recall point) vs. average network retention at recall point on set B instances. Length of sliding window is fixed throughout experiments and set to 5 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Leitner System. The train(.) function trains the network for one epoch using instances in the current batch, and the update queue(.) function promotes the recalled (correctly classified) instances, pmos, to the next queue and demotes the forgotten ones, dmos, to q 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: RbF kernel functions with τ = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Dataset</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of recall confidence η on the accuracy of different schedulers in predicting network retention (best seen in color.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Efficiency and Effectiveness of schedulers across three datasets and networks. RbF uses Cos η = .5 as kernel. CL starts with (small set of) easier instances and gradually incorporate slightly harder instances at each iteration. Lit and RbF start big and gradually delay reviewing easy instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Fig- ures 8(a) and 8(d), and 8(c) and 8(f) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>train, dev, test Network</head><label></label><figDesc></figDesc><table>Task 

IMDb 
20K, 5K, 25K 

MLP/fastext 
(Joulin et al., 
2017), 
best 
epoch=8 

sentiment 
analysis 

CIFAR10 45K, 5K, 10K 
CNN 
(Chan 
et al., 2015) 
best epoch=64 

image clas-
sification 

Addition 40K, 5K, 10K 
LSTM (Sutskever 
et al., 2014) 
best epoch=32 

arithmetic 
addition 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison of schedulers in terms of aver-
age network accuracy, average fraction of instances 
used for training per epoch (TIPE), and the extent 
to which a model runs faster than Rote training (X 
Times Faster). Gain column indicates the Accuracy 

T IP E 

</table></figure>

			<note place="foot" n="1"> Our code is available at scholar.harvard.edu/ hadi/RbF/</note>

			<note place="foot" n="2"> In this paper, we use the terms memory retention, recall, and learning interchangeably.</note>

			<note place="foot" n="5"> Note that in (Reddy et al., 2016) demoted instances are moved to qi−1. We observed significant improvement in Leitner system by moving such instances to q0 instead of qi−1.</note>

			<note place="foot" n="6"> https://github.com/fchollet/keras/ blob/master/examples/addition_rnn.py</note>

			<note place="foot" n="7"> k is the total number of iterations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Mitra Mohtarami for her constructive feedback during the development of this paper and anonymous reviewers for their thoughtful com-ments. This work was supported by National Insti-tutes of Health (NIH) grant R01GM114355 from the National Institute of General Medical Sciences (NIGMS). The content is solely the responsibil-ity of the authors and does not necessarily repre-sent the official views of the National Institutes of Health.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The form of the forgetting curve and the fate of memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Averell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Heathcote</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Psychology</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Teaching classification boundaries to humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janara</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="109" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed practice in verbal recall tasks: A review and quantitative synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Cepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Pashler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>John T Wixted</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Tsung-Han Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spacing effects and their implications for theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dempster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Educational Psychology Review</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="330" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Memory: A contribution to experimental psychology. 3. University Microfilms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ebbinghaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory: A contribution to experimental psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ebbinghaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of neurosciences</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Advances in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="issue">6245</biblScope>
			<biblScope unit="page" from="261" to="266" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2078" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-paced curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence, AAAI&apos;15</title>
		<meeting>the TwentyNinth AAAI Conference on Artificial Intelligence, AAAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2694" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr Bojanowski Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">427</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>M Pawan Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1189" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Replication and analysis of ebbinghaus&apos; forgetting curve</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jaap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joeri</forename><surname>Murre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">120644</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Education of a model student</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Timothy P Novikoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven H</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1868" to="1873" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unbounded human learning: Optimal scheduling for spaced repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1815" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From baby steps to leapfrog: How less is more in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Valentin I Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing the empirical course of forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John T Wixted</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">927</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
