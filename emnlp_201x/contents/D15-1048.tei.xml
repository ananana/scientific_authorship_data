<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">More Features Are Not Always Better: Evaluating Generalizing Models in Incident Type Classification of Tweets</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Schulz</surname></persName>
							<email>schulz.axel@gmx.net</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Guckelsberger</surname></persName>
							<email>c.guckelsberger@gold.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Schmidt</surname></persName>
							<email>benedikt.schmidt@tk.informatik.tu-darmstadt.de</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Computational Creativity Group Goldsmiths College, University of London United Kingdom</orgName>
								<orgName type="institution">Business Intelligence Marketing DB Fernverkehr AG</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Technische Universität Darmstadt</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">More Features Are Not Always Better: Evaluating Generalizing Models in Incident Type Classification of Tweets</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Social media represents a rich source of up-to-date information about events such as incidents. The sheer amount of available information makes machine learning approaches a necessity for further processing. This learning problem is often concerned with regionally restricted datasets such as data from only one city. Because social media data such as tweets varies considerably across different cities, the training of efficient models requires labeling data from each city of interest , which is costly and time consuming. In this study, we investigate which features are most suitable for training generalizable models, i.e., models that show good performance across different datasets. We re-implemented the most popular features from the state of the art in addition to other novel approaches, and evaluated them on data from ten different cities. We show that many sophisticated features are not necessarily valuable for training a generalized model and are outperformed by classic features such as plain word-n-grams and character-n-grams.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Incident information contained in social media has proven to frequently include information not cap- tured by standard emergency channels (e.g. 911 calls, bystander reports). Therefore, stakeholders like emergency management and city administra- tion can highly benefit from social media. Due to its unstructured and unfocused nature, automatic filtering of social media content is a necessity for further analysis. A standard approach for this fil- tering is automatic classification using a trained machine learning model ( <ref type="bibr" target="#b1">Agarwal et al., 2012;</ref><ref type="bibr" target="#b23">Schulz et al., 2013;</ref><ref type="bibr" target="#b25">Schulz et al., 2015b)</ref>.</p><p>A problem for the classification approach is that language, style and named entities used in social media highly vary across different regions. Con- sider the following two tweets as examples: "RT: @People 0noe friday afternoon in heavy traffic, car crash on I-90, right lane closed" and "Road blocked due to traffic collision on I-495". Both tweets comprise entities that might refer to the same thing with different wording, either on a se- mantically low ("accident" and "car collision") or more abstract level ("I90" and "I-495"). With simple syntactical text similarity approaches using standard bag of words features, it is not easily pos- sible to make use of this semantic similarity, even though it is highly valuable for classification.</p><p>These limitations impose constraints on the dataset, because tokens are likely to be related to the location where the text was created or con- tain location-or incident-sensitive topics. Models trained using spatially and temporally restricted data from one region are bound by the specific as- pects of language and style expressed in the train- ing data, thus, model reuse is not easily possible.</p><p>In this paper, we focus on the creation of gener- alized models. Such models avoid the use of fea- tures that -overfitting like -are only useful for a specific region. Generalized models are intended to work in different regions, even if training data originates only from one ore few regions. This can ensure high classification rates even in areas where only few training samples are available. Finally, in times of increasing growth of cities and the merg- ing with surrounding towns to large metropolitan areas, they allow to cope with the latent transitions in token use.</p><p>To create generalized models for incident type classification (and social media classification in general) the most important step is an appropri- ate feature generation. Therefore, in this paper we investigate the suitability of standard and novel features and different machine learning algorithms for the creation of generalized classification mod- els for incident type classification. We conduct intensive feature engineering and evaluation. For this purpose, we have collected and labeled 10 datasets with high regional variation. To the best of our knowledge, this is the first investigation of the challenges of heterogeneous datasets in this domain, and of the suitability of state of the art classification and feature extraction techniques.</p><p>In summary, our contributions are: 1) Investi- gation of features and feature groups for general- ized social media/incident type classification mod- els. 2) Identification of the best feature combina- tions and classifiers for a generalized model. For an evaluation (qualitative and inferential statistics) of ten tweet datasets with high regional variation we get an overall F-measure of &gt; 83%.</p><p>3) The evaluation shows that features extending a plain n- gram-based approach are not necessarily valuable for training a generalized model as these provide little improvement.</p><p>Following this introduction, we give an overview of related work in Section 2. In Sec- tion 3, we provide a description of our datasets followed by a comprehensive evaluation in Sec- tion 4. We close with our conclusion and future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A review of existing work on the classification of social media content shows which features, feature groups and algorithms are generally used (see <ref type="table" target="#tab_0">table 1</ref>). Furthermore, the number of classes and the dominating approaches unfold. We re- port the ratios of labeled tweets for the individual approaches; however, we omit performance mea- sures as these are directly related to the respective datasets used for evaluation.</p><p>Classifiers based on Support Vector Machines (SVM) or Naive Bayes (NB) clearly dominate in terms of performance for incident type classifi- cation. <ref type="bibr" target="#b21">(Sakaki and Okazaki, 2010;</ref><ref type="bibr" target="#b2">Carvalho et al., 2010;</ref><ref type="bibr" target="#b1">Agarwal et al., 2012;</ref><ref type="bibr" target="#b20">Robert Power, 2013;</ref><ref type="bibr" target="#b22">Schulz and Janssen, 2014</ref>) trained an SVM, whereas <ref type="bibr" target="#b1">(Agarwal et al., 2012;</ref><ref type="bibr" target="#b11">Imran et al., 2013;</ref><ref type="bibr" target="#b22">Schulz and Janssen, 2014</ref>) also eval- uated an NB classifier. In contrast to these works, (Wanichayapong et al., 2011) followed a dictionary-based approach using traffic-related keywords. ( <ref type="bibr" target="#b16">Li et al., 2012</ref>) do not provide any in- formation about the classifier used.</p><p>Feature groups are mostly based on word-n- grams, such as unigrams ( <ref type="bibr" target="#b2">Carvalho et al., 2010</ref>), bigrams <ref type="bibr" target="#b11">(Imran et al., 2013)</ref>, or the combination of unigrams and bigrams <ref type="bibr" target="#b20">(Robert Power, 2013;</ref><ref type="bibr" target="#b13">Karimi et al., 2013;</ref><ref type="bibr" target="#b1">Agarwal et al., 2012)</ref>. ( <ref type="bibr" target="#b22">Schulz and Janssen, 2014</ref>) combined unigrams, bigrams, and trigrams. Also, based on the words present in the text named entities such as locations, organi- zations, or persons were used by <ref type="bibr" target="#b1">(Agarwal et al., 2012;</ref><ref type="bibr" target="#b16">Li et al., 2012;</ref><ref type="bibr" target="#b22">Schulz and Janssen, 2014)</ref>.</p><p>Twitter-specific features were also used, includ- ing the number of hashtags, @-mentions or web- text features such as the presence of numbers or URLs ( <ref type="bibr" target="#b16">Li et al., 2012;</ref><ref type="bibr" target="#b1">Agarwal et al., 2012;</ref><ref type="bibr" target="#b20">Robert Power, 2013;</ref><ref type="bibr" target="#b13">Karimi et al., 2013;</ref><ref type="bibr" target="#b11">Imran et al., 2013;</ref><ref type="bibr" target="#b22">Schulz and Janssen, 2014)</ref>.</p><p>Keywords also play a crucial role in fea- ture design. ( <ref type="bibr" target="#b21">Sakaki and Okazaki, 2010</ref>) used earthquake-specific keywords, statistical features (the number of words in a tweet and the position of keywords), and word context features (the words before and after the earthquake-related keyword). ( <ref type="bibr" target="#b26">Wanichayapong et al., 2011</ref>) used traffic-related keywords in combination with location-related keywords. Furthermore, <ref type="bibr" target="#b16">Li et al. (2012)</ref> itera- tively refined a keyword-based search for retriev- ing a higher number of incident-related tweets.</p><p>Two approaches rely on more specific feature groups. The approach of ( <ref type="bibr" target="#b22">Schulz and Janssen, 2014</ref>) is the only one that uses TF-IDF scores. ( <ref type="bibr" target="#b11">Imran et al., 2013</ref>) use <ref type="bibr">Kipper et al.'s (2006)</ref> ex- tension of the Verbnet ontology for verbs.</p><p>The related approaches mostly use word-n- grams and a variety of Twitter-specific features. Datasets are spatially and temporally restricted and limited to a small number, complicating gen- eralizability.  <ref type="bibr" target="#b22">Schulz and Janssen, 2014)</ref> SVM, NB 4 2,000 x x x x x created in. For this purpose, we created 10 datasets with more than 20k labeled tweets to train and test models with respect to their generalization.</p><p>In the following, we describe how this data was collected, preprocessed, and which features were generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We focus on tweets as suitable example for un- structured textual information shared in social me- dia. The classification of incident-related tweets represents a challenge that is relevant for many cities. We use a complex four-class classification problem, where new tweets can be assigned to the classes "crash", "fire", "shooting", and a neutral class "not incident related". This goes beyond re- lated work with a focus on two-class classification. Our classes were identified as the most common incident types in Seattle using the Fire Calls data set (http://seattle.data.gov), an official incident in- formation source.</p><p>As ground truth data, we collected several city- specific datasets using the Twitter Search API. These datasets were collected in a 15 km ra- dius around the city centres of Boston (USA), Brisbane (AUS), Chicago (USA), Dublin (IRE), London (UK), Memphis (USA), New York City (USA), San Francisco (USA), Seattle (USA), Syd- ney (AUS).</p><p>We selected these cities because of their huge regional distance, which allows us to evaluate our approaches with respect not only to geographi- cal, but also to cultural variations. Also, for all cities, sufficiently many English tweets can be re- trieved. We chose 15 km as radius to collect a rep- resentative data sample even from cities with large metropolitan areas. Despite the limitations of the Twitter Search API with respect to the number of geotagged tweets, we assume that our sample is, although by definition incomplete, highly relevant to our experiments.</p><p>We collected all available Tweets during lim- ited time periods, resulting in three initial sets of tweets: 7.5M tweets collected from November, 2012 to February, 2013 for Memphis and Seattle (SET CITY 1); 2.5M tweets collected from Jan- uary, 2014 to March, 2014 for New York City, Chicago, and San Francisco (SET CITY 2); 5M tweets collected from July, 2014 to August, 2014 for Boston, Brisbane, Dublin, London, and Syd- ney (SET CITY 3).</p><p>For the manual labeling process, we had to se- lect a subset of our original tweet set which in- cluded our classes of interest for model training and testing. Generating subsets is required be- cause manual labeling of social media data is very expensive, especially if multiple annotators are involved. To generate subsets we used the ap- proach of ( <ref type="bibr" target="#b23">Schulz et al., 2013</ref>) of extracting mi- croposts using incident-related keywords. As a result, more than 200 keywords were identified for each class. Based on these incident-related keywords, we were able to accurately and effi- ciently filter the datasets. After applying keyword- filtering, we randomly selected 5.000 microposts for each city. Though one might assume that this pre-filtering leads to a biased dataset, ( <ref type="bibr" target="#b22">Schulz and Janssen, 2014)</ref> showed that keyword sampling does not influence the classification process as the performance of a keyword-based classifier is no- tably worse compared to supervised classifiers.</p><p>In the next step, we removed all redundant tweets as well as those with no textual con- tent from the resulting sets as a couple of tweets contain keywords that are part of hash- tags or @-mentions, but have no useful textual content. The tweets were then labeled manu- ally by five annotators using the CrowdFlower (http://www.crowdflower.com/) platform. We re- trieved the manual labels and selected those for   <ref type="table" target="#tab_0">Crash Fire Shooting  No  Boston  347  188  28  2257  Sydney  587  189  39  2208  Brisbane  497  164  12  1915  Chicago  129  81  4  1270  Dublin  131  33  21  2630  London  283  95  29  2475  Memphis  23  30  27  721  NYC  129  239  45  1446  SF  161  82  61  1176  Seattle  204  153  139  390</ref> which all coders agreed to at least 75%. In the case of disagreement, the tweets were removed. This resulted in ten datasets with regional diversity to be used for evaluation. <ref type="table" target="#tab_1">Table 2</ref> lists the class distributions for each dataset. The distributions vary considerably, al- lowing us to evaluate with typical city-specific samples. Also, the "crash" class seems to be the most prominent incident type, whereas "shoot- ings" are less frequent. One reason for this is that "shootings" do not occur as frequent as other inci- dents, whereas another less obvious reason might be that people tend to report more about specific incident types and that there is not necessarily a correlation between the real-world incidents and those mentioned in tweets. Although the data sets have been filtered based on keywords, the "no in- cident" class remains the largest class.</p><p>One of the key questions that motivates our work is to which extent the used words vary in each dataset as an effect of the spatial and cultural context. We thus analysed how similar all datasets are by calculating the intersection of tokens. We found that after preprocessing, between 14% and 23% tokens are shared between the datasets. We do not assume that every unique token is a city- specific token, but the large number of tweets in our evaluations gives a first indication that there is a diversity in the samples that either requires the training of several individual-or one generalizing model which is the focus of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing and Feature Generation</head><p>To use our datasets for feature generation, i.e., for deriving different feature groups that are used for training a classification model, we had to convert the texts into a structured representation by means of preprocessing. Following this, we extracted several features for training classification mod- els. To evaluate the best feature groups for inci- dent type classification, we re-implemented com- monly used feature extraction approaches from the state of the art. We further extended these feature groups by additional ones that seemed promising in this problem domain:</p><p>Preprocessing As a first step, the text was con- verted to Unicode to preserve non-Unicode char- acters. Specific URLs would not be useful for the classification process, therefore we replaced them with a common token "URL". We then removed stopwords and conducted tokenization. Every token was then analysed and non-alphanumeric characters were removed or replaced. Finally, we applied lemmatization to normalize all tokens. All preprocessing steps were performed by DKPro TC ( <ref type="bibr" target="#b3">de Castilho and Gurevych, 2014</ref>), a popular framework for text classification. After prepro- cessing, we generated several features (see <ref type="table" target="#tab_2">Table  3</ref>). In the following, we give a description of the different feature groups.</p><p>Baseline Feature Sets As the most simple ap- proach and as used in all related works, we repre- sented tweets as a set of words and also as a set of characters with varying lengths. As features, we used a vector with the frequency of each n-gram. Most importantly, we evaluated the powerset of all different combinations of n-grams. For instance, if a length of n = 2 was chosen, we evaluated the three combinations (n = 1), (n = 1, 2), (n = 2). Furthermore, as not all tokens are necessarily im- portant for the classification process, we evaluated several top-k selection strategies, i.e., taking only the k most frequent n-grams into account. For this, we tested k = 100, 1000, 5000 as well as the ap- proach using all n-grams. We treat these features as the baseline approach, and extend it by addi- tional features, e.g. similarity, sentiment scores, Twitter-specific features.</p><p>Sentiment Features Emoticons are widely used to express emotions in textual content. Various text classification approaches make use of these, e.g. for sentiment analysis <ref type="bibr" target="#b0">(Agarwal et al., 2011;</ref><ref type="bibr" target="#b6">Go et al., 2009</ref>). For incident type classification, they could also be useful as people link emotions with ongoing incidents, thus, we re-implemented three approaches for extracting sentiment features. </p><note type="other">: Overview of all feature groups implemented for comparison Feature Group Description Word-n-grams</note><p>Each tweet is represented as a powerset of word-n-grams of length n = 1 to n = 3. Char-n-grams Each tweet is represented as a powerset of char-n-grams of length n = 1 to n = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POS EMO</head><p>The Tweet NLP part-of-speech tagger ( <ref type="bibr" target="#b18">Owoputi et al., 2013</ref>) was used to identify emoticons. The ratio of emoticons to all tokens is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DICT EMO</head><p>An emoticon library that is based on the suggestions from <ref type="bibr" target="#b0">Agarwal et al. (Agarwal et al., 2011</ref>) was used comprising a set of 63 emoticons from Wikipedia. The number of positive and the number of negative emoticons in a tweet is calculated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AGG EMO</head><p>One single sentiment score based on the second approach by aggregating the number of positive and negative emoticons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NER</head><p>We used the Stanford Named Entity Recognizer ( <ref type="bibr" target="#b5">Finkel et al., 2005</ref>) and applied the three class model to count the number of location, organization, and person mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR CHAR</head><p>The number of characters in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR SENT</head><p>The number of sentences in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR TOKEN</head><p>The number of tokens in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUEST RT</head><p>The proportion of question marks and sentences in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXCLA RT</head><p>The proportion of exclamation marks and sentences in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR AT MN</head><p>The number of @-mentions in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR HASHTAG</head><p>The number of hashtags in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR URL</head><p>The number of URLs present in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR SLANG</head><p>The number of colloquial words (i.e., lol or ugh). Feature extraction is based on the Tweet NLP POS- tags ( <ref type="bibr" target="#b18">Owoputi et al., 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IS RT</head><p>A boolean to indicate whether a tweet is a retweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR CARD</head><p>In conjunction with the named entities present in tweets, people tend to refer to street names (e.g.,I-95) or the number of injured people (e.g.,2-people). Thus, we create a feature for the number of cardinal numbers present in a tweet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GREEDY ST</head><p>Similarity scores following Greedy String Tiling <ref type="bibr" target="#b27">(Wise, 1996)</ref> as a method to deal with shared sub- strings that do not appear in the same order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEVENST</head><p>The Levenshtein distance <ref type="bibr" target="#b15">(Levenshtein, 1966)</ref> as an edit-distance metrics, i.e., the minimum number of edit operations that transform one tweet into another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF IDF</head><p>As the baseline relies on plain frequency-based weighting, we calculate the traditional TF-IDF scores ( <ref type="bibr" target="#b17">Manning et al., 2009</ref>) for every tweet.</p><p>Named Entities: As shown in the state of the art, named entities, i.e. entities that have been assigned a name such as Seattle, are commonly used in tweets. Named entities might be valuable, as these are used frequently in incident-related tweets. Thus, we also incorporated Named Entity Recognition (NER) for feature extraction.</p><p>Stylistic Features: The style of a tweet could be an additional indicator for incident relatedness. For instance, a repetition of punctuations could point at a person that is expressing emotions re- sulting from an ongoing incident. Structured rep- resentation might indicate high quality.</p><p>Twitter-specific features As shown in related work, several Twitter-specific features seem to be valuable for incident type classification such as the number of @-mentions and hashtags.</p><p>Similarity Features The similarity of individual tweets might be helpful to identify common top- ics. We therefore implemented several similarity scores 1 . The rationale behind this is to embrace additional features that do not only take the raw frequencies of words into account, but also which words appear in which document.</p><p>To sum up, we re-implemented two approaches that will serve as a baseline, and 18 additional fea- ture groups to extend them. In the following sec- tion, we will evaluate the usefulness of these ap- proaches for training a generalizing model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>The goal of our evaluation is to determine which features were most useful for creating a generaliz- ing model. We first describe our method, includ- ing the feature sets, the classification algorithms used, and our sampling procedure. This is fol- lowed by a results section in which we report dif- ferences in performance by means of qualitative and inferential statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Method</head><p>The indicators for well-performing features in re- lated work allows us to perform a condensed evaluation, compared to similar studies such as ( <ref type="bibr" target="#b8">Hasanain et al., 2014</ref>).</p><p>Our approach comprises three steps: First, we evaluated the baseline approaches, i.e., word-and char-n-grams. Second, we combined each of the remaining features with the best performing base- line feature. Third, we again selected the best per- forming combinations and evaluated their power set. To evaluate the suitability of different features for training generalizing models, we picked one dataset from the 10 presented in Section 3.1 for training, and tested on the remaining 9 datasets. We did not evaluate different models on datasets from only one city, as we were interested in gen- eralizing models.</p><p>Selecting each city as training set resulted in 90 performance samples per model. The models were formed by combining the feature sets described in the previous section 3.2 or respectively, their com- binations, with an SVM and NB classifier. We de- cided for these classification algorithms since they were the most successful in related work. Another reason for the choice of NB is its good perfor- mance in text classification tasks, as demonstrated by <ref type="bibr" target="#b19">Rennie et al. (2003)</ref>. We relied on the Lib- Linear implementation of an SVM because it has been shown that for a large number of features and a small number of instances, a linear kernel is comparable to a non-linear one ( <ref type="bibr" target="#b10">Hsu et al., 2003)</ref>. As for SVMs parameter tuning is inevitable, we evaluated the best settings for the slack variable c whenever an SVM was used. For training and testing, we used the reference implementations in WEKA ( <ref type="bibr" target="#b7">Hall et al., 2009)</ref>.</p><p>We calculated the F1-Measure for assessing performance, because it is well established in text classification, cannot be manipulated by the classi- fication threshold parameter and allows to measure the overall performance of the approaches with an emphasis on the individual classes (Jakowicz and Shah, 2011). In Section 3.1, we demonstrated that the proportion of data representing individ- ual classes varies strongly. We therefore weighted the F1-measure by this ratio and report the micro- averaged results over all datasets F 1. Given our focus on training a generalizable model, we delib- erately did not focus on the performance variation in the individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In order to check whether our findings persist at least across the two learning algorithms, we did not aggregate the model performance samples but analyzed them for each algorithm separately. We therefore only have one independent variable, our feature groups, that affects the model perfor- mance. In order to keep p-value inflation low, we only compared the ten best performing models for each algorithm with respect to the F1-Measure. Note that even if the difference in performance be- tween these models appears small, there are thus many worse models that were not explicitly listed.</p><p>Our samples generally do not fulfill the assump- tions of normality and sphericity required by para- metric tests for comparing more than two groups. Under the violation of these assumptions, non- parametric tests have more power and are less prone to outliers <ref type="bibr" target="#b4">(Demsar, 2006</ref>). We therefore relied exclusively on the non-parametric tests sug- gested in literature: Friedman's test was used as non-parametric alternative to a repeated-measures one-way ANOVA, and Nemenyi's test 2 was used post-hoc as a replacement for Tukey's test.</p><p>In contrast to its parametric counterpart, Fried- man's test is based on a ranking of the models in- duced by the performance measure, and therefore only relies implicitly on the latter. Each model is ranked from best to worst, with mean ranks being <ref type="bibr">2</ref> We chose Nemenyi's test because it is widely accepted in the machine learning community. A discussion of alterna- tives can be found in <ref type="bibr" target="#b9">Herrera et al. (Herrera, 2008)</ref>.</p><p>Feature Group words(1000,1,2) words(1000,1,3) words(ALL,1,1) words(5000,1,1) words(100,1,1) words(100,1,2) words(100,1,3) words(5000,1,3) words <ref type="figure" target="#fig_2">(1000,1,1) words(5000,1,1</ref>  <ref type="table">Table 4</ref>: Average F1-Measure F 1 for the ten best performing baseline feature groups used in case of ties. The Friedman statistic is cal- culated by dividing the sum of squares of the mean ranks by the sum of squares error. For sufficiently many samples, the statistic follows a χ 2 distribu- tion with k − 1 degrees of freedom. The q statistic used in Nemenyi's test is similar to the one used by Tukey, but uses rank differences. It utilises the previous ranking from the Frieman test to calcu- late and relate the average ranks of two models, for each available pair. Two models are consid- ered significantly different, if their difference in mean ranks exceeds a critical value, which varies for different significance levels. For a detailed de- scription and examples of these tests, see <ref type="bibr" target="#b12">(Jakowicz and Shah, 2011</ref>). We illustrated the ranks and significant differ- ences between the feature groups by means of the critical difference (CD) diagram. Introduced by <ref type="bibr" target="#b4">Demsar (2006)</ref>, this diagram lists the feature groups ordered by their rank, where lower rank numbers indicate higher performance. Feature groups are connected with bars if they are not sig- nificantly different, given α = 0.05.</p><p>In the following, we will use shortcuts like words(1000,1,2) to denote the 1000 most frequent uni-and bigrams. The same applies for char-n- grams. Abbreviations can be found in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation using LibLinear Classifier</head><p>We first evaluated which of our 20 baseline fea- ture sets, as described in Section 3, lead to the best classification performance over different datasets. Notably, the ten best-performing approaches were all combinations of word-n-grams. <ref type="table">Table 4</ref> contains the average F-Measures for these ap- proaches. The Friedman test indicated strong sig- nificant differences between the performances of these groups (χ 2 r (9) = 112.467, p &lt; 0.001, α = 0.01). The subsequent Nemenyi test indicated strong significant pairwise differences between the performances of the models (α = 0.01), with p- values listed in <ref type="table" target="#tab_1">Table 2</ref> in the supplementary. <ref type="figure" target="#fig_2">Figure 1</ref> illustrates the differences by means of a CD diagram: the approaches of using simple un- igrams of the most frequent 5000 and all words provide the best results, i.e. they have the lowest rank. They are not significantly different from the 1000 most frequent word-uni and bigrams. Never- theless, they are significantly better than all other baseline approaches.</p><p>This also applies to the char-n-gram ap- proaches, that were not considered in this statisti-  <ref type="table" target="#tab_0">9  8  7  6  5  4  3  2  1</ref> words <ref type="formula">(</ref> cal comparison due to their inferior performance. It is important to note that the differences between the worst word-n-gram approaches and the best char-n-gram approaches could still be statistically non-significant.</p><p>The best performing baseline approach for LibLinear is using unigrams of the top 5000 words, i.e. words(5000,1,1), with F 1 = 82.87. We therefore picked this baseline feature group for the second part of our evaluation. We added each non-baseline feature individually to the selected baseline approach and compared the performances of these combinations and the non-extended base- line group. <ref type="table">Table 6</ref> lists the averaged F-Measure. When comparing the ten best-performing groups, the Friedman test showed strong significant dif- ferences between the performances of the models (χ 2 r (9) = 87.274, p &lt; 0.001, α = 0.01). The Nemenyi test partly showed strong significant dif- ferences between the performances of the models (for the corresponding p-values see <ref type="table" target="#tab_2">Table 3</ref> in the supplementary). They are illustrated in the CD di- agram in <ref type="figure" target="#fig_3">Figure 2</ref>. The tests indicate that adding NER and NR AT MT to the baseline approach pro- vides the best performances with F 1 = 83.32 and F 1 = 83.03 respectively.</p><p>Finally, we evaluated the power set of these feature groups, i.e. we compared the individual groups and their combination. <ref type="table" target="#tab_5">Table 5</ref> contains the corresponding averaged F-Measures. For the resulting performance samples, the Friedman test showed strong significant differences between the models (χ 2 r (3) = 72.014, p &lt; 0.001, α = 0.01). The Nemenyi test partly showed strong significant    <ref type="table">Table 4</ref> in the supplementary and illustrated in <ref type="figure" target="#fig_4">Fig- ure 3</ref>. The diagram shows that the combination of NER and NR AT MN with the words(5000,1,1) baseline outperforms all other models with respect to F1 (F 1 = 83.48), but does not differ sig- nificantly from the plain NER approach (F 1 = 83.32). This combination gives us the final and best feature set for training a generalizing model over our datasets. As can be seen, the plain n-gram approach (F 1 = 82.87) can be improved further by 0.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation using Naive Bayes Classifier</head><p>In this section, we repeat the previous steps for the NB classifier. As baseline feature sets, we first evaluated the word-n-gram and char-n-gram ap- proaches. The averaged F-Measures can be found in <ref type="table">Table 4</ref>. The Friedman test showed strong sig- nificant differences between the performances of the models (χ 2 r (9) = 110.293, p &lt; 0.001, α = 0.01). The Nemenyi test partly showed strong sig- nificant differences between the performances of the models (for the corresponding p-values see Ta- ble 1 in the supplementary). In contrast to the Li- bLinear classifier, using the 5000 most frequent combinations of two to five subsequent charac- ters, i.e. chars(5000,2,5) provide the best F1 score (F 1 = 80.48). Thus, char-n-grams outperform the word-n-gram approaches with respect to F1. The CD diagram in <ref type="figure" target="#fig_7">Figure 5</ref> shows that using either the 5000 most frequent char-n-grams with a length of two to five and two to four respec- tively, the 1000 most frequent word-n-grams with a length of one and one to two respectively, and the 1000 most frequent char-n-grams with a length of two to four do not differ significantly. However, using either the 5000 most frequent char-n-grams with a length of two to five and two to four respec- tively significantly outperform all other baseline approaches. As a subsequent step, we added each single feature to chars(5000,2,5) as the best base- line approach to find if these provide better per- formance for the NB classifier. <ref type="table">Table 6</ref> contains the corresponding averaged F-Measures. Though the Friedman test indicated strong significant dif- ferences between the performances of the mod- els (χ 2 r (9) = 22.209, p = 0.008, α = 0.01), the subsequent Nemenyi test did not indicate signifi- cant pairwise differences. We can therefore not re- peat the third step of our evaluation, but infer that for a NB classifier, the plain char-n-gram-based approach is sufficient for training a generalizing model for our dataset.</p><p>The results indicate that LibLinear provides a better avg. performance (F 1 = 83.32) when train- ing a generalizing model, compared to the NB classifier (F 1 = 80.48).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we compared the performance of different popular feature groups and classification algorithms for the task of training a generaliz- ing model for incident type classification. We carefully selected the most popular feature groups from related work, and separately evaluated them Feature Group words(5000,1,1) +DICT EMO +NER +NR CARD +NR AT MN +POS EMO +NR SLANG +EXCLA RT +QUEST RT +IS RT   <ref type="table">Table 6</ref>: Average F1-Measure F 1 for the ten best performing combinations of the best baseline and an additional feature  for the LibLinear and NB classification algorithms on ten spatially and temporally diverse datasets. The resulting F1-measure samples indicate that training a generalizing model, i.e., a model that is applicable on previously unseen incident-related data, is still a challenging task. We found that Li- bLinear provides a better averaged performance compared to the NB classifier. More surpris- ingly, additional feature groups that are commonly used in related work do not necessarily outperform a plain n-gram-based approach. This highlights the need for other novel approaches for training generalizing classification models. Especially in the domain of incident detection and emergency management, our findings are important because less time consuming techniques showed nearly the same performance as sophisticated ones.</p><p>There are two main topics for our future work. First, we will investigate the performance of mod- els generated with biased datasets on unfiltered datasets. This is relevant, if a technique like filter- ing is used to include more relevant class examples in a dataset than provided with an original sample -a necessary step to realize a labeled dataset for model learning of a rare-class task. Second, we will work on using novel features for the creation of generalized models. One example is the uti- lization of the Semantic Web to generate abstract features, utilizing a technique called Semantic Ab- straction ( <ref type="bibr" target="#b24">Schulz et al., 2015a</ref>). Semantic Ab- straction has shown to improve the generalization of tweet classification by deriving features from Linked Open Data and using location and tempo- ral mentions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CD diagram with the ranks of the ten best performing baseline feature groups for LibLinear. Feature groups are connected if they are not significantly different (α = 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CD diagram with the ranks of the ten best performing feature groups for LibLinear, comprising the baseline and the baseline with an additional feature. Feature groups are connected if they are not significantly different (α = 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CD diagram with the ranks of the best baseline feature groups, complemented with a combination of the best performing feature sets, for LibLinear. Feature groups are connected if they are not significantly different (α = 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CD diagram with the ranks of the ten best performing baseline feature groups for Naive Bayes. Feature groups are connected if they are not significantly different (α = 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ranks of NB baseline feature groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Overview of related approaches for incident type classification. (NEs = Named Entities)</head><label>1</label><figDesc></figDesc><table>Approach 
Classifier 
#Classes #Tweets N-Grams #NEs #URLs TF-IDF Twitter Other 
(Sakaki and Okazaki, 2010) 
SVM 
2 
597 
x 
Context 
(Carvalho et al., 2010) 
SVM 
2 
3,300 
x 
(Wanichayapong et al., 2011) 
Keyw. 
2 
1,249 
(Agarwal et al., 2012) 
NB, SVM 
2 
1,400 
x 
x 
x 
(Li et al., 2012) 
Undefined 
2 
Undef. 
x 
x 
(Robert Power, 2013) 
Keyw., SVM 
2 
794 
x 
x 
(Karimi et al., 2013) 
SVM 
6 
5,747 
x 
x 
(Imran et al., 2013) 
NB 
3 
1,233 
x 
x 
x 
Verbnet 
(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Class distributions for all datasets.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Average F1-Measure F 1 for the power set 
of best performing feature groups and LibLinear. 

differences (α = 0.001), with p-values listed in 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>5) +DICT EMO +QUEST RT +NER +NR AT MN +NR HASHTAG +POS EMO +NR SLANG +NR SENT +EXCLA RT</head><label></label><figDesc></figDesc><table>1 
82.87 
82.87 
83.32 83.06 
83.03 
82.87 
82.87 
82.88 
82.88 
82.88 

(a) LibLinear 

Feature Group chars(5000,2,F 1 
80.48 
80.48 
80.49 
80.55 80.51 
80.48 
80.48 
80.48 
80.48 
80.50 

(b) NaiveBayes 

</table></figure>

			<note place="foot" n="3"> Data Collection and Processing We are interested in generalizable models for different regions, user-generated content has been</note>

			<note place="foot" n="1"> The respective similarity scores have been calculated on the whole document corpus after preprocessing.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis of twitter data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilia</forename><surname>Vovsha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Languages in Social Media, LSM &apos;11</title>
		<meeting>the Workshop on Languages in Social Media, LSM &apos;11</meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Catching the long-tail: Extracting local news events from twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Vaithiyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Weblogs and Social Media, ICWSM 2012</title>
		<meeting>the Sixth International Conference on Weblogs and Social Media, ICWSM 2012</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time sensing of traffic information in twitter messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarmento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J F</forename><surname>Rossetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Artificial Transportation Systems and Simulation ATSS, ITSC&apos;10</title>
		<meeting>the 4th Workshop on Artificial Transportation Systems and Simulation ATSS, ITSC&apos;10</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="19" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A broad-coverage collection of portable nlp components for building shareable analysis pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Eckart De Castilho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings OIAF4HLT at COLING 2014</title>
		<meeting>OIAF4HLT at COLING 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janez</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Languages in Social Media, LSM &apos;11</title>
		<meeting>the Workshop on Languages in Social Media, LSM &apos;11</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="30" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The weka data mining software: an update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="18" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identification of answer-seeking questions in arabic microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Magdy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1839" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An Extension on Statistical Comparisons of Classifiers over Multiple Data Sets for all Pairwise Comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2677" to="2694" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A practical guide to support vector classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, National Taiwan University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Extracting information nuggets from disaster-Related messages in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shady</forename><surname>Elbassuoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Meier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="791" to="801" />
		</imprint>
	</monogr>
	<note>Karlsruher Institut fur Technologie (KIT</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evaluating Learning Algorithms. A Classification Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathalie</forename><surname>Jakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohak</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classifying microblogs for disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Australasian Document Computing Symposium, ADCS &apos;13</title>
		<meeting>the 18th Australasian Document Computing Symposium, ADCS &apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extending verbnet with novel verb classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Kipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neville</forename><surname>Ryant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings LREC&apos;06</title>
		<meeting>LREC&apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Binary Codes Capable of Correcting Deletions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vi Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Insertions and Reversals. Soviet Physics Doklady</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">707</biblScope>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tedas: A twitter-based event detection and analysis system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin</forename><forename type="middle">Hou</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Khadiwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin Chenchuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Data Engineering, ICDE&apos;12</title>
		<meeting>the 28th International Conference on Data Engineering, ICDE&apos;12</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1273" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">An Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schtze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tackling the poor assumptions of naive bayes text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML-03)</title>
		<editor>Tom Fawcett and Nina Mishra</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="616" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finding fires with twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David Ratcliffe Robert</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bella</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Language Technology Association Workshop</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Earthquake shakes twitter users: real-time event detection by social sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World Wide Web, WWW &apos;10</title>
		<meeting>the 19th international conference on World Wide Web, WWW &apos;10</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="851" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">What is good for one city may not be good for another one: Evaluating generalization for tweet classification based on semantic abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR, editor, Proceedings of the Fifth Workshop on Semantics for Smarter Cities a Workshop at the 13th International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1280</biblScope>
			<biblScope unit="page" from="53" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">I see a car crash: Real-time detection of small scale incidents in microblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="22" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic abstraction for generalization of tweet classification: An evaluation on incident-related tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Guckelsberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Janssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Semantic Web Journal: Special Issue on Smart Cities</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Small-scale incident detection based on microposts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Strufe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Conference on Hypertext and Social Media</title>
		<meeting>the 26th ACM Conference on Hypertext and Social Media</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Social-based traffic information extraction and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wanichayapong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pruthipunyaskul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pattaraatikom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaovalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on ITS Telecommunications, ITST&apos;11</title>
		<meeting>the 11th International Conference on ITS Telecommunications, ITST&apos;11</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Yap3: Improved detection of similarities in computer program and other texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Wise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCSEB: SIGCSE Bulletin</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
