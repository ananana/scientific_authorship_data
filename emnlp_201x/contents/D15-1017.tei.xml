<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monotone Submodularity in Opinion Summaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanth</forename><surname>Jayanth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IIT Bombay</orgName>
								<orgName type="department" key="dep2">IIT Bombay osjayaprakash</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayaprakash</forename><surname>Sundararaj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">IIT Bombay</orgName>
								<orgName type="department" key="dep2">IIT Bombay osjayaprakash</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monotone Submodularity in Opinion Summaries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
					<note>Pushpak Bhattacharyya IIT Bombay pb @cse.iitb.ac.in</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Opinion summarization is the task of producing the summary of a text, such that the summary also preserves the sentiment of the text. Opinion Summarization is thus a trade-off between summarization and sentiment analysis. The demand of compression may drop sentiment bearing sentences , and the demand of sentiment detection may bring in redundant sentences. We harness the power of submodularity to strike a balance between two conflicting requirements. We investigate an incipient class of submodular functions for the problem, and a partial enumeration based greedy algorithm that has performance guarantee of 63%. Our functions generate summaries such that there is good correlation between document sentiment and summary sentiment along with good ROUGE score, which outperforms the-state-of-the-art algorithms.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sentiment Analysis is often addressed as a classi- fication task, which aims at determining the sen- timent of a word, sentence, paragraph or a docu- ment as a whole into positive, negative or neutral classes ( <ref type="bibr" target="#b17">Pang et al., 2002</ref>). Summarization, on the other hand is the task of aggregating and represent- ing information content from a single document or multiple documents in a brief and fluent manner. Due to the explosive growth of data, fine grained sentiment analysis as well as summarization on the whole chunk of data can be a very time-consuming task. Sentiment Analysis also requires filtering of text portions as either objective (factual informa- tion) or subjective (expressing some sentiment or opinion) during pre-processing and then, classify- ing the subjective extracts as positive or negative.</p><p>Subjective extracts can also be provided to users as a summary of the sentiment-oriented content of the reviews in search engines. In this paper, we address the problem of generic extractive sum- marization of reviews, a task commonly known as Opinion Summarization ( <ref type="bibr" target="#b10">Liu, 2012)</ref>. The goals of opinion summarization are:</p><p>1. Present a short summary that conveys the essence as well as the sentiment of the review 2. Provide a short subjective extract to NLP pipeline for faster execution (e.g. sentiment analysis, review clustering etc.).</p><p>In this paper, we use movie reviews for opinion summarization task as they often have the follow- ing parts:</p><p>1. Plot -Description of the story, which is fac- tual in nature 2. Critique -Opinion about the movie, which is sentiment bearing</p><p>Clearly, opinion summary to be generated will have a trade-off between the two opposing parts -subjective critique and objective plot. Our goal is to strike a balance through linear combination of suitable submodular functions in our paper. Joint models of relevance and subjectivity have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In con- trast, conventional two-stage approach <ref type="bibr" target="#b16">Pang and Lee (2004)</ref>, which first generate candidate sub- jective sentences using min-cut and then selects top subjective sentences within budget to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For ex- ample, when we select subjective sentences first, the sentiment as well information content may be- come redundant for a particular aspect. On the other hand, when we extract sentences first, an im- portant subjective sentence may fail to be selected, simply because it is long. The two stage conflict in the sense that the demand of compression may drop sentiment bearing sentences, and the demand of sentiment detection may bring in redundant sen- tences. We then, use partial enumeration based greedy algorithm ( <ref type="bibr" target="#b3">Khuller et al., 1999</ref>), which gives performance guarantee of (1 − e −1 ) ≈ 0.632 <ref type="bibr" target="#b20">(Sviridenko, 2004</ref>). The performance guarantee reported is better than simple greedy algorithm, used by <ref type="bibr" target="#b7">Lin and Bilmes (2010)</ref> as their proof is erroneous ( <ref type="bibr" target="#b13">Morita et al., 2013)</ref>. Further, the same greedy algorithm, which was used again in <ref type="bibr" target="#b8">Lin and Bilmes (2011)</ref> gives only performance guarantee of 1 2 (1 − e 1 ) ≈ 0.316 ( <ref type="bibr" target="#b3">Khuller et al., 1999</ref>). The rest of the paper is as follows -in the next section, we look at previous work and establish further motivation for our work. Following that, we build the theory and formulate suitable objec- tives for opinion summarization task. In the final section, we present results based on implementa- tion and testing of the functions. Experimental re- sults show that the functions outperform the-state- of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Automatically generating opinion summaries from large review text corpora has long been stud- ied in both information retrieval and natural lan- guage processing.</p><p>In ( <ref type="bibr" target="#b16">Pang and Lee, 2004</ref>), a mincut-based algo- rithm was proposed to classify each sentence as being subjective or objective. The purpose of this work was to remove objective sentences from re- views to improve document level sentiment classi- fication. Interestingly, the cut functions are sym- metrical and submodular, and the problem of find- ing min-cut is equivalent to minimizing a symmet- ric submodular function. <ref type="bibr" target="#b6">Lerman et al. (2009)</ref> proposed three different models -sentiment match (SM), sentiment match + aspect coverage (SMAC) and sentiment-aspect match (SAM) to perform summarization of re- views of a product. The first model is called sen- timent match (SM), which extracts sentences so that the average sentiment of the summary is as close as possible to the average sentiment rating of reviews of the entity i.e. low MISMATCH but with high sentiment INTENSITY. The sec- ond model, called sentiment match + aspect cov- erage (SMAC), builds a summary that trades-off between DIVERSITY, maximally covering impor- tant aspects and MISMATCH, matching the over- all sentiment of the entity along with high INTEN- SITY. The third model, called sentiment-aspect match (SAM), not only attempts to cover impor- tant aspects, but cover them with appropriate sen- timent using KL-Divergence function. Here, IN- TENSITY and DIVERSITY in the first two mod- els are linear monotone submodular functions, while KL-Divergence function i.e. relative en- tropy in last model, unlike entropy is not mono- tone submodular.</p><p>In ( <ref type="bibr" target="#b15">Nishikawa et al., 2010b</ref>), a more sophis- ticated summarization technique was proposed, which generates a traditional text summary by se- lecting and ordering sentences taken from multi- ple reviews, considering both informativeness and readability of the final summary. The readability score in this paper would have been linear mono- tone submodular function, if the negative polarity was not penalizing. In ( <ref type="bibr" target="#b14">Nishikawa et al., 2010a</ref>), the authors further studied this problem using an integer linear programming formulation.</p><p>On the other hand, Lin et al .(2011) treated the task of generic summarization as monotone sub- modular function maximization. Further, they ar- gued that monotone non-decreasing submodular functions are an ideal class of functions to inves- tigate for document summarization. They also show, in fact, that many well-established meth- ods for summarization <ref type="bibr" target="#b0">(Carbonell and Goldstein, 1998;</ref><ref type="bibr" target="#b2">Filatova, 2004;</ref><ref type="bibr" target="#b19">Riedhammer et al., 2010)</ref> correspond to submodular function optimization, a property not explicitly mentioned in these publi- cations. Since many authors either in summariza- tion or opinion summarization have used functions similar to submodular functions as objective, we can take this fact as testament to the value of sub- modular functions for opinion summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introduction to Submodular Functions</head><p>A submodular function is a set function (f : 2 V → R) having a natural diminishing returns property. Diminishing returns property holds if the differ- ence in the value of the function that a single ele- ment makes when added to an input set decreases as the size of the input set increases i.e. for every A, B ⊆ V with A ⊆ B and every x ∈ V \B, we have that f (A∪{x})−f (A) ≥ f (B∪{x})−f (B).</p><p>A submodular function f is monotone if for every A ⊆ B, we have that f (A) ≤ f (B).</p><p>The extractive summarization task can be mod- eled as optimization problem i.e. finding a set S ⊆ V (S is set of sentences in summary, V is set of sentences in Document) which maximizes a submodular function f (S) subject to budget con- straints. In the following section, we will justify the use of submodular function for opinion sum- marization. Another advantage of choosing mono- tone submodular function is that there exists a polynomial-time greedy algorithm for constrained monotone submodular objective. The greedy al- gorithm guarantees that the summary solution ob- tained is almost as good as (63%) the best possi- ble summary solution according to the objective <ref type="bibr" target="#b20">(Sviridenko, 2004;</ref><ref type="bibr" target="#b21">Wolsey, 1982)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Submodularity in Opinion Summarization</head><p>Opinion Summarization should be modeled as a monotone submodular optimization problem, since opinion summary also holds following prop- erties:</p><p>1. Monotonicity -As more sentences are added to opinion summary, subjectivity increases along with information content as opinion- ated words are being added.</p><p>2. Diminishing Return -If multiple sentences of varying intensity are added to opinion sum- mary, the effect of a lower intensity polarity bearing sentence is diluted in the presence of a higher intensity one.</p><p>To show that opinion summarization inherently follow the diminishing return property, consider the following sentences 1 with positive polarity: A: "Even the acting in From Hell is solid, with the dreamy Depp turning in a typically strong perfor- mance and deftly handling a British accent." B: "Worth mentioning are the supporting roles by Ians Holm and Richardsonlog." (A ∪ B) : "Even the acting in From Hell is solid, with the dreamy Depp turning in a typically strong performance and deftly handling a British accent. Worth mentioning are the supporting roles by Ians Holm and Richardsonlog." Compare A and its su- perset, A ∪ B as candidate summaries. Sentence A and B convey positive sentiment, but sentence 1 http://www.imdb.com/reviews/295/29590.html B has less intensity compared to sentence A. After reading the text (A ∪ B), it is clear that the effect of sentence B has diminished in front of sentence A, though both are of same polarity. B can be thus, removed from the candidate summary as it does a diminishing addition in presence of sentence A to the positive sentiment over the "acting" aspect of the entity "movie". The diminishing return not only holds for same polarity but also, for opposite polarity. Consider another example 2 : A: "The movie is predictive with foreseeable end- ing." B: "Still it's very well-done that no movie in this entire year has a scene that evokes pure joy as this does." (A ∪ B) : "The movie is predictive with foresee- able ending. Still it's very well-done that no movie in this entire year has a scene that evokes pure joy as this does." Compare B and its superset, A ∪ B as candidate summaries. Sentence A has neg- ative sentiment whereas sentence B conveys posi- tive sentiment with more intensity. When we read the text (A ∪ B), it is clear that the effect of sen- tence A has diminished in front of sentence B in text , as usually polarity of higher intensity dom- inates over the polarity of lower intensity. Now, consider a general example 3 , "Laurence plays Neo's mentor Morpheus and he does an excellent job of it. His lines flow with con- fidence and style that makes his acting unique and interesting. The movie has lot of special effects and action-packed scenes with part of the appeal has philosophical and religious underpinnings."</p><p>If the budget for summary had been only two subjective sentences, then picking up first two would have redundantly captured only single as- pect (i.e. acting) and the redundancy of the con- cept (acting) also causes a diminishing return of the second sentence because of the difference in sentiment intensity. However, picking the last sentence with either one of the first two would have not just covered both the aspects (i.e. acting and visual effects) but since, the sentences are not overlapping in aspects, there would not have been any diminishing return of sentiment on shared as- pect (acting). Thus, it can be verified that opinion polarity also holds submodular property of dimin- ishing return, if they are on the same aspect of a distinct entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Formulation</head><p>Let V represent the set of the sentences in a doc- ument. The task of extractive opinion summariza- tion is to select a subset S ∈ V to represent the entirety (ground set V ) . Obviously, we should have |S| ≤ |V | as it is a summary and should be small. Therefore, constraints on S can naturally be modeled as knapsack constraints:</p><formula xml:id="formula_0">i∈S c i ≤ b (1)</formula><p>where c i is the non-negative cost of selecting unit i (e.g., the number of words in the sentence) and b is our budget. If we use a set function F : 2 V → R to measure the quality of the sum- mary set S, the summarization problem can then be formalized as the following combinatorial opti- mization problem:</p><formula xml:id="formula_1">S * ∈ argmax S⊂V F (S) s.t. i∈S c i ≤ b (2)</formula><p>where F (S) , total utility of summary is given as a linear combination of L(S), relevance and A(S), subjective coverage of aspects.</p><formula xml:id="formula_2">F (S) = αL(S) + βA(S)<label>(3)</label></formula><p>This formulation clearly brings out the trade-off between the subjective and the objective part. The intuition behind the combination of sentiment and aspect coverage in same function A(S) is that opin- ion polarity holds submodular property of dimin- ishing return only if the set of sentences talk about common aspect of the same entity as discussed in previous section. L(S) , relevance is modeled same as in ( <ref type="bibr" target="#b8">Lin and Bilmes, 2011)</ref> as it captures the summary property, while our novel function, A(S) has been modeled differently through a suit- able submodular function such that it captures the subjectivity property.</p><formula xml:id="formula_3">L(S) = i∈V min{c i (S), γc i (V )} (4) c i (S) = j∈S w i,j<label>(5)</label></formula><p>Here, w i,j &gt; 0 measures the similarity between i th and j th sentences and c i (S) measures the sim- ilarity of summary with the document.</p><p>Since, A(S), subjective coverage of aspects has to be modeled as monotone submodular function, it has been formulated as :</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A 1 : Modular Function</head><p>A 1 (S) is simple linear function, which is sum of weighted subjective scores for each sen- tence. No budgeting constraints are added to this formulation.</p><formula xml:id="formula_4">A 1 (S) = i j∈(P i ∩S) s j * w i<label>(6)</label></formula><p>Here P i ; i = 1...K is a partition of the ground set V (i.e., ∪ i P i = V ), which contains sen- tences pertaining to different distinct aspects. w i are the weights of the partitions, based on the corresponding aspects. s j is the subjec- tive score of the sentence j in summary. The subjective score s j is calculated using senti- wordnet as sum of the positive score ∈ [0, 1] and negative score ∈ [0, 1] (Esuli and <ref type="bibr" target="#b1">Sebastiani, 2006</ref>).</p><formula xml:id="formula_5">s j = word∈j (pos(word) + neg(word))<label>(7)</label></formula><p>2. A 2 : Budget-additive Function</p><formula xml:id="formula_6">A 2 (S)</formula><p>is an extension to A 1 (S), where max- imum subjectivity score is restricted with budget based on aspect. Here, λ ∈ [0, 1] is threshold coefficient for budget additive function to avoid redundancy of high senti- ment on same aspect. When aspect i is satu- rated by S (min( j∈(P i ∩S) s j , λ) = λ), any new sentence j cannot further improve cov- erage over i and thus, other aspects, which are not yet saturated will have a better chance of being covered. This formulation ensures that produced summary is diverse enough and conveys sentiment about different aspects by budgeting.</p><formula xml:id="formula_7">A 2 (S) = i min( j∈(P i ∩S) s j , λ i ) * w i (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A 3 : Polarity Partitioned Budget-additive Function</head><p>In previous formulation we have not consid- ered the polarity of the sentences. For ex- ample, if an aspect have many positive sen- tences with more intensity but few negative sentences with less intensity, A 2 more likely to reward more positive sentences because of intensity. In this formulation budgeting ap- plied not only on aspect but polarity scores too. This ensures that both positive and neg- ative polarity sentences are part of summary.</p><formula xml:id="formula_8">A 3 (S) = i min( j∈(P i ∩S∩Ppos) s j , λ i ) * w i + min( j∈(P i ∩S∩Pneg) s j , λ i ) * w i<label>(9)</label></formula><p>P pos and P neg are the partition of the sen- tences in the ground set V , based on their sign of polarity score. The polarity score pol j for partitioning sentences into P pos and P neg is calculated as difference of the positive and negative score.</p><formula xml:id="formula_9">pol j = word∈j (pos(word) − neg(word))<label>(10)</label></formula><p>Polarity based partitions bring out contrast view on a particular aspect, which is simi- lar to contrast view opinion summarization to give the reader a direct comparative view of different strong opinions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A 4 : Facility Location Function</head><p>In this formulation, we model the facil- ity location objective function <ref type="bibr" target="#b5">(Krause and Golovin, 2014</ref>) for opinion summarization as choosing possible sentences (facilities) out of document (set of locations) to serve as- pects (customers) giving service of value s j . If each aspect (customer) chooses the sen- tences (facility) with the highest value, the to- tal value provided to all aspects (customers) is modeled by this set function.</p><formula xml:id="formula_10">A 4 (S) = i max j∈(P i ∩S) s j * w i (11)</formula><p>So A 4 rewards only a sentence which has maximum subjectivity score in each aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">A 5 : Polarity Partitioned Facility Location Function</head><p>A 5 is similar to A 4 , but for each aspect, A 5 rewards two sentences with positive and neg- ative polarity but with maximum subjectivity scores in those polarity partitions.</p><formula xml:id="formula_11">A 5 (S) = i max j∈(P i ∩S∩Ppos) s j * w i + i max j∈(P i ∩S∩Pneg) s j * w i<label>(12)</label></formula><p>Each of the above functions are monotone sub- modular as the parameters s j and w i are positive. Since the first function is linear, it is both submod- ular and supermodular, thus modular. Budget ad- ditive and facility location functions ( <ref type="bibr" target="#b5">Krause and Golovin, 2014</ref>) are special types of monotone sub- modular functions. Since, monotone submodular- ity is preserved under non-negative linear combi- nations, polarity based partitioned function, whose sub-parts are monotone submodular is also mono- tone submodular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>We have created Movie ontology tree manually (figure 1). Further the ontology is enriched by adding clue words to all aspects using wordnet sense propagation algorithm ( <ref type="bibr" target="#b1">Esuli and Sebastiani, 2006</ref>) for three iterations. The algorithm does a hard clustering of the sentences by assigning the sentence aspect, which has maximum number of clue words in that sentence. Clue words for 'Plot' aspect are story, script, storyline, chief, commu- nicative, explain, narrate, narration, narrative, narrator, report, reporter, scheme, schemer, script, scriptural, storyteller, tell, write up..,.</p><p>For the experiments, we have used the polar- ity dataset from <ref type="bibr" target="#b16">Pang et al. (2004)</ref>. The dataset contains 1000 positive and 1000 negative movie reviews with size varying between 700 to 1000 words. As summary generation is time consum- ing task (DUC 4 only used 25 summaries to eval- uate the performance of systems), we picked 100 positive and 100 negative reviews randomly from the dataset and their abstract summaries are gener- ated manually with 200 words limit as budget for In the experiment, the partial enumeration based greedy algorithm ( <ref type="bibr" target="#b3">Khuller et al., 1999</ref>) is used for summary generation of 200 test documents within budget of 200 words. The algorithm has two parts. In the first part, the algorithm compares function values of all feasible solutions (sets) of cardinal- ity one or two. Let Summ 1 be a feasible set of cardinality one or two that has the largest value of the objective function F (S). In the second part, the algorithm enumerates all feasible sets of cardi- nality three. The algorithm, then completes each such set greedily and keeps the current solution feasible with respect to the knapsack constraint. Let Summ 2 be the solution obtained in the second part that has the largest value of objective function over all choices of the starting set for the greedy algorithm. Finally, the algorithm outputs Summ 1 if F (Summ 1 ) &gt; F (Summ 2 ) else Summ 2 oth- erwise. The algorithm does O(n 2 ) function cal- culations in first part, while O(n 5 ) in second part. This algorithm gives a performance guarantee of (1−e −1 ) for solving monotone submodular objec- tive with knapsack constraint ( <ref type="bibr" target="#b3">Khuller et al., 1999;</ref><ref type="bibr" target="#b20">Sviridenko, 2004</ref>). As far as we know, the algo- rithm has not been implemented for such prob- lems because of complexity constraints ( <ref type="bibr" target="#b8">Lin and Bilmes, 2011</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Overall Algorithm -Summary Ex- traction</head><formula xml:id="formula_12">B ⇐ 200 for Sentence s ∈ Document V do</formula><p>Assign sentence s to one of aspects in movie ontology. end for Summ1 ⇐ argmax { F(S), such that S ⊆ V, |S| &lt; 3, and cost(S) ≤ B } Summ2 ⇐ ∅ for all S ⊆ V, |S|=3, and cost(S) ≤ B do</p><formula xml:id="formula_13">U ⇐ V \S while U = ∅ do maxReturn ⇐ 0.0 newSentence ⇐ ∅ for Sentence s ∈ U do S * ⇐ S ∪ {s} F (S * ) ⇐ αL(S * ) + (1 − α)A(S * ) return ⇐ F (S * )−F (S) len(s) if return ≥ maxReturn then maxReturn ⇐ return newSentence ⇐ s end if end for if cost(S ∪ {newSentence}) ≤ B then S ⇐ S ∪ {newSentence} end if U ⇐ U \{newSentence} end while if F(S) ≥ F (Summ2) then Summ2 ⇐ S end if end for if F(Summ1) ≥ F(Summ2) then Summary ⇐ Summ1 else Summary ⇐ Summ2 end if</formula><p>In the algorithm, the sentences are clustered in different partitions, corresponding to different as- pects in the ontology tree using the clue words. In the experiment, hard clustering of the sentences in aspect-based partitions is considered but soft- clustering of the sentences will also work with this approach, which has been left out to avoid further parameter tuning for soft clustering assigments. The weights of the partitions as well as the thresh- old parameters for the A(S) are currently kept pro- portional to the inverse of the depth of that aspect in the ontology-tree as sentiment expressed on the concepts at higher level in the ontology tree should have more weightage. ∀ Aspects i,</p><formula xml:id="formula_14">w i = λ i = 1 Level(i)</formula><p>The linear combination parameter β is set as 1− α to bring out the trade-off between relevance and subjective coverage of aspects and α is varied from 0 to 1 with step size 0.05 to find optimal α. γ in L(S) is set to 0.5. The parameter learning, esp. α and its impact have been already studied in ( <ref type="bibr" target="#b8">Lin and Bilmes, 2011</ref>) and thus, is not addressed in the paper. We have used the same approach of grid search to find the optimal value of α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>We use ROUGE <ref type="bibr" target="#b9">(Lin, 2004</ref>) for evaluating the content of summaries. We have used the 200 test documents that are manually summarized as gold standard data for ROUGE evaluation. For figuring out the sentiment correlation between manual and system generated summaries, we trained Naive Bayes sentiment classifier ( <ref type="bibr" target="#b17">Pang et al., 2002</ref>) on training data using bag of words approach with features as unigrams and bigrams and then, us- ing minimum Pearson's chi-square score of 3 for feature extraction <ref type="bibr" target="#b18">(Pecina and Schlesinger, 2006</ref>) before calculating the sentiment. The measure of sentiment preservation is calculated as Pear- son correlation between the sentiment score of the document and the corresponding summary senti- ment, both calculated by the Naive Bayes senti- ment classifier while the measure of coverage of information content is given by ROUGE-1 and ROUGE-2 f-scores. Mathematically,</p><formula xml:id="formula_15">Correlation(X, Y ) = Covariance(X, Y ) std.dev(X) * std.dev(Y )<label>(13)</label></formula><p>Here, random variable X is the sentiment score of the document sample and random variable Y is the sentiment score of the correspond- ing summary sample. For 200 documents, it will be [(X 1 , Y 1 ), (X 2 , Y 2 ), ..., (X 200 , Y 200 )] sam- ple points for the above correlation function.</p><p>Following five baselines are used for compari- son:</p><p>have used same NaiveBayes classifier <ref type="bibr" target="#b17">(Pang et al., 2002</ref>) trained on imdb corpus to predict the sentiment of a sentence and document.</p><formula xml:id="formula_16">min S⊂V j∈S (|senti(V ) − senti(j)|) (14)</formula><p>4. Baseline-4/TEXTRANK : TextRank sum- marizer is based on Graph based unsuper- vised algorithm. Graph is constructed by cre- ating a vertex for each sentence in the docu- ment and edges between vertices based on the number of words two sentences (of vertices) have in common and then, ranking them by applying PageRank to the resulting graph. Summary is generated with sentences hav- ing more vertex score ( <ref type="bibr" target="#b12">Mihalcea and Tarau, 2004</ref>).</p><p>5. Baseline-5/MINCUT : Mincut algorithm ( <ref type="bibr" target="#b16">Pang and Lee, 2004</ref>) classifies the sentences as subjective and objective sentences, by finding minimum s-t cuts in graph of sen- tences using maximum flow algorithm. In the graph, each sentence is a vertex and the edge between the vertex to the source or sink is taken as probability of the sentence being subjective or objective (individual scores).</p><p>To ensure the graph connectivity, edges are drawn between every pair of sentence ver- tices, with edge weights taken proportional to the degree of proximity (association scores).</p><p>After maximum flow algorithm, the cut in which source vertex lies is classified as sub- jective and vice-versa. We pick top subjective sentences within the budget as summary.</p><p>Among the five baselines, TOP and TOP-SUBJ are simplistic. Though both TEXTRANK and MINCUT were not originally proposed for opin- ion summarization but a number of papers in opin- ion summarization have built over these two meth- ods and also, used them as baselines and thus, comparing with the "well-known" baselines will give the readers from the sentiment analysis field an intuitive idea of the performance of our sys- tem. MINCUT, however was reproposed specif- ically for subjective summarization by <ref type="bibr" target="#b16">Pang and Lee (2004)</ref> and we use that formulation for com- parison. proposed functions not only outperform the base- lines in terms of ROUGE scores for optimal pa- rameters but also, give better correlation with the document sentiment. This can be quantitively ver- ified by test of significance, unpaired one-tailed t-test without assuming equal variance between the baselines and the systems. The p-values are 0.0203 and 0.0066 respectively for ROUGE-1 F Score and Sentiment Correlation, justifying that the performance improvement by our system over the baselines is statistically significant at p &lt; 0.05. The main reason being that the functions with optimal values of trade-off parameter α strike out a balance between relevance and subjectivity. Clearly, the facility location based monotone sub- modular functions are the best choice as objective for opinion summarization task as they select sen- tences with maximum subjectivity (facilities giv- ing best service).</p><p>Our system is able to access the information of aspect and polarity of each sentence, while some baselines do not. So, the improvement over the baselines may be attributed to those additional in- formation rather than the optimality of the par- tial enumeration greedy algorithm over submodu- lar functions. So, we therefore, introduced the fol- lowing baseline to question this misdoubt on the experiment:</p><p>6. Baseline-6/LIN :</p><p>In this baseline, the greedy algorithm ( <ref type="bibr" target="#b7">Lin and Bilmes, 2010</ref>) is used for summary generation, us- ing the same functions and information in the for- mulation. This algorithm fills the empty summary set greedily by adding a single sentence in each   ), ensuring that current solution is feasible with respect to the knapsack constraint (cost(S ∪ {newSentence}) ≤ B). This algorithm has a complexity of O(n 2 ) but gives only perfor- mance guarantee of 1 2 (1 − e 1 ) ≈ 0.316 ( <ref type="bibr" target="#b3">Khuller et al., 1999)</ref>. <ref type="table" target="#tab_2">Table 2</ref> compares the same five functions in our system with (Lin and Bilmes, 2010) system based on optimal values of tradeoff α. From the table, it can be inferred that our system also outperforms this baseline both in terms of ROUGE scores and sentiment correlation, which can be quantitively verified by test of significance, unpaired one-tailed t-test without assuming equal variance between the baselines and the systems. The p-values are 0.02517 and 0.003965 respectively for ROUGE-1 F Score and Sentiment Correlation, justifying that the performance improvement by our system over LIN system is statistically significant at p &lt; 0.05.</p><p>The figures 2 and 3 plot the value of senti-  <ref type="table">Table 3</ref>: Maximum ROUGE F-score and their cor- responding sentiment correlation ment correlation and ROUGE-1 F score for the formulated submodular functions with respect to the trade-off parameter, α respectively. Looking at the graph 2, we can observe that more weightage to relevance over subjective coverage of aspects decreases the sentiment correlation, which was ex- pected because the summary generated misses out on subjective sentiment due to trade-off. Simi- larly, by looking at the graph 3, we also observe that more weightage to relevance over subjective coverage of aspects increases the ROUGE score as expected. The erratic behaviour in <ref type="figure">figure 3</ref> can be explained by arguing that subjective words are also important for summary and thus, giving less weightage to them over relevance, ROUGE score will increase but not properly.</p><p>The table 3 presents the value of sentiment correlation corresponding to maximum ROUGE score (for α ≈ 1). Clearly, A 4 and A 2 have maxi- mum ROUGE scores as they neglect polarities and instead, reward on aspect based partitions, thus in- creasing coverage. The table 4 presents the value of ROUGE score corresponding to maximum sen- timent correlation (for α ≈ 0). Clearly, A 4 also has maximum sentiment correlation as it rewards max- imum subjectivity, irrespective of polarities and   <ref type="figure">figure 3</ref> and the corresponding value of Sentiment Correlation from figure 2, at the same α.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we show that conflict between sub- jectivity and relevance naturally arises in opinion summarization. To address this problem, we intro- duce new monotone submodular functions that are well suited to document summarization ( <ref type="bibr" target="#b7">Lin and Bilmes, 2010;</ref><ref type="bibr" target="#b8">Lin and Bilmes, 2011;</ref><ref type="bibr" target="#b13">Morita et al., 2013</ref>) by modeling two important properties of opinion summary -relevance and subjective cover- age of aspects. We then, design different possible combinations of objective functions to model the task. To solve the algorithm effectively, we use the partial enumeration based algorithm, which is though computationally expensive (O(n 5 ) func- tion calls), gives a performance guarantee of 63% for an NP-hard problem like summarization <ref type="bibr" target="#b11">(McDonald, 2007</ref>). We have justified the submodular property of opinion summary through examples and significant performance of the system over the baselines. Further, this optimal trade-off between relevance and subjectivity can be used to design an evaluation framework for opinion summariza- tion task as both part of the objective functions are proportional to the ROUGE and Sentiment Corre- lation respectively, which are widely used evalua- tion measures <ref type="bibr" target="#b4">(Kim et al., 2011</ref>). As opinion sum- marization task lies in the intersection of opinion mining and summarization problems, both IR and NLP communities will benefit from our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Movie Ontology Tree</figDesc><graphic url="image-1.png" coords="6,72.00,63.80,249.42,323.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sentiment Correlation vs α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 compares</head><label>1</label><figDesc>the five functions with the above baselines based on optimal values of trade- off α. From the table, it can be inferred that all the</figDesc><table>System 

ROUGE1 ROUGE2 S. Corr. 

TOP 
0.43001 
0.16591 
0.86144 
TOP-SUBJ 
0.41807 
0.14362 
0.82953 
LER-SM 
0.42608 
0.14533 
0.96545 
TEXTRANK 0.41987 
0.14644 
0.88967 
MINCUT 
0.39368 
0.11047 
0.84017 
Submod-A 1 0.43223 
0.15702 
0.95306 
Submod-A 2 0.43594 
0.15977 
0.97538 
Submod-A 3 0.43247 
0.15436 
0.93155 
Submod-A 4 0.43602 
0.15760 
0.98566 
Submod-A 5 0.42976 
0.15551 
0.95415 

Table 1: ROUGE F-score and sentiment correla-
tion for optimal values of α with baselines 1-5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>ROUGE F-score and sentiment correla-
tion for optimal values of α with baseline 6 

0.78 

0.8 

0.82 

0.84 

0.86 

0.88 

0.9 

0.92 

0.94 

0.96 

0.98 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
Sentiment Correlation (Document and Summary) 

Alpha 

Alpha vs Sentiment Correlation 

A1 
A2 
A3 
A4 
A5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Maximum sentiment correlation and cor-
responding ROUGE F-Score 

the corresponding ROUGE-2 F-score is also high-
est among all functions. Tables 1 and 2 contain 
the ROUGE F-score and sentiment correlation for 
optimal values of α, found after grid search while 
tables 3 and 4 contain the peak values in the fig-
ures 2 and 3. For example, table 3 contains the 
peak value of ROUGE-1 F score from </table></figure>

			<note place="foot" n="2"> http://www.imdb.com/reviews/159/15918.html 3 http://www.imdb.com/title/tt0133093/reviews</note>

			<note place="foot" n="4"> Document Understanding Conferences, http://duc.nist.gov</note>

			<note place="foot" n="1">. Baseline-1/TOP : Sentences selected consecutively from the start of the review within the budget. 2. Baseline-2/TOP-SUBJ : Sentences ranked based on their subjectivity and then, selected with in the budget. 3. Baseline-3/LER-SM : (Lerman et al., 2009) Sentences which have sentiment close to document sentiment are chosen as Summary. We</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiwordnet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event-based extractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Filatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL Workshop on Summarization</title>
		<meeting>ACL Workshop on Summarization</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The budgeted maximum coverage problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samir</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph Seffi</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Comprehensive review of opinion summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Duk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Sondhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Submodular function maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tractability: Practical Approaches to Hard Problems</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentiment summarization: evaluating and learning user preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasha</forename><surname>Blair-Goldensohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="514" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-document summarization via budgeted maximization of submodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="912" to="920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A class of submodular functions for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="510" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out: Proceedings of the ACL-04 Workshop</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentiment analysis and opinion mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A study of global inference algorithms in multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="557" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Textrank: Bringing order into texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">275</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Subtree extractive summarization via submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Morita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opinion summarization with integer linear programming formulation for sentence extraction and ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="910" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing informativeness and readability for sentiment summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Nishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genichiro</forename><surname>Kikui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2010 Conference Short Papers</title>
		<meeting>the ACL 2010 Conference Short Papers</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="325" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd annual meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">271</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Thumbs up?: sentiment classification using machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 conference on Empirical methods in natural language processing</title>
		<meeting>the ACL-02 conference on Empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Combining association measures for collocation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Schlesinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL on Main conference poster sessions</title>
		<meeting>the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="651" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long story short-global unsupervised models for keyphrase based meeting summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korbinian</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilek</forename><surname>Benoit Favre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hakkani-Tür</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="801" to="815" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A note on maximizing a submodular set function subject to a knapsack constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Sviridenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="43" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An analysis of the greedy algorithm for the submodular set covering problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laurence A Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="385" to="393" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
