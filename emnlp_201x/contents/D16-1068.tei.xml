<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Tchernowitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Yedidsion</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Industrial Engineering and Management</orgName>
								<address>
									<settlement>Technion</settlement>
									<region>IIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="711" to="720"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Exact inference in high-order graph-based non-projective dependency parsing is intractable. Hence, sophisticated approximation techniques based on algorithms such as belief propagation and dual decomposition have been employed. In contrast, we propose a simple greedy search approximation for this problem which is very intuitive and easy to implement. We implement the algorithm within the second-order TurboParser and experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual dependency parsing. Our algorithm improves the run time of the parser by a factor of 1.43 while losing 1% in UAS on average across languages. Moreover , an ensemble method exploiting the joint power of the parsers, achieves an average UAS 0.27% higher than the TurboParser.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dependency parsing is instrumental in NLP appli- cations, with recent examples in information extrac- tion ( <ref type="bibr" target="#b32">Wu and Weld, 2010)</ref>, word embeddings ( <ref type="bibr" target="#b19">Levy and Goldberg, 2014)</ref>, and opinion mining ( <ref type="bibr" target="#b0">Almeida et al., 2015</ref>). The two main approaches for this task are graph based <ref type="bibr" target="#b23">(McDonald et al., 2005</ref>) and transi- tion based <ref type="bibr" target="#b26">(Nivre et al., 2007)</ref>.</p><p>The graph based approach aims to optimize a global objective function. While exact polyno- mial inference algorithms exist for projective pars- ing <ref type="bibr" target="#b10">(Eisner, 1996;</ref><ref type="bibr" target="#b23">McDonald et al., 2005;</ref><ref type="bibr" target="#b3">Carreras, 2007;</ref><ref type="bibr" target="#b17">Koo and Collins, 2010</ref>, inter alia), high order non-projective parsing is NP-hard <ref type="bibr" target="#b22">(McDonald and Pereira, 2006</ref>). The current remedy for this comes in the form of advanced optimization techniques such as dual decomposition <ref type="bibr" target="#b21">(Martins et al., 2013)</ref>, LP re- laxations ( <ref type="bibr" target="#b27">Riedel et al., 2012)</ref>, belief propagation ( <ref type="bibr" target="#b29">Smith and Eisner, 2008;</ref><ref type="bibr" target="#b14">Gormley et al., 2015</ref>) and sampling ( <ref type="bibr" target="#b36">Zhang et al., 2014b;</ref><ref type="bibr" target="#b35">Zhang et al., 2014a</ref>).</p><p>The transition based approach ( <ref type="bibr" target="#b34">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b1">Bohnet and Nivre, 2012;</ref><ref type="bibr" target="#b15">Honnibal et al., 2013;</ref><ref type="bibr" target="#b4">Choi and McCallum, 2013a</ref>, inter alia), and the easy first approach <ref type="bibr" target="#b11">(Goldberg and Elhadad, 2010)</ref> which extends it by training non-directional parsers that consider structural information from both sides of their decision points, lack a global ob- jective function. Yet, their sequential greedy solvers are fast and accurate in practice.</p><p>We propose a greedy search algorithm for high- order, non-projective graph-based dependency pars- ing. Our algorithm is a simple iterative graph-based method that does not rely on advanced optimization techniques. Moreover, we factorize the graph-based objective into a sum of terms and show that our basic greedy algorithm relaxes the global objective by se- quentially optimizing these terms instead of globally optimizing their sum.</p><p>Unlike previous greedy approaches to depen- dency parsing, transition based and non-directional, our algorithm does not require a specialized feature set or a training method that specializes in local deci- sions. In contrast, it supports global parameter train- ing based on the comparison between an induced tree and the gold tree. Hence, it can be integrated into any graph-based parser.</p><p>We first present a basic greedy algorithm that re- laxes the global graph-based objective (Section 3). However, as this simple algorithm does not provide a realistic estimation of the impact of an arc selection on uncompleted high-order structures in the partial parse forest, it is not competitive with state of the art approximations. We hence present an advanced version of our algorithm with an improved arc score formulation and show that this simple algorithm pro- vides high quality solutions to the graph-based infer- ence problem (Section 4).</p><p>Particularly, we implement the algorithm within the TurboParser ( <ref type="bibr" target="#b21">Martins et al., 2013</ref>) and exper- iment (Sections 8 and 9) with the datasets of the CoNLL 2006-2007 shared tasks on multilingual de- pendency parsing <ref type="bibr" target="#b2">(Buchholz and Marsi, 2006;</ref>). On average across languages our parser achieves UAS scores of 87.78% and 89.25% for first and second order parsing respectively, com- pared to respective UAS of 87.98% and 90.26% achieved by the original TurboParser.</p><p>We further implement (Section 6) an ensemble method that integrates information from the output tree of the original TurboParser and the arc weights learned by our variant of the parser into our search algorithm to generate a new tree. This yields an im- provement: average UAS of 88.03% and 90.53% for first and second parsing, respectively.</p><p>Despite being greedy, the theoretical runtime complexity of our advanced algorithm is not better than the best previously proposed approximations for our problem (O(n k+1 ), for n word sentences and k order parsing, Section 5). In experiments, our al- gorithms improve the runtime of the TurboParser by a factor of up to 2.41.</p><p>The main contribution of this paper is hence in providing a simple, intuitive and easy to implement solution for a long standing problem that has been addressed in past with advanced optimization tech- niques. Besides the intellectual contribution, we believe this will make high-order graph-based de- pendency parsing accessible to a much broader re- search and engineering community as it substan- tially relaxes the coding and algorithmic proficiency required for the implementation and understanding of parsing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>We start with a brief definition of the high order graph-based dependency parsing problem. Given an n word input sentence, an input graph G = (V, E) is defined. The set of vertices is V = {0, ..., n}, with the {1, . . . , n} vertices representing the words of the sentence, in their order of appearance, and the 0 vertex is a specialized root vertex. The set of arcs is E = {(u, v) : u ∈ {0, ..., n}, v ∈ {1, ..., n}, u = v}, that is, the root vertex has no incoming arcs.</p><p>We further define a part of order k to be a subset of E of size k, and denote the set of all parts with parts. For the special case of k = 1 a part is an arc. Different works employed different parts sets (e.g. ( <ref type="bibr" target="#b21">Martins et al., 2013;</ref><ref type="bibr" target="#b23">McDonald et al., 2005;</ref><ref type="bibr" target="#b17">Koo and Collins, 2010)</ref>). Generally, most parts sets consist of arcs connecting vertices either vertically (e.g. {(u, v), (v, z)} for k = 2) or horizontally (e.g. {(u, v), (u, z)}, for k = 2). In this paper we focus on the parts employed by <ref type="bibr" target="#b21">(Martins et al., 2013</ref>), a state-of-the-art parser, but our algorithms are gener- ally applicable for any parts set consistent with this general definition. <ref type="bibr">1</ref> In graph-based dependency parsing, each part p is given a score W p ∈ R. A Dependency Tree (DT) T is a subset of arcs for which the following conditions hold: (1) Every vertex, except for the root, has an incoming arc: ∀v ∈ V \ {0} : ∃u ∈ V s.t.(u, v) ∈ T ; (2) No vertex has multiple incom- ing arcs: ∀(u, u , v) ∈ V, (u, v) ∈ T → (u , v) / ∈ T ; and (3) There are no cycles in T . The score of a DT T is finally defined by:</p><formula xml:id="formula_0">score(T ) = part⊆T W part</formula><p>The inference problem in this model is to find the highest scoring DT in the input weighted graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Basic Greedy Inference</head><p>We start with a basic greedy algorithm (Algorithm 1), analyze the approximation it provides for the graph-based objective and its inherent limitations.</p><p>Algorithm 1 maintains a partial tree data struc- ture, T i , to which it iteratively adds arcs from the input graph G, one in each iteration, until a depen- dency tree T n is completed. For this end, in every iteration, i, a value, v i e , composed of loss i e and gain i e terms, is computed for every arc e ∈ E and the arc with the lowest v i e value is added to T i−1 to create the extended partial tree T i .</p><p>Due to the aforementioned conditions on the in- duced dependency tree, every arc that is added to T i−1 yields a set of lostArcs and lostParts that can- not be added to the partial tree in subsequent itera- tions. The loss value is defined to be:</p><formula xml:id="formula_1">loss i e := part∈lostP arts W part</formula><p>That is, every part that contains one or more arcs that violate the dependency tree conditions for a tree that extends the partial tree T i−1 ∪ {e} is considered a lost part as it will not be included in any tree ex- tending T i−1 ∪{e}. The loss value sums the weights of these parts. Likewise, the gain value is the sum of the weights of all the parts that are added to T i−1 when we add e to it. Denote this set of parts with P e := {part : part ⊆ T i−1 ∪ {e}, part / ∈ T i−1 }, then:</p><formula xml:id="formula_2">gain i e := part∈Pe W part</formula><p>Finally, v i e is given by:</p><formula xml:id="formula_3">v i e = loss i e − gain i e</formula><p>After the arc with the minimal value v i e is added to T i−1 , the arcs that violate the structural constraints on dependency trees are removed from G.</p><p>An example of an update iteration of the algo- rithm (lines 3-16) is given in <ref type="figure" target="#fig_0">Figure 1</ref>. In this ex- ample we consider two types of parts: first-order, arc, parts (ARC) and second-order grandparent parts (GP), consisting of arc pairs, {(g, u), (u, v)}. The upper graph shows the partial tree T 2 (solid arcs) as well as the rest of the graph G (dashed arcs). The parts included in T 2 are ARC(0,2), ARC(2,1) and GP <ref type="bibr">[(0,2)</ref>  graph that corresponds to T 3 all other incoming arcs to vertex 3 are removed. (in this instance there are no cycle forming arcs).</p><p>Analysis We now turn to an analysis of the relaxation that Algorithm 1 provides for the global graph-based objective. Recall that our objective in iteration i is:</p><formula xml:id="formula_4">v i e i = min{v i e }.</formula><p>For the inferred tree T n it holds that:</p><formula xml:id="formula_5">e i ∈T n v i e i − part⊆G W part = partT n W part − part⊆T n W part − part⊆G W part = − 2 × part⊆T n W part + partT n W part − partT n W part = − 2 × part⊆T n W part</formula><p>The first equation holds since e i ∈T n v i e i is the sum of all lost parts (parts that are not in T n ) minus all the gained parts (parts in T n ). Each of these parts was counted exactly once: when the part was added to the partial tree or when one of its arcs was re- moved from G. The second equation splits the term of part⊆G W part to two sums, one over parts in T n and the other over the rest. Since part⊆G W part and 2 are constants, we get:</p><formula xml:id="formula_6">arg min T n (− part⊆T n W part ) = arg min T n e i ∈T n v i e i</formula><p>From this argument it follows that our inference algorithm performs sequential greedy optimization over the presented factorization of the graph-based objective instead of optimizing the sum of terms, and hence the objective, globally.</p><p>The main limitation of Algorithm 1 is that it does not take into account high order parts contribution until the part is actually added to T . For exam- ple, in <ref type="figure" target="#fig_0">Figure 1</ref>, when the arc (2, 1) is added, the part GP[(2,1),(1,3)] is getting closer to completion. Yet, this is not taken into account when considering whether (2, 1) should be added to the tree or not. In- cluding this information in the gain and loss values of an arc can improve the accuracy of the algorithm, especially in high-order parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Basic Greedy Inference</head><formula xml:id="formula_7">1: T 0 = {} 2: for i ∈ 1..n do 3: for e = (u, v) ∈ E do 4: Pe := {part ∈ parts : part ⊆ T i−1 ∪ {e}, part T i−1 } 5:</formula><p>gain i e := part∈Pe Wpart 6:</p><formula xml:id="formula_8">incomingSet := {(u , v) ∈ E : u = u} 7: cycleSet := {(u , v ) ∈ E : T i−1 ∪ {e} ∪ (u , v ) contains a cycle} 8: lostArcs = (incomingSet ∪ cycleSet) 9:</formula><p>lostP arts = {part : ∃e ∈ lostArcs ∩ part} 10:</p><p>loss i e := part∈lostP arts Wpart 11:</p><formula xml:id="formula_9">v i e := loss i e − gain i e 12</formula><p>: end for 13:</p><formula xml:id="formula_10">e i = (u i , v i ) = arg min e {v i e } 14: T i = T i−1 ∪ {e i } 15:</formula><p>remove from G all incoming arcs to v i 16:</p><p>remove from G all cycle forming arcs w.r.t T i 17: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Greedy Inference with Partial Part Predictions</head><p>In order for the algorithm to account for information about partial high order parts, we estimate the prob- ability that such parts would be eventually included in T n . Our way to do this (Algorithm 2) is by es- timating these probabilities for arcs and from these derive parts probabilities. Particularly, for the set of incoming arcs of a ver- tex v, E v = {e = (u, v) : e ∈ E}, a probability measure p e is computed according to:</p><formula xml:id="formula_11">p e=(u,v) = exp α×We e =(u ,v) exp α×W e</formula><p>Where α is a hyper parameter of the model. For α = 0 we get a uniform distribution over all possible Algorithm 2 Greedy Inference with Partial Part Pre- dictions 1: T 0 = {} 2: for i ∈ 1..n do 3:</p><p>for e = (u, v) ∈ E do 4:</p><p>Pe := {part ∈ parts : e ∈ part} 5:</p><formula xml:id="formula_12">gain i e := part∈Pe Wpart × ppart|(T i−1 ∪ {e}) 6: incomingSet := {(u , v) ∈ E : u = u} 7: cycleSet := {(u , v ) ∈ E : T i−1 ∪ {e} ∪ (u , v ) contains a cycle} 8: lostArcs = (incomingSet ∪ cycleSet) 9:</formula><p>lostP arts = {part : ∃e ∈ lostArcs ∩ part} 10:</p><formula xml:id="formula_13">loss i e := part∈lostP arts Wpart × ppart|T i−1 11: v i e := β × loss i e − (1 − β) × gain i e 12: end for 13: e i = (u i , v i ) = arg min e {v i e } 14: T i = T i−1 ∪ {e i } 15:</formula><p>remove from G all incoming arcs to v i 16:</p><p>remove from G all cycle forming arcs w.r.t T i 17: end for heads of a vertex v, and for large α values arcs with larger weights get higher probabilities.</p><p>The intuition behind this measure is that arcs mostly compete with other arcs that have the same target vertex and hence their weight should be nor- malized accordingly. Using this measure, we define the arc-factored probability of a part to be:</p><formula xml:id="formula_14">p part = e∈part p e</formula><p>And the residual probability of a part given an exist- ing partial tree T :</p><formula xml:id="formula_15">p part |T = p part e∈(part T ) p e</formula><p>These probability measures are used in both the gain and the loss computations (lines 5 and 10 in Algorithm 2) as follows:</p><formula xml:id="formula_16">gain i e := part:e∈part W part × p part |(T i−1 ∪ {e}) loss i e := part∈lostP arts W part × p part |T i−1</formula><p>Finally, as adding an arc to the dependency sub- tree results in an exclusion of several arcs, the num- ber of lost parts is also likely to be much higher than the number of gained parts. In order to com- pensate for this effect, we introduce a balancing hyper-parameter, β ∈ [0, 1], and change the com- putation of v i e (line 10 in Algorithm 2) to be:</p><formula xml:id="formula_17">v i e := β × loss i e − (1 − β) × gain i e .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Runtime Complexity Analysis</head><p>In this section we provide a sketch of the runtime complexity analysis of the algorithm. Full details are in appendix A. In what follows, we denote the maximal indegree of a vertex with n in .  . The total runtime of Algorithm 2 is O(n k+1 in ×k+n×(n×n in ×(n in + min{n 2 in , n k−1 in })+n k in ×k 2 )). For unpruned graphs and k ≥ 2 this is equivalent to O(n k+1 ), the theo- retical runtime of the TurboParser's dual decompo- sition inference algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Algorithm 1 consists of two nested</head><formula xml:id="formula_18">(|parts| × k + n 2 × n in × (n in + min{n k−1 in , n 2 in })) time.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Error Propagation</head><p>Unlike modern approximation algorithms for our problem, our algorithm is greedy and determinis- tic. That is, in each iteration it selects an arc to be included in its final dependency tree and this de- cision cannot be changed in subsequent iterations. Hence, our algorithm is likely to suffer from error- propagation. We propose two solutions to this prob- lem described within Algorithm 2.</p><p>Beam search In each iteration (lines 3-16) the al- gorithm outputs its |B| best solutions to be subse- quently considered in the next iteration. That is, lines 4-10 are performed |B| times for each edge e ∈ E, one for each of the |B| partial solutions in the beam, b j ∈ B. For each such solution, we de- note its weight, as calculated by the previous itera- tion of the algorithm with beamV al b j . When evalu- ating v i e for an arc e with respect to b j (line 11), we set v i,j e = beamV al b j +β ×loss i e −(1−β)×gain i e .</p><p>Post-search improvements After Algorithm 2 is executed, we perform s iterations of local greedy arc swaps. That is, for every vertex v, s.t. (u, v) ∈ T n , we try to switch the arc (u, v) with the arc (u , v) as follows. Let T n v be the sub tree that is rooted at v, we distinguish between two cases:</p><formula xml:id="formula_19">(1) If u / ∈ T n v then T n = T n \ {(u, v)} ∪ {(u , v)}. (2) If u ∈ T n</formula><p>v then let w be the first vertex on the path from v to u (if (v, u ) ∈ T then w = u ):</p><formula xml:id="formula_20">T n = T n \ {(u, v), (v, w)} ∪ {(u , v), (u, w)}.</formula><p>After inspecting all possible substitutions, we choose the one that yields the best increase in the tree score (if such a substitution exists) and perform the substitution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Parser Combination</head><p>In our experiments (see below), we implemented our algorithms within the TurboParser so that each of them, in turn, serves as its inference algorithm. In development data experiments with Algorithm 2 we found that for first order parsing, both our algorithm and the TurboParser predict on average over all lan- guages around 1% of the gold arcs that are not in- cluded in the output of the other algorithm. For sec- ond order parsing, the corresponding numbers are 1.75% (for gold arcs in the output of our algorithm but not of the original TurboParser) and 4.3% (for the other way around). This suggests that an ensem- ble method may improve upon both parsers.</p><p>We hence introduce a variation of Algorithm 2 that accepts a dependency tree T o as an input, and biases its output towards that tree. As different parsers usually generate weights on different scales, we do not directly integrate part weights. Instead, we change the weight of each part part ⊆ T o of order j, to be W part = W part + γ j , where γ j is an hyperparameter reflecting our belief in the pre- diction of the other parser on parts of order j. The change is applied only at test time, thus integrating two pre-trained parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Experimental Setup</head><p>We implemented our algorithms within the Tur- boParser ( <ref type="bibr" target="#b21">Martins et al., 2013)</ref>  <ref type="bibr">2</ref> . That is, every other aspect of the parser -feature set, pruning algorithm, cost-augmented MIRA training ( <ref type="bibr" target="#b8">Crammer et al., 2006</ref>) etc., is kept fixed but our algorithms replace the inference algorithms: Chu-Liu-Edmonds ( <ref type="bibr" target="#b9">(Edmonds, 1967)</ref>, first order) and dual-decomposition (higher order). We implemented two variants, for algorithm 1 and 2 respectively, and compare their results to those of the original TurboParser.</p><p>We experiment with the datasets of the CoNLL 2006 and 2007 shared task on multilingual depen- dency parsing <ref type="bibr" target="#b2">(Buchholz and Marsi, 2006;</ref>, for a total of 17 languages. When a language is represented in both sets, we used the 2006 version. We followed the standard train/test split of these datasets and, for the 8 languages with a training set of at least 10000 sentences, we ran- domly sampled 1000 sentences from the training set to serve as a development set. For these languages, we first trained the parser on the training set and then used the development set for hyperparameter tuning (|B|, s, α, β, and γ 1 , . . . , γ k for k order parsing). <ref type="bibr">34</ref> We employ four evaluation measures, where ev- ery measure is computed per language, and we re- port the average across all languages: (1) Unlabeled Attachment Score (UAS); (2) Undirected UAS (U- UAS) -for error analysis purposes; (3) Shared arcs (SARC) -the percentage of arcs shared by the pre- dictions of each of our algorithms and of the origi- nal TurboParser; and (4) Tokens per second (TPS) -for ensemble models this measure includes the TurboParser's inference time. <ref type="bibr">5</ref> We also report a gold(x,y) = (a,b) measure: where a is the percentage of gold standard arcs included in trees produced by algorithm x but not by y, and b is the corresponding number for y and x. We consider two setups. . <ref type="bibr">4</ref> The original TurboParser is trained on the training set of each language and tested on its test set, without any further di- vision of the training data to training and development sets. <ref type="bibr">5</ref> Run times where computed on an Intel(R) Xeon(R) CPU E5-2697 v3@2.60GHz machine with 20GB RAM memory.</p><p>Fully Supervised Training In this setup we only consider the 8 languages with a development set. For each language, the parser is trained on the train- ing set and then the hyperparameters are tuned. First we set the beam size (|B|) and number of improve- ment iterations (s) to 0, and tune the other hyperpa- rameters on the language-specific development set. Then, we tune |B| and s, using the optimal parame- ters of the first step, on the English dev. set.</p><p>Minimally Supervised Training Here we con- sider all 17 languages. For each language we ran- domly sampled 20 training sets of 500 sentences from the original training set, trained a parser on each set and tested on the original test set. Results for each language were calculated as the average over these 20 folds. The hyper parameters for all languages were tuned once on the English develop- ment set to the values that yielded the best average results across the 20 training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Results</head><p>Fully Supervised Training Average results for this setup are presented in table 1 (top). Unsur- prisingly, UAS for second order parsing with basic greedy inference (Algorithm 1, BGI) is very low, as this model does not take information about partial high order parts into account in its edge scores. We hence do not report more results for this algorithm.</p><p>The table further reflects the accuracy/runtime tradeoff provided by Algorithm 2 (basic greedy in- ference with partial part predictions, BGI-PP): a UAS degradation of 0.34% and 2.58% for first and second order parsing respectively, with a runtime improvement by factors of 1.01 and 2.4, respec- tively. Employing beam search and post search im- provements (BGI-PP+i+b) to compensate for error propagation improves UAS but harms the runtime gain: for example, the UAS gap in second order parsing is 1.01% while the speedup factor is 1.43.</p><p>As discussed in footnote 1 and Section 11, our algorithm does not support the third-order parts of the TurboParser. However, the average UAS of the third-order TurboParser is 90.62% (only 0.36% above second order TurboParser) and its TPS is 72.12 (almost 5 times slower).</p><p>The accuracy gaps according to UAS and undi- rected UAS are similar, indicating that the source  <ref type="table" target="#tab_2">Individual Models  Ensemble Models  UAS  TPS  SARC  U-UAS  UAS  TPS  SARC  U-UAS   TurboParser  order1  87.98 5621.30  - 88.82  - - - - order2  90.26 356.63  - 90.98  - - - -  BGI  order1  83.78 5981.91  90.87  90.87  - - - - order2</ref> 27  of differences between the parsers is not arc direc- tionality. The percentage of arcs shared between the parsers increases with model complexity but is still as low as 94.79% for BGI-PP+i+b in second or- der parsing. In this setup, gold(BGI-PP+i+b, Tur- boParser) = (1.6%,2.6%) which supports the devel- opment data pattern reported in Section 6 and further justifies an ensemble approach.</p><p>The right column section of the table indeed shows consistent improvements of the ensemble models over the TurboParser for second order pars- ing: the ensemble models achieve UAS of 90.5- 90.53% compared to 90.26% of the TurboParser. Naturally, running the TurboParser alone is faster by a factor of 1.67. Like for the individual inference algorithms, the undirected UAS measure indicates that the gain does not come from arc directionality improvements. The ensemble methods share almost all of their arcs with the TurboParser, but in cases of disagreement ensembles tend to be more accurate. <ref type="table" target="#tab_4">Table 2</ref> complements our results, providing UAS values for each of the 8 languages participat- ing in this setup. The UAS difference between BGI+PP+i+b and the TurboParser are (+0.24)-(- 0.71) in first order parsing and (+0.18)-(-2.46) in second order parsing. In the latter case, combining these two models (BGI+PP+i+b+e) yields improve- ments over the TurboParser in 6 out of 8 languages.</p><p>Minimally Supervised Training Results for this setup are in table 1 (bottom). While result pat- terns are very similar to the fully supervised case, two observations are worth mentioning. First, the percentage of arcs shared by our algorithms and the original parser is much lower than in the fully supervised case. This is true also for shared gold arcs: gold(BGI-PP+b+i,TurboParser) = (4.86%,5.92%) for second order parsing. This sug- gests that more sophisticated ensemble techniques may be useful in this setup.</p><p>Second, ensemble modeling improves UAS over the TurboParser also for first order parsing, lead- ing to a gain of 0.3% in UAS for the BGI+i+b ensemble (79.29% vs. 78.99%). As the percent- age of shared arcs between the ensemble mod- els and the TurboParser is particularly low in first order parsing, as well as the shared gold arcs  Best results for each language and parsing model order are highlighted in bold.</p><p>(gold(BGI+i+b,TurboParser) = (4.98%,5.5%)), im- proving the ensemble techniques is a promising fu- ture research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work</head><p>Our work brings together ideas that have been con- sidered in past, although in different forms.</p><p>Greedy Inference Goldberg and Elhadad <ref type="formula">(2010)</ref> introduced an easy-first, greedy, approach to depen- dency parsing. Their algorithm adds at each iteration the best candidate arc, in contrast to the left to right ordering of standard transition based parsers. This work is extended at ( <ref type="bibr" target="#b31">Tratz and Hovy, 2011;</ref><ref type="bibr" target="#b13">Goldberg and Nivre, 2013)</ref>. The easy-first parser consists of a feature set and a specialized variant of the structured perceptron training algorithm, both dedicated to greedy infer- ence. In contrast, we show that a variant of the Tur- boParser that employs Algorithm 2 for inference and is trained with its standard global training algorithm, performs very similarly to the same parser that em- ploys dual decomposition inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Propagation in Deterministic Parsing</head><p>Since deterministic algorithms are standard in transition-based parsing, the error-propagation prob- lem has been dealt with in that context. Various methods were employed, with beam search being a prominent idea ( <ref type="bibr" target="#b28">Sagae and Lavie, 2006;</ref><ref type="bibr" target="#b30">Titov and Henderson, 2007;</ref><ref type="bibr" target="#b33">Zhang and Clark, 2008;</ref><ref type="bibr" target="#b16">Huang et al., 2009;</ref><ref type="bibr" target="#b34">Zhang and Nivre, 2011;</ref><ref type="bibr" target="#b1">Bohnet and Nivre, 2012;</ref><ref type="bibr" target="#b6">Choi and McCallum, 2013b</ref>, inter alia).</p><p>Post Search Improvements Several previous works employed post-search improvements tech- niques. Like in our case, these techniques improve the tree induced by an initial, possibly more princi- pled, search technique through local, greedy steps. <ref type="bibr" target="#b22">McDonald and Pereira (2006)</ref> proposed to ap- proximate high-order graph-based non-projective parsing, by arc-swap iterations over a previously in- duced projective tree. <ref type="bibr" target="#b18">Levi et al. (2016)</ref> proposed a post-search improvements method, different than ours, to compensate for errors of their graph-based, undirected inference algorithm. Finally, <ref type="bibr" target="#b35">Zhang et al. (2014a)</ref> demonstrated that multiple random ini- tialization followed by local improvements with re- spect to a high-order parsing objective result in ex- cellent parsing performance. Their algorithm, how- ever, shouldbhhb employ hundreds of random ini- tializations in order to provide state-of-the-art re- sults.</p><p>Ensemble Approaches Finally, several previous works combined dependency parsers. These include <ref type="bibr" target="#b25">Nivre and McDonald (2008)</ref> who used the output of one parser to provide features for another, <ref type="bibr" target="#b33">Zhang and Clark (2008)</ref> that proposed a beam-search based parser that combines two parsers into a single sys- tem for training and inference, and <ref type="bibr" target="#b20">Martins et al. (2008)</ref> that employed stacked learning, in which a second predictor is trained to improve the perfor- mance of the first. Our work complements these works by integrating information from a pre-trained TurboParser in our algorithm at test time only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Discussion</head><p>We presented a greedy inference approach for graph-based, high-order, non-projective dependency parsing. Our experiments with 17 languages show that our simple and easy to implement algorithm is a decent alternative for dual-decomposition inference.</p><p>A major limitation of our algorithm is in-cluding information from parts that require a given set of arcs not to be included in the de- pendency tree (footnote 1). For example, the nextSibling((1, 2), (1, 5)) part of the TurboParser would fire iff the tree includes the arcs (1, 2) and (1, 5) but not the arcs (1, 3) and <ref type="bibr">(1,</ref><ref type="bibr">4)</ref>. In order to account for such parts, we should de- cide how to compute their probabilities and, addi- tionally, at which point they are considered part of the tree. We explored several approaches, but failed to improve our results. Hence, we did not experi- ment with the third-order TurboParser as all of its third-order parts contain "non-included" arcs. This is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Runtime Complexity Analysis</head><p>Here we analyze the complexity of our algorithms, denoting the maximal indegree of a vertex with n in .</p><p>Algorithm 1 Algorithm 1 consists of two nested loops (lines 2-k3) and hence lines 4-11 are repeated O (n × |E|) = O(n × n × n in ) times. At each repe- tition, loss (lines 6-10) and gain (lines 4-5) values are computed. Afterwards the graph's data struc- tures are updated (lines 13-16).</p><p>For every arc that we examine (line 3), there are O(n in ) lost arcs, as there are O(n in ) incoming arcs (set 1) and O(n in ) cycles to break (set 2). Since every lost arc translates to a set of lost parts, we can avoid repeating computations by storing the par- tial loss of every arc in a data structure (DS): e → part:e∈part w part . Now, instead of summing all the lost parts, (every edge participates in O(n k−1 in ) parts, <ref type="bibr">6</ref> thus there are O(n k in ) lost parts per added arc), we can sum only O(n in ) partial loss values. However, since some lost parts may contain an arc from set 1 and an arc from set 2, we need to sub- tract the values that were summed twice, this can be done in O(min{n k−1 in , n 2 in }) time by holding a sec- ond DS: e 1 × e 2 → part:e 1 ∈part∧e 2 ∈part w part . 7 In order to efficiently compute the gain values, we hold a mapping from arcs to the sum of weights of parts that can be completed in the current iteration by adding the arc to the tree. With this DS, gain val-ues can be computed in constant time. In total, the runtime of lines 4-11 is O(n in + min{n k−1 in , n 2 in }). The DSs are initialized in O(|parts| × k) time. Since every part is deleted at most once, and gets updated (its arcs are added to the tree) at most k times, the total DS update time is O(k × |parts|) = O(n k+1 in ). Thus algorithm 1 runs in O(|parts| × k + n 2 × n in × (n in + min{n k−1 in , n 2 in })) time. Algorithm 2 Algorithm 2 is similar in structure to Algorithm 1 but the loss and gain computations are more complex. To facilitate efficiency, we hold two DSs: (a) a mapping from arcs to the sum of lost parts values, which are now w part × P part for part ∈ parts; and (b) a mapping from arc pairs to the sum of part values for parts that contain both arcs. The loss and gain values can be computed, as above, in O(n in + min{n k−1 in , n 2 in }) time. The initialization of the DSs takes O(|parts| × k) time. In the i-th iteration we add e = (u, v) to T i , and remove the lostArcs from E. Every lost arc participates in O(n k−1 in ) parts, and we need to update O(k) entries for each lost part in DS(a) (as the value of the other arcs of that part should no longer account for that part's weight) and O(k 2 ) en- tries in DS (b). Thus, the total update time of the DSs is O(n k in × k 2 ) and the total runtime of Algo- rithm 2 is O(n k+1 in × k + n × (n × n in × (n in + min{n 2 in , n k−1 in })+n k in ×k 2 )). For unpruned graphs and k ≥ 2 this is equivalent to O(n k+1 ), the theo- retical runtime of the TurboParser's dual decompo- sition inference algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of an iteration of Algorithm 1 (lines 3-16)). See description in text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>loops (lines 2-3) and hence lines 4-11 are repeated O (n × |E|) = O(n × n × n in ) times. At each repe- tition, loss (lines 6-10) and gain (lines 4-5) values are computed. Afterwards the graph's data struc- tures are updated (lines 13-16). We define data structures (DSs) that keep our computations effi- cient. With these DSs the total runtime of lines 4-11 is O(n in + min{n k−1 in , n 2 in }). The DSs are initial- ized in O(|parts| × k) time and their total update time is O(k × |parts|) = O(n k+1 in ). Thus algo- rithm 1 runs in O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Algorithm 2 Algorithm 2 is similar in structure to Algorithm 1. The enhanced loss and gain computa- tions take O(n in + min{n k−1 in , n 2 in }) time. The ini- tialization of the DSs takes O(|parts| × k) time and their update time is O(n k in × k 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Our first order part weights are in [−9, 4], and second order part weights in [−3, 13]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Results for the fully supervised (top table) and minimally supervised (bottom table) setups. The left column 

section of each table is for individual models while the right column section is for ensemble models (Section 7). BGI-
PP is the basic greedy inference algorithm with partial part predictions, +i indicates post-search improvements and 
+b indicates beam search (Section 6). The Tokens per Second (TPS) measure for the ensemble models reports the 
additional inference time over the TurboParser inference. All scores are averaged across individual languages. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Per language UAS for the fully supervised setup. Model names are as in Table 1, 'e' stands for ensemble. 

</table></figure>

			<note place="foot" n="1"> More generally, a part is defined by two arc subsets, A and B, such that a part p belongs to a tree T if ∀e ∈ A : e ∈ T and ∀e ∈ B : e / ∈ T. In this paper we assume B = φ. Hence, we cannot experiment with the third order TurboParser as in all its third order parts B = φ. Also, when we integrate our algorithms into the second order TurboParser we omit the nextSibling part for which B = φ. For the original TurboParser to which we compare our results, we do not omit this part as it improves the parser&apos;s performance.</note>

			<note place="foot" n="6"> Assuming that a part is a connected component. 7 For first order parsing this is not needed; for second order parsing it is done in O(nin) time.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The third author was partly supported by a research grant from the Microsoft/Technion research center for electronic commerce: Context Sensitive Sen-tence Understanding for Natural Language Process-ing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aligning opinions: Cross-lingual opinion mining with dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Mariana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cláudia</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-CoNLL. Association for Computational Linguistics</title>
		<meeting>EMNLP-CoNLL. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Conll-x shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experiments with a higher-order projective dependency parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Carreras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccallum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with selectional branching</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shai ShalevShwartz, and Yoram Singer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Online passiveaggressive algorithms</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Journal of Research of the National Bureau of Standards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
	<note>Optimum branchings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient normal-form parsing for combinatory categorial grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An efficient algorithm for easy-first non-directional dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elhadad</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dynamic oracle for arc-eager dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training deterministic parsers with non-deterministic oracles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation-aware dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A non-monotonic arc-eager transition system for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bilingually-constrained (monolingual) shift-reduce parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Edgelinear first-order dependency parsing with undirected minimum spanning tree inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Effi</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacking dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>In EACL</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-projective dependency parsing using spanning tree algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Ribarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The conll 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL shared task session of EMNLP-CoNLL</title>
		<meeting>the CoNLL shared task session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Integrating graph-based and transition-based dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-08: HLT</title>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maltparser: A languageindependent system for data-driven dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atanas</forename><surname>Chanev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülsen</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetoslav</forename><surname>Marinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="95" to="135" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parse, price and cut-delayed column and row generation for graph based parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLPCoNLL</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A best-first probabilistic shift-reduce parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the COLING/ACL on Main conference poster sessions</title>
		<meeting>of the COLING/ACL on Main conference poster sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dependency parsing by belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast and robust multilingual dependency parsing with a generative latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A fast, accurate, non-projective, semantically-enriched parser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Open information extraction using wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing using beamsearch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with rich non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Greed is good if randomized: New inference for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Steps to excellence: Simple inference with refined scoring of dependency trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
