<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington Seattle</orgName>
								<address>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces the task of question-answer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb &quot;intro-duce&quot; in the previous sentence would be labeled with the questions &quot;What is in-troduced?&quot;, and &quot;What introduces some-thing?&quot;, each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for prede-fined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifier-based models for predicting which questions to ask and what their answers should be. Our results show that non-expert anno-tators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic role labeling (SRL) is the widely stud- ied challenge of recovering predicate-argument structure for natural language words, typically verbs. The goal is to determine "who does what to whom," "when," and "where," etc. ( <ref type="bibr" target="#b9">Johansson and Nugues, 2008)</ref>. However, this intuition is difficult to formalize and funda- mental aspects of the task vary across efforts, for example FrameNet ( <ref type="bibr" target="#b1">Baker et al., 1998</ref>) models a large set of interpretable thematic roles (AGENT, PATIENT, etc.) while PropBank ( <ref type="bibr" target="#b19">Palmer et al., 2005</ref>) uses a small set of verb-specific roles  (ARG0, ARG1, etc.). Existing task definitions can be complex and require significant linguistic ex- pertise to understand, 1 causing challenges for data annotation and use in many target applications. In this paper, we introduce a new question- answer driven SRL task formulation (QA-SRL), which uses question-answer pairs to label verbal predicate-argument structure. For example, for the sentence in <ref type="figure">Figure 1</ref>, we can ask a short ques- tion containing a verb, e.g. "Who finished some- thing?", and whose answer is a phrase from the original sentence, in this case "UCD." The answer tells us that "UCD" is an argument of "finished," while the question provides an indirect label on the role that "UCD" plays. Enumerating all such pairs, as we will see later, provides a relatively complete representation of the original verb's ar- guments and modifiers.</p><p>The QA-SRL task formulation has a number of advantages. It can be easily explained to non- expert annotators with a short tutorial and a few examples. Moreover, the formulation does not depend on any pre-defined inventory of semantic roles or frames, or build on any existing gram-mar formalisms. Nonetheless, as we will show, it still represents the argument and modifier attach- ment decisions that have motivated previous SRL definitions, and which are of crucial importance for semantic understanding in a range of NLP tasks, such as machine translation ( <ref type="bibr" target="#b15">Liu and Gildea, 2010)</ref> and coreference resolution <ref type="bibr" target="#b21">(Ponzetto and Strube, 2006</ref>). The annotations also, perhaps sur- prisingly, capture other implicit arguments that cannot be read directly off of the syntax, as was re- quired for previous SRL approaches. For example, in "It was his mother's birthday, so he was going to play her favorite tune", annotators created the QA pair "When would someone play something? His mother's birthday" which describes an im- plicit temporal relation. Finally, QA-SRL data can be easily examined, proofread, and improved by anyone who speaks the language and understands the sentence; we use natural language to label the structure of natural language.</p><p>We present a scalable approach for QA-SRL an- notation and baseline models for predicting QA pairs. Given a sentence and target word (the verb), we ask annotators to provide as many question- answer pairs as possible, where the question comes from a templated space of wh-questions 2 and the answer is a phrase from the original sen- tence. This approach guides annotators to quickly construct high quality questions within a very large space of possibilities. Given a corpus of QA- SRL annotated sentences, we also train baseline classifiers for both predicting a set of questions to ask, and what their answers should be. The ques- tion generation aspect of QA-SRL is unique to our formulation, and corresponds roughly to identify- ing what semantic role labels are present in pre- vious formulations of the task. For example, the question "Who finished something" in <ref type="figure">Figure 1</ref> corresponds to the AGENT role in FrameNet. Ta- ble 1 also shows examples of similar correspon- dences for PropBank roles. Instead of pre-defining the labels, as done in previous work, the questions themselves define the set of possibilities.</p><p>Experiments demonstrate high quality data an- notation with very little annotator training and es- tablish baseline performance levels for the task. We hired non-expert, part-time annotators on Up- work (previously oDesk) to label over 3,000 sen- tences (nearly 8,000 verbs) across two domains (newswire and Wikipedia) at a cost of approxi- mately $0.50 per verb. We show that the data is high quality, rivaling PropBank in many aspects including coverage, and easily gathered in non- newswire domains. <ref type="bibr">3</ref> The baseline performance levels for question generation and answering re- inforce the quality of the data and highlight the potential for future work on this task.</p><p>In summary, our contributions are:</p><p>• We introduce the task of question-answer driven semantic role labeling (QA-SRL), by using question-answer pairs to specify ver- bal arguments and the roles they play, without predefining an inventory of frames or seman- tic roles.</p><p>• We present a novel, lightweight template- based scheme (Section 3) that enables the high quality QA-SRL data annotation with very little training and no linguistic expertise.</p><p>• We define two new QA-SRL sub-tasks, ques- tion generation and answer identification, and present baseline learning approaches for both (Sections 4 and 5). The results demonstrate that our data is high-quality and supports the study of better learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The success of syntactic annotation projects such as the Penn Treebank ( <ref type="bibr" target="#b17">Marcus et al., 1993</ref>) has led to numerous efforts to create semantic annotations for large corpora. The major distinguishing fea- tures of our approach are that it is not tied to any linguistic theory and that it can be annotated by non-experts with minimal training. Existing SRL task formulations are closely re- lated to our work. FrameNet ( <ref type="bibr" target="#b1">Baker et al., 1998</ref>) contains a detailed lexicon of verb senses and the- matic roles. However, this complexity increases the difficulty of annotation. While the FrameNet project is decades old, the largest fully anno- tated corpus contains about 3,000 sentences ( <ref type="bibr" target="#b5">Chen et al., 2010)</ref>. We were able to annotate over 3,000 sentences within weeks. PropBank <ref type="bibr" target="#b11">(Kingsbury and Palmer, 2002</ref>), NomBank ( <ref type="bibr" target="#b18">Meyers et al., 2004</ref>) and OntoNotes ( <ref type="bibr" target="#b8">Hovy et al., 2006</ref>) circum- vent the need for a large lexicon of roles, by defin- QA-SRL</p><p>(1) Stock-fund managers , meantime , went into October with less cash on hand than they held earlier this year . A0 they Who had held something? Stock-fund managers / they AM-TMP year When had someone held something? earlier this year What had someone held? less cash on hand Where had someone held something? on hand (2) Mr. Spielvogel added pointedly : " The pressure on commissions did n't begin with Al Achenbaum . "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A0</head><p>Spielvogel ing the core semantic roles in a predicate-specific manner. This means that frames need to be created for every verb, and it requires experts to distin- guish between different senses and different roles.</p><p>Our work is also related to recent, more gen- eral semantic annotation efforts. Abstract Mean- ing Representation ( <ref type="bibr" target="#b2">Banarescu et al., 2013)</ref> can be viewed as an extension of PropBank with ad- ditional semantic information. Sentences take 8- 13 minutes to annotate-which is slower than ours, but the annotations are more detailed. Uni- versal Cognitive Conceptual Annotation (UCCA) <ref type="bibr" target="#b0">(Abend and Rappoport, 2013</ref>) is an attempt to cre- ate a linguistically universal annotation scheme by using general labels such as argument or scene. The UCCA foundational layer does not distin- guish semantic roles, so Frogs eat herons and Herons eat frogs will receive identical annotation -thereby discarding information which is po- tentially useful for translation or question answer- ing. They report similar agreement with Prop- Bank to our approach (roughly 90%), but an- notator training time was an order-of-magnitude higher (30-40 hours). The Groningen Meaning Bank ( <ref type="bibr" target="#b3">Basile et al., 2012</ref>) project annotates text by manually correcting the output of existing seman- tic parsers. They show that some annotation can be crowdsourced using "games with a purpose" -however, this does not include its predicate- argument structure, which requires expert knowl- edge of their syntactic and semantic formalisms. Finally, Reisinger et al. (2015) study crowdsourc- ing semantic role labels based on Dowty's proto- roles, given gold predicate and argument men- tions. This work directly complements our focus on labeling predicate-argument structure.</p><p>The idea of expressing the meaning of natural language in terms of natural language is related to natural logic ( <ref type="bibr" target="#b16">MacCartney and Manning, 2007)</ref>, in which they use natural language for logical in- ference. Similarly, we model predicate-argument structure of a sentence with a set of question- Example of Values No. Values WH* Question words (wh-words) who, what, when, where, why, how, how much 7 AUX Auxiliary verbs is, have, could, is n't 36 SBJ Place-holding words for the subject position someone, something 2 TRG* Some form of the target word built, building, been built ≈ 12 OBJ1</p><p>Place-holding words for the object position someone, something 2 PP Frequent prepositions (by, to, for, with, about) and prepositions (unigrams or bigrams) that oc- cur in the sentence to, for, from, by ≈ 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OBJ2</head><p>Similar to OBJ1, but with more options someone, something, do, do something, doing something 9  answer pairs. While existing work on natural logic has relied on small entailment datasets for train- ing, our method allows practical large-scale anno- tation of training data. Parser evaluation using textual entailment <ref type="bibr">(Yuret et al., 2010</ref>) is a method for evaluating syn- tactic parsers based on entailment examples. In a similar spirit to our work, they abstract away from linguistic formalisms by using natural lan- guage inference. We focus on semantic rather than syntactic annotation, and introduce a scal- able method for gathering data that allows both training and evaluation. <ref type="bibr">Stern and Dagan (2014)</ref> applied textual entailment to recognize implicit predicate-argument structure that are not explicitly expressed in syntactic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">QA-based Semantic Dataset</head><p>This section describes our annotation process in more detail, and discusses agreement between our annotations and PropBank. <ref type="table">Table 1</ref> shows exam- ples provided by non-expert annotators. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Annotation Task Design</head><p>We annotate verbs with pairs of questions and an- swers that provide information about predicate- argument structure. Given a sentence s and a ver- bal predicate v in the sentence, annotators must produce a set of wh-questions that contain v and whose answers are phrases in s.</p><p>To speed annotation and simplify downstream processing, we define a small grammar over possi- ble questions. The questions are constrained with a template with seven fields, q ∈ WH × AUX × SBJ × TRG × OBJ1 × PP × OBJ2, each asso- ciated with a list of possible options. Descriptions for each field are shown in <ref type="table" target="#tab_1">Table 2</ref>. The gram- mar is sufficiently general to capture a wide-range of questions about predicate-argument structure- some examples are given in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>The precise form of the question template is a function of the verb v and sentence s, for two of the fields. For the TRG field, we generate a list of inflections forms of v using the Wiktionary dictio- nary. For the PP field, the candidates are all the prepositions that occurred in the sentence s, and some frequently-used prepositions -by, to, for, with, and about. We also include preposition bi- grams (e.g., out for) from s.</p><p>Answers are constrained to be a subset of the words in the sentence but do not necessarily have to be contiguous spans. We also allow questions to have multiple answers, which is useful for annotat- ing graph structured dependencies such as those in examples 3 and 6 in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Preparation</head><p>We annotated over 3000 sentences (nearly 8,000 verbs) in total across two domains: newswire (PropBank) and Wikipedia.  2009), excluding questions and sentences with fewer than 10 words. For the Wikipedia do- main, we randomly sampled sentences from the English Wikipedia, excluding questions and sen- tences with fewer than 10 or more than 60 words.</p><p>In each sentence, we need to first identify the candidates for verbal predicates. In princi- ple, a separate stage of annotation could iden- tify verbs-but for simplicity, we instead used POS-tags. We used gold POS-tags for newswire, and predicted POS-tags (using Stanford tagger ( <ref type="bibr">Toutanova et al., 2003)</ref>) in Wikipedia. Annota- tors can choose to skip a candidate verb if they are unable to write questions for it. Annotators skipped 136 verbs (3%) in Wikipedia data and 50 verbs (1.5%) in PropBank data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Annotation Process</head><p>For annotation, we hired 10 part-time, non-expert annotators from Upwork (previously oDesk) and paid $10 per hour for their work. The average cost was $0.58 per verb ($1.57 per sentence) for newswire text and $0.45 per verb ($1.01 per sen- tence) on the Wikipedia domain. The annotators are given a short tutorial and a small set of sam- ple annotations (about 10 sentences). Annotators were hired if they showed good understanding of English and our task. The entire screening process usually took less than 2 hours.</p><p>Writing QA pairs for each sentence takes 6 min- utes on average for Wikipedia and 9 minutes on newswire, depending on the length and complex- ity of the sentence and the domain of the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Agreement with Gold PropBank Data (CoNLL-2009)</head><p>PropBank is the most widely used annotation of predicate-argument structure. While our anno- tation captures different information from Prop- Bank, it is closely related. To investigate the sim- ilarity between the annotation schemes, we mea- sured the overlap between the newswire domain All Roles Core Adjuncts Precision 81.4</p><p>85.9 59.9 Recall 86.3 89.8 63.6 (1241 sentences) of our QA-SRL dataset and the PropBank dataset.</p><p>For each PropBank predicate that we have an- notated with our scheme, we compute the agree- ment between the PropBank arguments and the QA-SRL answers. We ignore modality, reference, discourse and negation roles, as they are outside the scope of our current annotation. An annotated answer is judged to match the PropBank argument if either (1) the gold argument head is within the annotated answer span, or (2) the gold argument head is a preposition and at least one of its chil- dren is within the answer span.</p><p>We measure the macro-averaged precision and recall of our annotation against PropBank, with the proportion of our QA-pairs that are match a PropBank relation, and the proportion of Prop- Bank relations covered by our annotation. The re- sults are shown in <ref type="table" target="#tab_5">Table 5</ref>, and demonstrate high overall agreement with PropBank. Agreement for core arguments 5 is especially strong, showing much of the expert linguist annotation in Prop- Bank can be recovered with our simple scheme. Agreement for adjuncts is lower, because the an- notated QAs often contain inferred roles, espe- cially for why, when and where questions (See ex- amples 4, 7 and 8 in <ref type="table">Table 1</ref>). These inferred roles are typically correct, but outside of the scope of PropBank annotations; they point to exciting op- portunities for future work with QA-SRL data. On the other hand, the adverbial arguments in Prop- Bank are sometimes neglected by annotators, thus becoming a major source of recall loss. <ref type="table">Table 6</ref> shows the overlap between our anno- tated question words and PropBank argument la- bels. There are many unsurprising correlations- who questions are strongly associated with Prop-  <ref type="figure">Figure 2</ref>: Inter-annotator agreement measured on 100 newswire sentences and 108 Wikipedia sentences, comparing the total number of annotators to the number of unique QA pairs produced and the number of agreed pairs. A pair is considered agreed if two or more annotators produced it.  <ref type="table">Table 6</ref>: Co-occurrence of wh-words in QA-SRL annoations and role labels in PropBank.</p><p>Bank agents (A0), and where and when ques- tions correspond to PropBank temporal and loca- tive roles, respectively. Some types of questions are divided much more evenly among PropBank roles, such as How much. These cases show how our questions can produce a more easily inter- pretable annotation than PropBank labels, which are predicate-specific and can be difficult to un- derstand without reference to the frame files. Together, these results suggest that non-experts can annotate much of the information contained in PropBank, and produce a more easily interpretable annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Inter-Annotator Agreement</head><p>To judge the reliability of the data, we measured agreement on a portion of the data (100 sentences in the newswire domain and 108 sentences in the Wikipedia domain) annotated by five annotators.</p><p>Measuring agreement is complicated by the fact that the same question can be asked in multiple ways-for example "Who resigned?" and "Who resigned from something?"-and annotators may choose different, although usually highly overlap- ping, answer spans. We consider two QA pairs to be equivalent if (1) they have the same wh-word and (2) they have overlapping answer spans. In this analysis, Who and What are considered to be the same wh-word. <ref type="figure">Figure 2</ref> shows how the number of different QA pairs (both overall and agreed) increases with number of annotations. A QA pair is considered to be agreed upon if it is proposed by at least two of the five annotators. After five annotators, the num- ber of agreed QA pairs starts to asymptote. A sin- gle annotator finds roughly 80% of the agreed QA pairs that are found by five annotators, suggesting that high recall can be achieved with a single stage of annotation. To further improve precision, future work should explore a second stage of annotation where annotators check each other's work, for ex- ample by answering each other's questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Question Generation</head><p>Given a sentence s and a target verb v, we want to automatically generate a set of questions con- taining v that are answerable with phrases from s. This task is important because generating answerable questions requires understanding the predicate-argument structure of the sentence. In essence, questions play the part of semantic roles in our approach. <ref type="bibr">6</ref> We present a baseline that breaks down ques- tion generation into two steps: (1) we first use a classifier to predict a set of roles for verb v that are likely present in the sentence, from a small, heuristically defined set of possibilities and then (2) generate one question for each predicted role, using templates extracted from the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mapping Question Fields to Semantic Roles</head><p>To generate questions, we first have to decide the primary role we want to target; each question's an- swer is associated with a specific semantic role. For example, given the sentence UCD finished the 2006 championship and target verb finished, we could ask either: (Q1) Who finished something? or (Q2) What did someone finish?. Q1 targets the role associated with the person doing the finish- ing, while Q2 focuses on the thing being finished.</p><p>To generate high quality questions, it is also often necessary to refer to roles other than the primary role, with pronouns. For example, Q2 uses "some- one" to refer to the finisher.</p><p>Although it is difficult to know a priori the ideal set of possible roles, our baseline uses a simple discrete set, and introduces heuristics for identi- fying the roles a question refers to. The roles R include:</p><formula xml:id="formula_0">R ={R0, R1, R2, R2[p], w, w[p]}</formula><p>w ∈{Where, When, Why, How, HowMuch} p ∈Prepositions</p><p>We then normalize the annotated questions by mapping its fields WH, SBJ, OBJ1 and OBJ2 to the roles r ∈ R, using a small set of rules listed in <ref type="table">Table 7</ref>. In our example, the WH field of the Q1 (Who) and the SBJ of Q2 (someone) are both mapped to role R0. The WH of Q2 (What) and the OBJ1 of Q1 (something) are mapped to role R1. Some roles can be subclassed with prepositions. For example, the WH field of the question What did something rise from? is mapped to R2 <ref type="bibr">[from]</ref>.</p><p>In most cases, R0 is related to the A0/AGENT roles in PropBank/FrameNet, and R1/R2 are re- lated to A1/PATIENT roles. Since our questions are defined in a templated space, we are able to do</p><formula xml:id="formula_1">wh ∈ {Who, What} ∧ voice = active WH → R0 SBJ = φ OBJ1 → R1 OBJ2 → R2[p] WH → R1 SBJ → R0 OBJ1 = φ OBJ2 → R2[p] WH → R2[p] SBJ → R0 OBJ1 → R1 OBJ2 = φ wh ∈ {Who, What} ∧ voice = passive WH → R1 SBJ = φ OBJ1 → R2 OBJ2 → R2[p] WH → R2 SBJ → R1 OBJ1 = φ OBJ2 → R2[p]</formula><p>wh ∈ {When, Where, Why, How, HowMuch} ∧ voice = active WH → wh[p] <ref type="table">Table 7</ref>: Mapping question fields to roles in R.</p><formula xml:id="formula_2">SBJ → R0 OBJ1 → R1 OBJ2 = φ WH → wh SBJ → R0 OBJ1 → R1 OBJ2 → R2[p] wh ∈ {When, Where, Why, How, HowMuch} ∧ voice = passive WH → wh[p] SBJ → R1 OBJ1 → R2 OBJ2 = φ WH → wh SBJ → R1 OBJ1 → R2 OBJ2 → R2[p]</formula><p>The mapping is based on whether certain question fields are empty and the voice of the verb in the question (active or passive). φ indicates that a field is either an empty string or equals "do/doing". If a question is in passive voice and contains the preposition "by", then OBJ2 is tagged with R0 in- stead, as in What is built by someone?</p><p>this mapping heuristically with reasonable accu- racy. In the future, we might try to induce the set of possible roles given each target verb, follow- ing the semantic role induction work of <ref type="bibr">Titov and Klementiev (2012)</ref> and <ref type="bibr" target="#b13">Lang and Lapata (2011)</ref>, or use crowdsourcing to label proto-roles, follow- ing <ref type="bibr">Reisinger et al. (2015)</ref>.</p><p>Predicting Question Roles Given this space of possible roles, our first step in generation is to de- termine which roles are present in a sentence, and select the pronouns that could be used to refer to them in the resulting questions. We formulate this task as a supervised multi-label learning problem. We define the set of possible labels L by combin- ing the roles in R with different pronoun values:</p><formula xml:id="formula_3">L ={role:val | role ∈ R}</formula><p>val ∈{φ, someone, something, do something, doing something}</p><p>For example, to support the generation of the questions Who finished something? and What did someone finish?, we need to first predict the labels R0:someone and R1:something. Adjunct roles, such as When and How, always take an empty pro- noun value.  <ref type="table" target="#tab_1">Question  WH  SBJ  Voice  OBJ1  OBJ2  Who finished something?  R0  /  active  R1  /  What did someone finish?  R1  R0  active  /  /   Table 8</ref>: Example surface realization templates from abstract questions.</p><p>For each sentence s and verb v, the set of posi- tive training samples corresponds to the set of la- bels in the annotated questions, and the negative samples are all the other labels in L train , the sub- set of labels appeared in training data. <ref type="bibr">7</ref> We train a binary classifier for every label in L train using L2- regularized logistic regression by <ref type="bibr">Liblinear (Fan et al., 2008)</ref>, with hyper-parameter C = 0.1. Fea- tures of the binary classifiers are listed in <ref type="table">Table  10</ref>. For each sentence s and verb v in the test data, we take the k highest-scoring labels, and generate questions from these.</p><p>Question Generation After predicting the set of labels for a verb, we generate a question to query each role. First, we define the concept of an ab- stract question, which provides a template that specifies the role to be queried, other roles to in- clude in the question, and the voice of the verb. Abstract questions can be read directly from our training data.</p><p>We can map an abstract question to a sur- face realization by substituting the slots with the pronoun values of the predicted labels. Ta- ble 8 shows the abstract questions we could use to query roles R0 and R1; and the generated questions, based on the set of predicted labels {R0:someone, R1:something}.</p><p>Therefore, to generate a question to query a role r ∈ R, we simply return the most frequent ab- stract question that occurred in training data that matches the role being queried, and the set of other predicted labels.</p><p>Experiments Native English speakers manually evaluated 500 automatically generated questions (5 questions per verb). Annotators judged whether the questions were grammatical 8 and answerable from the sentence.</p><p>We evaluated the top k questions produced by Newswire Wikipedia Ans. Gram. Ans. Gram. prec@1 66.0 84.0 72.0 90.0 prec@3 51.3 78.7 53.3 86.0 prec@5 38.4 77. <ref type="bibr">2</ref> 40.0 82.0 <ref type="table">Table 9</ref>: Manual evaluation results for question generation in two domains, including the averaged number of distinct questions that are answerable given the sentence (Ans.) and the averaged num- ber of questions that are grammatical (Gram.).</p><p>our baseline technique. The results in <ref type="table">Table 9</ref> show that our system is able to produce questions which are both grammatical and answerable. The average number of QA pairs per verb collected by human annotator is roughly 2.5, demonstrating significant room for improving these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Answer Identification</head><p>The goal of the answer identification task is to pre- dict an answer a given sentence s, target verb v and a question q. Our annotated answers can be a series of spans, so the space of all possible an- swers is 2 |s| . To simplify the problem, we trans- form our span-based answer annotation to answer head words, thus reducing the answer space to |s|. We model whether a word is the head of an answer as a binary classification problem. Each training sample is a tuple s, v, q, a, ±1. The answer head a is extracted from the k-best de- pendency parses and the annotated answer span. Given a dependency tree, if any word in the an- notated answer span has a parent coming from outside the span, then it is considered an answer head. Therefore, a gold question-answer pair can be transformed into multiple positive training sam- ples. The negative samples come from all the words in the sentence that are not an answer head. For learning, we train a binary classifier for every word in the sentence (except for the verb v).</p><p>Experiments We use L2-regularized logistic re- gression by <ref type="bibr">Liblinear (Fan et al., 2008</ref>) for binary classification. Features are listed in <ref type="table">Table 10</ref>.</p><p>The performance of our answer identification approach is measured by accuracy. For evaluation, given each test sentence s, verb v and question q, we output the word with highest predicted score using the binary classifier. If the predicted word is contained inside the annotated answer span, it is considered a correct prediction. We also use the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Generation</head><p>Answer Identification Predicate Token, Predicted POS-tag, Lemma extracted from Wiktionary Dependency parent and edge label, dependency children and edge label Question Question role label, Wh-word, Preposition Answer Word / Syntactic parent and edge label, Left/Right-most syntactic children, Predicate-Answer / Relative position (left or right), Syntactic relation, Syntactic path <ref type="table">Table 10</ref>: Indicator features that are included in our role classifiers for question generation (Section 4) and the answer identification classifier (Section 5). Many come from previous work in SRL <ref type="bibr" target="#b9">(Johansson and Nugues, 2008;</ref><ref type="bibr">Xue and Palmer, 2004</ref>). To mitigate syntactic errors, we used 10-best dependency parses from the Stanford parser ( <ref type="bibr" target="#b12">Klein and Manning, 2003)</ref>.</p><p>Newswire Wikipedia Classifier 78.7 82.3 Random 26.3 26.9 baseline method that predicts a random syntactic child from the 1-best parse for each question.</p><p>In each of the two domains, we train the bi- nary classifiers on the training set of that domain (See <ref type="table" target="#tab_3">Table 4</ref> for dataset size). <ref type="table" target="#tab_8">Table 11</ref> shows experiment results for answer identification. Our classifier-based method outputs a correct answer head for 80% of the test questions, establishing a useful baseline for future work on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>We introduced the task of QA-SRL, where question-answer pairs are used to specify predicate-argument structure. We also presented a scalable annotation approach with high coverage, as compared to existing SRL resources, and intro- duced baselines for two core QA-SRL subtasks: question generation and answering.</p><p>Our annotation scheme has a number of advan- tages. It is low cost, easily interpretable, and can be performed with very little training and no lin- guistic expertise. These advantages come, in large part, from the relatively open nature of the QA- SRL task, which does not depend on any linguis- tic theory of meaning or make use of any frame or role ontologies. We are simply using natural lan- guage to annotate natural language.</p><p>Although we studied verbal predicate-argument structure, there are significant opportunities for fu- ture work to investigate annotating nominal and adjectival predicates. We have also made few language-specific assumptions, and believe the an- notation can be generalized to other languages- a major advantage over alternative annotation schemes that require new lexicons to be created for each language.</p><p>The biggest challenge in annotating sentences with our scheme is choosing the questions. We in- troduced a method for generating candidate ques- tions automatically, which has the potential to en- able very large-scale annotation by only asking the annotators to provide answers. This will only be possible if performance can be improved to the point where we achieve high recall question with acceptable levels of precision.</p><p>Finally, future work will also explore applica- tions of our annotation. Most obviously, the anno- tation can be used for training question-answering systems, as it directly encodes question-answer pairs. More ambitiously, the annotation has the potential to be used for training parsers. A joint syntactic and semantic parser, such as that of <ref type="bibr" target="#b14">Lewis et al. (2015)</ref>, could be trained directly on the annotations to improve both the syntactic and semantic models, for example in domain transfer settings. Alternatively, the annotation could be used for active learning: we envisage a scheme where parsers, when faced with ambiguous attach- ment decisions, can generate a human-readable question whose answer will resolve the attach- ment. <ref type="bibr">Durme. 2015</ref>. Semantic proto-roles. Transac- tions of the Association for Computational <ref type="bibr">Linguistics, 3:475</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>UCD finished the 2006 championship as Dublin champions , by beating St Vincents in the final .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: QA-SRL annotations for a Wikipedia sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FieldDescription</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Fields in our question annotation template, with descriptions, example values, and the total 
number of possible values for each. WH* and TRG* are required; all other fields can be left empty. 

WH* 
AUX 
SBJ 
TRG* 
OBJ1 
PP 
OBJ2 
Who 
built 
something 
? 
What 
had 
someone 
said 
? 
What 
was 
someone 
expected 
to 
do 
? 
Where might something rise 
from 
? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Four example questions written with our question annotation template.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 shows</head><label>4</label><figDesc></figDesc><table>the 
full data statistics. In the newswire domain, 
we sampled sentences from the English training 
data of CoNLL-2009 shared task (Hajič et al., </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Annotated data statistics. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Agreement with gold PropBank (CoNLL-
2009) for all roles, core roles, and adjuncts. Preci-
sion is the percentage of QA pairs covering exactly 
one PropBank relation. Recall is the percentage 
of PropBank relations covered by exactly one QA 
pair. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>of QA Pairs per Predicate Average Number of QA Pairs vs. Number of Annotators on Newswire Dataof QA Pairs per Predicate Average Number of QA Pairs vs. Number of Annotators on Wikipedia Data Average Number of QA Pairs Average Number of Agreed QA Pairs</head><label></label><figDesc></figDesc><table>1 

2 
3 
4 
5 
2 

2.2 

2.4 

2.6 

2.8 

3 

3.2 

3.4 

3.6 

Number of Annotators 
Average Number Average Number of QA Pairs 
Average Number of Agreed QA Pairs 

1 
2 
3 
4 
5 
2 

2.2 

2.4 

2.6 

2.8 

3 

3.2 

3.4 

3.6 

Number of Annotators 
Average Number </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Answer identification accuracy on 
newswire and Wikipedia text. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>- 488 .</head><label>488</label><figDesc></figDesc><table>Josef Ruppenhofer, Michael Ellsworth, Miriam RL 
Petruck, Christopher R Johnson, and Jan Schef-
fczyk. 2006. Framenet ii: Extended theory and 
practice. 

Asher Stern and Ido Dagan. 2014. Recognizing im-
plied predicate-argument relationships in textual in-
ference. Proceedings of the 52nd Annual Meeting of 
the Association for Computational Linguistics: Hu-
man Language Technologies. 

Ivan Titov and Alexandre Klementiev. 2012. A 
bayesian approach to unsupervised semantic role in-
duction. In Proceedings of the 13th Conference of 
the European Chapter of the Association for Com-
putational Linguistics, pages 12-22. Association for 
Computational Linguistics. 

Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network. 
In Proceedings of the 2003 Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology, 
volume 1, pages 173-180. Association for Compu-
tational Linguistics. 

Nianwen Xue and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. In Proceedings 
of the 2004 Conference on Empirical Methods in 
Natural Language Processing, pages 88-94. 

Deniz Yuret, Aydin Han, and Zehra Turgut. 2010. 
Semeval-2010 task 12: Parser evaluation using tex-
tual entailments. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages 
51-56. Association for Computational Linguistics. </table></figure>

			<note place="foot" n="1"> The PropBank annotation guide is 89 pages (Bonial et al., 2010), and the FrameNet guide is 119 pages (Ruppenhofer et al., 2006). Our QA-driven annotation instructions are 5 pages.</note>

			<note place="foot" n="2"> Questions starting with a wh-word, such as who, what, when, how, etc.</note>

			<note place="foot" n="3"> Our hope is that this approach will generalize not only across different domains in English, as we show in this paper, but also to other languages. We will leave those explorations to future work.</note>

			<note place="foot" n="4"> Our dataset is freely available at: https://dada.cs.washington.edu/qasrl .</note>

			<note place="foot" n="5"> In PropBank, A0-A5 are the core arguments. In QASRL, the core arguments include QA pairs with a question that starts with Who or What.</note>

			<note place="foot" n="6"> The task also has applications to semi-automatic annotation of sentences with our scheme, if we could generate questions with high enough recall and only require annotators to provide all the answers. We leave this important direction to future work.</note>

			<note place="foot" n="7"> We pruned the negative samples that contain prepositions that are not in the sentence or in the set of frequently-used prepositions (by, to, for, with, about). 8 Some automatically generated questions are ungrammatical because of label prediction errors, such as Who sneezed someone?, where the label R1:someone shouldn&apos;t be predicted.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the NSF (IIS-1252835), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We are grateful to Kenton Lee and Mark Yatskar for evaluating the question gener-ation task, and Eunsol Choi, Yejin Choi, Chloé Kiddon, Victoria Lin, and Swabha Swayamdipta for their helpful comments on the paper. We would also like to thank our freelance workers on oDesk/Upwork for their annotation and the anony-mous reviewers for their valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Universal conceptual cognitive annotation (ucca)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="228" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Collin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John B</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Computational Linguistics</title>
		<meeting>the 17th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linguistic Annotation Workshop</title>
		<meeting>the Linguistic Annotation Workshop<address><addrLine>Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Developing a large semantically annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Evang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noortje</forename><surname>Venhuizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Language Resources and Evaluation</title>
		<meeting>the 2012 International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3196" to="3200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Propbank annotation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Babko-Malaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Center for Computational Language and Education Research ; CU-Boulder</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semafor: Frame argument resolution with log-linear models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
		<meeting>the 5th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="264" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lluís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Padó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaň</forename><surname>Stěpánek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning: Shared Task</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ontonotes: the 90% solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Nugues</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dependency-based semantic role labeling of propbank</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From treebank to propbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Conference on Language Resources and Evaluation</title>
		<meeting>the 2002 International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate unlexicalized parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised semantic role induction via split-merge clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1117" to="1126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint a* ccg parsing and semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic role features for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="716" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural logic for textual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The nombank project: An interim report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Reeves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Macleod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Zielinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL 2004 workshop: Frontiers in corpus annotation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="24" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The proposition bank: An annotated corpus of semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="106" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Human Language Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="103" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Simone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ponzetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Drew</forename><surname>Reisinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<imprint>
			<pubPlace>Craig Harman, Kyle Rawlins, and Benjamin Van</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
