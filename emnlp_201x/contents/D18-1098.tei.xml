<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Topic Intrusion for Automatic Topic Model Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shraey</forename><surname>Bhatia</surname></persName>
							<email>shraeybhatia@gmail.com, jeyhan.lau@gmail.com, tb@ldwin.net</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computing and Information Systems</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Topic Intrusion for Automatic Topic Model Evaluation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="844" to="849"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>844</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Topic coherence is increasingly being used to evaluate topic models and filter topics for end-user applications. Topic coherence measures how well topic words relate to each other, but offers little insight into the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task-the task of guessing an outlier topic given a document and a set of topics-and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Topic models have traditionally been evaluated us- ing model perplexity, but there is an increasing trend to use topic coherence as a task-independent evaluation <ref type="bibr" target="#b11">(Newman et al., 2010;</ref><ref type="bibr" target="#b10">Mimno et al., 2011;</ref><ref type="bibr" target="#b0">Aletras and Stevenson, 2013;</ref><ref type="bibr" target="#b9">Lau et al., 2014;</ref><ref type="bibr" target="#b14">Röder et al., 2015)</ref>. In earlier work <ref type="bibr" target="#b1">(Bhatia et al., 2017)</ref>, we showed that topic coherence as a standalone evaluation can be misleading, which we illustrated with an adversarial topic model that produces highly coherent topics that collectively tell us little about the content of the document col- lection.</p><p>We went on to explore an alternative approach to assessing topics using topic intrusion, based on the manual task of <ref type="bibr" target="#b7">Chang et al. (2009)</ref>. In the original topic intrusion setup, users are presented with a document, a set of associated topics (from a topic model) and an intruder topic, and are tasked to find the intruder. Success in the task demonstrates that the topics learnt by the topic model are relevant to the document. We proposed a method to automate this ( <ref type="bibr" target="#b1">Bhatia et al., 2017)</ref>, by training a support vec- tor regression model based on information retrieval (IR) and word co-occurrence features to predict the intruder topic.</p><p>Although our earlier method is able to distin- guish between good and bad topic models (at the model-level), we provided no evaluation at the doc- ument level other than the observation that "there are still slight disparities between human annota- tors and the automated method in intruder topic selection". Additionally, the method involves a number of dependencies on complex external sys- tems such as Indri, and no implementation of the method was ever released. In this paper, we extend our earlier work ( <ref type="bibr" target="#b1">Bhatia et al., 2017)</ref> as follows:</p><p>(1) we improve the results based on a novel neural model and provide additional analysis of document- level evaluation via mean-absolute-error; (2) we propose a new metric to measure the performance of the system; and (3) we release an open source implementation of our system. 1 <ref type="bibr" target="#b7">Chang et al. (2009)</ref> introduced the word and topic intrusion tasks to assess topic models. Since then, various automatic measures to assess topics have been proposed <ref type="bibr" target="#b11">(Newman et al., 2010;</ref><ref type="bibr" target="#b10">Mimno et al., 2011;</ref><ref type="bibr" target="#b0">Aletras and Stevenson, 2013)</ref>. <ref type="bibr" target="#b9">Lau et al. (2014)</ref> compared and contrasted these approaches, and proposed a variant method based on normalised pointwise mutual information. <ref type="bibr">Röder et al. (2015)</ref> conducted a systematic search using a framework that combines various existing measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref>, we revisited the topic intrusion task of <ref type="bibr" target="#b7">Chang et al. (2009)</ref>, and explored its viability as an alternative task-independent ap- proach for topic model evaluation. We tested a number of topic models and found that there can be large discrepancies between conventional topic coherence measures and topic intrusion results, sug- gesting that topics can be individually coherent but poor descriptors of the documents. In addition, we proposed a method to automate the topic intrusion task and reported encouraging correlation levels with human judgements for model-level evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets and Topic Models</head><p>We conduct our experiments using the datasets of <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref>: (1) APNEWS, a collec- tion of Associated Press news articles; and (2) the British National Corpus ("BNC": Burnard (1995)), made up of excerpts from diverse sources such as journals, books, letters, and articles. For the topic models we experiment with the following: standard LDA (lda: <ref type="bibr" target="#b3">Blei et al. (2003)</ref>), corre- lated topic model (ctm: <ref type="bibr" target="#b2">Blei and Lafferty (2006)</ref>), non-parametric topic model (hca: <ref type="bibr" target="#b4">Buntine and Mishra (2014)</ref>), neural topic model (ntm: <ref type="bibr" target="#b6">Cao et al. (2015)</ref>), and an adversarial topic model (cluster: <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref>). cluster is adversarial in the sense that it is designed to pro- duce topics that are coherent but poor descriptors of documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we briefly describe the topic intru- sion task and propose an improved methodology to automate it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task</head><p>Chang et al. (2009) first proposed the topic intru- sion task with the aim of assessing whether topics associated with a document capture its content. In this task, an annotator is presented with a docu- ment along with its top-3 highest probability topics and a low probability intruder topic, and are asked to identify the outlier intruder topic. <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref> incorporate an additional constraint: the in- truder topic has to have high probability for at least one other document. Their rationale is to ensure that the intruder is interpretable. We follow the approach of our earlier work ( <ref type="bibr" target="#b1">Bhatia et al., 2017)</ref> when generating intruder topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human Judgements</head><p>To assess our methodology, we need human anno- tations for the topic intrusion task. We collect hu- man judgements using Amazon Mechanical Turk. Each HIT is comprised of 5 documents, and each document is paired with 4 topics (3 real and 1 in- truder). To control for annotation quality, an addi- tional document-topics pair is inserted as part of the HIT. The control item's intruder topic is gener- ated by randomly sampling words from the corpus vocabulary. To pass the quality control, an anno- tator has to select the correct intruder topic; they are filtered out if their pass rate over all controls is &lt; 0.6. <ref type="bibr">2</ref> Each HIT is judged by 10 workers. We col- lect additional annotations by releasing the task internally to a small number of local workers. We needed to carry out some annotations internally to make sure that each HIT had at least 4 anno- tations. The average number of internal annota- tions was approximately 1.6. For each topic model, we collected annotations for 100 documents on 2 corpora (5 topic models × 100 documents × 2 collections = 1000). After filtering and including internal judgements we have an average of 6.7 and 6.9 annotations for APNEWS and BNC, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Intruder Topic Detection</head><p>We propose a neural network model to automati- cally predict intruder topics. Our model is inspired by <ref type="bibr" target="#b15">Severyn and Moschitti (2015)</ref>, where they com- bine a learn-to-rank deep learning architecture in an IR setting to rank the documents for a given query. We adapt it to our topic intrusion task by ranking topics for a given document. Our task takes the form of a document d i with correspond- ing topics T i = {t 1 i , t 2 i , t 3 i , t 4 i }, where 3 topics are real and 1 is the intruder. The topic set T i has la- bels Y i = {y 1 i , y 2 i , y 3 i , y 4 i } ("1" denotes the intruder topic, or "0" otherwise). We train using a point- wise ranking approach, where training examples  <ref type="figure">Figure 2</ref>: mp GOLD vs. System Scores at the model level are triples of (d i , t j i , y j i ) -essentially the task is formulated as a binary classification problem.</p><p>The architecture of our network is given in <ref type="figure" target="#fig_0">Fig- ure 1</ref>. The input to our model is a document-topic pair, with each represented as a sequence of words. These words are mapped to embeddings, via em- bedding matrix W ∈ R |V |×d , where V is the vocab- ulary and d the dimensionality of the embeddings. The document embeddings E d ∈ R k×d (k = doc- ument length) and topic embeddings E t ∈ R m×d (m = number of topic words) are processed via convolutional layers <ref type="bibr" target="#b8">(Kim, 2014;</ref><ref type="bibr" target="#b15">Severyn and Moschitti, 2015</ref>) to produce two hidden representations for the document and topic. The convolution oper- ation is performed using feature maps of varying size followed by a max-pooling operation to pro- duce a constant-length vector. The document and topic hidden representations are concatenated and fed to 2 dense layers and ultimately reduced to a sigmoid-activated score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">External IR Feature</head><p>A good topic model learns common themes in the document collection. A limitation of our network is the lack of global-or collection-level informa- tion (as the input consists of only a document and topic). To incorporate collection-level information, we include an IR feature where we query document d i using the topic words of t j i . We use Okapi BM25 ( <ref type="bibr" target="#b13">Robertson and Walker, 1994)</ref> to compute the rel- evance score of the document with respect to its N topic words independently, thereby constructing an N -dimensional feature vector. <ref type="bibr">3</ref> This external feature vector is incorporated into the network after the convolutional layers (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>3 N = 5 in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Aggregating Human and System Scores for a Document</head><p>For each document we have a number of workers identifying the intruder topic. To aggregate the results, <ref type="bibr" target="#b7">Chang et al. (2009)</ref> define model precision (mp GOLD ), which is the proportion of workers who correctly identified the intruder, as a proxy for how clearly the intruder topic is inappropriate for the document.</p><p>Our system and that of <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref> com- pute several scores for a document (one for each topic). <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref> select the topic with the maximum score as the intruder, and compute model precision (mp) based on that. This yields binary precision scores (i.e. the model either predicts the intruder correctly or not) and ignores the relative magnitude of the system score. We additionally propose using the normalised sigmoid score (nss) as a means of scoring the intruder topic for a given document, which is computed by normalising the raw sigmoid scores over all topics.  <ref type="table">Table 2</ref>: Examples of best topics (top-half) and worst topics (bottom-half) based on nss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Implementation Details</head><p>For our experiments, we train the model on outputs from all topics models over one dataset, and test it on the other (cross-domain training). We use a single channel for the convolutional networks, pad the documents as necessary (k = 200), and use the top-10 words to represent a topic (i.e. m = 10). Word embeddings are initialised using pre-trained GloVe ( <ref type="bibr" target="#b12">Pennington et al., 2014</ref>) vectors (d = 100), and their weights are fixed during training. We use kernel windows of width = {3, 5, 7} with 100 fea- ture maps each and two (fully-connected) hidden layers, with dimensionality of 50 and 10. We use a dropout rate of 0.5, 0.5 and 0.25 after the docu- ment, topic and first hidden layer, respectively. We set the batch size to 100, and use Adam as the opti- mizer with a learning rate of 0.001. For activation functions, we use ReLU for the fully-connected layers and sigmoid for the final layer. To reduce variance, we run the models with 8 different seeds for initialisation and take the average for a topic's sigmoid score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>By taking the mean of mp GOLD and mp over doc- uments, <ref type="bibr" target="#b1">Bhatia et al. (2017)</ref> compute a single human/system score for each topic model. Al- though this resulted in a strong correlation between mp GOLD and mp, the evaluation is limited to model- level comparison: it separates good topic models from bad topic models, but does not provide any insights into the performance of each top model over individual documents. We aim to improve model-level correlation in this work, in addition to analysing document-level evaluation, i.e. investi- gating how well the system predicts mp GOLD for each individual document. We present plots of human and system scores in <ref type="figure">Figure 2</ref>. There are 3 system scores: mp of Bha- tia et al. (2017) (mp ORIG ), and mp and nss of our proposed system. In general, we found strong cor- relation for all systems, but nss of our proposed system performs substantially better than mp ORIG , though our mp is lower than mp ORIG .</p><p>To compare the performance of our system with human judgements at the document level, we com- pute mean absolute error (mae) between mp GOLD and nss/mp, as summarised in <ref type="table" target="#tab_2">Table 1</ref>. We find for both datasets nss consistently outperforms mp ORIG and mp by a substantial margin, and also has a score close to human judgements. We can attribute this to the fact that nss provides more nuanced system predictions (over the full range <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>), whereas mp tends to be binary. <ref type="bibr">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>One motivation we have in this paper is to test whether topic intrusion can be used as an alternative means for assessing topics. Given the encouraging mae results, we attempt to use nss to rank topics produced by a topic model.</p><p>To accomplish this, we first filter out the topics that occur in less than 5 documents as top 1-topic: these topics tend to be noisy, and as such do not ap- pear with significant weight in any documents. For each of the filtered topics we randomly select 5-10 documents for which it is a top topic and calculate its mean nss over these documents. We then use the topics' mean nss to rank them; in <ref type="table">Table 2</ref> we show some selected best and worst topics for differ- ent topic models. Overall, the top-ranked topics ap- pear to be more descriptive than the bottom-ranked topics. Having said that, we found instances where coherent topics have low nss ranking (e.g. ctm topics in the bottom half of <ref type="table">Table 2</ref>), but stress that ultimately the topic intrusion approach to assess- ing topics is very different to topic coherence. We include a more comprehensive list of best/worst topics in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We explore an alternative approach to evaluate topic models based on topic allocations in docu- ments, i.e. via topic intrusion. We propose an auto- mated method that improves upon the state-of-the- art substantially at the model-and document-levels, and demonstrate that it can be used to rank/filter topics. As future work we intend to explore ways that combine both the topic coherence and topic intrusion for topic model evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture diagram of our method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>mae between mp GOLD and nss/mp. "BNC → APNEWS" means the model is trained on BNC and tested on APNEWS. Boldface indicates optimal performance for each dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Model Best/Worst Topics nss lda share revenue cents billion quarter earnings analysts net rose income 0.001 european greece europe billion debt country crisis minister french france 0.002</figDesc><table>ctm 
building lodge bauer buildings fee part stephens hall property council 
0.007 
military army afghanistan killed soldiers forces troops iraq war attacks 
0.013 

hca 
shares earnings keywords insights profit thomson cents reuters premarket net 
0.011 
upheld ruling appeals justices appellate supreme injunction plaintiffs unconstitutional rulings 
0.051 

ntm 
rose shrank pct decliners quadrupled exhibitors parade spectrum index outperform 
0.110 
arraigned burglarizing arrested bigamy detectives motorcyclist arraignment coroner accomplice fondled 
0.141 

cluster 
soared plummeted climbed surged dipped tumbled dropped fell slipped rose 
0.005 
students teachers kindergarten tutors elementary coursework curriculum teaching tutoring education 
0.013 

lda 
lot good things long put start number making kind place 
0.291 
political issue called issues policy decision long change statement support 
0.271 

ctm 
online information internet book video media facebook phone computer technology 
0.263 
show music film movie won festival tickets game band play 
0.233 

hca 
richter riverboat sheppard lander plazas tam mandarin amarillo colosseum nassau 
0.376 
deplorable interaction foresee envelope handwriting knot quickest scrambled alarmed mum 
0.368 

ntm 
aboard spacewalks bushels budget lifeboats flotilla lifeboat spacewalk millage spaceflight 
0.364 
evacuated evacuations evacuate evacuating airlifted twisters aftershocks evacuation driest barricaded 
0.323 

cluster 
accord delegations accords cooperation consultations negotiators negotiation committees intergovernmental negotiations 0.323 
summaries summary critiques excerpts articles responses quotes references descriptions critique 
0.309 

</table></figure>

			<note place="foot" n="1"> Source code and dataset can be downloaded at: https: //github.com/sb1992/Topic-Intrusion-forAutomatic-Topic-Model-Evaluation.</note>

			<note place="foot" n="2"> We fixed the threshold to 0.6 based on preliminary experiments. We found that it was a challenging task, and this value provides quality without filtering out most of the workers.</note>

			<note place="foot" n="4"> Strictly speaking, it is continuous as it is averaged over the runs for the multiple random seeds, but in general, it tends to be (close to) 0 or 1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluating topic coherence using distributional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Computational Semantics (IWCS-10)</title>
		<meeting>the Tenth International Workshop on Computational Semantics (IWCS-10)<address><addrLine>Potsdam, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An automatic approach for document-level topic model evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shraey</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experiments with non-parametric topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Buntine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">User guide for the British National Corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename><surname>Burnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2015</title>
		<meeting>AAAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2210" to="2216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading tea leaves: How humans interpret topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21 (NIPS-09)</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2014</title>
		<meeting>EACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="530" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimizing semantic coherence in topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Talley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Leenders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic evaluation of topic coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jey</forename><forename type="middle">Han</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Grieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-Poisson Model for probabilistic weighted retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;94)</title>
		<meeting>17th International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR&apos;94)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring the space of topic coherence measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Röder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Both</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hinneburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining (WSDM 2015)</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining (WSDM 2015)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
