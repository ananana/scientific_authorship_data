<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuening</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California Los Angeles</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3761" to="3771"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3761</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Previous work on grounded language learning did not fully capture the semantics underlying the correspondences between structured world state representations and texts, especially those between numerical values and lexical terms. In this paper, we attempt at learning explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of values and texts. We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced annotations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework. 1</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The meaning of natural language should always be accompanied by a context. Grounded lan- guage acquisition aims at learning the meaning of language in the context of an observed world state. A solution framework typically addresses the following subproblems: segmenting the text into meaningful phrasal units, determining which world state information is being referred to, and finding proper alignments from these units to the events of values in the world state.</p><p>The task has attracted much attention from the NLP community with a special focus on align- ing text descriptions onto processed, structured event records <ref type="bibr" target="#b36">(Snyder and Barzilay, 2007;</ref><ref type="bibr" target="#b21">Liang et al., 2009;</ref><ref type="bibr" target="#b14">Hajishirzi et al., 2011</ref>). Various sta- tistical models have been proposed, attempting at * Contribution during internship at Microsoft Research Asia. <ref type="bibr">1</ref> Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.  characterizing the interaction between text spans and categorical values (e.g., direction='East') or strings (e.g., person names). The previously ad- dressed term semantic correspondences narrowly describes the process of aligning natural language spans to different data fields. However, there still exists a gap between align- ment results and the underlying semantics. People tend to use various phrases to describe information that are inferred from different amounts of numer- ical values in a data field, or values derived from additional operations over fields. Consider the ex- ample description for a basketball game shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The phrase edged out in the first sen- tence implies the fact that the Toronto Raptors had beaten their opponent by a relatively narrow mar- gin. This could only be derived from an operation of subtraction between two scores that correspond to the field PTS for both teams in the event table, which leads to a relatively small difference of only four points. Previous efforts on learning seman- tic correspondences relying on categorical distri- butions ( <ref type="bibr" target="#b21">Liang et al., 2009</ref>) or string pattern fea- tures ( <ref type="bibr" target="#b15">Hajishirzi et al., 2012;</ref><ref type="bibr" target="#b19">Koncel-Kedziorski et al., 2014</ref>) do not have the capability to accu- rately capture numerical information, especially for the part that does not appear explicitly in the table and needs to be inferred.</p><p>Such kind of language grounding is important both for natural language understanding and for natural language generation. For language un- derstanding, establishing explicit connections be- tween symbols and values beyond ungrounded symbolic meaning representations will be useful for acquisition and inference of numerical com- monsense ( <ref type="bibr" target="#b28">Narisawa et al., 2013</ref>). For language generation, properly aligned information is the key to acquiring patterns of various lexical choices un- der different world states <ref type="bibr" target="#b33">(Roy and Reiter, 2005)</ref>.</p><p>In this work, we make a step towards more explicit semantic correspondences between struc- tured data and texts. Rather than only produc- ing coarse alignments between data fields and text spans, we try to detect the latent semantics under- lying these alignments by prompting explicit se- mantic annotations. We make the first attempt at utilizing publicly available datasets originally pre- pared for data-to-text language generation to pro- duce such annotations for words and phrases in natural language without additional supervision.</p><p>Specifically, we conduct our study on a recently released dataset of descriptions for NBA basket- ball games with structured tables of game records. Different from a few popular datasets that have been well conjectured to be produced from rules <ref type="bibr" target="#b32">(Reiter, 2017)</ref>, the summaries are all written by humans. The text contains some numbers and proper nouns, which are easier to establish corre- spondence with data. However, the majority of texts contain many informative words, some of which need to be inferred indirectly from various types of values in the data cells. We want to estab- lish explicit correspondences for them.</p><p>We derive a set of semantic labels from orig- inal data fields (Sec 4.1). These labels could be executed to establish direct correspondences to one or more values in the structured table. No annotation on the original dataset means unsu- pervised learning from weak distant supervision should be conducted. We design a semi-hidden Markov model to address this problem (Sec 4.2), which could align a semantic label to a word span. In the model, we leverage continuous probabil- ity distributions to model the correspondences be- tween numerical values and lexical terms, which has not been well captured in previous work. To address the emerged issue of "garbage collection" that commonly appears in statistical alignment models (Sec 4.5), we add a soft statistical con- straint via posterior regularization ( . As a by-product, we also show how the derived semantic annotations could be used to in- duce descriptive templates for data-to-text genera- tion (Sec 5). Experimental results (Sec 6) suggest the feasibility of the setting in this study, and show the effectiveness of our proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Grounded language acquisition has aroused wide interest in various disciplines <ref type="bibr" target="#b35">(Siskind, 1996;</ref><ref type="bibr" target="#b38">Yu and Ballard, 2004;</ref><ref type="bibr" target="#b12">Gorniak and Roy, 2007;</ref><ref type="bibr" target="#b39">Yu and Siskind, 2013;</ref><ref type="bibr" target="#b8">Chrupała et al., 2015)</ref>. Later work in the community of natural language pro- cessing also moved in this direction by relax- ing the amount of supervision to enable a model to learn from ambiguous alignments <ref type="bibr" target="#b17">(Kate and Mooney, 2007;</ref><ref type="bibr" target="#b6">Chen and Mooney, 2008)</ref>. Some research aimed at establishing coarse alignments between simulated robot soccer game records and commentary sentences <ref type="bibr" target="#b6">(Chen and Mooney, 2008;</ref><ref type="bibr" target="#b5">Chen et al., 2010;</ref><ref type="bibr" target="#b3">Bordes et al., 2010;</ref><ref type="bibr" target="#b14">Hajishirzi et al., 2011</ref>). For weather forecast domain, <ref type="bibr" target="#b21">Liang et al. (2009)</ref> used a hierarchical hidden Markov model in order to map utterances to world states, which coped with segmentation and alignment to- gether. More recently, <ref type="bibr" target="#b19">Koncel-Kedziorski et al. (2014)</ref> tried to obtain the correspondences be- tween real commentaries and structured football (soccer) events in multiple resolutions. We are distinct from this line of work in the fact that we aim at producing explicit semantic annotations that could capture information from structured ta- bles. To achieve this goal, we need additional scal- ing or operations to enable data fields and values to be faithfully mapped onto texts. This will address the issue of the lack of consideration for the rela- tionship between lexical terms and numerical val- ues. Our approach makes a significant difference in that our framework could generalize to numeri- cal values or value combinations that are unseen in training, and will not be simply reciting cooccur- rence patterns of exact values in the training data.</p><p>Our work relates to learning executable se- mantic parsers under weak supervision. Early semantic parsing started from fully supervised training with annotated meaning representations available <ref type="bibr" target="#b40">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b11">Ge and Mooney, 2006;</ref><ref type="bibr" target="#b36">Snyder and Barzilay, 2007)</ref>, but more recent work focused on reducing the amount of supervision required <ref type="bibr" target="#b1">(Artzi and Zettlemoyer, 2013</ref>). The intuition behind weakly supervised executable semantic parsing is that once the la- tent semantic representation has been executed, one could test whether the execution results could match the information with available weak super- vision signals such as answers to natural language queries ( <ref type="bibr" target="#b9">Clarke et al., 2010;</ref><ref type="bibr" target="#b22">Liang et al., 2011</ref>), or task completion from instructional navigations ( <ref type="bibr" target="#b25">Misra et al., 2017)</ref>. Such formulations have been adapted for question answering over structured tables <ref type="bibr" target="#b29">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b20">Krishnamurthy et al., 2017)</ref>. However, the current research fo- cus is to convert a natural language question into executable table queries and to directly retrieve re- sults. They do not have the need of inference in- volving numerical commonsense implied by var- ious lexical patterns. A few unsupervised ap- proaches exist <ref type="bibr" target="#b31">(Poon and Domingos, 2009;</ref><ref type="bibr" target="#b30">Poon, 2013)</ref> but only specific to translating language into queries in the highly structured database and can- not be applied to our domain.</p><p>Our approach is implemented as assigning tag annotations over text spans, which is conceptu- ally related to fine-grained named entity tagging ( <ref type="bibr" target="#b23">Ling and Weld, 2012)</ref>. Our setting only requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells. Similar modeling and learning strate- gies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future ( <ref type="bibr" target="#b7">Choi et al., 2018</ref>).</p><p>The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation. Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data ( <ref type="bibr" target="#b24">Mei et al., 2016;</ref><ref type="bibr" target="#b18">Kiddon et al., 2016;</ref><ref type="bibr" target="#b26">Murakami et al., 2017;</ref><ref type="bibr" target="#b37">Wiseman et al., 2017)</ref>, with- out establishing underlying semantic correspon- dences. Texts generated thereby can be fluent but not conforming to the input data, unlike template- based approaches where lexical choices could be directly controlled. In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, ei- ther with simple heuristics to automatically ex- tract description patterns (how to say) and corre- sponding triggers (what &amp; when to say), or with more crafted discriminative learning approaches (cf. <ref type="bibr" target="#b0">Angeli et al. (2010)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Technical overview</head><p>Task Let S be the set of all world states, W be the set of all texts, O be the set of all executable operators, and V be the output space of O. A world state s ∈ S is a table storing some infor- mation, or more specifically in this work, a tabular recording for a sports game. An operator o ∈ O can be executed on a world state to retrieve val- ues, i.e., each o could be treated as a mapping of S → V. The result of an operation can be a string, a continuous values or a discrete value. Mean- while, each world state s is accompanied with a piece of description w ∈ W. Here w consists of a sequence of word tokens {w i ∈ w}. We fur- ther define a segmentation variable π, which could convert w into a sequence of word spans c, con- taining each span of tokens c t ∈ c that could be interpreted as a phrase. Note that we use super- script t to denote indices of phrases, and subscript i to denote indices of individual words. We further define l as a sequence of latent labels, and value of each l t is an operator o i ∈ O. 2 For each world state s and corresponding description w, we want to jointly find a proper segmentation π to obtain c, and assign labels to every word span c t .</p><p>We conduct this study on the ROTOWIRE sub- set of the openly available dataset released by <ref type="bibr" target="#b37">Wiseman et al. (2017)</ref>, containing text descrip- tions for NBA basketball games with structured tables of game statistics. Take <ref type="figure" target="#fig_0">Figure 1</ref> as an ex- ample. The proper nouns (e.g. Toronto Raptors) appeared in the sentence can be assigned with a tag Team Name in our tag set, which could then be aligned to the Team Name field in the table. Some numbers appearing in the text, such as 15 in the example, can be assigned with the label Team Losses, with the executable annotation to extract the number of previously lost matches of the mentioned team. What we are more interested in is where the phrase edge out comes from. We are aiming at a model which is capable to capture the semantics behind the phrase edge out, which is used to describe an event that one team has beaten the other with close scores. In our an- notation scheme, this phrase should be assigned with the tag Team Points Delta, and execut- ing that will return the score difference between two teams.</p><p>The task is challenging in that there does not  exist any other additional supervision signal. The learning process will mostly rely on statistical cooccurrences of information between the struc- tured data and its text descriptions. Note that we assume a consistent structure (schema) throughout the whole dataset upon which the learning process will be performed.</p><p>Model To jointly learn word segmentations c and latent semantic annotations l between world state s and text w in a unified framework, we pro- pose a generative model to characterize the joint distribution P s (l, π, w; θ), parameterized by θ.</p><p>Learning The data contain paired texts and ta- bles only, thus our model must learn segmenta- tions and latent semantic annotations in an unsu- pervised fashion. The target is to maximize the complete data likelihood</p><formula xml:id="formula_0">L(θ) = (s,w)∈ D l,π P s (l, π, w; θ),</formula><p>where D represents the whole training data. To reduce the search space in inference and to cap- ture some patterns of content planning in the text descriptions, we adopt a Markov assump- tion over phrase segments, which leads to a hid- den semi-Markov model (semi-HMM) <ref type="bibr" target="#b27">(Murphy, 2002;</ref><ref type="bibr" target="#b34">Sarawagi and Cohen, 2005</ref>). The key part is to characterize different types of correspondences (Sec 4.2). We derive an expectation-maximization (EM) algorithm to perform maximum likelihood estimation, and introduce a soft statistical regular- ization to guide the model towards a better solu- tion (Sec 4.5). Inference Once the model has been trained, we use a Viterbi-like dynamic programming process to perform MAP inference to segment the texts and to assign the most likely tags for each span.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The set of annotations</head><p>We describe the process of how we derived our set of semantic annotations here. There are two kinds of specific tables for each NBA game in the dataset: Box-Scores and Line-Scores, respectively showing the performance statistics for individual players and the whole teams. The types of pos- sible values for data fields are strings, categorical values and numerical values. Box-Score consists of 24 fields, of which four are string-values, one is categorical, the other 19 being numerical. Line- Score consists of 16 fields, containing two string- valued, one categorical, and 13 numerical fields. A semantic tag works on a specified kind of field type, from either team statistics or player statis- tics. For a tag that could align to one single field in the table, we let it return the exact value in the cell that could maximize the likelihood. For ex- ample, the tag Team City is used to extract the name of the team in the table that corresponds to the current word span. For fields taking numeri- cal values, we additionally allow tags to be able to perform mathematical operations, such as sub- traction, between two values in the same field. 3 For example, the tag Team Points Delta can be executed to return the score difference between two teams. We list the different types of tags with examples shown in <ref type="table" target="#tab_2">Table 1</ref> and leave the entire tag set to Appendix A. Along with all these tags de- rived from the original data fields, we also include a special NULL tag which are supposed to be as- signed to non-informative words or words contain- ing information not contained in the given table.</p><p>Note that we impose little prior knowledge in this step. We simply over-generate all possible la- bels, and let the model figure out which part of them should be eventually used. Although the only compositional operation we used in this work is numeric subtraction, common operations that could produce string, categorical values or num- bers could be easily introduced for other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-HMMs with continuous values</head><p>As previously mentioned, we will be modeling the joint distribution of word segmentation c and the latent semantic annotations l between paired world state s and text w, which could be factorized as:</p><formula xml:id="formula_1">P s (l, π, w) = P s (w, π|l) · P s (l),</formula><p>and we write P s (w, π|l) as P s (c|l). In this sec- tion, we focus on the probability of the alignments between word spans and labels, namely, P s (c|l).</p><p>Following <ref type="bibr" target="#b21">Liang et al. (2009)</ref>, we consider two aspects. One is salience that captures the intuition that some fields should be more frequently men- tioned than others (henceforth some latent tags should be more frequently triggered). The other is (local) coherence, which refers to the order in which the writer mentioning certain information tends to follow some patterns. To capture these two phenomena, we define a Markov model:</p><formula xml:id="formula_2">P s (c, l) = t P (l t |l t−1 ) · P s (c t |l t ),</formula><p>where l t is the annotated label at time stamp t, and we assume that the transition probabilities are in- dependent of world state s. It resembles a standard form of HMMs, despite the subscript s in P s (c|l). For different types of correspondences between l t and c t , we define different probability distributions to model P s (c t |l t ):</p><p>(1) Numerics-to-numerics:</p><p>The numbers in texts could sometimes be inaccurate due to some rounding customs, thus we use a Gaussian model for this type:</p><formula xml:id="formula_3">Sof tIndicator(x, y|σ) = N (x − y|0, σ),</formula><p>where N is the Gaussian density. When the output type of tag l t is numeric and the word span c t is a number, we set P s (c|l) = Sof tIndicator(c, v l |σ l ), where σ l is different for different tags, and v l is the corresponded value in the table for tag l. Note that when σ → 0, Sof tIndicator reduces to an indicator function that only allows exact matching.</p><p>(2) String-to-string: Similarly for strings, since simple matching could fail if the text con- tains Bob to refer to Bob Smith. We simply use string matching to model the probability:</p><formula xml:id="formula_4">P s (c|l) ∝ M atch(v l , c)</formula><p>, where the M atch function returns the number of shared words between cell value v l and word c.</p><p>(3) Category-to-string: For labels correspond to discrete categorical values, such as Sunday, PG (point guard, a basketball position), we adopt the same method used by <ref type="bibr" target="#b21">Liang et al. (2009)</ref>: using a multinomial distribution over all word spans for each possible category:</p><formula xml:id="formula_5">P s (c|l) = ν c,v l , c ν c,v = 1,<label>(1)</label></formula><p>where v l is again the output value of tag l.</p><p>the Clippers took down the T e a m _ N a m e - P o in ts _ D e lt a - n u ll - n u ll - <ref type="figure">Figure 2</ref>: A Semi-HMM that can yield an entire phrase from one tag.</p><p>(4) Numerics-to-string: When the tag corre- spond to a numeric value v l while the word span c is not a number, the problem resembles speech modeling ( <ref type="bibr" target="#b16">Huang et al., 1990</ref>). Apply- ing the Bayes rule, we get: 4</p><formula xml:id="formula_6">P s (c|l) = P (c|l, v l ) ∝ P (v l |c, l) · P (c|l),</formula><p>where we collapse the relevant part from world state s into v l . The intuition be- hind P (v l |c, l) is that when an informa- tive word (e.g. routed) appears in the text, the corresponding values should have different chances to happen in the world, e.g. P (30|routed, Points Delta) &gt; P (3|routed, Points Delta).</p><p>Due to the lack of prior knowledge on the dis- tribution, we also model this term as Gaussian. <ref type="bibr">5</ref> The result resembles a Gaussian mixture:</p><formula xml:id="formula_7">P s (c|l) ∝ N (v l ; µ c,l , σ c,l ) · η c,l , c η c,l = 1,</formula><p>where P (c|l) = η c,l is also multinomial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modeling phrasal spans</head><p>Our model can enable phrase segmentation. Pre- viously, <ref type="bibr" target="#b21">Liang et al. (2009)</ref> treated the words in- side a phrase individually and independently. This could be problematic in our scenario. For exam- ple, take down is used to describe a team defeating another, while separately both take and down are frequent words in general, making them difficult to be jointly assigned with the correct label as a whole. Instead, in our model we treat the phrasal T e a m _ N a m e - ... the Clippers took down the Rockets ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P o in ts _ D e lt a -</head><p>n u ll - n u ll - T e a m _ N a m e - word span as a unit. The probability is assigned to the whole span of phrase instead of individual words, which will break the token-wise Markov property <ref type="figure">(Fig. 2</ref>, henceforth Semi-HMM). For effi- cient parameter estimation, We use a variant of the standard forward-backward algorithm by adding a parameter k, which is the maximum length of word spans, onto the Markov chain. We leave de- tailed descriptions to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Skipping null labels</head><p>Preliminary experiments suggest that the initial model have too many words assigned to the NULL tag. Informative alignments may not be adja- cent, which breaks the simplest Markov assump- tions. In our model, the transition score of two non-NULL labels can be calculated by skipping all the NULLs in between, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. This is implemented without breaking the over- all Markov property with the following trick used in earlier work on statistical alignments ( <ref type="bibr" target="#b4">Brown et al., 1993)</ref>: Suppose we have m labels (i.e., m latent states), we can design m different NULL la- bels that share the same emission score, while pre- serving their original outward-transition probabil- ities. The types of NULL labels are inherited from the previous label. This might seem to be waste- ful at first sight as we use two-fold latent states, but the Markov property is successfully preserved, therefore simplifying our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Posterior regularization</head><p>The structured tables and text descriptions of the dataset were originally crawled from different sources. As a consequence, a non-negligible pro- portion of texts is in fact describing information outside the given table, such as historical records (e.g., "win streak"). Ideally, words in these parts of the text should remain unaligned, or in our set- ting, be annotated with the NULL tag. However, due to the notorious effect of garbage collection from statistical alignment ( <ref type="bibr" target="#b4">Brown et al., 1993)</ref>, these words tend to be aligned to some irrelevant fields in the table which are rarely mentioned. We address this issue by adding a soft statisti- cal constraint in the form of posterior regulariza- tion ( . With posterior regularization, we could add cer- tain types of statistical constraints to the E-step in the EM procedure, while keeping the inference tractable. The constraints should be in the form of:</p><formula xml:id="formula_8">E[f (w, l)] = i E[f (w, l i )] ≤ b w ,<label>(2)</label></formula><p>where the features f should be defined on local cliques for tractable inference. We use projected gradient descent to solve the E-step sub-problem in this work. The statistical constraint we add to the posterior is rather simple: For each sentence, we "encourage" at least a pro- portion of words to be aligned to NULL labels:</p><formula xml:id="formula_9">E[−f (w, l)] ≤ −r 0 · n,<label>(3)</label></formula><formula xml:id="formula_10">f (w, l) = n i=1 1(l i = NULL),<label>(4)</label></formula><p>where r 0 is a adjustable ratio, n is the length of w. We also tried other constraints but found this simple soft regularization performing well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">By-product: template induction</head><p>Intuitively, the assigned semantic correspondences could be useful to derive templates and trigger rules for language generation. In this work, we use the most straightforward heuristics to perform template induction, utilizing the established cor- respondences and inferred parameters. Specifi- cally, we first blank out the correspondences of numerics-to-numerics and string-to-string to be empty slots and replace with the tag names. In the example of <ref type="figure" target="#fig_0">Figure 1</ref>, we could replace Raptors with Team Name, and 120 with Team Points, if they have been correctly aligned.</p><p>We also need to know when to use each tem- plate. We define a template trigger to be a quadru- ple <ref type="figure">(c, l, µ c,l , σ c,l )</ref>, where c is a phrase, l is a tag, µ c,l and σ c,l are estimated Gaussian parameters. <ref type="bibr">6</ref> We assign each template with a score to be the minimum probability for all triggers inside: score(s, t) = min i N (t i .l(s); t i .µ, t i .σ), (5) <ref type="bibr">6</ref> To use a unified notation, for categorical-value triggers we set µ c,l = arg max v l νc,v l (defined in (1)) and σ c,l = .</p><p>where t = {t i } denotes all possible triggers in the template, and the tag l can be executed over the world state s to retrieve a value l(s). We only consider sentences satisfying both of the following conditions in order to extract templates with high quality: (1) sentences aligned to ≤ two teams or one player, and (2) sentences with triggers derived from continuous distributions. Now that the templates and triggers are ready for use, we will experiment with the following straightforward rules to perform data-to-text gen- eration: For every game, we first generate a sen- tence describing the scoreline result, followed by three sentences describing other information about team performance. While keeping that no tem- plate is repeatedly used, we will then choose the template with the highest score for top ten players sorted by their game points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental setup</head><p>We conducted experiments on the ROTOWIRE sub- set of the <ref type="bibr" target="#b37">Wiseman et al. (2017)</ref> dataset. In our experiment, we restricted the maximum length of word span to two as a trade-off of speed and per- formance. Empirically, most of the phrases in the dataset are at a length of at most two. We empiri- cally set the expected NULL ratio to be r 0 = 0.5.</p><p>We did the following pre-processing steps for all systems in comparison: we lemmatized all to- kens in the sentences, and filtered out sentences containing less than five words since they are meaningless short sentences. To utilize the game dates, we converted them from calendar date to the day of week, e.g. 11/28/2016 is converted to Mon- day as a categorical value. Due to the huge noise in the ROTOWIRE dataset, containing many sen- tences irrelevant with their corresponding tables, we filtered out the sentences that contain no team or player names, or those that mention more than 2 players, as most of them are irrelevant texts.</p><p>Following <ref type="bibr" target="#b21">Liang et al. (2009)</ref>, we also used the parameters of a simpler model without Markov de- pendency (which was uniformly initialized) to ini- tialize our complete model with obtained param- eters, and then trained it until convergence. We adapted <ref type="bibr" target="#b21">Liang et al. (2009)</ref>'s framework to the ta- ble schema in the ROTOWIRE dataset, and ran ex- periments accordingly as our baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Intrinsic evaluation</head><p>It is difficult to evaluate the accuracy of tag as- signments for the entire dataset, since the tags are not annotated in the original data. We recruited three human annotators with familiarity in the do- main of basketball games to label 300 sentences (around 8,000 tokens in total, and 30% of them are annotated with tags) from the test set. There exists a fraction (18%) where agreements were not made, we included all the proposed tags from the annotators to be correct. Also, because of the ambiguity of annotations, we use the base names of derived tags (e.g. Rebounds Delta) for numerics-to-string relationship evaluation. (e.g. Rebounds Delta is reduced to Rebounds) Finally, we calculated the precision and recall for non-NULL tag assignments at word-level.   The results are shown in <ref type="table" target="#tab_3">Table 2</ref>. The Liang et al. (2009) framework could still achieve around 65% recall, because there exist a large proportion of correspondences that could be easily captured by exact matches and simple categorical distribu- tions. Our model without PR achieves lower pre- cision than the baseline, because the baseline did not model numerics-to-string relationships and en- countered less severe issues of garbage collection. We can observe that our initial model indeed out- performs the baseline system in recall, while PR helps a lot to avoid distraction from irrelevant in- formation that should be tagged as NULL.</p><p>We also include more fine-grained results for different types of correspondences, shown in Ta- ble 3. As expected, numerics-to-string correspon-The Boston Celtics ( 7 -5 ) blew out the Brooklyn Nets ( 2 -11 ) 120 -95 on Friday .</p><formula xml:id="formula_11">n u l l - T e a m _ N a m e - P o i n t s _ D e l t a - n u l l - n u l l - T e a m _ P o i n t s - n u l l - n u l l - T e a m _ W i n s - T e a m _ N a m e - n u l l - n u l l - n u l l - T e a m _ C i t y - T e a m _ L o s s e s - n u l l - T e a m _ C i t y - n u l l - T e a m _ W i n s - T e a m _ L o s s e s - T e a m _ P o i n t s - D a t e - (a)</formula><p>Hessan Whiteside led the charge of Miami with 23 points on super efficient 10 -of -11 shooting , ...    dences are the most difficult part in this study. Another notable thing is that although we found that around 40% of numerics-to-numerics corre- spondences were ambiguous due to the appear- ance of identical values from different table cells, our model could still achieve a high accuracy of 95.0%.</p><formula xml:id="formula_12">F T _ M a d e - M i n u t e - n u l l - n u l l - F T _ M a d e -n u l l -n u l l - n u l l - n u l l - n u l l - n u l l - n u l l -n u l l - (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Extrinsic evaluation</head><p>We also tested how the derived templates could perform in language generation, when compared with the baseline using the same heuristics de- scribed in Sec 5. We report automatic metrics in- cluding BLEU scores and those based on relation extraction as proposed by <ref type="bibr" target="#b37">Wiseman et al. (2017)</ref>: precision &amp; number of unique relations in genera- tion (RG), precision &amp; recall for content selection (CS), and content ordering (CO) score. These au- tomatic metrics were designed for various aspects in NLG and may not all suit our main focus well, so we also conducted human evaluation on infor- mation correctness (1-5 scale ratings, the higher the better). We asked four human raters who are fluent in English and with familiarity in basketball terms to rate over outputs for 30 random games. Results are shown in <ref type="table" target="#tab_8">Table 4</ref>. We can observe that templates derived from our model indeed outper- form those from the baseline. We put some inducted templates and generated text examples in the Appendix. <ref type="figure" target="#fig_4">Figure 4</ref> shows some examples produced from our methods. Some of the alignments are meaning- ful, for example, the model assigned the word per- fect with the annotation FT Percent, which rep- resents the percentage of free throws. Without PR, our model performed poorly by aligning many common words to those rarely mentioned cells. In this example, the FT Made and FT Attempt fields in the input table both have the same value 8, making it difficult for a model without proper local coherence modeling to distinguish between them. Because our initial model without PR can- not annotate NULL correctly, the Markov transi- tion between these two numbers was intercepted by three meaningless tokens. However, after in- jecting the PR constraint, most of the unmentioned words were successfully identified. The model captured the pattern that FT Attempt almost al- ways follows FT Made, making it correctly as- signed these two labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>We conducted ablation experiments for some  of the components <ref type="table" target="#tab_10">(Table 5)</ref>. When setting the maximum phrase length to be k = 1, the model degenerates to a normal HMM. The performance measured by F1-score drops for only a little. One possible reason is that Semi-HMMs tend to out- put some meaningless combinations of words as phrases, such as the phrase points on in <ref type="figure" target="#fig_4">Fig- ure 4 (b)</ref>, which could lead to many redundant annotations that hampers precision albeit its help to recall. We also tried to disable the transition probabilities during both training and inference, which led to lower precision and lower recall natu- rally as there was no modeling for local coherence. Finally, by canceling the NULL-skipping mecha- nism, we found that the numerics-to-numerics an- notation accuracy dropped from 95.0% to 88.8%. Many of the spurious numerics-to-numerics anno- tations, such as the 8 -of -8 in <ref type="figure" target="#fig_4">Figure 4</ref> (d), could be corrected using transition probabilities under the skipping-NULL mechanism <ref type="figure" target="#fig_4">(Figure 4 (c)</ref>).  and we list the top 12 weighted words in <ref type="figure" target="#fig_5">Fig- ure 5</ref>. We can observe that most of the displayed phrases have strong semantic relationship with score differences. More interestingly, we found the mean and variance values estimated by the Gaussian distributions rather informative. When l = Teams Points Delta, we observed that µ l,narrowly ≈ 2 while µ l,blow out ≈ 26. We could infer the conditions under which some phrases should be used, providing useful insights for lexi- cal choices in language generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RG(P%) RG(#) CS(P%) CS(R%) CO BLEU Correctness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we attempt to learn executable la- tent semantic annotations from paired structured tables and texts. We model the joint probability of data fields, texts, phrasal spans, and latent annota- tions with an adapted semi-hidden Markov model and impose a soft statistical constraint via poste- rior regularization. Experimental results suggest the feasibility of the setting in study and the effec- tiveness of our framework. This is a preliminary study for using weak su- pervision from structured data and texts to address the challenging problem of language grounding. For future study, one could collect large-scale data and texts in other domains where more complex grounding on phrases such as "increasing trends" should be done. To enhance modeling power, un- supervised discriminative models that utilize rich features <ref type="bibr" target="#b2">(Berg-kirkpatrick et al., 2010</ref>) could also be explored. We are also interested in collecting more high-quality parallel data to induce grounded compositional logic representations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the summary and the corresponding table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A slightly different Semi-HMM whose transition score is calculated by skipping NULL fields.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of latent tag assignment. Words in blue dashed boxes are phrases recognized by our model. The labels red with asterisks (*) are false assignments. (a), (b), (c) are example of Semi-HMMs-PR, while (d) is produced from the initial Semi-HMMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Mean±std for the top 12 weighted phrases assigned with Team Points Delta. (circle sizes are proportional to weights)</figDesc><graphic url="image-1.png" coords="9,314.36,158.83,204.09,112.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Examples from the derived tag set, where each tag could correspond to one of three types of values.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Word-level tag assignment results. 

Correspondence type Proportion Accuracy 
numerics-to-numerics 0.369 
0.950 
numerics-to-string 
0.283 
0.402 
string-to-string 
0.252 
0.892 
category-to-string 
0.095 
0.936 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The performance of Semi-HMMs+PR for dif-
ferent types of correspondences. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 4 : Results for data-to-text generation (Kendall's W=0.83 from correctness raters)</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 5 : Ablation results.</head><label>5</label><figDesc></figDesc><table>One additional advantage of our model is that 
we can easily verify what the model has captured. 
For the latent annotation Team Points Delta, 
we sort its corresponding phrases by weights P (c|l) 

P (c) 

</table></figure>

			<note place="foot" n="2"> We will interchangeably use labels, operators, tags to refer to the latent executable semantic annotations in this paper.</note>

			<note place="foot" n="3"> We limit the number of arguments within two in this work and leave more complex operators for future study.</note>

			<note place="foot" n="4"> We noticed that in parallel with our work, another study (Zhang et al., 2018) on verb selection for data-to-text generation also use the same strategy of Bayes rule to estimate parameters, and provide a noisy-channel interpretation. 5 The double tails in Gaussian pdf have different interpretations: unlikely to occur, or unlikely to occur conditionally.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Hongyuan Mei and all the anonymous reviewers for giving helpful com-ments on an earlier draft of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Simple Domain-Independent Probabilistic Approach to Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Painless unsupervised learning with features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bouchard-Côté</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Label Ranking under Ambiguous Supervision for Learning Semantic Correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training a multilingual sportscaster: Using perceptual context to learn language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to sportscast: a test of grounded language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning language through pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chrupała</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">´</forename><forename type="middle">A</forename><surname>Kádár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alishahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Driving Semantic Parsing from the World&apos;s Response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Posterior Regularization for Structured Latent Variable Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative reranking for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Situated language understanding as filtering perceived affordances. Cognitive Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gorniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<title level="m">Learning Tractable Word Alignment Models with Complex Constraints. Computational Linguistics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reasoning about RoboCup Soccer Narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">T</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic Understanding of Proefessional Soccer Commentaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hidden Markov Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ariki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Jack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Edinburgh University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning language semantics from ambiguous supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Kate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Globally Coherent Text Generation with Neural Checklist Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Resolution Language Grounding with Weak Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural Semantic Parsing with Type Constraints for Semi-Structured Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Semantic Correspondences with Less Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Dependency-Based Compositional Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fine-Grained Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics and Human Language Technology (NAACLHLT)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mapping Instructions and Visual Observations to Actions with Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to Generate Market Comments from Stock Prices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hidden Semi-Markov Models (HSMMs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is a 204 cm Man Tall or Small? Acquisition of Numerical Common Sense from the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mizuno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<title level="m">Compositional Semantic Parsing on Semi-Structured Tables</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics (ACL)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grounded Unsupervised Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">You Need to Understand your Corpora! The Weathergov Example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Connecting language to the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-Markov Conditional Random Fields for Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A computational study of crosssituational techniques for learning word-to-meaning mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Database-text alignment via structured multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Challenges in Data-to-Document Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Shieber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the Integration of Grounding Language and Learning Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grounded Language Learning from Video Described with Sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Probabilistic Verb Selection for Data-to-Text Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
