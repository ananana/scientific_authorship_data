<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Parde</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of North Texas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Finding Patterns in Noisy Crowds: Regression-based Annotation Aggregation for Crowdsourced Data</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1907" to="1912"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Crowdsourcing offers a convenient means of obtaining labeled data quickly and inexpensively. However, crowdsourced labels are often noisier than expert-annotated data, making it difficult to aggregate them meaningfully. We present an aggregation approach that learns a regression model from crowdsourced annotations to predict aggregated labels for instances that have no expert adjudications. The predicted labels achieve a correlation of 0.594 with expert labels on our data, outperforming the best alternative aggregation method by 11.9%. Our approach also outperforms the alternatives on third-party datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Publicly-available labeled datasets are scarce for many NLP tasks, and crowdsourcing services such as Amazon Mechanical Turk 1 (AMT) offer re- searchers a quick, inexpensive means of labeling their data. However, workers employed by these services are typically unfamiliar with the anno- tation tasks, and they may have little motivation to perform high-quality work due to factors such as low pay and anonymity. To further complicate matters, some workers may produce spam or ma- licious responses. Thus, it is not uncommon for workers to correlate poorly with one another.</p><p>Researchers using crowdsourcing services com- monly aggregate the labels they receive via sim- ple strategies such as using the majority or av- erage label. These methods are best suited for simple, straightforward tasks; with noisier data such as that which may be obtained for more dif- ficult or subjective tasks, these strategies may pro- duce skewed labels that misrepresent the instance. <ref type="bibr">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>www.mturk.com</head><p>Thus, it is desirable to devise more effective aggre- gation strategies that consider factors such as label distribution and worker quality, while still avoid- ing manual adjudication of all instances.</p><p>In this work, our contributions are as follows: (1) we develop a regression-based method for au- tomatically aggregating crowdsourced annotations of varying quality, with poor agreement and mini- mal expert-adjudicated data, that addresses multi- ple potential flaws or biases in non-expert human annotation. To do so, we (2) crowdsource anno- tations for a difficult NLP task, metaphor novelty scoring, and (3) describe a process by which we automatically detect untrustworthy workers. We then (4) introduce a feature set that captures label distribution and trustworthiness, and extract the features from our crowdsourced annotations. Fi- nally, (5) we train a regression model that predicts aggregated labels for unseen instances and com- pare the predictions to expert annotations, finding that our method outperforms the best alternative approach. We evaluate our approach both on our data and on existing crowdsourcing datasets. All datasets and source code are available for the re- search community to improve on our results. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several methods have been proposed to identify low-quality workers in crowdsourced data. <ref type="bibr" target="#b9">Jagabathula et al. (2016)</ref> filtered adversarial workers in binary labeling tasks by identifying those with out- lier labeling patterns, and <ref type="bibr" target="#b11">Lin et al. (2014)</ref> identi- fied when additional labels for binary tasks should be crowdsourced to optimize classifier accuracy. Unlike these approaches, our filtering algorithm is suitable for multi-class annotation tasks.</p><p>Various methods have also been explored as in- telligent modes of label aggregation. Most ( <ref type="bibr" target="#b14">Snow et al., 2008;</ref><ref type="bibr" target="#b13">Raykar et al., 2010;</ref><ref type="bibr" target="#b10">Karger et al., 2011;</ref><ref type="bibr" target="#b12">Liu et al., 2012;</ref><ref type="bibr" target="#b7">Hovy et al., 2013;</ref><ref type="bibr" target="#b2">Felt et al., 2014;</ref><ref type="bibr" target="#b8">Huang et al., 2015</ref>) have built upon the probabilistic item-response model first pro- posed by <ref type="bibr" target="#b1">Dawid and Skene (1979)</ref>, which simul- taneously estimates annotator quality and aggre- gated labels using an expectation-maximization algorithm. MACE ( <ref type="bibr" target="#b7">Hovy et al., 2013</ref>) is a popular implementation inspired by this that aggregates la- bels as a function of the annotation and a learned binary variable indicating whether the annotator is a spammer. We posit that although annotator qual- ity is an important factor in predicting accurate ag- gregations, the interplay between it and other fac- tors is more nuanced. Thus, rather than adapting the item-response method, our learning approach incorporates features that address multiple poten- tial flaws or biases in crowdsourced annotations.</p><p>Some researchers have also used data-aware ap- proaches to predict aggregations ( <ref type="bibr" target="#b13">Raykar et al., 2010;</ref><ref type="bibr" target="#b2">Felt et al., 2014</ref><ref type="bibr" target="#b3">Felt et al., , 2015</ref><ref type="bibr" target="#b4">Felt et al., , 2016</ref>). We do not use the data itself in this work, to avoid skewing labels in a way that makes it trivial to learn clas- sifiers based on the same data. To the best of our knowledge, our work is the first to frame label ag- gregation as a regression task, with features based solely on workers and their labels, that learns en- tirely from a small amount of expert-adjudicated crowdsourced annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Collection</head><p>We evaluated our approach on our new metaphor novelty dataset, as well as on third-party datasets. To build our dataset, we crowdsourced annota- tions for 3112 potentially metaphoric word pairs, and randomly divided the instances into training (1036), validation (1038), and test (1038) subsets. We developed features and selected our regression algorithm using the training and validation sets only; the test set was withheld until the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Annotation Task</head><p>Instances were comprised of pairs of words from 1840 sentences in the VU Amsterdam Metaphor Corpus (VUAMC) <ref type="bibr" target="#b15">(Steen et al., 2010</ref>). The VUAMC consists of documents for which individ- ual words are labeled as metaphors. The novelty of those metaphors varies widely, from highly con-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example</head><p>Score Alice looked up, and there stood the Queen in front of them, with her arms folded, frowning like a thunderstorm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novel</head><p>Metaphor <ref type="formula">(3)</ref> 'Once,' said the Mock Turtle at last, with a deep sigh, 'I was a real Turtle.' Conventional Metaphor (1) A large rose-tree stood near the entrance of the garden: the roses growing on it were white, but there were three garden- ers at it, busily painting them red.</p><p>Non- Metaphor (0) <ref type="table">Table 1</ref>: Sample word pairs provided to Turkers.</p><p>ventional to quite novel. Each sentence for which we collected annotations contained a content word (noun, verb, adjective, or adverb) labeled as being metaphoric, and one or more other content words or personal pronouns that were syntactically re- lated to the metaphoric word. Word pairs contain- ing a metaphoric word and a syntactically-related content word or personal pronoun were considered instances. AMT workers ("Turkers") were asked to score each instance on a discrete scale from non-metaphoric (0) to highly novel metaphor (3). Some examples are shown in <ref type="table">Table 1</ref>. <ref type="bibr">3</ref> Instances were grouped into Human Intelli- gence Tasks (HITs) containing all instances asso- ciated with 10 sentences each. Five worker assign- ments were requested per HIT, and Turkers were paid $0.20 per HIT. Overall, 237 Turkers anno- tated 942 assignments, with an average correlation of 0.269 per HIT (the poor agreement suggests this is a very difficult annotation task). An expert adju- dicated all 3112 instances; those labels were con- sidered the gold standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Data Filtering</head><p>Spam and malicious workers were identified dur- ing data collection using a filtering algorithm that compared annotations with those completed by "potentially good annotators" (P GA). Alg. 1 de- scribes this process. Letting H i be a set of HITs collected, A i be the set of annotators who anno- tated H i , and A=∪(A 1 , . . . , A j ) be the set of all annotators, the algorithm computes three sets of annotators: good annotators (GA), spammers or malicious annotators (Bad Robots, or BR), and annotators of currently unknown quality U QA.</p><p>R(a j , a k ) computes the correlation coefficient between two annotators a j and a k , where a k is a potentially good annotator whose annotations overlap with a j 's, and AVG R(a j ) computes the average correlation between a j and all a k . HITs</p><formula xml:id="formula_0">Algorithm 1 Worker Filtering for Annotation Set i P GA ← A \ BR repeat for aj in A do A j ← {a ∈ P GA} who annotated ≥ 1 unfiltered HIT in common with aj for a k in A j do r j,k ← R(aj , a k ) rj ← AVG R(aj ) B − ← {aj ∈ A|rj &lt; 0.0} B 0 ← {aj ∈ A|rj == 0.0 or rj == ∞} 4 B + ← {aj ∈ A}, of size |B − |, with the lowest rj &gt; 0.0 B &lt;.1 ← {aj ∈ A|rj &lt; 0.1} P GA = A − (B − + B 0 + B + + B &lt;.1 ) until convergence or iterations = max GA ← {aj ∈ A|rj &gt; 0.35} BR ← B − + B 0 + BOTTOM(ROUND( 2 3 |B − |), B + )</formula><p>completed under a minimum time threshold were also filtered. Following algorithm completion, fil- tered HITs and unpaid HITs from members of BR were rejected, and annotators in BR were disqual- ified from accepting future HITs. 116 total as- signments were rejected by the filtering algorithm. Annotators in U QA (U QA=A−GA−BR) who had completed ≥ 2 HITs and had an r j &lt; 0.1 were also disqualified. All other HITs were accepted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>We designed features to capture the distribution and trustworthiness of crowdsourced labels for each instance. The features are described in Ta- ble 2. ANNOTATIONS are designed to provide the regression algorithm with label distributions based on label value and worker trustworthiness. AVG. R features are intended to further clarify worker quality, and AVG. R (GOOD) is meant to provide a more selective view of the same characteristic. AVG., WEIGHTED AVG., and WEIGHTED AVG. (GOOD) allow the regressor to consider three dif- ferent versions of a popular aggregation strategy, and finally, HIT R supplies the algorithm with an estimate of agreement on the current instance to consider when making its prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regression Algorithm</head><p>The approach utilizes a random subspace regres- sor, which was selected based on its performance on the training and validation data relative to a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Description ANNOTA- TIONS From highest to lowest label, the five an- notations for the instance. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVG. R</head><p>For each annotator, in order of label value, his/her avg. correlation with other workers across all instances he/she annotated. <ref type="bibr">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVG. R (GOOD)</head><p>AVG. R in which each annotator is com- pared only to annotators with rj&gt;0.35. If the annotator has no overlapping annota- tions with those, AVG. R is repeated. AVG.</p><p>Average of the five ANNOTATIONS.</p><p>WEIGHTED AVG.</p><p>Let li be the i th ANNOTATION, and ri be its annotator's AVG. R. Then,</p><formula xml:id="formula_1">WEIGHTED AVG. = 5 i=1 (l i ×r i ) 5 i=1 r i . WEIGHTED AVG.<label>(GOOD)</label></formula><p>Similar to WEIGHTED AVG., with weights (ri) taken from AVG. R. (GOOD) instead of AVG. R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIT R</head><p>The average weighted correlation among annotators for the HIT containing the in- stance. Letting wi,j be the weight for a pair of annotators equal to</p><formula xml:id="formula_2">r i +r j 2</formula><p>, where ri and rj are the AVG. R associated with an- notators ai and aj, ri,j be the correlation between annotators ai and aj for the HIT, and P contain all annotator pairs <ref type="bibr">(ai, aj)</ref> for the HIT, HIT R = p∈P r i,j ×w i,j p∈P w i,j Label Range 0-100 -100-100 0-2 0-3 <ref type="table">Table 3</ref>: Dataset Details large variety of other regression algorithms. Ran- dom subspace is similar in nature to bagging and random forests, using multiple decision trees con- structed from subsets of features selected ran- domly without replacement to make its predictions <ref type="bibr" target="#b6">(Ho, 1998)</ref>. We used the implementation from the Weka library ( <ref type="bibr" target="#b5">Frank et al., 2016)</ref>, with Weka's REPTree classifier as the base decision tree model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Other Datasets</head><p>In addition to evaluating our approach on our data, we evaluate it on three existing crowdsourcing datasets that differ in terms of their size, noise level, and number of annotators. Details about each dataset are shown in <ref type="table">Table 3</ref>, with additional information below. Each third-party dataset was randomly divided into 66% training and 34% test.</p><p>Affect (Emotion and Valence). Affect (Emo- tion) and Affect (Valence) were created for <ref type="bibr">Snow et al.'s (2008)</ref> work, and contain emotion (anger, fear, disgust, joy, sadness, and surprise) and va- lence ratings for 100 headlines from the SemEval affective text annotation task <ref type="bibr" target="#b16">(Strapparava and Mihalcea, 2007</ref>) test set. Annotations indicate the de- gree of emotion in an emotion-headline pair (Af- fect (Emotion)) and the overall positive or negative valence of a headline (Affect (Valence)). Snow et al. report an average correlation among annotators of 0.669 (emotion) and 0.844 (valence).</p><p>WebRel. WebRel was originally created for the <ref type="bibr">TREC 2010</ref><ref type="bibr">Relevance Feedback Track (Buckley et al., 2010</ref>, and its annotations indicate the rele- vance of web documents retrieved for queries. The full dataset contains crowdsourced annotations for 20,232 topic-document pairs; 3277 of those pairs additionally have gold-standard labels. The num- ber of annotations collected per instance varied. We used the subset of instances with gold stan- dard labels and at least five annotations, and recon- structed their HIT groupings based on the workers that annotated each instance (we assumed all in- stances annotated by the exact same set of work- ers were originally from the same HIT). Average correlation per HIT was 0.102 (quite noisy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We compare our approach to a number of alter- native methods, detailed with justifications in Ta- ble 4. The alternatives are popular aggregation techniques that address different potential flaws in non-expert annotation. We train our approach on the training (and validation, for our dataset) data, and test on the test set. Since MACE (used for Item-Response) learns from and outputs pre- dictions for the same data, we provide it with the entire dataset (training, validation if available, and test), but report its results for the test in- stances only. We provide input to MACE in an n- dimensional sparse matrix (1 row per instance and 1 column per each of n distinct annotators in the dataset, with filled values only for the annotators who provided annotations for that instance), since the approach requires knowledge of which annota- tor provided each annotation to function properly. <ref type="bibr">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Majority Vote</head><p>The most frequent label given by annotators for the instance. Ties were broken by taking the highest of the tied labels-assumes the most popular opinion should be trusted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highest</head><p>The highest label for the instance-assumes those who see a metaphor should be trusted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Item- Response</head><p>The prediction expected from an item- response model. We use MACE ( <ref type="bibr" target="#b7">Hovy et al., 2013</ref>) to generate predictions since it is a well-documented item-response ap- proach that is publicly available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mode Average</head><p>The real-valued average of the mode(s) of the instance's labels (if only one mode, this feature is that mode)-assumes popu- lar opinions should be trusted, and equally popular opinions are equally trustworthy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average</head><p>The average of all five labels-assumes each annotator's opinion is equally valid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule- Based</head><p>Assigns a value of 0 if 4+ annotators labeled the instance as such; otherwise, takes the avg. non-zero label-assumes annotators frequently miss tricky or subtle instances. We also evaluate the performance of different feature subsets on our data. All−Averages con- tains all features except for AVG., WEIGHTED AVG., and WEIGHTED AVG. (GOOD). Each other subset contains all features except for the respec- tive feature type noted from <ref type="table" target="#tab_0">Table 2</ref>. The corre- lation coefficient (r) and root mean squared er- ror (RMSE) were recorded for each test condition since our estimator produced continuous-valued scores. Since Mode Average, Average, and Rule- Based result in continuous values and Majority Vote, Highest, and Item-Response result in discrete values, we present two versions of our results; in one, predictions were rounded to the nearest inte- ger (forcing a 0, 1, 2, or 3) and in the other, they were left as-is. For the discrete approaches on our data, we also report accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The results are presented in Tables 5, 6, and 7. <ref type="table" target="#tab_3">Table 5</ref> compares our method with each alter- native approach on our data, and <ref type="table" target="#tab_4">Table 6</ref> com- pares our method with the alternatives on each third-party dataset. <ref type="table">Table 7</ref> shows the results of the feature ablation. On our dataset, our ap- proach outperformed all other approaches, with r = 0.594 with the gold standard and RMSE (0-3) = 0.605. This represented correlation improve- ments of 18.6%, 11.9%, and 69.2% relative to the continuous alternative approaches (Mode Aver- age, Average, and Rule-Based, respectively). The   rounded predictions also outperformed all discrete alternatives (Majority Vote, Highest and Item- Response) with relative correlation improvements of 10.6%, 66.1%, and 35.4%, respectively. All ap- proaches had strong positive statistically signifi- cant (p&lt;&lt;0.0001) correlations and the improve- ment of our results over the alternatives was sta- tistically significant (p&lt;&lt;0.0001). On WebRel and Affect (Valence), our approach outperformed all other approaches for both the dis- crete and continuous conditions. On Affect (Emo- tion), our approach outperformed all alternatives for the discrete condition and had a lower RMSE than all other approaches for the continuous condi- tion (relative reductions in error to RULE-BASED, AVERAGE, and MODE AVERAGE were 37.4%, 0.6%, and 24.2%, respectively), but the predic- tions from AVERAGE correlated better with the gold standard than did those of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rounded</head><p>Continuous Feature Set r RMSE r RMSE All 0.490 0.690 0.594 0.605 All−Annotations 0.440 0.716 0.557 0.627 All−Avg. R 0.480 0.701 0.581 0.611 All−Avg. R (G.) 0.494 0.692 0.582 0.611 All−Averages 0.465 0.703 0.594 0.607 All−HIT R 0.486 0.693 0.587 0.608 <ref type="table">Table 7</ref>: Feature subset performance comparison.</p><p>Interestingly, <ref type="table">Table 7</ref> shows that the discrete version of our approach performed slightly better when the features indicating annotators' correla- tions with good annotators were removed; this was not the case for the continuous-labeled version. The raw annotations themselves were the most valuable features for both cases. Their removal led to a correlation reduction of 10.2% (rounded) and 6.2% (continuous) relative to using all features.</p><p>The results suggest that our approach is a suit- able means of automatically aggregating noisy crowdsourced labels, and that reasonable results can be obtained even when training on only a small amount of expert-adjudicated instances. Further, the performance of the alternative approaches sug- gests that typical aggregation techniques may be less suitable for tasks with many workers who completed relatively few annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present a regression-based ag- gregation method that addresses multiple poten- tial flaws or biases in non-expert human annota- tion. We show that the predictions from our ap- proach correlate at r=0.594 with expert adjudica- tions for a noisy, difficult task, outperforming the best alternative approach by 11.9% on our data and by up to 63.7% on third-party crowdsourcing datasets. This improvement shows that a learn- ing approach can overcome some of the challenges faced by simple label aggregation techniques for these types of tasks. Our data and source code is publicly available for further research by others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 : Features used.</head><label>2</label><figDesc></figDesc><table>Affect 
(Emo.) 
Affect 
(Val.) 
WebRel Ours 

Instances 
600 
100 
2439 
3112 
Annotators 
38 
38 
722 
237 
Annotators / 
Instance 
10 
10 
5 
5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 4 : Alternative Approaches.</head><label>4</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with alternative methods. 

Method 
r 
RMSE 

Affect (Emotion) 

Majority Vote 
0.510 23.2 
Highest 
0.416 52.4 
Item-Response 0.526 21.8 
Ours (R) 
0.578 16.6 
Mode Average 0.506 21.9 
Average 
0.613 16.7 
Rule-Based 
0.462 26.5 
Ours (C) 
0.578 16.6 

Affect (Valence) 

Majority Vote 
0.423 50.1 
Highest 
0.573 75.3 
Item-Response 0.483 46.0 
Ours (R) 
0.938 18.4 
Mode Average 0.644 37.4 
Average 
0.926 22.4 
Rule-Based 
0.913 19.7 
Ours (C) 
0.938 18.4 

WebRel 

Majority Vote 
0.325 1.0 
Highest 
0.219 1.2 
Item-Response 0.385 0.9 
Ours (R) 
0.412 0.8 
Mode Average 0.350 0.9 
Average 
0.372 0.8 
Rule-Based 
0.282 0.9 
Ours (C) 
0.523 0.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison on third-party datasets. 

</table></figure>

			<note place="foot" n="2"> Our data can be downloaded at http://hilt.cse. unt.edu/resources.html, and our source code is available at https://github.com/natalieparde/ label-aggregation.</note>

			<note place="foot" n="3"> Sentences are from Lewis Carroll&apos;s Alice in Wonderland.</note>

			<note place="foot" n="4"> Turkers who assigned the same label to every instance, or whose assignments had already been filtered for some other reason (e.g., violating the minimum time threshold). 5 We also include a second copy of these features ordered by the annotators&apos; average r values.</note>

			<note place="foot" n="6"> Note: Item-response approaches are better-suited to scenarios in which fewer workers annotate more instances each, but our results would also improve under such circumstances where a worker&apos;s trustworthiness, as measured by average r value, is more reliable.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the NSF Graduate Research Fellowship Program under Grant 1144248, and the NSF under Grant 1262860. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Overview of the trec 2010 relevance feedback track (notebook)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Joon</forename><surname>Smucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Nineteenth Text Retrieval Conference (TREC) Notebook</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="88" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Momresp: A bayesian model for multi-annotator document labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Felt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbie</forename><surname>Haertel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making the most of crowdsourced document annotations: Confused supervised lda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Felt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="194" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic annotation aggregation with conditional crowdsourcing models and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Felt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ringger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Seppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The weka workbench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Appendix for &quot;Data Mining: Practical Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>fourth edition. edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kam</forename><surname>Tin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning whom to trust with mace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1120" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimation of discourse segmentation labels from crowd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><forename type="middle">J</forename><surname>Passonneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2190" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identifying unreliable and adversarial workers in crowdsourced labeling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Jagabathula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshminarayanan</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devavrat</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">To re (label), or not to re (label)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Christopher H Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second AAAI Conference on Human Computation and Crowdsourcing</title>
		<meeting>the Second AAAI Conference on Human Computation and Crowdsourcing</meeting>
		<imprint>
			<publisher>HCOMP</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational inference for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander T</forename><surname>Ihler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="692" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Vikas C Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good?: Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A method for linguistic metaphor identification: From MIP to MIPVU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerard J Steen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Aletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Kaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trijntje</forename><surname>Krennmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pasma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Benjamins Publishing</publisher>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semeval2007 task 14: Affective text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
