<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Encoding Gated Translation Memory into Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
							<email>dyxiong@suda.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<settlement>Suzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Encoding Gated Translation Memory into Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3042" to="3047"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3042</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Translation memories (TM) facilitate human translators to reuse existing repetitive translation fragments. In this paper, we propose a novel method to combine the strengths of both TM and neural machine translation (NMT) for high-quality translation. We treat the target translation of a TM match as an additional reference input and encode it into NMT with an extra encoder. A gating mechanism is further used to balance the impact of the TM match on the NMT decoder. Experiment results on the UN corpus demonstrate that when fuzzy matches are higher than 50%, the quality of NMT translation can be significantly improved by over 10 BLEU points.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural machine translation, an emerging machine translation (MT) technology, has made remarkable progress in the past few years ( <ref type="bibr" target="#b11">Sutskever et al., 2014</ref>), which strongly encourages many translation agencies to embrace it for prod- uct deployment. A natural question during this deployment is how the strengths of both the tra- ditional TM and new NMT technologies can be combined together for professional high-quality translation.</p><p>Such attempts to the TM and MT combination have been already conducted in the context of sta- tistical machine translation (SMT). A variety of efforts have been made to incorporate matched translation segments from TM into SMT ( <ref type="bibr" target="#b7">Koehn and Senellart, 2010)</ref>. Partially inspired by these efforts, we aim at combining TM and NMT in this paper.</p><p>Different from TM and SMT, both of which use symbolic fragments to construct translations, NMT induces translations from a real-valued con- tinuous space. Furthermore, NMT is trained in an * Corresponding author end-to-end fashion, which makes it not easy to be amenable to external intervention. Therefore, in- corporating TM as external knowledge into NMT is challenging.</p><p>In this paper, we propose a novel and effective method to address this issue in the combination of TM and NMT. The key idea behind this method is to mimic human translators in translating a source sentence given a similar source sentence with a translation. We treat the matched TM translation as an additional signal and try to encode it with a new encoder to guide the NMT decoder to trans- late the current sentence. Specifically, we first find the sentence that is most similar to the current source sentence from TM by calculating their se- mantic similarity based on sentence embeddings. In order to prevent the TM matched translation from dominating the decoding process, we intro- duce a gate mechanism to balance the TM transla- tion signal and the current source sentence which are encoded separately by two different encoders.</p><p>A series of experiments on the Chinese-English UN corpus demonstrate that when fuzzy matches are over 50%, the proposed method can signifi- cantly improve NMT with the gated TM signal. We also conduct an in-depth analysis on the TM gate, which shows that the gate can indeed reg- ulate the information flow from TM to the NMT decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Encoding Gated TM into NMT</head><p>In this section, we elaborate our proposed method that encodes translation memories into neural ma- chine translation with a gating mechanism. We re- fer to our method as NMT-GTM, which consists of three essential components: i) coupled encoders that encodes both the source sentence and matched TM translation separately, ii) a TM gating net- work that controls the encoded signal from the TM matched translation and iii) a TM-guided decoder that incorporates the gated TM signal into the de- coding. The diagram of NMT-GTM is shown in <ref type="figure">Figure 1</ref>.</p><p>For each source sentence src, we retrieve TM to find the most similar sentence to it. Different from the combination of TM and SMT, we de- fine the best TM match as the sentence with the highest cosine similarity which is calculated based on sentence embeddings ( <ref type="bibr" target="#b8">Le and Mikolov, 2014</ref>), instead of being selected based on fuzzy match score. This is consistent with NMT that performs in an embedding-defined semantic space. But we display our results in experiments according to fuzzy match scores for easy understanding. We use tm s to denote the most semantically similar sentence to src from TM and tm t its translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Coupled Encoders</head><p>We use a pair of encoders to separately encode the source sentence src and its matched TM transla- tion tm t. Both encoders are running indepen- dently of each other with bidirectional GRU re- current neural networks 1 ( <ref type="bibr" target="#b3">Chung et al., 2014</ref>). Ac- cordingly, two separate attention networks are em- ployed to obtain context representations for both src and tm t, which we denote as c src and c tm t respectively. The attention network for the TM matched translation is able to help detect matched translation segments from tm t for the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TM Gating Network</head><p>When we translate a source sentence, in addition to the input of the sentence itself, we also have a TM matched translation (tm t) semantically simi- lar to the sentence as an additional input. We want the additional input to act as a translation example for providing positive guide to target word predic- tion. In order to balance the information flow from the two inputs (src and tm t) into the decoder, we further introduce a TM gating network to control the respective proportions of tm t and src, par- tially inspired by <ref type="bibr" target="#b12">Tu et al. (2017)</ref> who propose a gating mechanism to combine source and target contexts. We formulate the TM gating network as follows:</p><formula xml:id="formula_0">g tm = f (s t−1 , y t−1 , c src , c tm t )</formula><p>where s t−1 is the previous hidden state, y t−1 is the previously predicted target word, and f is a train dev test #Sentences 1, 117, 452 804 1, 614 Average FMS 0.1890 0.5493 0.5392 <ref type="table">Table 1</ref>: Statistics of the training data, develop- ment and test set. FMS: fuzzy match score.</p><p>logistic sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">TM-Guided Decoder</head><p>In the TM-guided decoder, we integrate the gated TM information into the decoding process and use the context representations of src and tm t to pre- dict the hidden state of the decoder in each time step. The decoder hidden state s t is computed as follows:</p><formula xml:id="formula_1">s t = GRU (s t−1 , y t−1 , c src * (1−g tm ), c tm t * g tm )</formula><p>where * is an element-wise multiplication.</p><p>The conditional probability of the next word y t is calculated as follows:</p><formula xml:id="formula_2">p(y t |y &lt;t , src) = g(f (s t , y t−1 , c src ))</formula><p>Please notice that we only incorporate the gated TM into the hidden state of the decoder, rather than the prediction of the next word. Our goal is to correctly translate the source sentence with ref- erence to the translation of the TM match tm t. In other words, tm t only plays a supporting role in translation. We don't want too much infor- mation from TM to affect the translation of the source sentence. Therefore, we incorporate the gated TM in a way that it can only indirectly influ- ence the target generation via hidden states. In our experiments, we observe that this helps our pro- posed model to faithfully translate a source sen- tence, instead of copying all information from the TM matched translation, especially for source sen- tences with slight differences (e.g., dates or num- bers) from TM matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We conducted a series of experiments on Chinese- English corpus to evaluate the effectiveness of the proposed NMT-GTM and analyzed the TM gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Our data come from the Chinese-English United Nations Parallel Corpus ( <ref type="bibr" target="#b10">Rafalovitch et al., 2009)</ref>, which consists of official records and other par- liamentary documents. Since large-scale public  </p><formula xml:id="formula_3">F M S(s a , s b ) = 1 − Levenshtein(s a , s b ) max(|s a |, |s b |)</formula><p>where Levenshtein(s a , s b ) is the word-based Levenshtein Distance between s a and s b . The fuzzy match score can also be calculated with other methods, e.g., the method introduced in ( <ref type="bibr" target="#b1">Bloodgood and Strauss, 2015)</ref>. We leave FMS es- timated with different methods to our future work. We selected all pairs (s a /t a , s b /t b ) with a fuzzy match score F M S &gt;= 0.5. From those pairs with F M S &lt; 0.5, we randomly selected 20% of them. These selected pairs were then divided into 9 groups according to their fuzzy match scores (e.g., F M S ∈ [0.5, 0.6)). We randomly chose approximately the same number of sentences from each group to create a development set and test set. The remaining data were used to create the train- ing data (i.e., {(s a , t b , t a ) selected }) and translation memory (i.e., {(s b , t b ) selected }). Statistics of the training data, development and test set are shown in <ref type="table">Table 1</ref>. The numbers of sentences of the test set in each fuzzy match score group are presented in <ref type="table" target="#tab_0">Table 2</ref>.</p><p>We used RNNSearch as our NMT baseline. We set the maximum sentence length of training cor- pus to 50 words both for the Chinese and English sides. The sizes of vocabularies of both sides were   BLEU scores for translations from RNNSearch, NMT-GTM and TM.  set to 30k. For those words that are not in the vo- cabulary, we replaced them with a special token UNK. We set the dropout to 0.5. All the other set- tings were the same as those described by . We used the stochastic gra- dient descent algorithm with Adam ( <ref type="bibr" target="#b6">Kingma and Ba, 2014</ref>) to train NMT models. The learning rate was set to 0.0004. The size of mini-batch was set to 80 sentences. The beam size was set to 10 dur- ing decoding.</p><p>For the proposed NMT-GTM model, we used tuples (src, tm t, tgt) as input. The rest of the parameter settings were consistent with the base- line model. To calculate the cosine similarity, we used the fasttext tool 2 with the dimension of 100 to obtain sentence embeddings. <ref type="table" target="#tab_2">Table 3</ref> shows the results of different NMT sys- tems measured by BLEU ( <ref type="bibr" target="#b9">Papineni et al., 2002</ref>). From the table, we can find that when fuzzy match scores are over 50%, the extra introduction of TM information can significantly help NMT to better translate. Even when fuzzy match scores are lower than 50%, the translation quality does not drop too much. On the entire test set, the proposed gated combination model of TM and NMT improves the translation quality by 10.32 BLEU points over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>In addition, in order to investigate how simi- lar the matched TM translations tm t are to the reference translations ref , we also measured the BLEU scores of the matched TM translations against the reference translations. The results are also shown in <ref type="table" target="#tab_2">Table 3</ref>, indicated as TM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Analysis</head><p>We further took a deep look into how the TM gate is varying when we incorporate TM matches with different fuzzy match scores. As a comparison, we used the reference translations as the matched TM translations and incorporated them into NMT- GTM to check the changes of the gate. The BLEU scores measured when we used reference transla- tions as matched TM translations as well as aver- age gate values are shown in <ref type="table" target="#tab_3">Table 4</ref>. The results demonstrate that when the matched TM is seman- tically closer to the current source sentence, the TM gate is larger, indicating that more informa- tion from the matched TM translation is used to guide the decoder. <ref type="table" target="#tab_4">Table 5</ref> shows an example from our test set. The highlighted fragments of the source sentence and the matched TM source sentence are not actually the same in terms of their surface forms. However, they are semantically close and can be translated into the same target translation. Our proposed NMT-GMT is able to successfully incorporate the translation of such a fragment into the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Various strategies have been proposed to combine TM and SMT ( <ref type="bibr" target="#b7">Koehn and Senellart, 2010;</ref><ref type="bibr" target="#b5">He et al., 2010)</ref>. Their key ideas are to integrate the translations of the same fragments from TM into SMT, and let SMT only translate those different parts. In order to better model this process, <ref type="bibr" target="#b13">Wang et al. (2013</ref><ref type="bibr" target="#b14">Wang et al. ( , 2014</ref> use different features to allow relevant TM information to guide SMT decoding. src 43 ref the chairman said that the representative of zimbabwe asked to participate in the discussion of the item in accordance with rule 43 of the rules of procedure . tm s 43 tm t the chairman said that the representative of serbia had asked to participate in the discussion of the item in accordance with rule 43 of the rules of procedure . RNNSearch the chairman said that the representative of zimbabwe, in accordance with rule 43, requested a discussion of the item . NMT- GTM the chairman said that the representative of zimbabwe had asked to participate in the discussion of the item in accordance with rule 43 of the rules of procedure . The related work on combining TM and NMT is quite limited. <ref type="bibr" target="#b4">Gu et al. (2017)</ref> propose a TM- NMT model that first finds the most similar seg- ments through search engines according to fuzzy match scores and saves them as key-value pairs in memory. In the subsequent decoding, the saved information is used to help decoding. Our work is significantly different from theirs in two aspects. First, we use semantic similarity based on sen- tence embeddings to detect the best TM matches rather than the fuzzy match score. Second, we en- code the entire TM matched translation rather than segments into NMT with coupled encoders and a gating network.</p><p>Our work is also related to multi-source NMT ( <ref type="bibr" target="#b15">Zoph and Knight, 2016)</ref>. The difference is that in our case, the multiple source inputs are just se- mantically similar, rather than identical. This is the reason that we use a gate to combine these in- puts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>In this paper, we have presented a novel gated method to encode translation memory into NMT so as to convey the information of the matched TM translation into the NMT decoder. Extensive ex- periments verify that our method can indeed effec- tively improve translation quality, especially when fuzzy match scores are higher than 50%. Further analysis reveals that the proposed TM gate is able to vary according to the similarity between the matched TM translation and the current sentence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>í</head><label></label><figDesc>Figure 1: Model Architecture of NMT-GTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FMS</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The numbers of sentences of the test set 
in each fuzzy match score group. 

translation memories are not easily available, we 
built a translation memory from the UN corpus. 
Specifically, we divided the Chinese-English UN 
corpus into two parts U N a and U N b with equal 
size. For each source sentence s a from U N a , 
we chose the source sentence s b from U N b that 
has the highest semantically similarity to s a , com-
puted in the way described in the last section. In 
doing so, we built a corpus with matched pairs 
(s a /t a , s b /t b ) where t a/b are translations corre-
sponding to s a/b . Then we computed the fuzzy 
match score for each pair of source sentences as 

follows: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Changes of the TM gate. The second col- umn shows the BLEU scores with reference trans- lations being used as additional TM inputs. The third column represents the average gate values of the standard setting, while the last column repre- sents the average gate values when references are used as additional TM inputs.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>A translation example from the test set. Semantically similar fragments are highlighted with red color.</figDesc><table></table></figure>

			<note place="foot" n="1"> In this paper, we use GRU encoders and decoders. However, our method can be applicable to other encoders and decoders.</note>

			<note place="foot" n="2"> Available at: https://fasttext.cc/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The present research was supported by the National Natural Science Foundation of China (Grants No. 61622209 and 61861130364 ). We would like to thank three anonymous reviewers for their insightful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Translation memory retrieval methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bloodgood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Strauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07267</idno>
		<title level="m">Search Engine Guided NonParametric Neural Machine Translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bridging SMT and TM with translation recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">Way</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="622" to="630" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convergence of translation memory and statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AMTA Workshop on MT Research and the Translation Industry</title>
		<meeting>AMTA Workshop on MT Research and the Translation Industry</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">United nations general assembly resolutions: A sixlanguage parallel corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Rafalovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the MT Summit</title>
		<meeting>the MT Summit</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="292" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context gates for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="99" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating translation memory into phrase-based machine translation during decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamically integrating cross-domain translation memory into phrase-based machine translation during decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Yih</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="398" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-source neural translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="30" to="34" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
