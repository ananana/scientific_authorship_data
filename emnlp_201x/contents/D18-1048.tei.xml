<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Multi-pass Decoder for Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwei</forename><surname>Geng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Multi-pass Decoder for Neural Machine Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="523" to="532"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>523</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although end-to-end neural machine translation (NMT) has achieved remarkable progress in the recent years, the idea of adopting multi-pass decoding mechanism into conventional NMT is not well explored. In this paper, we propose a novel architecture called adaptive multi-pass decoder, which introduces a flexible multi-pass polishing mechanism to extend the capacity of NMT via reinforcement learning. More specifically, we adopt an extra policy network to automatically choose a suitable and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on Chinese-English translation demonstrate the effectiveness of our proposed adaptive multi-pass de-coder upon the conventional NMT with a significant improvement about 1.55 BLEU.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the past several years, end-to-end neural ma- chine translation (NMT) <ref type="bibr" target="#b10">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b17">Sutskever et al., 2014;</ref>) has attracted increas- ing attention from both academic and industry communities. Compared with conventional sta- tistical machine translation (SMT) ( <ref type="bibr" target="#b1">Brown et al., 1993;</ref><ref type="bibr" target="#b12">Koehn et al., 2003)</ref>, which needs to explic- itly model latent structures, NMT adopts a uni- fied encoder-decoder framework to directly trans- form a source sentence into a target sentence. Fur- thermore, the introduction of attention mechanism ( ) enhances the capability of NMT in capturing long-distance dependencies.</p><p>Recently, a number of authors have endeav- ored to adopt the polishing mechanism into NMT. Similar to human cognitive process for writing a good paper, their models first create a complete * Corresponding author.</p><p>Source hésh`hésh`ızhù xi¯ ansh¯ eng d¯ e wěirènq¯ ı wéi y¯ ı nián , yˇıyˇı p` eihé qí wéifángwěihù ı wěiyuán d¯ e r` engq ¯ ijì emˇanemˇan r` ıq¯ ı , qít¯ a x¯ ın wěirèngwěiyu¯ an d¯ e r` engq¯ ı zé wéi liˇangníanliˇangnían .</p><p>Reference all appointments are for two years , except that of mr ho sai -chu 's which is for one year in order to tie in with the expiry date of his appointment as an ha member .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1st-pass</head><p>mr ho sai -chu 's UNK is a year -long term of two years with a term of two years as the term of his term of office of the ha .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2nd-pass</head><p>mr ho sai -chu 's UNK is a year -long term of two years with a term of two years to serve as the term of office of the ha .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3rd-pass</head><p>mr ho sai -chu 's UNK is a year -long term of two years with a term of two years to tie in with the expiry date of his term of office .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4th-pass</head><p>mr ho sai -chu has been serving as a member of authority for a term of two years with a term of two years . draft and then polish it based on global under- standing of the whole draft ( <ref type="bibr" target="#b14">Niehues et al., 2016;</ref><ref type="bibr" target="#b2">Chatterjee et al., 2016;</ref><ref type="bibr" target="#b22">Zhou et al., 2017;</ref><ref type="bibr" target="#b20">Xia et al., 2017;</ref><ref type="bibr" target="#b9">Junczys Dowmunt and Grundkiewicz, 2017)</ref> . Moreover, <ref type="bibr" target="#b21">Zhang et al. (2018)</ref> introduces a backward decoder to better exploit the right-to- left target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task.</p><p>Although these polishing mechanism-based ap- proaches demonstrate their effectiveness with two- pass decoding, the idea of multi-pass decoding is not well explored for NMT. Motivated by it, we first propose a novel multi-pass decoder to per- form the translation procedure with a fixed number of decoding passes, referred to as decoding depth. According to the preliminary results, just as ex- pected, multi-pass decoding really benefit to most translations. However, in some cases, the more decoding passes perhaps lead to the poor trans- lation. For example in <ref type="table" target="#tab_0">Table 1</ref>, the 3rd-pass de-coding achieves a better result compared to 1st- and 2nd-pass decoding. Nevertheless, a drastic de- crease arises, when we perform the 4th-pass de- coding. Therefore, it's necessary to introduce a flexible multi-pass decoding, which has the ability to adaptively choose the suitable decoding passes.</p><p>Towards above goal, we further propose a novel framework called adaptive multi-pass decoder to automatically choose a proper decoding depth us- ing reinforcement learning. Our model considers multi-pass decoding as a sequential decision mak- ing process, where continuing decoding or halt is chosen at each step. An extra policy network is employed to learn to automatically choose to con- tinue next pass decoding or halt via reinforcement learning. For the purpose of making accurate and effective choices, the policy network employs re- current neural network to capture the complexity of source sentence as well as the difference be- tween the consecutive generated translations. Ex- tensive experiments on Chinese-English transla- tion show the proposed adaptive multi-pass de- coder is capable of choosing a suitable decoding depth and significantly improves translation per- formance over conventional NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Given a source sentence x = x 1 , . . . , x m , . . . , x M and a target sentence y = y 1 , . . . , y n , . . . , y N , end-to-end neural machine translation directly models translation probability word by word as a single, large neural network:</p><formula xml:id="formula_0">P (y|x; θ) = N n=1 P (y n |x, y &lt;n ; θ)<label>(1)</label></formula><p>where θ is a set of model parameters and y &lt;n de- notes a partial translation. Prediction of n-th word is generally made in an encoder-decoder frame- work:</p><formula xml:id="formula_1">P (y n |x, y &lt;n ; θ) = g(y n−1 , s n , c n )<label>(2)</label></formula><p>where g(·) is a non-linear function, y n−1 denotes the previously generated word, s n is n-th decoding hidden state, and c n is a context vector for gener- ating n-th target word. The decoder state s n is computed by RNNs as follows:</p><formula xml:id="formula_2">s n = f (s n−1 , y n−1 , c n )<label>(3)</label></formula><p>where f (·) is an activation function. Actually it's found gated RNN alternatives such as LSTM <ref type="bibr" target="#b7">(Hochreiter and Schmidhuber, 1997</ref>) or GRU ( ) often achieve better performance than vanilla ones. c n is a dynamic vector that se- lectively summarizes certain parts of source sen- tence at each decoding step:</p><formula xml:id="formula_3">c n = M m=1 α n,m h m (4)</formula><p>where α m,n measures how well x m and y n are aligned, calculated by attention model ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>, and h m is the en- coder hidden state of the m-th source word. For the purpose of capturing both forward and back- ward contexts, bidirectional RNN ( <ref type="bibr" target="#b16">Schuster and Paliwal, 1997</ref>) is often employed as the encoder which converts the source sentence into an annota-</p><formula xml:id="formula_4">tion sequence h = {h 1 , . . . , h m , . . . , h M }, where h m = [ − → h m , ← − h m ]</formula><p>captures information about m- th word with respect to the preceding and follow- ing words in the source sentence respectively.</p><p>Although the introduction of RNNs as a de- coder has resulted in substantial improvements in terms of translation quality, simultaneously it im- poses a serious restriction on the capability of encoder-decoder framework caused by the struc- ture of RNNs. That is, when the RNN decoder generates the t-th word y t in decoding phase, only y &lt;t can be utilized, while the possible words y &gt;t are directly neglected. Thus, it's difficult to capture global information especially the un- generated words for the current dominant RNN decoder without new significant innovation. Un- der the premise of preserving the original struc- ture, a promising alternative to address the afore- mentioned issue is to incorporate with auxiliary neural networks to extend the RNN decoder.</p><p>Towards above goal, polishing mechanism- based methods first capture the global information through a complete draft created by SMT or NMT, and then take it as input to finally generate a trans- lation. Compared with conventional NMT, polish- ing mechanism-based methods make a more accu- rate prediction at each time-step due to the extra global understanding, resulting in more fluent and grammatically correct translation. While these approaches have demonstrated the effectiveness, previous approaches follow pre-defined routes to perform the decoding procedure, not considering choosing a suitable decoding depth for the com- plexity of source sentences completely.</p><p>Therefore, it's important to develop a novel framework for making an accurate and effective choice about which decoding depth is appropriate for the source sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive Multi-pass Decoder</head><p>In this section, we present an adaptive multi-pass decoder for neural machine translation, as illus- trated in <ref type="figure">Figure 1</ref>. It could choose a proper de- coding depth, depending on the complexity of the source sentence. As shown in <ref type="figure">Figure 1</ref>, our model includes three major components: an encoder to summarize source sentences with parameter set θ e , a multi-pass decoder for multi-pass decoding with parameter set θ d , and a policy network to choose a suitable depth with parameter set θ p . The en- coder of our model is identical to that of the domi- nant NMT which is modeled using a bidirectional RNN. Please refer to  for more details. We will elaborate the multi-pass de- coder and policy network for adaptive multi-pass decoding in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-pass Decoder</head><p>The multi-pass decoder is extended from the one of the dominant NMT model to leverage the target-side context. Similar to the dominant NMT model, our multi-pass decoder also performs the decoding under the semantic guide of source-side context captured by the encoder, whereas more importantly and differently, the global understand- ing through the target-side context provided by last pass decoding, is able to strongly assist our model to produce a better translation. Given the source-side and target-side contexts separately captured by the encoder and last pass decoding, the multi-pass decoder learns to gener- ate next target word, based on previous generated words. Using the multi-pass decoder with parame- ter set θ d , we calculate the conditional probability of the translationˆytranslationˆ translationˆy l at the l-th decoding pass as follows:</p><formula xml:id="formula_5">P (ˆ y l |x, ˆ y l−1 ; θ e , θ d ) = N l n=1 P (ˆ y l n |x, ˆ y l &lt;n , ˆ y l−1 ; θ e , θ d ) = N l n=1 g dec (ˆ y l n−1 , s l,dec n , c l,enc n , c l,dec n ) (5)</formula><p>where g dec (·) is a non-linear function, and s l,dec n denotes the n-th decoding state within the l-th de- coding pass. N l indicates the length of generated translation at the l-th decoding pass. The decoding state s l,dec n is obtained by RNNs as follows:</p><formula xml:id="formula_6">s l,dec n = f dec (s l,dec n−1 , ˆ y l n−1 , c l,enc n , c l,dec n ) (6)</formula><p>where f dec (·) is the GRU activation function. c l,enc n and c l,dec n denote source-side and target-side contexts at the n-th time step within the l-th de- coding pass, respectively. It should be noted that when the multi-pass decoder performs the first de- coding, there doesn't exist any generated transla- tion. To address this case, the first-pass target-side context c 1,dec is set to zero.</p><p>Among the aforementioned contexts, c l,enc n is obtained as the weighted sum of the source-side hidden states {h m }, while we take the target-side hidden states {s l−1,dec n } produced by last pass de- coding as input to compute c l,dec n . Similar to the dominant NMT model, we adopt the atten- tion model ( <ref type="bibr" target="#b13">Luong et al., 2015</ref>) to calculate the weights, which indicate the alignment probability. We assume that attn enc de- notes the encoder-decoder attention model, which takes the source annotations {h m } as input, while attn dec are introduced to calculate the weight which measures how well the decoding state s l,dec attends the last-pass hidden states {s l−1,dec n }. Assuming s a indicates the decoding state, which attends the annotations {s b k } with a length K, our attention model calculates the context vec- tor s c as follows:</p><formula xml:id="formula_7">s c = K k=1 α k s b k (7) α k = exp(e k ) K k =1 exp(e k ) (8) e k = (v a ) T tanh(W a s a + U a s b k ) (9)</formula><p>where v a , W a and U a are the parameters of atten- tion model. Given a training (x, y), the translation route can be demonstrated as:</p><formula xml:id="formula_8">x → ˆ y 1 → . . . → ˆ y l → . . . → ˆ y L (x,y) −1 → y.</formula><p>The intermediate transla- tions {ˆy{ˆy l } are generated by decoding. Given a training corpus D = {x, y}, we define the object function using cross-entropy at last pass decoding as follows: </p><formula xml:id="formula_9">J dec (θ e , θ d ) = − 1 |D| arg min θe,θ d (x,y)∈D {log P (y|x, ˆ y L (x,y) −1 ; θ e , θ d )}<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Policy network</head><p>Policy state representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>Figure 1: The architecture of our adaptive multi-pass decoder. Given the annotation sequence produced by the encoder, a policy network is adopted to choose a suitable action from the set {Continue, Stop}, which indicates continuing next pass decoding, or halt respectively. Different from the conventional decoder which only obtains the source-side context with the source attention model attn enc , our multi-pass decoder also captures the target- side context of last-pass decoding with decoder attention model attn dec . The policy network also use attn policy to collect useful information from the multi-pass decoding to choose an accurate and effective action to generate a good translation. Note that in this work the same parameters set of decoder and the corresponding attention is shared among different decoding passes. For this figure, we demonstrate a translation procedure with 3-pass decoding controlled by adaptive multi-pass decoder.</p><p>where</p><formula xml:id="formula_10">P (y|x, ˆ y L (x,y) −1 ; θ e , θ d ) is conditional probability computed by multi-pass decoder.</formula><p>L (x,y) indicates the decoding depth for the in- stance (x, y). For effectiveness, note that all the intermediate translations {ˆy{ˆy l } are generated by greedy search in training and testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Policy Network</head><p>The multi-pass decoding can be converted into se- quential decision making process, in which a pol- icy is adopted to choose next pass decoding or halt. It's expected to automatically choose an accurate and effective decoding depth to generate a good translation. For example, if the source sentence is exhausted to obtain the corresponding transla- tion such as the long sentences, we assume more decoding passes are needed to improve the trans- lation, while only one pass decoding is enough to tackle the simple case.</p><p>Our main idea is to use reinforcement learn- ing to control the decoding depth. We parameter- ize the available action a l ∈ {Continue, Stop}, where Continue and Stop indicate continuing next decoding pass and halt respectively, by a pol- icy network π(a l |s policy l ; θ p ), where s policy l repre- sents the policy state at the l-th decoding pass. For the purpose of making a better choice about the de- coding depth and direction, it's necessary to con- sider whether or not the source sentence is easy to obtain a good translation and compared with the last pass decoding, whether the quality of trans- lation can be improved. Thus, supervised by this guideline, the policy state s policy l is calculated by GRU to model the difference between the consec- utive two decoding passes as follows:</p><formula xml:id="formula_11">s policy l = f policy (s policy l−1 , m l )<label>(11)</label></formula><p>where f policy is the activation function, and m l captures the useful information with respect to the policy network at the l-th decoding pass. In this work, we use the attention models attn policy to collect the decoding progress, denoted as m l of the l-th decoding pass. In order to take account of the complexity of source sentence itself, the ini- tial policy state s policy </p><p>where W p and b p are the parameters of the policy network. In this work we use REINFORCE algo- rithm <ref type="bibr" target="#b19">(Williams, 1992)</ref>, which is an instance of a broader class of algorithms called policy gradient methods <ref type="bibr" target="#b18">(Sutton and Barto, 1998)</ref>, to learn the pa- rameter set θ p such that the sequence of actions a = {a 1 , . . . , a l , . . . , a L (x,y) } maximizes the to- tal expected reward. The expected reward for an instance is defined as:</p><formula xml:id="formula_13">J policy (θ p ) = E π(a|s policy ;θp) r(ˆ y L (x,y) ) (13)</formula><p>where r(ˆ y L <ref type="bibr">(x,y)</ref> ) is the reward at the L (x,y) -th de- coding pass. In this work, we use BLEU ( <ref type="bibr" target="#b15">Papineni et al., 2002</ref>) of the final translationˆytranslationˆ translationˆy L (x,y) gener- ated by greedy search as input to compute our re- ward as follows:</p><formula xml:id="formula_14">r(ˆ y L (x,y) ) = BLEU(ˆ y L (x,y) , y)<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe experimental settings and report empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluated the proposed adaptive multi- pass decoder on Chinese-English translation task. To effectively train the NMT model, we trained each model with sentences of length up to 50 words. Besides, we limited vocabulary size to 30K for both languages and map all the out-of- vocabulary words in the Chinese-English corpus to a special token UNK. We applied Rmsprop (Graves, 2013) to train models and selected the best model parameters according to the model per- formance on the development set. During this procedure, we set the following hyper-parameters: word embedding dimension as 620, hidden layer size as 1000, learning rate as 5 × 10 −4 , batch size as 80, gradient norm as 1.0, and dropout rate as 0.3.</p><p>In the experiments, we compared our approach against the following state-of-the-art SMT and NMT systems:</p><p>1 https://github.com/moses- smt/mosesdecoder/blob/master/scripts/generic/multi- bleu.perl <ref type="bibr">2</ref> The training corpus includes LDC2002E18, LDC2003E07, LDC2003E14, part of LDC2004T07, LDC2004T08 and LDC2005T06 1. Moses <ref type="bibr">3</ref> : an open source phrase-based transla- tion system with default configuration and a 4-gram language model trained on the target portion of training data. Note that we used all data to train MOSES ( <ref type="bibr" target="#b11">Koehn et al., 2007</ref>).</p><p>2. RNNSearch: a variant of the attention-based NMT system ( ) with slight changes from dl4mt tutorial 4 .</p><p>3. Deliberation Network 5 : a re-implementation of attention-based NMT system with two in- dependent left-to-right decoders ( <ref type="bibr" target="#b20">Xia et al., 2017</ref>). The first-pass decoder is identical to one of RNNSearch to generate a draft transla- tion, while the second-pass decoder polishes it with an extra attention over the first pass de- coder. The second-pass decoder is integrated with the first-pass decoder via reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ABDNMT:</head><p>As a comparison with the De- liberation Network, ABDNMT utilizes first- pass backward decoder to generate a trans- lation with greedy search, and the second- pass forward decoder refines it with attention model ( <ref type="bibr" target="#b21">Zhang et al., 2018</ref>). For fairness, we replace the first-pass backward decoder with a forward decoder.</p><p>We set the beam size of all above-mentioned models as 10 in our work. Deliberation Net- work and ABDNMT were initialized with the pre- trained RNNSearch as <ref type="bibr" target="#b20">Xia et al. (2017)</ref> and <ref type="bibr" target="#b21">Zhang et al. (2018)</ref> described. Our multi-pass decoder was also initialized with RNNSearch and other pa- rameters were randomly initialized from a uniform distribution on [−0.1, 0.1]. Besides, for effective- ness, we set the maximum decoding depth of our adaptive multi-pass decoder as 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Chinese-English Translation</head><p>The experimental results of our model and base- line models on Chinese-English machine transla- tion datasets are depicted in <ref type="table">Table 2</ref> .   <ref type="table">Table 2</ref>: Evaluation of the NIST Chinese-English translation task. The BLEU scores are case-insensitive. "Params" denotes the number the parameters in each model. The "Speed" denotes the generation speed in seconds on the development set. RNNSearch is an attention-based neural machine translation model( ) with one-pass left-to-right decoding. RNNSearch(R2L) is a variant of RNNSearch with one-pass right-to-left decoding. As a comparison, Deliberation Network ( <ref type="bibr" target="#b20">Xia et al., 2017</ref>) and ABDNMT ( <ref type="bibr" target="#b21">Zhang et al., 2018</ref>) involve two independent decoders to adopt polishing mechanism to extend the ability of conventional NMT. Deliberation Network utilizes two left-to-right decoders coupled with reinforcement learning. However, ABDNMT exploits a backward decoder to perform first-pass right-to-left decoding. {2,3,4,5}-pass decoder utilizes our multi-pass decoder with a fixed number of decoding passes. Furthermore, adaptive multi-pass decoder involves a policy network to enhance our multi-pass decoder to choose a proper decoding depth.</p><p>Parameters RNNSearch, Deliberation Network and ABDNMT have 83.99M, 125.16M and 122.86M parameters, respectively. And the pa- rameter size of our {2,3,4,5}-pass decoder and adaptive multi-pass decoder are about 87.81M and 96.01M, respectively.</p><p>Fixed Decoding Depth {2,3,4,5}-pass decoders perform the left-to-right decoding by the multi- pass decoder with a fixed number of decoding passes. In contrast to the related machine transla- tion systems, our fixed number-pass decoder sig- nificantly outperforms Moses and RNNSearch by 7.53 and 1.05 BLEU points at least, as <ref type="table">Table 2</ref> presents. More importantly, our proposed multi- pass decoder obtains much better performance with an increase of only 3.82M parameters over RNNSearch. As a comparison with Deliberation Network involves two-pass decoding, the multi- pass decoder has a minimum increase of 0.24 BLEU score. Nevertheless, our multi-pass de- coder proves its effectiveness due to the less pa- rameters consumption of 37.35M in contrast to Deliberation Network. These results verify our hypothesis that the more decoding passes can pol- ish the generated output to improve the translation quality. The underlying reason is that the attention component attn dec within our multi-pass decoder can capture the extra target-side contexts to obtain a global understanding to assist the translation pro- cedure. Towards the effect of the decoding depth set {2,3,4,5}, our multi-pass decoders obtain the ap- proximate results, but the whole curve of BLEU is on an upward trend. Specifically, the multi-pass decoder with decoding depth 5 achieves the best performance with 38.64 BLEU, while the one with decoding depth 3 performs the worst among the decoding depth set with 38.55 BLEU. Although the average results of {2,3,4,5}-pass decoder are approximate, the distinction of {2,3,4,5}-pass de- coder on NIST03, NIST04, NIST05, NIST06 and NIST08 is not negligible. These results indirectly prove the necessity of flexibility mechanism.</p><p>Adaptive Decoding Depth our proposed adap- tive multi-pass decoder involves an extra pol- icy network which controls the decoding depth according to the complexity of the source sen- tence and the differences between the consecu- tive generated translations. As shown in <ref type="table">Table  2</ref>, the proposed adaptive multi-pass decoder ob- tains an improvement about 0.41 to 0.5 BLEU on average over the {2,3,4,5}-pass decoder, which demonstrates the effectiveness of the policy net- work. Specifically, the adaptive multi-pass de- coder outperforms the multi-pass decoder with a fixed decoding depth by 0.69, 0.71, 0.68 and 0.45 BLEU scores on NIST03, NIST04, NIST05 and NIST06 datasets at most. In contrast to the Moses, RNNSearch, Deliberation Network and ABDNMT, the adaptive multi-pass decoder has the corresponding improvement about 8.03, 1.55, 0.74 and 0.34 BLEU points, respectively. More importantly, our adaptive multi-pass decoder out- performs ABDNMT, Deliberation Network model with a decrease of 26.85M, 29.15M parameters.</p><p>In order to further demonstrate the effective- ness of adaptively choosing the decoding depth, we investigate the ratio of decoding passes con- sumed by our multi-pass decoder on the devel- opment dataset, as shown in <ref type="table">Table 3</ref>. Our adap- tive multi-pass decoder chooses one-pass decod- ing in a high ratio of 46.57%, while in most about 53.43% cases our model leverages more than one pass decoding to produce a translation. The av- erage decoding depth of our model is calculated as: (1 × 46.36% + 2 × 20.84% + 3 × 13.10% + 4 × 13.55% + 5 × 6.15%) = 2.12. Moreover, our ratio of the samples tends to decrease as the decod- ing depth rises on a whole. Since time consump- tion correlates with decoding depth, our adap- tive multi-pass decoder proves its superior perfor- mance due to fewer parameters and less decoding passes.</p><p>Depth 1 2 3 4 5 Ratio(%) 46.57 20.45 13.00 13.60 6.38 <ref type="table">Table 3</ref>: The ratio of decoding depth chosen by adap- tive multi-pass decoder on the development dataset.</p><p>Time Consumption Due to the multi-pass de- coding mechanism, the major limitation of our proposed multi-pass decoder is time cost. In train- ing phrase, we spend more time training the multi- pass decoder than RNNSearch, Deliberation Net- work and ABDNMT. However, in testing phrase, as illustrated in <ref type="table">Table 2</ref>, our adaptive multi-pass decoder spends about 180s completing the entire testing procedure, in comparison with the corre- sponding 87s, 162s, 132s of RNNSearch, Deliber- ation Network and ABDNMT, due to the auxiliary policy network. These results are consistent with above conclusion drew according to the decoding <ref type="figure">Figure 3</ref>: Ratio of decoding depth set {1,2,3,4,5} con- trolled by our adaptive multi-pass decoder with respect to each length segment of the source sentences on the development dataset.</p><p>depth. Therefore, it's proven the necessity of our proposed auxiliary policy network to choose the decoding depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Source Sentence Length</head><p>Following , we group sen- tences of similar lengths together and compute the BLEU score for each group, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Obviously, our proposed adaptive multi-pass de- coder outperforms RNNSearch in all length seg- ments. Compared with {2,3,4,5}-pass decoders, our adaptive multi-pass decoder outperforms most even all the multi-pass decoders with fixed decod- ing depth in the length segments.</p><p>For the purpose of investigating the flexibility of policy network, we calculate the ratios of decoding depth set {1,2,3,4,5} on each sentence group with similar length, as illustrated in <ref type="figure">Figure 3</ref>. The ratio of one-pass decoding remains high level on each length segment, but explicitly is dominant on the length segment <ref type="bibr">[0,</ref><ref type="bibr">10)</ref>. In contrast, the ratios of remaining decoding depths show upwards trends on a whole. These results indirectly proves that our policy network has the capability of choos- ing the proper decoding depth. That is, when the source sentence is difficult to be translated such as some long sentences, more decoding passes are consumed to improve the translation quality, while in simple cases such as short source sentences, one-pass decoding is adequate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Case Study</head><p>To better understand the effectiveness of flexi- ble polishing mechanism adopted by policy net- work, <ref type="table" target="#tab_4">Table 4</ref> provides a Chinese-English transla-Source x¯ ınhuáshè běij¯ ıng 4yù e 5 r` ıdì an d` amíngdˇamíngdˇıngdˇamíngdˇıngdˇıng d¯ e w¯ eiruˇaneiruˇan g¯ ongs¯ ı zˇongcáizˇongcái bˇıěrbˇıěr · g` aicí r` ıqıán b` ei ji¯ anádà y` ı ji¯ a guˇangb¯guˇangb¯ odì an tái "shù an le " y` ı bˇabˇa , z` ai yúrénjié n` ati¯ an b` ei k¯ ai le y ¯ igè d`awángxì ao .</p><p>Reference xinhua news agency, beijing, april 5 , bill gates , the all -famous microsoft chairman, was duped by a canadian radio station the the other day and fell a victim to a big prank on april fools ' day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1st-pass</head><p>xihua news agency report of april 5 th from beijing (by staff reporter UNK UNK ) -the president of microsoft 's microsoft corporation , gates , was recently " UNK " by a radio station in canada and was hit by a UNK day on the day of the day .</p><p>2nd-pass xinhua news agency, beijing, april 5, a fews days ago, microsoft 's president , microsoft corporation , was " UNK " by a radio station in canada .</p><p>3rd-pass xinhua news agency, beijing, april 5 , microsoft 's president bill gates, the president of microsoft , was " UNK " by a radio station in canada in few days ago .</p><p>4th-pass xinhua news agency, beijing, april 5 , microsoft 's president bill gates, the president of microsoft , was " UNK " by a radio station in canada in few days ago .</p><p>5th-pass xinhua news agency, beijing, april 5 , microsoft 's president bill gates, the president of microsoft , was " UNK " by a radio station in canada in few days ago . tion example. Our proposed adaptive multi-pass decoder has the ability to polish the generated hy- pothesis again and again. As shown in <ref type="table" target="#tab_4">Table 4</ref>, we force our adaptive multi-pass decoder to per- form the multi-pass decoding with fixed depth sets {1,2,3,4,5}. The translation quality has an up- wards trend with decoding depth 1 to 3, and the decoding with depth set {4,5} generates the iden- tical translation as the decoding depth 3. More- over, given the same source sentence, we use the proposed adaptive multi-pass decoder to choose the decoding depth. As expected, our adaptive multi-pass chooses 3-pass decoding which gen- erates best translation and consumes least time, rather than {4,5}-pass decoding. Therefore, these results proves the effectiveness of our adaptive multi-pass decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this work, we mainly focus on how to adopt adaptive polishing mechanism into NMT model, which has attracted intensive attention in recent years. We will elaborate polishing mechanism- based methods in the following pages. The polishing mechanism-based approaches first generate a complete draft, and then improve the quality of it based on the global understanding of the whole draft. A related work is post-editing ( <ref type="bibr" target="#b14">Niehues et al., 2016;</ref><ref type="bibr" target="#b2">Chatterjee et al., 2016;</ref><ref type="bibr" target="#b22">Zhou et al., 2017;</ref><ref type="bibr" target="#b9">Junczys Dowmunt and Grundkiewicz, 2017</ref>): a source sentence e is first trans- lated to f , and then f is refined by another model. <ref type="bibr" target="#b14">Niehues et al. (2016)</ref> used phrase-based statisti- cal machine translation (PBMT) to pre-translate the source sentence into target language, which was taken as input of NMT to generate the final translation. <ref type="bibr" target="#b22">Zhou et al. (2017)</ref> combined phrase- based statistical machine translation (PBMT), hi- erarchical phrase-based statistical machine trans- lation (HPMT) and NMT with a unified architec- ture, similar to the dominant NMT model. Com- pared with the dominant NMT model, two atten- tion models were involved to compute the context vectors. Specifically, an attention model is utilized to calculate the context vector for each machine system, while the other attention model obtains the context vector over the all context vectors of ma- chine systems.</p><p>In above works, the generating and refining are two separate processes. As a comparison, <ref type="bibr" target="#b20">Xia et al. (2017)</ref> proposed deliberation network, which consists of two decoders: a first-pass decoder gen- erates a draft, which is taken as input of second- pass decoder to obtain a better translation. All the components of deliberation network are cou- pled together and jointly optimized in an end-to- end way via reinforcement learning. Instead of first-pass forward decoder, <ref type="bibr" target="#b21">Zhang et al. (2018)</ref> adopted a backward decoder to capture the right- to-left target-side contexts, which is taken as input to assist the second-pass forward decoder to ob- tain a better translation. Besides, the another dif- ference with deliberation network is the second- pass decoder is integrated with the first-pass de- coder without reinforcement learning.</p><p>For the purpose of exploring polishing mech- anism, our model adopts adaptive multi-pass de- coding strategy. Compared with the previous works which consumes no more than two decod- ing passes, our multi-pass decoder makes an at- tempt to perform the multi-pass decoding. More importantly, we adopt adaptive decoding depth controlled by policy network to extend the capac- ity of our multi-pass decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel architecture called adaptive multi-pass decoder to adopt pol- ishing mechanism into the NMT model via rein- forcement learning. Towards this goal, a novel multi-pass decoder is introduced to generate the translation, conditioned on the source-and target- side contexts. Simultaneously, the multi-pass de- coding is supervised by a policy network which learns to choose a suitable action from continu- ing next pass decoding or halt at each time step to maximize the BLEU of the final translation. As a result, our model has the capability of controlling the decoding depth to generate a better translation. Extensive experiments on Chinese-English trans- lation demonstrate the effectiveness of the pro- posed adaptive multi-pass decoder.</p><p>In this paper, we focus on utilizing multi-pass decoder to polish the translation. Our proposed multi-pass decoder performs the multi-pass decod- ing mechanism with only forward decoding. One promising direction is to incorporate the backward decoding into our architecture. More specifically, we can extend the policy network to choose the backward decoding except for forward decoding and halting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>0</head><label></label><figDesc>is computed by s policy 0 = tanh(W init h M ), where h M is last state source an- notations, and W init is the parameters of initializ- ing the policy state. Finally, we take the policy state s policy l as input to calculate the policy as fol- lows: π(a l |s policy l ; θ p ) = sof tmax(W p s policy l + b p )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>System</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of the generated translations with respect to the lengths of the source sentences on the development dataset.</figDesc><graphic url="image-1.png" coords="7,72.56,62.81,214.42,175.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Translation examples of more decoding passes with the proposed multi-pass decoder.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Source encoder Hidden h i Source Attention</head><label></label><figDesc></figDesc><table>Source context 
attention 

last state 

… 
… 

he 
shi 
zhu 
liang nian 

1-pass decoder 

mr 

1-pass target 
decoder state 
ho 
sai 
the 
ha 

… 

1-pass decoder 
attention 
2-pass decoder 

mr 

2-pass target 
decoder state 
ho 
sai 
the 
ha 

… 

2-pass decoder 
attention 
3-pass decoder 

mr 

3-pass target 
decoder state 
ho 
sai 
of 
office 

… 

3-pass decoder 
attention 

Multi-pass decoder 

s 1 

policy 

Continue 

s 2 

policy 

Continue 

s 3 

policy 

Stop 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Translation examples at each decoding depth of adaptive multi-pass decoder.</figDesc><table></table></figure>

			<note place="foot" n="3"> http://www.statmt.org/moses 4 https://github.com/nyu-dl/dl4mt-tutorial 5 We reproduce the deliberation network based on REINFORCE and gumbel-softmax (Jang et al., 2016), separately, but there still exists a gap with its best performance. We attribute this to that our reimplementation may be different from the original model in some unknown details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous review-ers for their insightful comments. We also thank Heng Gong and Shuang Chen for helpful discus-sion. This work was supported by the National Natural Science Foundation of China (NSFC) via grant 61632011, 61772156 and 61502120.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The fbk participation in the wmt 2016 automatic post-editing shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
	<note>Shared Task Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effective deep memory networks for distant supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="19" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A language-independent neural network for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">92106</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<title level="m">Generating sequences with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An exploration of neural sequence-tosequence architectures for automatic post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junczys</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grundkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Herbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</title>
		<meeting>the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume the Demo and Poster Sessions</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pre-translation for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunah</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1828" to="1836" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deliberation networks: Sequence generation beyond one-pass decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1784" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongji</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05122</idno>
		<title level="m">Asynchronous bidirectional decoding for neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural system combination for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
