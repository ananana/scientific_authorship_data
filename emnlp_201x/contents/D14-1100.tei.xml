<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T13:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ambiguity Resolution for Vt-N Structures in Chinese</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 25-29, 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ming</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing-Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National Tsing-Hua University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
							<email>kchen@iis.sinica.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ambiguity Resolution for Vt-N Structures in Chinese</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
						<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) <address><addrLine>Doha, Qatar. c</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="928" to="937"/>
							<date type="published">October 25-29, 2014</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The syntactic ambiguity of a transitive verb (Vt) followed by a noun (N) has long been a problem in Chinese parsing. In this paper, we propose a classifier to resolve the ambiguity of Vt-N structures. The design of the classifier is based on three important guidelines, namely, adopting linguistically motivated features, using all available resources, and easy integration into a parsing model. The linguistically motivated features include semantic relations, context, and morphological structures; and the available resources are treebank, thesaurus, affix database , and large corpora. We also propose two learning approaches that resolve the problem of data sparseness by auto-parsing and extracting relative knowledge from large-scale unlabeled data. Our experiment results show that the Vt-N classifier outperforms the current PCFG parser. Furthermore, it can be easily and effectively integrated into the PCFG parser and general statistical parsing models. Evaluation of the learning approaches indicates that world knowledge facilitates Vt-N disambigua-tion through data selection and error correction .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In Chinese, the structure of a transitive verb (Vt) followed by a noun (N) may be a verb phrase (VP), a noun phrase (NP), or there may not be a dependent relation, as shown in (1) below. In general, parsers may prefer VP reading because a transitive verb followed by a noun object is nor- mally a VP structure. However, Chinese verbs can also modify nouns without morphological inflection, e.g., 養殖/farming 池/pond. Conse- quently, parsing Vt-N structures is difficult be- cause it is hard to resolve such ambiguities with- out prior knowledge. The following are some typical examples of various Vt-N structures:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>解決/solve 問題/problem  VP 解決/solving 方案/method  NP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>解決/solve 人類/mankind (問題/problem)None</head><p>To find the most effective disambiguation fea- tures, we need more information about the Vt-N  NP construction and the semantic relations between Vt and N. Statistical data from the Sini- ca Treebank ( <ref type="bibr" target="#b1">Chen et al., 2003)</ref> indicates that 58% of Vt-N structures are verb phrases, 16% are noun phrases, and 26% do not have any de- pendent relations. It is obvious that the semantic relations between a Vt-N structure and its con- text information are very important for differen- tiating between dependent relations. Although the verb-argument relation of VP structures is well understood, it is not clear what kind of se- mantic relations result in NP structures. In the next sub-section, we consider three questions: What sets of nouns accept verbs as their modifi- ers? Is it possible to identify the semantic types of such pairs of verbs and nouns? What are their semantic relations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem Analysis</head><p>Analysis of the instances of NP(Vt-N) structures in the Sinica Treebank reveals the following four types of semantic structures, which are used in the design of our classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type 1. Telic(Vt) + Host(N):</head><p>Vt denotes the telic function (purpose) of the head noun N, e.g., 研 究 /research 工 具 /tool; 探 測 /explore 機 /machine; 賭/gamble 館/house; 搜尋/search 程 式/program. The telic function must be a salient property of head nouns, such as tools, buildings, artifacts, organizations and people. To identify such cases, we need to know the types of nouns which take telic function as their salient property. Furthermore, many of the nouns are monosyl- labic words, such as 員/people, 器/instruments, 機/machines. Type 2. Host-Event(Vt) + Attribute(N): Head nouns are attribute nouns that denote the attributes of the verb, e.g., 研究/research 方法 /method (method of research); 攻擊/attack 策略 /strategy (attacking strategy); 書寫/write 內容 /context (context of writing); 賭/gamble 規/rule (gambling rules). An attribute noun is a special type of noun. Semantically, attribute nouns de- note the attribute types of objects or events, such as weight, color, method, and rule. Syntactically, attribute nouns do not play adjectival roles <ref type="bibr" target="#b7">(Liu, 2008)</ref>. By contrast, object nouns may modify nouns. The number of attributes for events is limited. If we could discover all event-attribute relations, then we can solve this type of construc- tion.</p><p>Type 3. Agentive + Host: There is only a lim- ited number of such constructions and the results of the constructions are usually ambiguous, e.g., 炒飯/fried rice (NP), 叫聲/shouting sound. The first example also has the VP reading.</p><p>Type 4. Apposition + Affair: Head nouns are event nouns and modifiers are verbs of apposi- tion events, e.g. 追撞/collide 事故/accident, 破 壞 /destruct 運 動 /movement, 憤 恨 /hate 行 為 /behavior. There is finite number of event nouns.</p><p>Furthermore, when we consider verbal modi- fiers, we find that verbs can play adjectival roles in Chinese without inflection, but not all verbs play adjectival roles. According to <ref type="bibr" target="#b0">Chang et al. (2000)</ref> and our observations, adjectival verbs are verbs that denote event types rather than event instances; that is, they denote a class of events which that are concepts in an upper-level ontolo- gy. One important characteristic of adjectival verbs is that they have conjunctive morphologi- cal structures, i.e., the words are conjunct with two nearly synonymous verbs, e.g., 研/study 究 /search (research), 探 /explore 測 /detect (ex- plore), and 搜/search 尋/find (search). Therefore, we need a morphological classifier that can de- tect the conjunctive morphological structure of a verb by checking the semantic parity of two morphemes of the verb.</p><p>Based on our analysis, we designed a Vt-N classifier that incorporates the above features to solve the problem. However, there is a data sparseness problem because of the limited size of the current Treebank. In other words, Treebank cannot provide enough training data to train a classifier properly. To resolve the problem, we should mine useful information from all availa- ble resources.</p><p>The remainder of this paper is organized as follows. Section 2 provides a review of related works. In Section 3, we describe the disambigua- tion model with our selected features, and intro- duce a strategy for handling unknown words. We also propose a learning approach for a large- scale unlabeled corpus. In Section 4, we report the results of experiments conducted to evaluate the proposed Vt-N classifier on different feature combinations and learning approaches. Section 5 contains our concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most works on V-N structure identification focus on two types of relation classification: modifier- head relations and predicate-object relations <ref type="bibr" target="#b14">(Wu, 2003;</ref><ref type="bibr" target="#b10">Qiu, 2005;</ref><ref type="bibr" target="#b2">Chen, 2008;</ref><ref type="bibr" target="#b15">Yu et al., 2008)</ref>. They exclude the independent structure and conjunctive head-head relation, but the cross-bracket relation does exist between two adjacent words in real language. For example, if "遍佈/all over 世界/world " was included in the short sentence "遍佈/all over 世界/world 各國 /countries", it would be an independent structure. A conjunctive head-head relation between a verb and a noun is rare. However, in the sentence "服 務 設備 都 甚 周到" (Both service and equip- ment are very thoughtful.), there is a conjunctive head-head relation between the verb 服 務 /service and the noun 設備/equipment. Therefore, we use four types of relations to describe the V- N structures in our experiments. The symbol 'H/X' denotes a predicate-object relation; 'X/H' denotes a modifier-head relation; 'H/H' denotes a conjunctive head-head relation; and 'X/X' de- notes an independent relation.</p><p>Feature selection is an important task in V-N disambiguation. Hence, a number of studies have suggested features that may help resolve the am- biguity of V-N structures ( <ref type="bibr" target="#b16">Zhao and Huang, 1999;</ref><ref type="bibr" target="#b12">Sun and Jurafsky, 2003;</ref><ref type="bibr" target="#b4">Chiu et al., 2004;</ref><ref type="bibr" target="#b10">Qiu, 2005;</ref><ref type="bibr" target="#b2">Chen, 2008)</ref>. Zhao and Huang used lexi- cons, semantic knowledge, and word length in-formation to increase the accuracy of identifica- tion. Although they used the Chinese thesaurus CiLin ( <ref type="bibr" target="#b8">Mei et al., 1983</ref>) to derive lexical seman- tic knowledge, the word coverage of CiLin is insufficient. Moreover, none of the above papers tackle the problem of unknown words. Sun and Jurafsky exploit the probabilistic rhythm feature (i.e., the number of syllables in a word or the number of words in a phrase) in their shallow parser. Their results show that the feature im- proves the parsing performance, which coincides with our analysis in Section 1.1. Chiu et al.'s study shows that the morphological structure of verbs influences their syntactic behavior. We follow this finding and utilize the morphological structure of verbs as a feature in the proposed Vt- N classifier. Qiu's approach uses an electronic syntactic dictionary and a semantic dictionary to analyze the relations of V-N phrases. However, the approach suffers from two problems: (1) low word coverage of the semantic dictionary and (2) the semantic type classifier is inadequate. Finally, Chen proposed an automatic VN combination method with features of verbs, nouns, context, and the syllables of words. The experiment re- sults show that the method performs reasonably well without using any other resources.</p><p>Based on the above feature selection methods, we extract relevant knowledge from Treebank to design a Vt-N classifier. However we have to resolve the common problem of data sparseness. Learning knowledge by analyzing large-scale unlabeled data is necessary and proved useful in previous works <ref type="bibr" target="#b14">(Wu, 2003;</ref><ref type="bibr" target="#b15">Yu et al., 2008)</ref>. Wu developed a machine learning method that acquires verb-object and modifier- head relations automatically. The mutual infor- mation scores are then used to prune verb-noun whose scores are below a certain threshold. The author found that accurate identification of the verb-noun relation improved the parsing perfor- mance by 4%. Yu et al. learned head-modifier pairs from parsed data and proposed a head- modifier classifier to filter the data. The filtering model uses the following features: a PoS-tag pair of the head and the modifier; the distance be- tween the head and the modifier; and the pres- ence or absence of punctuation marks (e.g., commas, colons, and semi-colons) between the head and the modifier. Although the method im- proves the parsing performance by 2%, the filter- ing model obtains limited data; the recall rate is only 46.35%. The authors also fail to solve the problem of Vt-N ambiguity.</p><p>Our review of previous works and the obser- vations in Section 1.1 show that lexical words, semantic information, the syllabic length of words, neighboring PoSs and the knowledge learned from large-scale data are important for Vt-N disambiguation. We consider more features for disambiguating Vt-N structures than previous studies. For example, we utilize (1) four relation classification in a real environment, including 'X/H', 'H/X', 'X/X' and 'H/H' relations; (2) un- known word processing of Vt-N words (includ- ing semantic type predication and morph- structure predication); (3) unsupervised data se- lection (a simple and effective way to extend knowledge); and (4) supervised knowledge cor- rection, which makes the extracted knowledge more useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design of the Disambiguation Model</head><p>The disambiguation model is a Vt-N relation classifier that classifies Vt-N relations into 'H/X' (predicate-object relations), 'X/H' (modifier- head relations), 'H/H' (conjunctive head-head relations), or 'X/X' (independent relations). We use the Maximum Entropy toolkit <ref type="bibr" target="#b17">(Zhang, 2004)</ref> to construct the classifier. The advantage of us- ing the Maximum Entropy model is twofold: (1) it has the flexibility to adjust features; and (2) it provides the probability values of the classifica- tion, which can be easily integrated into our PCFG parsing model.</p><p>In the following sections, we discuss the de- sign of our model for feature selection and ex- traction, unknown word processing, and world knowledge learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Selection and Extraction</head><p>We divide the selected features into five groups: PoS tags of Vt and N, PoS tags of the context, words, semantics, and additional information. <ref type="table">Table 1</ref> shows the feature types and symbol nota- tions. We use symbols of t 1 and t 2 to denote the PoS of Vt and N respectively. The context fea- ture is neighboring PoSs of Vt and N: the sym- bols of t -2 and t -1 represent its left PoSs, and the symbol t 3 and t 4 represent its right PoSs. The se- mantic feature is the lexicon's semantic type ex- tracted from E-HowNet sense expressions ( <ref type="bibr" target="#b6">Huang et al., 2008</ref>). For example, the E- HowNet expression of " 車 輛 /vehicles" is {LandVehicle| 車 :quantity={mass| 眾 }}, so its semantic type is {LandVehicle|車}. We discuss the model's performance with different feature combinations in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Description PoS PoS of Vt and N t 1 ; t 2 Context Neighboring PoSs t -2 ; t -1 ; t 3 ; t 4 Word Lexical word w 1 ; w 2 Semantic</head><p>Semantic type of word st 1 ; st 2 Additional Information</p><p>Morphological structure of verb Vmorph Syllabic length of noun Nlen <ref type="table">Table 1</ref>. The features used in the Vt-N classifier</p><p>The example in <ref type="figure" target="#fig_0">Figure 1</ref> illustrates feature la- beling of a Vt-N structure. First, an instance of a Vt-N structure is identified from Treebank. Then, we assign the semantic type of each word with- out considering the problem of sense ambiguity for the moment. This is because sense ambigui- ties are partially resolved by PoS tagging, and the general problem of sense disambiguation is beyond the scope of this paper. Furthermore, <ref type="bibr" target="#b16">Zhao and Huang (1999)</ref> demonstrated that the retained ambiguity does not have an adverse im- pact on identification. Therefore, we keep the ambiguous semantic type for future processing.  <ref type="table">Table 2</ref> shows the labeled features for "學習 /learn 中文/Chinese" in <ref type="figure" target="#fig_0">Figure 1</ref>. The column x and y describe relevant features in "學習/learn" and "中文/Chinese" respectively. Some features are not explicitly annotated in the Treebank, e.g., the semantic types of words and the morphologi- cal structure of verbs. We propose labeling methods for them in the next sub-section.   <ref type="table">Table 2</ref>. The feature labels of Vt-N pair in <ref type="figure" target="#fig_0">Figure  1</ref> 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.2 Unknown Word Processing</head><p>In Chinese documents, 3% to 7% of the words are usually unknown <ref type="bibr" target="#b11">(Sproat and Emerson, 2003)</ref>. By 'unknown words', we mean words not listed in the dictionary. More specifically, in this paper, unknown words means words without se- mantic type information (i.e., E-HowNet expres- sions) and verbs without morphological structure information. Therefore, we propose a method for predicting the semantic types of unknown words, and use an affix database to train a morph- structure classifier to derive the morphological structure of verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Morph-Structure Predication of Verbs:</head><p>We use data analyzed by <ref type="bibr" target="#b4">Chiu et al. (2004)</ref> to devel- op a classifier for predicating the morphological structure of verbs. There are four types of mor- phological structures for verbs: the coordinating structure (VV), the modifier-head structure (AV), the verb-complement structure (VR), and the verb-object structure (VO). To classify verbs automatically, we incorporate three features in the proposed classifier, namely, the lexeme itself, the prefix and the suffix, and the semantic types of the prefix and the suffix. Then, we use train- ing data from the affix database to train the clas- sifier. <ref type="table">Table 3</ref> shows an example of the unknown verb " 傳 播 到 /disseminate" and the morph- structure classifier shows that it is a 'VR' type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature</head><p>Feature  <ref type="table">Table 3</ref>. An example of an unknown verb and feature templates for morph-structure predication Semantic Type Provider: The system ex- ploits WORD, PoS, affix and E-HowNet infor- mation to obtain the semantic types of words (see <ref type="figure" target="#fig_2">Figure 2</ref>). If a word is known and its PoS is giv- en, we can usually find its semantic type by searching the E-HowNet database. For an un- known word, the semantic type of its head mor- pheme is its semantic type; and the semantic type of the head morpheme is obtained from E- HowNet <ref type="bibr">1</ref> . For example, the unknown word "傳 播 到 /disseminate", its prefix word is " 傳 播 /disseminate" and we learn that its semantic type is {disseminate|傳播} from E-HowNet. There- fore, we assign {disseminate|傳播} as the se- mantic type of " 傳 播 到 /disseminate". If the word or head morpheme does not exist in the affix database, we assign a general semantic type based on its PoS, e.g., nouns are {thing|萬物} and verbs are {act|行動}. In this matching pro- cedure, we may encounter multiple matching data of words and affixes. Our strategy is to keep the ambiguous semantic type for future pro- cessing.  The E-HowNet function in <ref type="figure" target="#fig_2">Figure 2</ref> will return a null ST value where words do not exist in E-HowNet or Affix data- base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input: WORD, PoS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning World Knowledge</head><p>Based on the features discussed in the previous sub-section, we extract prior knowledge from Treebank to design the Vt-N classifier. However, the training suffers from the data sparseness problem. Furthermore most ambiguous Vt-N relations are resolved by common sense knowledge that makes it even harder to construct a well-trained system. An alternative way to ex- tend world knowledge is to learn from large- scale unlabeled data <ref type="bibr" target="#b14">(Wu, 2003;</ref><ref type="bibr" target="#b15">Yu et al., 2008)</ref>. However, the unsuper- vised approach accumulates errors caused by automatic annotation processes, such as word segmentation, PoS tagging, syntactic parsing, and semantic role assignment. Therefore, how to extract useful knowledge accurately is an im- portant issue.</p><p>To resolve the error accumulation problem, we propose two methods: unsupervised NP selection and supervised error correction. The NP selec- tion method exploits the fact that an intransitive verb followed by a noun can only be interpreted as an NP structure, not a VP structure. It is easy to find such instances with high precision by parsing a large corpus. Based on the selection method, we can extend contextual knowledge about NP(V+N) and extract nouns that take ad- jectival verbs as modifiers. The error correction method involves a small amount of manual edit- ing in order to make the data more useful and reduce the number of errors in auto-extracted knowledge. The rationale is that, in general, high frequency Vt-N word-bigram is either VP or NP without ambiguity. Therefore, to obtain more accurate training data, we simply classify each high frequency Vt-N word bigram into a unique correct type without checking all of its instances. We provide more detailed information about the method in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>We classify Vt-N structures into four types of syntactic structures by using the bracketed in- formation (tree structure) and dependency rela- tion (head-modifier) to extract the Vt-N relations from treebank automatically. The resources used in the experiments as follows.</p><p>Treebank: The Sinica Treebank contains 61,087 syntactic tree structures with 361,834 words. We extracted 9,017 instances of Vt-N structures from the corpus. Then, we randomly selected 1,000 of the instances as test data and used the remainder (8,017 instances) as training data. Labeled information of word segmentation and PoS-tagging were retained and utilized in the experiments.</p><p>E-HowNet: E-HowNet contains 99,525 lexi- cal semantic definitions that provide information about the semantic type of words. We also im- plement the semantic type predication algorithm in <ref type="figure" target="#fig_2">Figure 2</ref> to generate the semantic types of all Vt and N words, including unknown words.</p><p>Affix Data: The database includes 13,287 ex- amples of verbs and 27,267 examples of nouns, each example relates to an affix. The detailed statistics of the verb morph-structure categoriza- tion are shown in <ref type="table">Table 4</ref>. The data is used to train a classifier to predicate the morph-structure of verbs. We found that verbs with a conjunctive structure (VV) are more likely to play adjectival roles than the other three types of verbs. The classifier achieved 87.88% accuracy on 10-fold cross validation of the above 13,287 verbs. <ref type="table">Table 4</ref>. The statistics of verb morph-structure categorization Large Corpus: We used a Chinese parser to analyze sentence structures automatically. The auto-parsed tree structures are used in Experi- ment 2 (described in the Sub-section 4.3). We obtained 1,262,420 parsed sentences and derived 237,843 instances of Vt-N structure as our da- taset (called as ASBC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VV VR AV VO Prefix 920 2,892 904 662 Suffix 439 7,388 51 31</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 1: Evaluation of the Vt-N Classifier</head><p>In this experiment, we used the Maximum En- tropy Toolkit <ref type="bibr" target="#b17">(Zhang, 2004</ref>) to develop the Vt-N classifier. Based on the features discussed in Sec- tion 3.1, we designed five models to evaluate the classifier's performance on different feature combinations.</p><p>The features and used in each model are de- scribed below. The feature values shown in brackets refer to the example in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>• M1 is the baseline model. It uses PoS-tag pairs as features, such as (t 1 =VC, t 2 =Na).</p><p>• M2 extends the M1 model by adding con- text features of (t -1 =VK, t 1 =VC), (t 2 =Na, t 3 =DE), (t -2 =Nep, t -1 =VK, t 1 =VC), (t 2 =Na, t 3 =DE, t 4 =Na) and (t -1 =VK, t 3 =DE).</p><p>• M3 extends the M2 model by adding lexi- con features of (w 1 =學習, t 1 =VK, w 2 =中 文, t 2 =Na), (w 1 ＝學習, w 2 =中文), (w 1 =學 習) and (w 2 =中文).</p><p>• M4 extends the M3 model by adding se- mantic features of (st 1 =study|學習, t 1 =VK , st 2 =language|語言, t 2 =Na), (st 1 =study|學 習 , t 1 =VK) and (st 2 =language| 語 言 , t 2 =Na).</p><p>• M5 extends the M4 model by adding two features: the morph-structure of verbs; and the syllabic length of nouns (Vmorph='VV') and (Nlen=2). <ref type="table">Table 5</ref> shows the results of using different feature combinations in the models. The symbol P1(%) is the 10-fold cross validation accuracy of the training data, and the symbol P2(%) is the accuracy of the test data. By adding contextual features, the accuracy rate of M2 increases from 59.10% to 72.30%. The result shows that contex- tual information is the most important feature used to disambiguate VP, NP and independent structures. The accuracy of M2 is approximately the same as the result of our PCFG parser be- cause both systems use contextual information. By adding lexical features (M3), the accuracy rate increases from 72.30% to 80.20%. For se- mantic type features (M4), the accuracy rate in- creases from 80.20% to 81.90%. The 1.7% in- crease in the accuracy rate indicates that seman- tic generalization is useful. Finally, in M5, the accuracy rate increases from 81.90% to 83.00%. The improvement demonstrates the benefits of using the verb morph-structure and noun length features.</p><p>Models Feature for Vt-N P1(%) P2(%) M1</p><p>(  <ref type="table">Table 5</ref>. The results of using different feature combinations Next, we consider the influence of unknown words on the Vt-N classifier. The statistics shows that 17% of the words in Treebank lack semantic type information, e.g., 留在/StayIn, 填飽/fill, 貼 出/posted, and 綁好/tied. The accuracy of the Vt-N classifier declines by 0.7% without seman- tic type information for unknown words. In other words, lexical semantic information improves the accuracy of the Vt-N classifier. Regarding the problem of unknown morph-structure of words, we observe that over 85% of verbs with more than 2 characters are not found in the affix data- base. If we exclude unknown words, the accura- cy of the Vt-N prediction decreases by 1%. Therefore, morph-structure information has a positive effect on the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment 2: Using Knowledge Ob- tained from Large-scale Unlabeled Data by the Selection and Correction Meth- ods.</head><p>In this experiment, we evaluated the two methods discussed in Section 3, i.e., unsuper- vised NP selection and supervised error correc- tion. We applied the data selection method (i.e., distance=1, with an intransitive verb (Vi) fol- lowed by an object noun (Na)) to select 46,258 instances from the ASBC corpus and compile a dataset called Treebank+ASBC-Vi-N. <ref type="table" target="#tab_3">Table 6</ref> shows the performance of model 5 (M5) on the training data derived from Treebank and Tree- bank+ASBC-Vi-N. The results demonstrate that learning more nouns that accept verbal modifiers improves the accuracy.  We had also try to use the auto-parsed results of the Vt-N structures from the ASBC corpus as supplementary training data for train M5. It de- grades the model's performance by too much error when using the supplementary training data. To resolve the problem, we utilize the supervised error correction method, which manually correct errors rapidly because high frequency instances (w 1 , w 2 ) rarely have ambiguous classifications in different contexts. So we designed an editing tool to correct errors made by the parser in the classi- fication of high frequency Vt-N word pairs. After the manual correction operation, which takes 40 man-hours, we assign the correct classifications (w 1 , t 1 , w 2 , t 2 , rt) for 2,674 Vt-N structure types which contains 10,263 instances to creates the ASBC+Correction dataset. Adding the corrected data to the original training data increases the precision rate to 88.40% and reduces the number of errors by approximately 31.76%, as shown in the Treebank+ASBC+Correction column of Ta- ble 7.  We also used the precision and recall rates to evaluate the performance of the models on each type of relation. The results are shown in <ref type="table">Table 8</ref>. Overall, the Treebank+ASBC+Correction meth- od achieves the best performance in terms of the precision rate. The results for Treebank+ASBC- Vi-N show that the unsupervised data selection method can find some knowledge to help identi- fy NP structures. In addition, the proposed mod- els achieve better precision rates than the PCFG parser. The results demonstrate that using our guidelines to design a disambiguation model to resolve the Vt-N problem is successful.  <ref type="table">Table 8</ref>. Performance comparison of different classification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment 3: Integrating the Vt-N classifier with the PCFG Parser</head><p>Identifying Vt-N structures correctly facilitates statistical parsing, machine translation, infor-mation retrieval, and text classification. In this experiment, we develop a baseline PCFG parser based on feature-based grammar representation by  to find the best tree struc- tures (T) of a given sentence (S). The parser then selects the best tree according to the evaluation score Score(T,S) of all possible trees. If there are n PCFG rules in the tree T, the Score(T,S) is the accumulation of the logarithmic probabilities of the i-th grammar rule (RPi). Formula 1 shows the baseline PCFG parser.</p><formula xml:id="formula_0">∑ = = n i i RP S T Score 1 ) ( ) , (<label>(1)</label></formula><p>The Vt-N models can be easily integrated into the PCFG parser. Formula 2 represents the inte- grated structural evaluation model. We combine RPi and VtNPi with the weights w 1 and w 2 re- spectively, and set the value of w 2 higher than that of w 1 . VtNPi is the probability produced by the Vt-N classifier for the type of the relation between Vt-N bigram determined by the PCFG parsing. The classifier is triggered when a <ref type="bibr">[Vt, N]</ref> structure is encountered; otherwise, the Vt-N model is not processed.</p><formula xml:id="formula_1">∑ = × + × = n i i i VtNP w RP w S T Score 1 2 1 ) ( ) , (<label>(2)</label></formula><p>The results of evaluating the parsing model in- corporated with the Vt-N classifier (see Formula 2) are shown in <ref type="table" target="#tab_8">Table 9</ref> and   <ref type="table" target="#tab_7">Table 10</ref>. The performance of the PCFG parser with and without model M5 from Tree- bank+ASBC+Correction data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment 4: Comparison of Various Chinese Parsers</head><p>In  The evaluation results on the testing data, i.e. in P2 metric, are as follows. The accuracy of PCFG parser is 77.09%; CDM parser reaches 78.45% of accuracy; and Berkeley parser is 70.68%. The results show that the problem of Vt- 3</p><p>The "-treebank CHINESE -SMcycles 4" is the best train- ing parameter in Traditional Chinese Parsing task of SIGHAN Bake-offs 2012.</p><p>N cannot be well solved by any general parser including CDM parser and Berkeley's parser. It is necessary to have a different approach aside from the general model. So we set the target for a better model for Vt-N classification which can be easily integrated into the existing parsing model. So far our best model achieved the P2 accuracy of 87.88%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>We have proposed a classifier to resolve the am- biguity of Vt-N structures. The design of the classifier is based on three important guidelines, namely, adopting linguistically motivated fea- tures, using all available resources, and easy in- tegration into parsing model. After analyzing the Vt-N structures, we identify linguistically moti- vated features, such as lexical words, semantic knowledge, the morphological structure of verbs, neighboring parts-of-speech, and the syllabic length of words. Then, we design a classifier to verify the usefulness of each feature. We also resolve the technical problems that affect the prediction of the semantic types and morph- structures of unknown words. In addition, we propose a framework for unsupervised data se- lection and supervised error correction for learn- ing more useful knowledge. Our experiment re- sults show that the proposed Vt-N classifier sig- nificantly outperforms the PCFG Chinese parser in terms of Vt-N structure identification. Moreo- ver, integrating the Vt-N classifier with a parsing model improves the overall parsing performance without side effects.</p><p>In our future research, we will exploit the pro- posed framework to resolve other parsing diffi- culties in Chinese, e.g., N-N combination. We will also extend the Semantic Type Predication Algorithm <ref type="figure" target="#fig_2">(Figure 2</ref>) to deal with all Chinese words. Finally, for real world knowledge learn- ing, we will continue to learn more useful knowledge by auto-parsing to improve the pars- ing performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. An example of a tree with a Vt-N structure</figDesc><graphic url="image-1.png" coords="4,70.88,444.64,226.00,128.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Feature</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The Pseudo-code of the Semantic Type Predication Algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Sinica Treebank: S(NP(Head:Nh:他們)|Head:VC:散播 |NP(Head:Na:熱情)) Penn Treebank: ( (S (NP (Head:Nh (Nh 他們))) (Head:VC (VC 散播)) (NP (Head:Na (Na 熱情)))))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Description</head><label></label><figDesc></figDesc><table>Word=傳播到 
Lexicon 
PW=傳播 
Prefix word 
PWST={disseminate|傳播} Semantic Type of 
Prefix Word 傳播 
SW=到 
Suffix Word 
SWST={Vachieve|達成} 
Semantic Type of 
Suffix Word 到 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table>Experiment results on the test data for 
various knowledge sources 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table>Experiment results of classifiers with 
different training data 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 10 .</head><label>10</label><figDesc></figDesc><table>The P2 is 
the accuracy of Vt-N classification on the test 
data. The bracketed f-score (BF 2 ) is the parsing 
performance metric. Based on these results, the 
integrated model outperforms the PCFG parser in 
terms of Vt-N classification. Because the Vt-N 
classifier only considers sentences that contain 
Vt-N structures, it does not affect the parsing 
accuracies of other sentences. 

PCFG + 
M5 (Treebank) 

PCFG 

P2(%) 80.68 
77.09 
BF(%) 83.64 
82.80 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table>The performance of the PCFG parser 
with and without model M5 from Treebank. 

2 

The evaluation formula is (BP*BR*2) / (BP+BR), where 
BP is the precision and BR is the recall. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their val-uable comments. This work was supported by National Science Council under Grant NSC99-2221-E-001-014-MY3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alternation Across Semantic Fields: A Study on Mandarin Verbs of Emotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Li</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internal Journal of Computational Linguistics and Chinese Language Processing (IJCLCLP)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sinica Treebank: Design Criteria, Representational Issues and Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Ren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Ching</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Jan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Ming</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abeille 2003) Treebanks: Building and Using Parsed Corpora</title>
		<meeting><address><addrLine>Dordrecht, the Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="231" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autolabeling of VN Combination Based on Multi-classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jiang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="79" to="81" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dependency Parsig with Short Dependency Relations in Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyotaka</forename><surname>Uchimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitoshi</forename><surname>Isahara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third International Joint Conference on Natural Language Processing (IJCNLP)</title>
		<meeting>the third International Joint Conference on Natural Language Processing (IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="88" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compositional Semantics of Mandarin Affix Verbs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ming</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Chin</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Research on Computational Linguistics Conference (ROCLING)</title>
		<meeting>the Research on Computational Linguistics Conference (ROCLING)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving PCFG Chinese Parsing with Context-Dependent Probability Reestimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ming</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second CIPSSIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>the Second CIPSSIGHAN Joint Conference on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="216" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">E-HowNet: the Expansion of HowNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Shan</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keh-Jiann</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First National HowNet workshop</title>
		<meeting>the First National HowNet workshop<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhi</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Xiandai Hanyu Shuxing Fanchou Yianjiu (現代漢語屬性範疇研究). Chengdu: Bashu Books</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Dictionary of Synonyms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxian</forename><surname>Ying</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
	<note>Shanghai Cishu Chubanshe</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Accurate, Compact, and Interpretable Tree Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceesings of COLING/ACL</title>
		<meeting>eesings of COLING/ACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Constitutive Relation Analysis for V-N Phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Likun</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Language and Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="173" to="183" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The first International Chinese Word Segmentation Bakeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Sproat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Emerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Effect of Rhythm on Structural Disambiguation in Chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tranditional Chinese Parsing Evaluation at SIGHAN Bake-offs 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuen-Hsieh</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chih</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing</title>
		<meeting>the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="199" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Verb-Noun Relations to Improve Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second SIGHAN workshop on Chinese Language Processing</title>
		<meeting>the Second SIGHAN workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chinese Dependency Parsing with Large Scale Automatically Constructed Case Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (COLING2008)</title>
		<meeting>the 22nd International Conference on Computational Linguistics (COLING2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1049" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Complex-feature-based Model for Acquisition of VNconstruction Structure Templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ning</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Maximum Entropy Modeling Toolkit for Python and C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Reference Manual</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
