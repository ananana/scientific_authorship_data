<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DOC: Deep Open Classification of Text Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>September 7-11, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DOC: Deep Open Classification of Text Documents</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="2911" to="2916"/>
							<date type="published">September 7-11, 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training. This also applies to text learning or text classification. As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem. This problem is called open-world classification or open classification. This paper proposes a novel deep learning based approach. It outperforms existing state-of-the-art techniques dramatically.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key assumption made by classic supervised text classification (or learning) is that classes appeared in the test data must have appeared in training, called the closed-world assumption <ref type="bibr" target="#b3">Chen and Liu, 2016)</ref>. Although this as- sumption holds in many applications, it is violated in many others, especially in dynamic or open en- vironments. For example, in social media, a classi- fier built with past topics or classes may not be ef- fective in classifying future data because new top- ics appear constantly in social media . This is clearly true in other domains too, e.g., self-driving cars, where new objects may ap- pear in the scene all the time.</p><p>Ideally, in the text domain, the classifier should classify incoming documents to the right existing classes used in training and also detect those doc- uments that don't belong to any of the existing classes. This problem is called open world classi with a positive region bounded by two parallel hyperplanes. Similar works were also done in a probability setting by  and . Both approaches use probabil- ity threshold, but choosing thresholds need prior knowledge, which is a weakness of the methods. <ref type="bibr" target="#b5">Dalvi et al. (2013)</ref> proposed a multi-class semi- supervised method based on the EM algorithm. It has been shown that these methods are poorer than the method in .</p><p>The work closest to ours is that in <ref type="bibr" target="#b0">(Bendale and Boult, 2016)</ref>, which leverages an algorithm called OpenMax to add the rejection capability by uti- lizing the logits that are trained via closed-world softmax function. One weak assumption of Open- Max is that examples with equally likely logits are more likely from the unseen or rejection class, which can be examples that are hard to classify. Another weakness is that it requires validation ex- amples from the unseen/rejection class to tune the hyperparameters. Our method doesn't make these weak assumptions and performs markedly better.</p><p>Our proposed method, called DOC (Deep Open Classification), uses deep learning ( <ref type="bibr" target="#b10">Goodfellow et al., 2016;</ref><ref type="bibr" target="#b14">Kim, 2014)</ref>. Unlike traditional clas- sifiers, DOC builds a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than soft- max to reduce the open space risk. It reduces the open space risk further for rejection by tightening the decision boundaries of sigmoid functions with Gaussian fitting. Experimental results show that DOC dramatically outperforms state-of-the-art ex- isting approaches from both text classification and image classification domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed DOC Architecture</head><p>DOC uses <ref type="bibr">CNN (Collobert et al., 2011;</ref><ref type="bibr" target="#b14">Kim, 2014)</ref> as its base and augments it with a 1-vs- rest final sigmoid layer and Gaussian fitting for classification. Note: other existing deep mod- els like RNN ( <ref type="bibr" target="#b23">Williams and Zipser, 1989;</ref><ref type="bibr" target="#b19">Schuster and Paliwal, 1997</ref>) and LSTM <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b9">Gers et al., 2002</ref>) can also be adopted as the base. Similar to RNN, CNN also works on embedded sequential data (using 1D convolution on text instead of 2D convolution on images). We choose CNN because OpenMax uses CNN and CNN performs well on text <ref type="bibr" target="#b14">(Kim, 2014)</ref>, which enables a fairer comparison with OpenMax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CNN and Feed Forward Layers of DOC</head><p>The proposed DOC system (given in <ref type="figure" target="#fig_0">Fig. 1</ref> </p><formula xml:id="formula_0">d = W (ReLU(W h + b)) + b ,<label>(1)</label></formula><p>where W ∈ R r×k , b ∈ R r , W ∈ R m×r , and b ∈ R m are trainable weights; r is the output dimension of the first fully connected layer. The output layer of DOC is a 1-vs-rest layer applied to d 1:m , which allows rejection. We describe it next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">1-vs-Rest Layer of DOC</head><p>Traditional multi-class classifiers ( <ref type="bibr" target="#b10">Goodfellow et al., 2016;</ref><ref type="bibr" target="#b0">Bendale and Boult, 2016)</ref> typically use softmax as the final output layer, which does not have the rejection capability since the prob- ability of prediction for each class is normalized across all training/seen classes. Instead, we build a 1-vs-rest layer containing m sigmoid functions for m seen classes. For the i-th sigmoid function corresponding to class l i , DOC takes all examples with y = l i as positive examples and all the rest examples y = l i as negative examples.</p><p>The model is trained with the objective of sum- mation of all log loss of the m sigmoid functions on the training data D.</p><formula xml:id="formula_1">Loss = m i=1 n j=1 −I(y j = l i ) log p(y j = l i ) −I(y j = l i ) log(1 − p(y j = l i )),<label>(2)</label></formula><p>where I is the indicator function and p(y j = l i ) = Sigmoid(d j,i ) is the probability output from ith sigmoid function on the jth document's ith- dimension of d.</p><p>During testing, we reinterpret the prediction of m sigmoid functions to allow rejection, as shown in Eq. 3. For the i-th sigmoid function, we check if the predicted probability Sigmoid(d i ) is less than a threshold t i belonging to class l i . If all pre- dicted probabilities are less than their correspond- ing thresholds for an example, the example is re- jected; otherwise, its predicted class is the one with the highest probability. Formally, we havê</p><formula xml:id="formula_2">havê y = reject, if Sigmoid(d i ) &lt; t i , ∀l i ∈ Y; arg max l i ∈Y Sigmoid(d i ), otherwise.<label>(3)</label></formula><p>Note that although multi-label classification ( <ref type="bibr" target="#b12">Huang et al., 2013;</ref><ref type="bibr" target="#b24">Zhang and Zhou, 2006;</ref><ref type="bibr" target="#b22">Tsoumakas and Katakis, 2006</ref>) may also leverage multiple sigmoid functions, Eq. 3 forbids multi- ple predicted labels for the same example, which is allowed in multi-label classification. DOC is also related to multi-task learning ( <ref type="bibr" target="#b12">Huang et al., 2013;</ref><ref type="bibr" target="#b1">Caruana, 1998)</ref>, where each label l i is re- lated to a 1-vs-rest binary classification task with shared representations from CNN and fully con- nected layers. However, Eq. 3 performs classifi- cation and rejection based on the outputs of these binary classification tasks.</p><p>Comparison with OpenMax: OpenMax builds on the traditional closed-world multi-class classi- fier (softmax layer). It reduces the open space for each seen class, which is weak for rejecting unseen classes. DOC's 1-vs-rest sigmoid layer provides a reasonable representation of all other classes (the rest of seen classes and unseen classes), and en- ables the 1 class forms a good boundary. Sec. 3.5 shows that this basic DOC is already much better than OpenMax. Below, we improve DOC further by tightening the decision boundaries more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reducing Open Space Risk Further</head><p>Sigmoid function usually uses the default prob- ability threshold of t i = 0.5 for classification of each class i. But this threshold does not con- sider potential open space risks from unseen (re- jection) class data. We can improve the bound- ary by increasing t i . We use <ref type="figure" target="#fig_2">Fig. 2</ref> to illustrate. The x-axis represents d i and y-axis is the predicted probability p(y = l i |d i ). The sigmoid function tries to push positive examples (belonging to the i-th class) and negative examples (belonging to the other seen classes) away from the y-axis via a high gain around d i = 0, which serves as the de- fault decision boundary for d i with t i = 0.5. As demonstrated by those 3 circles on the right-hand side of the y-axis, during testing, unseen class ex- amples (circles) can easily fill in the gap between the y-axis and those dense positive (+) examples, which may reduce the recall of rejection and the precision of the i-th seen class prediction. Obvi- ously, a better decision boundary is at d i = T , where the decision boundary more closely "wrap" those dense positive examples with the probability threshold t i 0.5 .</p><p>To obtain a better t i for each seen class i-th, we use the idea of outlier detection in statistics:</p><p>1. Assume the predicted probabilities p(y = l i |x j , y j = l i ) of all training data of each class i follow one half of the Gaussian dis- tribution (with mean µ i = 1), e.g., the three positive points in <ref type="figure" target="#fig_2">Fig. 2</ref> projected to the y-axis (we don't need d i ). We then artifi- cially create the other half of the Gaussian distributed points (≥ 1): for each existing point p(y = l i |x j , y j = l i ), we create a mir- ror point 1 + (1 − p(y = l i |x j , y j = l i ) (not a probability) mirrored on the mean of 1.</p><p>2. Estimate the standard deviation σ i using both the existing points and the created points.</p><p>3. In statistics, if a value/point is a certain num- ber (α) of standard deviations away from the mean, it is considered an outlier. We thus set the probability threshold t i = max(0.5, 1 − ασ i ). The commonly used number for α is 3, which also works well in our experiments.</p><p>Note that due to Gaussian fitting, different class l i can have a different classification threshold t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We perform evaluation using two publicly avail- able datasets, which are exactly the same datasets used in .</p><p>(1) 20 Newsgroups 2 (Rennie, 2008): The 20 newsgroups data set contains 20 non-overlapping classes. Each class has about 1000 documents.</p><p>(2) 50-class reviews (Chen and Liu, 2014): The dataset has Amazon reviews of 50 classes of prod- ucts. Each class has 1000 reviews. Although prod- uct reviews are used, we do not do sentiment clas- sification. We still perform topic-based classifica- tion. That is, given a review, the system decides what class of product the review is about.</p><p>For every dataset, we keep a 20000 frequent word vocabulary. Each document is fixed to 2000- word length (cutting or padding when necessary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Test Settings and Evaluation Metrics</head><p>For a fair comparison, we use exactly the same set- tings as in . For each class in each dataset, we randomly sampled 60% of docu- ments for training, 10% for validation and 30% for testing.  did not use a valida- tion set, but the test data is the same 30%. We use the validation set to avoid overfitting. For open- world evaluation, we hold out some classes (as un- seen) in training and mix them back during testing. We vary the number of training classes and use 25%, 50%, 75%, or 100% classes for training and all classes for testing. Here using 100% classes for training is the same as the traditional closed- world classification. Taking 20 newsgroups as an example, for 25% classes, we use 5 classes (we randomly choose 5 classes from 20 classes for 10 times and average the results, as in ) for training and all 20 classes for testing (15 classes are unseen in training). We use macro   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We compare DOC with two state-of-the-art meth- ods published in 2016 and one DOC variant. cbsSVM: This is the latest method published in NLP . It uses SVM to build 1-vs-rest CBS classifiers for multiclass text classi- fication with rejection option. The results of this system are taken from .</p><p>OpenMax: This is the latest method from com- puter vision <ref type="bibr" target="#b0">(Bendale and Boult, 2016)</ref>. Since it is a CNN-based method for image classification, we adapt it for text classification by using CNN with a softmax output layer, and adopt the Open- Max layer 3 for open text classification. When all classes are seen (100%), the result from softmax is reported since OpenMax layer always performs rejection. We use default hyperparameter values of OpenMax (Weibull tail size is set to 20). DOC(t = 0.5): This is the basic DOC (t = 0.5). Gaussian fitting isn't used to choose each t i .</p><p>Note that ) compared with several other baselines. We don't compare with them as it was shown that cbsSVM was superior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hyperparameter Setting</head><p>We use word vectors pre-trained from Google News 4 (3 million words and 300 dimensions). For the CNN layers, 3 filter sizes are used <ref type="bibr">[3,</ref><ref type="bibr">4,</ref><ref type="bibr">5]</ref>. For each filter size, 150 filters are applied. The dimen- sion r of the first fully connected layer is 250.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Result Analysis</head><p>The results of 20 newsgroups and 50-class reviews are given in <ref type="table" target="#tab_0">Tables 1 and 2</ref>, respectively. From the tables, we can make the following observations:</p><p>1. DOC is markedly better than OpenMax and cbsSVM in macro-F 1 scores for both datasets in the 25%, 50%, and 75% settings. For the 25% and 50% settings (most test examples are from unseen classes), DOC is dramati- cally better. Even for 100% of traditional closed-world classification, it is consistently better too. DOC(t = 0.5) is better too.</p><p>2. For the 25% and 50% settings, DOC is also markedly better than DOC(t = 0.5), which shows that Gaussian fitting finds a better probability threshold than t = 0.5 when many unseen classes are present. In the 75% setting (most test examples are from seen classes), DOC(t = 0.5) is slightly better for 20 newsgroups but worse for 50-class re- views. DOC sacrifices some recall of seen class examples for better precision, while t = 0.5 sacrifices the precision of seen classes for better recall. DOC(t = 0.5) is also worse than cbsSVM for 25% setting for 50-class re- views. It is thus not as robust as DOC.</p><p>3. For the 25% and 50% settings, cbsSVM is also markedly better than OpenMax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper proposed a novel deep learning based method, called DOC, for open text classification. Using the same text datasets and experiment set- tings, we showed that DOC performs dramatically better than the state-of-the-art methods from both the text and image classification domains. We also believe that DOC is applicable to images.</p><p>In our future work, we plan to improve the cu- mulative or incremental learning method in ) to learn new classes without training on all past and new classes of data from scratch. This will enable the system to learn by self to achieve continual or lifelong learning <ref type="bibr" target="#b3">(Chen and Liu, 2016)</ref>. We also plan to improve model per- formance during testing ( <ref type="bibr" target="#b20">Shu et al., 2017</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall Network of DOC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) is a variant of the CNN architecture (Collobert et al., 2011) for text classification (Kim, 2014) 1 . The first layer embeds words in document x into dense vectors. The second layer performs convolution over dense vectors using different filters of var- ied sizes (see Sec. 3.4). Next, the max-over-time pooling layer selects the maximum values from the results of the convolution layer to form a k- dimension feature vector h. Then we reduce h to a m-dimension vector d = d 1:m (m is the number of training/seen classes) via 2 fully connected lay- ers and one intermediate ReLU activation layer:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Open space risk of sigmoid function and desired decision boundary d i = T and probability threshold t i .</figDesc><graphic url="image-5.png" coords="3,309.10,70.88,198.47,129.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F</head><label></label><figDesc>1 -score over 5 + 1 classes (1 for rejection) for 2 http://qwone.com/ ˜ jason/20Newsgroups/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Macro-F 1 scores for 20 newsgroups 
% of seen classes 25% 50% 75% 100% 
cbsSVM 
59.3 70.1 72.0 
85.2 
OpenMax 
35.7 59.9 76.2 
91.9 
DOC (t = 0.5) 
75.9 84.0 87.4 
92.6 
DOC 
82.3 85.2 86.2 
92.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Macro-F 1 scores for 50-class reviews 
% of seen classes 25% 50% 75% 100% 
cbsSVM 
55.7 61.5 58.6 
63.4 
OpenMax 
41.6 57.0 64.2 
69.2 
DOC (t = 0.5) 
51.1 63.6 66.2 
69.8 
DOC 
61.2 64.8 66.6 
69.8 

evaluation. Please note that examples from unseen 
classes are dropped in the validation set. 

</table></figure>

			<note place="foot" n="1"> https://github.com/alexander-rakhlin/ CNN-for-Sentence-Classification-in-Keras</note>

			<note place="foot" n="3"> https://github.com/abhijitbendale/ OSDN 4 https://code.google.com/archive/p/ word2vec/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by grants from National Science Foundation (NSF) under grant no. IIS-1407927 and IIS-1650900.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards open set deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1563" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning to learn</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="95" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining topics in documents: standing on the shoulders of big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1116" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lifelong Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploratory learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>William W Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="128" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social media text classification under negative covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Breaking the closed world assumption in text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning cumulatively to become more knowledgeable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geli</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning precise timing with lstm recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task deep neural network for multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2897" to="2900" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-class open set recognition using probability of inclusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lalit P Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="393" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Archana</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probability models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><forename type="middle">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boult</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2317" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuldip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lifelong learning crf for supervised aspect extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2017</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics (ACL-2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Pw Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-label classification: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Warehousing and Mining</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilabel neural networks with applications to functional genomics and text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1338" to="1351" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
