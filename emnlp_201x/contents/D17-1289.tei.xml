<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Predict Charges for Criminal Cases with Legal Basis</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingfeng</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Institute of Big Data Research</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science and Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Predict Charges for Criminal Cases with Legal Basis</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2727" to="2736"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The charge prediction task is to determine appropriate charges for a given case, which is helpful for legal assistant systems where the user input is fact description. We argue that relevant law articles play an important role in this task, and therefore propose an attention-based neural network method to jointly model the charge prediction task and the relevant article extraction task in a unified framework. The experimental results show that, besides providing legal basis, the relevant articles can also clearly improve the charge prediction results, and our full model can effectively predict appropriate charges for cases with different expression styles.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of automatic charge prediction is to deter- mine appropriate charges, such as fraud, larceny or homicide, for a case by analyzing its textual fact description. Such techniques are crucial for legal assistant systems, where users could find similar cases or possible penalties by describing a case with their own words. This is helpful for non-legal professionals to get to know the legal basis of their interested cases, e.g., cases they or their friends are involved in, since the massive legal materials and the lack of knowledge of legal jargons make it hard for outsiders to do it on their own.</p><p>However, predicting appropriate charges based on fact descriptions is not trivial: (1) The differ- ences between two charges can be subtle, for ex- ample, in the context of criminal cases in China, distinguishing intentional homicide from inten- tional injury would require to determine, from the fact description, whether the defendant intended to kill the victim, or just intended to hurt the vic- tim, who, unfortunately died of severe injury. (2) Multiple crimes may be involved in a single case, which means we need to conduct charge predic- tion in the multi-label classification paradigm. <ref type="formula">(3)</ref> Although we can expect an off-the-shelf classifi- cation model to learn to label a case with corre- sponding charges through massive training data, it is always more convincing to make the prediction with its involved law articles explicitly shown to the users, as legal basis to support the prediction. This issue is crucial in countries using the civil law system, e.g., China (except Hong Kong), where judgements are made based on statutory laws only. For example, in <ref type="figure">Fig. 1</ref>, a judgement document in China always includes relevant law articles (in the court view part) to support the decision. Even in countries using the common law system, e.g., the United States (except Louisiana), where the judgement is based mainly on decisions of previ- ous cases, there are still some statutory laws that need to be followed when making decisions. Existing attempts formulate the task of auto- matic charge prediction as a single-label classi- fication problem, by either adopting a k-Nearest Neighbor (KNN) ( <ref type="bibr" target="#b12">Liu et al., 2004</ref>; <ref type="bibr" target="#b13">Liu and Hsieh, 2006</ref>) as the classifier with shallow textual fea- tures, or manually designing key factors for spe- cific charges to help text understanding ( <ref type="bibr" target="#b11">Lin et al., 2012)</ref>, which make those works hard to scale to more types of charges. There are also works ad- dressing a related task, finding the law articles that are involved in a given case. A simple solution is to convert this multi-label problem into a multi- class classification task by only considering a fixed set of article combinations (Liu and <ref type="bibr" target="#b14">Liao, 2005</ref>; <ref type="bibr" target="#b13">Liu and Hsieh, 2006</ref>), which can only be applied to a small set of articles and does not fit to real applications. Recent improvement takes a two- step approach by performing a preliminary classi- fication first and then re-ranking the results with word-level and article-level features ( . These efforts advance the applications of machine learning and natural language process- ing methods into legal assistance services, how- ever, they are still in an early stage, e.g., relying on expert knowledge, using relatively simple clas- sification paradigms, and shallow textual analysis. More importantly, related tasks, e.g., charge pre- diction and relevant article extraction, are treated independently, ignoring the fact that they could benefit from each other.</p><p>Recent advances in neural networks enable us to jointly model charge prediction and relevant article extraction in a unified framework, where the latent correspondence from the fact descrip- tion about a case to its related law articles and further to its charges can be explicitly addressed by a two-stack attention mechanism. Specifi- cally, we use a sentence-level and a document- level Bi-directional Gated Recurrent Units (Bi- GRU) ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) with a stack of fact- side attention components to model the correla- tions among words and sentences, in order to cap- ture the whole story as well as important details of the case. Given the analysis of the fact description, we accordingly learn a stack of article-side atten- tion components to attentively select the most sup- portive law articles from the statutory laws to sup- port our charge prediction, which is investigated in the multi-label paradigm.</p><p>We evaluate our model in the context of pre- dicting charges for criminal cases in China. We collect publicly available judgement documents from China's government website, from which we can automatically extract fact descriptions, rele- vant law articles and the charges using simple rules, as shown in <ref type="figure">Figure 1</ref>. Experimental results show that our neural network method can effec- tively predict appropriate charges for a given case, and also provide relevant law articles as legal ba- sis to support the prediction. Our experiments also provide quantitive analysis about the effect of fact- side and article-side information on charge predi- cion, and confirm that, apart from providing le- gal basis, relevant articles also contain useful in- formation that can help to improve charge pre- diction in the civil law system. We also exam- ine our model on the news reports about criminal cases. Although trained on judgement documents, our model can still achieve promising performance on news data, showing a reasonable generalization ability over different expression styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The charge prediction task aims at finding appro- priate charges based on the facts of a case. Previ- ous works consider this task in a multi-class clas- sification framework, which takes the fact descrip- tion as input and outputs a charge label. ( <ref type="bibr" target="#b12">Liu et al., 2004;</ref><ref type="bibr" target="#b13">Liu and Hsieh, 2006</ref>) use KNN to clas- sify 12 and 6 criminal charges in Taiwan. How- ever, except for the inferior scalability of the KNN method, their word-level and phrase-level features are too shallow to capture sufficient evidence to distinguish similar charges with subtle differences. ( <ref type="bibr" target="#b11">Lin et al., 2012)</ref> propose to make deeper under- standing of a case by identifying charge-specific factors that are manually designed for 2 charges. This method also suffers from the scalability issue due to the human efforts required to design and an- notate these factors for each pair of charges. Our method, however, employs Bi-GRU and a two- stack attention mechanism to make comprehensive understanding of a case without relying on explicit human annotations.</p><p>Within the civil law system, there are some works focusing on identifying applicable law ar- ticles for a given case. ( <ref type="bibr" target="#b14">Liu and Liao, 2005;</ref><ref type="bibr" target="#b13">Liu and Hsieh, 2006</ref>) convert this multi-label problem into a multi-class classification problem by only considering a fixed set of article combinations, which cannot scale well since the number of pos- sible combinations will grow exponentially when a larger set of law articles are considered. ( ) instead design a scalable two-step approach by first using Support Vector Machine (SVM) for preliminary article classification, and then re-ranking the results using word level fea- tures and co-occurence tendency among articles. We also use SVM to extract top k candidate arti- cles, but further adopt Bi-GRU and article-side at- tention to better understand the texts and exploring the correlation among articles. More importantly, we optimize the article extraction task within our charge prediction framework, which not only pro- vides another view to understand the facts, but also serves as legal basis to support the final decision.</p><p>Another related thread of work is to predict the overall outcome of a case. The target can be which party will the outcome side with ( <ref type="bibr" target="#b0">Aletras et al., 2016)</ref>, or whether the present court will affirm or reverse the decision of a lower court (Katz et al., <ref type="figure">Figure 1</ref>: An example judgement document excerpt of a criminal case in our dataset. Names are anonymized as AA and BB. Rectangulars, ellipses and dashed rectangulars refer to the clauses that usually indicate the beginning of the facts part, the court view part and the decision part, respectively. Articles and charges are extracted with regular expressions and a charge list. 2016). Our work differs from them in that, instead of binary outcome (the latter one also contains an other class), we step further to focus on the de- tailed results of a case, i.e., the charges, where the output may contain multiple labels.</p><p>We also share similar spirit with the legal ques- tion answering task ( <ref type="bibr" target="#b7">Kim et al., 2014a)</ref>, which aims at answering the yes/no questions in the Japanese legal bar exams, that we all believe that relevant law articles are important for decisions in the civil law system. Different from ours, this task requires participants to extract relevant Japanese Civil Code articles first, and then use them to an- swer the yes/no questions. The former phase is of- ten treated as an information retrieval task, and the latter phase is considered as a textual entailment task ( <ref type="bibr" target="#b8">Kim et al., 2014b;</ref><ref type="bibr" target="#b2">Carvalho et al., 2016</ref>).</p><p>In the field of artificial intelligence and law, there are also works trying to find relevant cases given the input query ( <ref type="bibr" target="#b18">Raghav et al., 2016;</ref>, which is crutial for decision making in the common law system. Rather than finding relevant cases, our work focuses on predicting spe- cific charges, and we also emphasize the impor- tance of law articles in decision making, which is important in the civil law system where the deci- sions are made based solely on statutory laws.</p><p>Our work is also related to the task of docu- ment classification, but mainly differs in that we also need to automatically identify applicable law articles to support and improve the charge predic- tion. Recently, various neural network (NN) ar- chitectures such as Convolutional Neural Network (CNN) <ref type="bibr" target="#b9">(Kim, 2014)</ref> and Recurrent Neural Net- work (RNN) have been used for document em- bedding, which is further used for classification. ( <ref type="bibr" target="#b19">Tang et al., 2015)</ref> propose a two-layer scheme, RNN or CNN for sentence embedding, and an- other RNN for document embedding. ( <ref type="bibr" target="#b22">Yang et al., 2016</ref>) further use global context vectors to at- tentively distinguish informative words or sen- tences from non-informative ones during embed- ding, which we share similar spirit with. But, we take a more flexible and descriptive two-stack at- tention mechanism, one stack for fact embedding, and the other for article embedding which is dy- namically generated for each instance according to the fact-side clues as extra guidance. Another difference is the multi-label nature of our task, where, rather than optimizing as multiple binary classification tasks <ref type="bibr" target="#b17">(Nam et al., 2014</ref>), we convert the multi-label target to label distribution during training with cross entropy as loss function ( <ref type="bibr" target="#b10">Kurata et al., 2016)</ref>, and use a threshold tuned on val- idation set to produce the final prediction, which performs better in our pilot experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Preparation</head><p>Our data are collected from China Judgements Online 1 , where the Chinese government has been publishing judgement documents since 2013. We randomly choose 50,000 documents for training, 5,000 for validation and 5,000 for testing. To en- sure enough training data for each charge, we only classify the charges that appear more than 80 times in the training data, and treat documents with other charges as negative data. As for law articles, we consider those in the Criminal Law of the People's Republic of China. The resulting dataset contains 50 distinct charges, 321 distinct articles, averagely 383 words per fact description, 3.81 articles per case, and 3.56% cases with more than one charges.</p><p>An example judgement document is shown in <ref type="figure">Figure 1</ref>, where we highlight the indicator clauses that we used to divide a document into three pieces and extract fact description, articles, and charges from each piece, respectively. We use a manually collected charge list to identify all the charges, and law articles are extracted by regular expressions 2 . The extracted charges and articles are considered as gold standard charges and articles for the corre- sponding fact description. We also masked all the charges in fact descriptions, since although rare, charge names sometimes may appear in the fact description part.</p><p>Currently, it is hard and expensive to match the facts related to different defendants with their cor- responding charges. We therefore only consider the cases with one defendant, and leave the chal- lenging multi-defendant cases for future work. Al- though this simplification may change the real- world charge distribution, it enables us to automat- ically build large scale high quality dataset without relying on annotations from legal practitioners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>As depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>, our approach contains the following steps: (1) The input fact description is fed to a document encoder to generate the fact embedding d f , where u f w and u f s are global word-level and sentence-level context vectors used to attentively select informative words and sen- tences. (2) Concurrently, the fact description is also passed to an article extractor to find top k relevant law articles. (3) These articles are em- bedded by another document encoder, and passed to an article aggregator to attentively select sup- portive articles, and produce the aggregated article embedding d a . Specifically, three context vectors, i.e., u aw , u as and u ad , are dynamically generated from d f , to produce attention values within the ar- ticle document encoder and the article aggregator. (4) Finally, d f and d a are concatenated and passed to a softmax classifier to predict the charge distri- bution for the input case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Document Encoder</head><p>Intuitively, a sentence is a sequence of words and a document is a sequence of sentences. The document embedding problem, therefore, can be converted to two sequence embedding problems ( <ref type="bibr" target="#b19">Tang et al., 2015;</ref><ref type="bibr" target="#b22">Yang et al., 2016</ref>). As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, we can first embed each sentence us- ing a sentence-level sequence encoder, and then <ref type="bibr">2</ref> The regular expression used to extract law articles:  Bi-GRU Sequence Encoder A challenge in building a sequence encoder is how to take the correlation among different elements into consid- eration. A promising solution is Bi-directional Gated Recurrent Units (Bi-GRU) ( <ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, which encodes the context of each element by using a gating mechanism to track the state of sequence. Specifically, Bi-GRU first uses a for- ward and a backward GRU ( <ref type="bibr" target="#b4">Cho et al., 2014</ref>), which is a kind of RNN, to encode the sequence in two opposite directions, and then concatenates the states of both GRUs to form its own states.</p><formula xml:id="formula_0">"[○0-9]+([ ])?)"</formula><p>Given a sequence [x 1 , x 2 , ..., x T ] where x t is the input embedding of element t, the state of Bi- GRU at position t is:</p><formula xml:id="formula_1">h t = [h f t , h bt ]<label>(1)</label></formula><p>where h f t and h bt are the states of the forward and backward GRU at position t. The final sequence embedding is either the concatenation of h f T and h b1 or simply the average of h t .</p><p>Attentive </p><formula xml:id="formula_2">g = T t=1 α t h t ; α t = exp(tanh(Wh t ) T u) t exp(tanh(Wh t ) T u)<label>(2)</label></formula><p>where W is a weight matrix, and u is the context vector to distinguish informative elements from non-informative ones.</p><p>By using this sequence encoder for fact em- bedding, the fact-side attention module actually contains two components, i.e., the word-level and sentence-level, using u f w and u f s as their global context vectors, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Using Law Articles</head><p>One of the challenges of using law articles to sup- port charge prediction lies in the fact that statu- tory laws contain a large number of articles, which makes applying complex models to these articles directly time-consuming, and thus hard to scale. The multi-label nature of relevant article extrac- tion also requires a model that can output multi- ple articles. We thus adopt a two-step approach, specifically, we first build a fast and easy-to-scale classifier to filter out a large fraction of irrelevant articles, and retain the top k articles. Then, we use neural networks to make comprehensive un- derstanding of the top k articles, and further use the article-side attention module to select the most supportive ones for charge prediction.</p><p>Top k Article Extractor We treat the relevant article extraction task as multiple binary classifica- tions. Specifically, we build a binary classifier for each article, focusing on its relevance to the input case, which results in 321 binary classifiers corre- sponding to the 321 distinct law articles appearing in our dataset. When more articles are considered, we can simply add more binary classifiers accord- ingly, with the existing classifiers untouched.</p><p>Similar to the preliminary classification phase of ( , we also use word-based SVM as our binary classifier, which is fast and performs well in text classification <ref type="bibr" target="#b5">(Joachims, 2002;</ref><ref type="bibr" target="#b21">Wang and Manning, 2012</ref>). Specifically, we use bag-of- words TF-IDF features, chi-square for feature se- lection and linear kernel for binary classification.</p><p>Article Encoder Since each law article may contain multiple sentences, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, we also use the document encoder described in Sec. 4.1 to produce an embedding a j , j ∈ <ref type="bibr">[1, k]</ref>, for each article in the top k extracted articles. While using similar architecture, this article en- coder differs from the fact encoder that, instead of using global context vectors, its word-level con- text vector u aw and sentence-level context vector u as are dynamically generated for each case ac- cording to its corresponding fact embedding d f :</p><formula xml:id="formula_3">u aw = W w d f + b w ; u as = W s d f + b s (3)</formula><p>where W * is the weight matrix and b * is the bias. The context vectors, u aw and u as , are used to pro- duce the word-level and sentence-level attention values, respectively. Through the dynamic context vectors, the fact embedding d f actually guides our model to attend to informative words or sentences with respect to the facts of each case, rather than just selecting generally informative ones.</p><p>Attentive Article Aggregator The article ag- gregator aims to find supportive articles for charge prediction from the top k extractions, and accord- ingly produce an aggregated article embedding. Although the order of the top k extracted articles is not fully reliable, ( <ref type="bibr" target="#b20">Vinyals et al., 2016</ref>) sug- gests that it is still beneficial to use a bi-directional RNN to embed the context of each element even in a set, where the order does not exist. In our task, bi-directional RNN can help to utilize the co- occurrence tendency of relevant articles.</p><p>Specifically, we use the attentive sequence en- coder in Sec. 4.1 to produce the aggregated arti- cle embedding d a . Again, to guide the attention with fact descriptions, we dynamically generate the article-level context vector u ad by:</p><formula xml:id="formula_4">u ad = W d d f + b d (4)</formula><p>The attention values produced by the attentive sequence encoder can be seen as the relevance of each article to the input case, which can be used to rank and filter the top k articles. The results can be shown to users as legal basis for charge prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Output</head><p>To make the final charge prediction, we first con- catenate the document embedding d f and the ag- gregated article embedding d a , and feed them to two consecutive full connection layers to generate a new vector d , which is then passed to a soft- max classifier to produce the predicted charge dis- tribution. We use the validation set to determine a threshold τ , and consider all the charges with out- put probability higher than τ as positive predic- tions. The input to the first full connection layer can also be only d f or d a , which means we use only fact or article to make the prediction.</p><p>The loss function for training is cross entropy:</p><formula xml:id="formula_5">Loss = − N i=1 L l=1 y il log(o il )<label>(5)</label></formula><p>where N is the number of training data, L is the number of charges, y il and o il are the target and predicted probability of charge l for case i. The target charge distribution y i is produced by setting positive labels to 1 m i and negative ones to 0, where m i is the number of positive labels in case i.</p><p>Supervised Article Attention We can also uti- lize the gold-standard law articles naturally in the judgement documents to supervise the article at- tention during training. Specifically, given the top k articles, we want the article attention distribution α ∈ R k to simulate the target article distribution t ∈ R k , where t j = 1 k if article j belongs to the gold-standard articles and t j = 0 otherwise. Here k is the number of gold-standard articles in the top k extractions. We, again, use cross entropy, and the loss function is:</p><formula xml:id="formula_6">Loss = − N i=1 ( L l=1 y il log(o il )+β k j=1 t ij log(α ij )) (6)</formula><p>where β is the weight of the article attention loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We use HanLP 3 for Chinese word segmentation and POS tagging. Word embeddings are trained using word2vec <ref type="bibr" target="#b16">(Mikolov et al., 2013</ref>) on judge- ment documents, web pages from several legal fo- rums and Baidu Encyclopedia. The resulting word embeddings contain 573,353 words, with 100 di- mension. We randomly initialize a 50-d vector for each POS tag, which is concatenated with the word embedding as the final input. Each GRU in the Bi-GRU is of size 75, the two full connection layers are of size 200 and 150. The relevant arti- cle extractor generates top 20 articles, the weight of the article attention loss (β in Eq. 6) is 0.1, and prediction threshold τ is 0.4. We use Stochastic Gradient Descent (SGD) for training, with learn- ing rate 0.1, and batch size 8.</p><p>We compare our full model with two variations: without article attention supervision and only us- ing facts for charge prediction. The latter one is similar to the state-of-art document classification model ( <ref type="bibr" target="#b22">Yang et al., 2016)</ref>, but adapted to the multi- label nature of our problem. We also implement an SVM model, which is effective and scales well in many fact-description-related tasks in the field of artificial intelligence and law ( <ref type="bibr" target="#b0">Aletras et al., 2016)</ref>. Specifically, the SVM model takes bag-of-words TF-IDF features as input, and uses chi-square to select top 2,000 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Charge Prediction Results</head><p>The charge distribution is imbalanced, and the top 5 charges take more than 60% of the cases. There- fore, we evaluate the charge prediction task us- ing precision, recall and F1, in both micro-and macro-level. The macro-precision/recall are cal- culated by averaging the precision and recall of each charge, and the micro-precision/recall are av- eraged over each prediction.</p><p>As shown in <ref type="table">Table 1</ref>, the basic SVM fact model, which only takes fact descriptions as in- put, indeed proves to be a strong baseline. By   <ref type="table">Table 1</ref>: Charge prediction results. Left and right side of the slash refer to micro and macro statistics, respectively. gold art refers to using gold standard articles mentioned in judgements (marked in blue in <ref type="figure">Fig. 1)</ref>, which is the upper bound for article-related modules. contrast, our corresponding neural network model (NN fact), which also only uses facts for pre- diction, outperforms SVM fact by about 4% in micro-F1. Since NN fact benefits from the pre- trained word embeddings, the two-level Bi-GRU architecture, and the fact-side attention module, it can attentively recognize informative expressions from the description and better capture the under- lying correspondence from fact descriptions to ap- propriate charges, even when there is less over- lap in the words used among cases with the same charge, or when there are limited data (i.e., infre- quent charges). This may explain that NN mod- els have more balanced performance over different charges, leading to more prominent improvements over SVM ones in macro metrics, which usually have a strong bias towards frequent charges.</p><p>When we use both facts and extracted relevant law articles (that are admittedly noisy), the SVM version (SVM fact art) drops by around 5% than SVM fact, showing that the SVM model cannot benefit from the extracted, thus noisy, relevant articles in such a straightforward way. However, our NN version (NN fact article) can still learn from the noisy article extractions through attentively aggregating those extracted ar- ticles even without direct guidance, thus improves NN fact by around 0.4%. Furthermore, if we use the gold standard articles during training as su- pervision for the article attention (our full model, NN fact supv art), the results can be fur- ther improved, achieving 90.21% and 80.48% in micro-and macro-F1, respectively. The improve- ments made by using relevant law articles actually indicates the nature of the civil law system that judgements are made based on statutory laws.</p><p>However, if we only use the extracted rele- vant articles to make prediction (SVM art and NN art 4 ), the performance becomes worse. Even with the proved-helpful attentive aggregator, the model performs worst among all NN variants (though still better than SVM fact). This indi- cates that it is necessary to consider both facts and relevant law articles for charge prediction, and, the fact that NN fact outperforms NN art also indi- cates that although the judgments are made based on the statutory laws in the civil law system, the logic employed by the court when making deci- sions, to some extent, may be implicitly captured through massive fact-charges paris. Now the question is: how much improve- ment can we have if we can make full use of the relevant law articles within the civil law system?</p><p>Let us consider an ideal situation where we can access both fact descriptions and gold standard law articles during testing, which could be considered as an upper bound sce- nario. The SVM version (SVM fact gold art) significantly outperforms SVM fact art by more than 30% in macro-F1.</p><p>And the NN version (NN fact gold art) outperforms NN fact supv art by over 8%. These compar- isons confirm again that law articles play an im- portant role for automatic judgement prediction, but the extracted relevant articles inevitably con- tain noise, which should be properly handled, e.g., using an attentively aggregation mechanism to dis- till valuable evidence to support charge prediction.</p><p>Case Study We study the model outputs and find certain star-like confusion patterns among the charges. For example, intentional injury is of- ten confused with multiple charges like intentional homicide (when the victim is dead, the difference is whether the defendant intends to kill or just hurt the victim) and picking quarrels and provok- ing troubles (there may also exist injuries here). These charges usually share some similar fact de- scriptions, e.g., how the injuries are caused, and since intentional injury appears more frequently than the others, SVM fact thus outputs inten- tional injury in most situations, and fails to distin- guish these charges. However, by using Bi-GRU and the attention mechanism, NN fact can at-  tend to important details of the facts and signifi- cantly improves the performance on these charges. When the direct supervision for articles is avail- able, NN fact supv art can enhance the inter- action between certain pairs of fact descriptions and law articles, which helps to capture the sub- tle differences among similar charges, and further improves the performance on these situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Article Extraction Results</head><p>We also evaluate our SVM article extractor, which achieves 77.60%, 88.96%, 94.21% and 96.53% re- call regarding the top 5, 10, 20 and 30 articles, respectively. Although simple, the SVM extrac- tor can obtain over 94% recall for top 20 arti- cles, which is good enough for further refinement. However, the micro-F1 of the extractor is only 61.08% in the test set, which will lead to severe error propagation problem if we use the prediction results of the extractor directly. Therefore, we de- sign the article attention mechanism to handle the noise in the top 20 articles. <ref type="table" target="#tab_3">Table 2</ref> shows the re-ranking results of our ar- ticle attention module (column 2-3) and the corre- sponding charge prediction performances (column 4), under different weights for article attention (β in Eq. 6). Prec@1 refers to top 1 precision, and MAP refers to mean average precision. We can see that, even if there is no supervision over the article attention (β = 0), our model still has reasonable performance on re-ranking the k articles. When the attention supervision is employed, the extrac- tion quality improves significantly, and keeps in- creasing as β goes up. However, the charge pre- diction performance does not always increase with the article extraction quality, and the best perfor- mance is achieved when β = 0.1. This is not sur- prising, since there exists a tradeoff between the benefits of more accurate article extraction and the less model capacity left for charge classification due to the increased emphasis on the article extrac- tion performance. The promising article extraction results also confirm the ability of our model to pro- vide legal basis for the charge prediction.  <ref type="table">Table 3</ref>: Performance (micro statistics) on News</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance on News Data</head><p>There are usually clear differences between the ex- pressions used by legal practitioners and people without legal background, thus it is important to see how our model will perform on fact descrip- tions written by non-legal professionals.</p><p>We create a news dataset by asking 3 law school students to annotate the appropriate charges for 100 social news reports about criminal cases from two news websites <ref type="bibr">5</ref> , with 262 words on average and 25 distinct charges. The κ value is 0.83, indi- cating good consistency. The annotators are asked to have a disscussion to achieve an aggreement on inconsistent annotations. The results are shown in <ref type="table">Table 3</ref>, where we only report micro statistics due to the relatively small size of the dataset compared with the number of distinct charges.</p><p>We can see that, SVM fact suffers a signifi- cant drop in F1 on the news data, confirming the gap between the expressions used by legal prac- titioners and non-legal professionals, given the BOW nature of SVM fact. Although SVM fact cannot generalize well, the patterns learned by SVM fact are reliable in themselves, leading to a high precision. It is not surprising that our NN models also suffer from the expression differ- ences, but due to the effectiveness of our NN archi- tecture, with about 10%∼15% less absolute drop in F1, and NN fact supv art can still achieve 79.12% in F1. For example, the word (beat up) is seldom used in judgement documents, mak- ing it hard for SVM fact to correctly utilize as an indicator for injury related charges, but, our NN models can associate it with its near- synonymy (hit), which is a formal expression in judgement documents. Furthermore, the clear improvements from NN fact to NN fact art, and further to NN fact supv art prove again the importance of relevant law articles in support- ing the charge prediction, even in news domain. The reasonable performance on news data also shows that our method do have the ability to help non-legal professionals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose an attention-based neu- ral network framework that can jointly model the charge prediction task and the relevant article ex- traction task, where the weighted relevant articles can serve as legal basis to support the charge pre- diction. The experimental results on judgement documents of criminal cases in China show the ef- fectiveness of our model on both charge prediction and relevant article extraction. The comparison of different variants of our model also indicates the importance of law articles in making judicial de- cisions in the civil law system. By experiment- ing on news data, we show that, although trained on judgement documents, our model also has rea- sonable generalization ability on fact descriptions written by non-legal professionals. While promis- ing, our model still cannot explicitly handle multi- defendant cases, and there is also a clear gap be- tween our model and the upper bound improve- ment that relevant articles can achieve. We will leave these challenges for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Our Model aggregate them with a document-level sequence encoder to produce the document embedding d. While these two encoders can have different architectures, we use the same here for simplicity.</figDesc><graphic url="image-2.png" coords="4,307.56,62.81,217.71,123.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Document Encoder Framework</figDesc><graphic url="image-3.png" coords="4,325.70,287.72,181.43,115.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Model</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Refined Article Extraction Performance</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://wenshu.court.gov.cn</note>

			<note place="foot" n="3"> https://github.com/hankcs/HanLP</note>

			<note place="foot" n="4"> NN art uses fact embeddings to attentively aggregate relevant articles, but only use the aggregated article embedding da, without fact embedding d f , for charge prediction.</note>

			<note place="foot" n="5"> http://news.cn and http://people.com.cn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Predicting judicial decisions of the european court of human rights: A natural language processing perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Aletras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tsarapatsanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preot¸iucpreot¸iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Lampos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Lexicalmorphological modeling for legal text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Tien</forename><surname>Danilo S Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">Le</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.00799</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A text mining approach to assist the general public in the retrieval of legal documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hung</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Liang</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="280" to="290" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to classify text using support vector machines: Methods, theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><forename type="middle">Joachims</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Kluwer Academic Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A general approach for predicting the behavior of the supreme court of the united states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel Martin Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bommarito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blackman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03473</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Satoh</surname></persName>
		</author>
		<ptr target="http://webdocs.cs.ualberta.ca/˜miyoung2/jurisin_task/index.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">COLIEE-14</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Legal question answering using ranking svm and syntactic/semantic similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi-Young</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JSAI International Symposium on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="244" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved neural network-based multi-label classification with better initialization leveraging label cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gakuto</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="521" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting machine learning models for chinese legal documents labeling, case classification, and sentencing prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Ting</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung-Jia</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ROCLING XXIV</title>
		<imprint>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Case instance generation and refinement for case-based criminal summary judgments in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Tsung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim-How</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="783" to="800" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring phrase-based classification of judicial documents for criminal charges in chinese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chwen-Dar</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Methodologies for Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="681" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Classifying criminal charges in chinese for web-based legal services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Predicting associated statutes for legal problems. Information Processing &amp; Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hung</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Liang</forename><surname>Ho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="194" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Largescale multi-label text classification-revisiting neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungi</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneldo</forename><surname>Loza Mencía</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Analyzing the extraction of relevant legal judgments using paragraph-level and citation information. AI4J-Artificial Intelligence for Justice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Raghav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reddy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Xiaodong He, Alex Smola, and Eduard Hovy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
