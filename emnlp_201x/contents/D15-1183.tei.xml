<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-09">September 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY)</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY)</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Word Embedding Model and its Low Rank Positive Semidefinite Solution</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Lisbon, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="17" to="21"/>
							<date type="published" when="2015-09">September 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Most existing word embedding methods can be categorized into Neural Embedding Models and Matrix Factorization (MF)-based methods. However some models are opaque to probabilistic interpretation , and MF-based methods, typically solved using Singular Value Decomposition (SVD), may incur loss of corpus information. In addition, it is desirable to incorporate global latent factors, such as topics, sentiments or writing styles, into the word embedding model. Since gen-erative models provide a principled way to incorporate latent factors, we propose a generative word embedding model, which is easy to interpret, and can serve as a basis of more sophisticated latent factor models. The model inference reduces to a low rank weighted positive semidefinite approximation problem. Its optimization is approached by eigendecomposition on a submatrix, followed by online blockwise regression, which is scalable and avoids the information loss in SVD. In experiments on 7 common benchmark datasets, our vectors are competitive to word2vec, and better than other MF-based methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of word embedding is to model the distri- bution of a word and its context words using their corresponding vectors in a Euclidean space. Then by doing regression on the relevant statistics de- rived from a corpus, a set of vectors are recovered which best fit these statistics. These vectors, com- monly referred to as the embeddings, capture se- mantic/syntactic regularities between the words.</p><p>The core of a word embedding method is the link function that connects the input -the embed- dings, with the output -certain corpus statistics.</p><p>Based on the link function, the objective function is developed. The reasonableness of the link func- tion impacts the quality of the obtained embed- dings, and different link functions are amenable to different optimization algorithms, with different scalability. Based on the forms of the link func- tion and the optimization techniques, most meth- ods can be divided into two classes: the traditional neural embedding models, and more recent low rank matrix factorization methods.</p><p>The neural embedding models use the softmax link function to model the conditional distribution of a word given its context (or vice versa) as a function of the embeddings. The normalizer in the softmax function brings intricacy to the optimiza- tion, which is usually tackled by gradient-based methods. The pioneering work was ( <ref type="bibr" target="#b3">Bengio et al., 2003)</ref>. Later <ref type="bibr" target="#b26">Mnih and Hinton (2007)</ref> propose three different link functions. However there are interaction matrices between the embeddings in all these models, which complicate and slow down the training, hindering them from being trained on huge corpora. <ref type="bibr" target="#b23">Mikolov et al. (2013a)</ref> and <ref type="bibr" target="#b24">Mikolov et al. (2013b)</ref> greatly simplify the condi- tional distribution, where the two embeddings in- teract directly. They implemented the well-known "word2vec", which can be trained efficiently on huge corpora. The obtained embeddings show ex- cellent performance on various tasks.</p><p>Low-Rank Matrix Factorization (MF in short) methods include various link functions and opti- mization methods. The link functions are usu- ally not softmax functions. MF methods aim to reconstruct certain corpus statistics matrix by the product of two low rank factor matrices. The ob- jective is usually to minimize the reconstruction error, optionally with other constraints. In this line of research, <ref type="bibr" target="#b18">Levy and Goldberg (2014b)</ref> find that "word2vec" is essentially doing stochastic weighted factorization of the word-context point- wise mutual information (PMI) matrix. They then factorize this matrix directly as a new method. <ref type="bibr" target="#b27">Pennington et al. (2014)</ref> propose a bilinear regres- sion function of the conditional distribution, from which a weighted MF problem on the bigram log- frequency matrix is formulated. Gradient Descent is used to find the embeddings. Recently, based on the intuition that words can be organized in se- mantic hierarchies,  add hi- erarchical sparse regularizers to the matrix recon- struction error. With similar techniques,  reconstruct a set of pretrained embed- dings using sparse vectors of greater dimensional- ity. <ref type="bibr" target="#b8">Dhillon et al. (2015)</ref> apply Canonical Corre- lation Analysis (CCA) to the word matrix and the context matrix, and use the canonical correlation vectors between the two matrices as word embed- dings. <ref type="bibr" target="#b30">Stratos et al. (2014)</ref> and <ref type="bibr" target="#b31">Stratos et al. (2015)</ref> assume a Brown language model, and prove that doing CCA on the bigram occurrences is equiva- lent to finding a transformed solution of the lan- guage model. <ref type="bibr" target="#b2">Arora et al. (2015)</ref> assume there is a hidden discourse vector on a random walk, which determines the distribution of the current word. The slowly evolving discourse vector puts a con- straint on the embeddings in a small text window. The maximum likelihood estimate of the embed- dings within this text window approximately re- duces to a squared norm objective.</p><p>There are two limitations in current word em- bedding methods. The first limitation is, all MF- based methods map words and their context words to two different sets of embeddings, and then em- ploy Singular Value Decomposition (SVD) to ob- tain a low rank approximation of the word-context matrix M . As SVD factorizes M M , some in- formation in M is lost, and the learned embed- dings may not capture the most significant regu- larities in M . Appendix A gives a toy example on which SVD does not work properly.</p><p>The second limitation is, a generative model for documents parametered by embeddings is absent in recent development. Although <ref type="bibr" target="#b30">(Stratos et al., 2014;</ref><ref type="bibr" target="#b31">Stratos et al., 2015;</ref><ref type="bibr" target="#b2">Arora et al., 2015</ref>) are based on generative processes, the generative pro- cesses are only for deriving the local relationship between embeddings within a small text window, leaving the likelihood of a document undefined. In addition, the learning objectives of some mod- els, e.g. ( <ref type="bibr" target="#b24">Mikolov et al., 2013b</ref>, Eq.1), even have no clear probabilistic interpretation. A genera- tive word embedding model for documents is not only easier to interpret and analyze, but more im- portantly, provides a basis upon which document- level global latent factors, such as document topics <ref type="bibr" target="#b34">(Wallach, 2006</ref>), sentiments <ref type="bibr" target="#b20">(Lin and He, 2009)</ref>, writing styles ( <ref type="bibr" target="#b41">Zhao et al., 2011b</ref>), can be incor- porated in a principled manner, to better model the text distribution and extract relevant information.</p><p>Based on the above considerations, we pro- pose to unify the embeddings of words and con- text words. Our link function factorizes into three parts: the interaction of two embeddings capturing linear correlations of two words, a residual captur- ing nonlinear or noisy correlations, and the uni- gram priors. To reduce overfitting, we put Gaus- sian priors on embeddings and residuals, and ap- ply Jelinek-Mercer Smoothing to bigrams. Fur- thermore, to model the probability of a sequence of words, we assume that the contributions of more than one context word approximately add up. Thereby a generative model of documents is con- structed, parameterized by embeddings and resid- uals. The learning objective is to maximize the corpus likelihood, which reduces to a weighted low-rank positive semidefinite (PSD) approxima- tion problem of the PMI matrix. A Block Co- ordinate Descent algorithm is adopted to find an approximate solution. This algorithm is based on Eigendecomposition, which avoids information loss in SVD, but brings challenges to scalability. We then exploit the sparsity of the weight matrix and implement an efficient online blockwise re- gression algorithm. On seven benchmark datasets covering similarity and analogy tasks, our method achieves competitive and stable performance.</p><p>The source code of this method is provided at https://github.com/askerlee/topicvec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notations and Definitions</head><p>Throughout the paper, we always use a uppercase bold letter as S, V to denote a matrix or set, a low- ercase bold letter as v w i to denote a vector, a nor- mal uppercase letter as N, W to denote a scalar constant, and a normal lowercase letter as s i , w i to denote a scalar variable. Suppose a vocabulary S = {s 1 , · · · , s W } con- sists of all the words, where W is the vocab- ulary size. We further suppose s 1 , · · · , s W are sorted in decending order of the frequency, i.e. s 1 is most frequent, and s W is least frequent. A document d i is a sequence of words  In a document, a sequence of words is referred to as a text window, denoted by w i , · · · , w i+l , or w i :w i+l in shorthand. A text window of chosen size c before a word w i defines the context of w i as w i−c , · · · , w i−1 . Here w i is referred to as the focus word. Each context word w i−j and the focus word w i comprise a bigram w i−j , w i .</p><formula xml:id="formula_0">d i = (w i1 , · · · , w iL i ), w ij ∈ S. A corpus is a collec-Name Description S Vocabulary {s1, · · · , sW } V Embedding matrix (vs 1 , · · · , vs W ) D Corpus {d1, · · · , dM</formula><p>The Pointwise Mutual Information between two words s i , s j is defined as</p><formula xml:id="formula_1">PMI(s i , s j ) = log P (s i , s j ) P (s i )P (s j ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Link Function of Text</head><p>In this section, we formulate the probability of a sequence of words as a function of their embed- dings. We start from the link function of bigrams, which is the building blocks of a long sequence. Then this link function is extended to a text win- dow with c context words, as a first-order approx- imation of the actual probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Link Function of Bigrams</head><p>We generalize the link function of "word2vec" and "GloVe" to the following:</p><formula xml:id="formula_2">P (s i , s j ) = exp v s j v s i + a s i s j P (s i )P (s j ) (1)</formula><p>The rationale for (1) originates from the idea of the Product of Experts in <ref type="bibr" target="#b14">(Hinton, 2002)</ref>. Sup- pose different types of semantic/syntactic regu- larities between s i and s j are encoded in differ-</p><formula xml:id="formula_3">ent dimensions of v s i , v s j . As exp{v s j v s i } = l exp{v s i ,l · v s j ,l },</formula><p>this means the effects of dif- ferent regularities on the probability are combined by multiplying together. If s i and s j are indepen- dent, their joint probability should be P (s i )P (s j ).</p><p>In the presence of correlations, the actual joint probability P (s i , s j ) would be a scaling of it. The scale factor reflects how much s i and s j are pos- itively or negatively correlated. Within the scale factor, v s j v s i captures linear interactions between s i and s j , the residual a s i s j captures nonlinear or noisy interactions. In applications, only v s j v s i is of interest. Hence the bigger magnitude v s j v s i is of relative to a s i s j , the better.</p><p>Note that we do not assume a s i s j = a s j s i . This provides the flexibility P (s i , s j ) = P (s j , s i ), agreeing with the asymmetry of bigrams in natu- ral languages. At the same time, v s j v s i imposes a symmetric part between P (s i , s j ) and P (s j , s i ).</p><p>(1) is equivalent to</p><formula xml:id="formula_4">P (s j |s i )=exp v s j v s i + a s i s j + log P (s j ) ,<label>(2)</label></formula><formula xml:id="formula_5">log P (s j |s i ) P (s j ) = v s j v s i + a s i s j .<label>(3)</label></formula><p>(3) of all bigrams is represented in matrix form:</p><formula xml:id="formula_6">V V + A = G,<label>(4)</label></formula><p>where G is the PMI matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Gaussian Priors on Embeddings</head><p>When <ref type="formula">(1)</ref> is employed on the regression of empir- ical bigram probabilities, a practical issue arises: more and more bigrams have zero frequency as the constituting words become less frequent. A zero-frequency bigram does not necessarily imply negative correlation between the two constituting words; it could simply result from missing data. But in this case, even after smoothing, (1) will force v s j v s i + a s i s j to be a big negative number, making v s i overly long. The increased magnitude of embeddings is a sign of overfitting.</p><p>To reduce overfitting of embeddings of infre- quent words, we assign a Spherical Gaussian prior</p><formula xml:id="formula_7">N (0, 1 2µ i I) to v s i : P (v s i ) ∼ exp{−µ i v s i 2 },</formula><p>where the hyperparameter µ i increases as the fre- quency of s i decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Gaussian Priors on Residuals</head><p>We wish v s j v s i in (1) captures as much corre- lations between s i and s j as possible. Thus the smaller a s i s j is, the better. In addition, the more frequent s i , s j is in the corpus, the less noise there is in their empirical distribution, and thus the residual a s i s j should be more heavily penalized.</p><p>To this end, we penalize the residual a s i s j by f (˜ P (s i , s j ))a 2 s i s j , where f (·) is a nonnega- tive monotonic transformation, referred to as the weighting function. Let h ij denote˜Pdenote˜ denote˜P (s i , s j ), then the total penalty of all residuals are the square of the weighted Frobenius norm of A:</p><formula xml:id="formula_8">s i ,s j ∈S f (h ij )a 2 s i s j = A 2 f (H) .<label>(5)</label></formula><p>By referring to "GloVe", we use the following weighting function, and find it performs well:</p><formula xml:id="formula_9">f (h ij ) =        √ h ij Ccut h ij &lt; C cut , i = j 1 h ij ≥ C cut , i = j 0 i = j ,</formula><p>where C cut is chosen to cut the most frequent 0.02% of the bigrams off at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Jelinek-Mercer Smoothing of Bigrams</head><p>As another measure to reduce the impact of miss- ing data, we apply the commonly used Jelinek- Mercer Smoothing ( <ref type="bibr" target="#b39">Zhai and Lafferty, 2004</ref>) to smooth the empirical conditional probability˜P probability˜ probability˜P (s j |s i ) by the unigram probability˜Pprobability˜ probability˜P (s j ) as:</p><formula xml:id="formula_10">˜ P smoothed (s j |s i ) = (1−κ) ˜ P (s j |s i )+κP (s j ). (6)</formula><p>Accordingly, the smoothed bigram empirical joint probability is defined as˜P as˜ as˜P</p><formula xml:id="formula_11">(s i , s j ) = (1−κ) ˜ P (s i , s j )+κP (s i )P (s j ).<label>(7)</label></formula><p>In practice, we find κ = 0.02 yields good re- sults. When κ ≥ 0.04, the obtained embeddings begin to degrade with κ, indicating that smoothing distorts the true bigram distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Link Function of a Text Window</head><p>In the previous subsection, a regression link func- tion of bigram probabilities is established. In this section, we adopt a first-order approximation based on Information Theory, and extend the link function to a longer sequence w 0 , · · · , w c−1 , w c .</p><p>Decomposing a distribution conditioned on n random variables as the conditional distributions on its subsets roots deeply in Information The- ory. This is an intricate problem because there could be both (pointwise) redundant information and (pointwise) synergistic information among the conditioning variables <ref type="bibr" target="#b35">(Williams and Beer, 2010)</ref>. They are both functions of the PMI. Based on an analysis of the complementing roles of these two types of pointwise information, we assume they are approximately equal and cancel each other when computing the pointwise interaction infor- mation. See Appendix B for a detailed discussion.</p><p>Following the above assumption, we have PMI(w 2 ; w 0 , w 1 ) ≈ PMI(w 2 ; w 0 )+PMI(w 2 ; w 1 ):</p><formula xml:id="formula_12">log P (w 0 , w 1 |w 2 ) P (w 0 , w 1 ) ≈log P (w 0 |w 2 ) P (w 0 ) +log P (w 1 |w 2 ) P (w 1 ) .</formula><p>Plugging <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_5">(3)</ref> into the above, we obtain</p><formula xml:id="formula_13">P (w 0 , w 1 , w 2 ) ≈ exp 2 i,j=0 i =j (v w i v w j + a w i w j ) + 2 i=0</formula><p>log P (w i ) .</p><p>We extend the above assumption to that the pointwise interaction information is still close to 0 within a longer text window. Accordingly the above equation extends to a context of size c &gt; 2:</p><formula xml:id="formula_14">P (w 0 , · · · , w c ) ≈ exp c i,j=0 i =j (v w i v w j + a w i w j ) + c i=0</formula><p>log P (w i ) .</p><p>From it derives the conditional distribution of w c , given its context w 0 , · · · , w c−1 :</p><formula xml:id="formula_15">P (w c | w 0 : w c−1 )= P (w 0 , · · · , w c ) P (w 0 , · · · , w c−1 ) ≈P (w c ) exp v wc c−1 i=0 v w i + c−1 i=0 a w i wc .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generative Process and Likelihood</head><p>We proceed to assume the text is generated from a Markov chain of order c, i.e., a word only depends on words within its context of size c. Given the hyperparameter µ = (µ 1 , · · ·, µ W ), the generative process of the whole corpus is: </p><formula xml:id="formula_16">a s i s j from N 0, 1 2f (h ij ) ;</formula><p>3. For each document d i , for the j-th word, draw word w ij from S with probability P (w ij | w i,j−c : w i,j−1 ) defined by <ref type="bibr">(8)</ref>.</p><formula xml:id="formula_17">v w0 v w1 v wc · · · d V A µ i v si</formula><p>h ij a ij <ref type="figure" target="#fig_0">Figure 1</ref>: The Graphical Model of PSDVec</p><p>The above generative process for a document d is presented as a graphical model in <ref type="figure" target="#fig_0">Figure 1</ref>. Based on this generative process, the probabil- ity of a document d i can be derived as follows, given the embeddings and residuals V , A:</p><formula xml:id="formula_18">P (d i |V , A) = L i j=1 P (w ij ) exp v w ij j−1 k=j−c v w ik + j−1 k=j−c a w ik w ij .</formula><p>The complete-data likelihood of the corpus is:</p><formula xml:id="formula_19">p(D, V , A) = W i=1 N (0, I 2µ i ) W,W i,j=1 N 0, 1 2f (h ij ) M i=1 p(d i |V, A) = 1 Z(H, µ) exp − W,W i,j=1 f (h i,j )a 2 s i s j − W i=1 µ i v s i 2 · M,L i i,j=1 P (w ij ) exp v w ij j−1 k=j−c v w ik + j−1 k=j−c a w ik w ij ,</formula><p>where Z(H, µ) is the normalizing constant. Taking the logarithm of both sides of p(D, A, V ) yields</p><formula xml:id="formula_20">log p(D, V , A) =C 0 − log Z(H, µ) − A 2 f (H) − W i=1 µ i v s i 2 + M,L i i,j=1 v w ij j−1 k=j−c v w ik + j−1 k=j−c a w ik w ij ,<label>(9)</label></formula><p>where C 0 = M,L i i,j=1 log P (w ij ) is constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Learning Objective</head><p>The learning objective is to find the embeddings V that maximize the corpus log-likelihood (9). Let x ij denote the (smoothed) frequency of bi- gram s i , s j in the corpus. Then (9) is sorted as:</p><formula xml:id="formula_21">log p(D, V , A) =C 0 − log Z(H, µ) − A 2 f (H) − W i=1 µ i v s i 2 + W,W i,j=1 x ij (v s i v s j + a s i s j ).<label>(10)</label></formula><p>As the corpus size increases, W,W i,j=1 x ij (v s i v s j +a s i s j ) will dominate the parameter prior terms. Then we can ignore the prior terms when maximizing <ref type="bibr">(10)</ref>.</p><formula xml:id="formula_22">max x ij (v s i v s j +a s i s j ) = x ij · max˜P max˜ max˜P smoothed (s i , s j ) log P (s i , s j ).</formula><p>As both { ˜ P smoothed (s i , s j )} and {P (s i , s j )} sum to 1, the above sum is maximized when P (s i , s j ) = ˜ P smoothed (s i , s j ). The maximum likelihood estimator is then:</p><formula xml:id="formula_23">P (s j |s i ) = ˜ P smoothed (s j |s i ), v s i v s j + a s i s j = log ˜ P smoothed (s j |s i ) P (s j ) .<label>(11)</label></formula><p>Writing (11) in matrix form:</p><formula xml:id="formula_24">B * = ˜ P smoothed (s j |s i ) s i ,s j ∈S G * = log B * − log u ⊗ (1 · · · 1),<label>(12)</label></formula><p>where "⊗" is the outer product.</p><p>Now we fix the values of v s i v s j + a s i s j at the above optimal. The corpus likelihood becomes</p><formula xml:id="formula_25">log p(D, V , A) =C 1 − A 2 f (H) − W i=1 µ i v s i 2 , subject to V V + A = G * ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_26">C 1 = C 0 + x ij log˜Plog˜ log˜P smoothed (s i , s j ) − log Z(H, µ) is constant.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Learning V as Low Rank PSD Approximation</head><p>Once G * has been estimated from the corpus using (12), we seek V that maximizes <ref type="bibr">(13)</ref>. This is to find the maximum a posteriori (MAP) estimates of V , A that satisfy V V + A = G * . Applying this constraint to (13), we obtain Algorithm 1 BCD algorithm for finding a unreg- ularized rank-N weighted PSD approximant. <ref type="formula" target="#formula_6">(14)</ref> Let X = V V . Then X is positive semidef- inite of rank N . Finding V that minimizes <ref type="formula" target="#formula_6">(14)</ref> is equivalent to finding a rank-N weighted posi- tive semidefinite approximant X of G * , subject to Tikhonov regularization. This problem does not admit an analytic solution, and can only be solved using local optimization methods.</p><formula xml:id="formula_27">Input: matrix G * , weight matrix W = f (H), iteration number T , rank N Randomly initialize X (0) for t = 1, · · · , T do G t = W • G * + (1 − W ) • X (t−1) X (t) = PSD Approximate(G t , N ) end for λ, Q = Eigen Decomposition(X (T ) ) V * = diag(λ 1 2 [1:N ]) · Q [1:N ] Output: V * arg max V log p(D, V , A) = arg min V G * −V V f (H) + W i=1 µ i v s i 2 .</formula><p>First we consider a simpler case where all the words in the vocabulary are enough frequent, and thus Tikhonov regularization is unnecessary. In this case, we set ∀µ i = 0, and <ref type="formula" target="#formula_6">(14)</ref> becomes an unregularized optimization problem. We adopt the Block Coordinate Descent (BCD) algorithm 1 in ( <ref type="bibr" target="#b29">Srebro et al., 2003)</ref> to approach this problem. The original algorithm is to find a generic rank-N ma- trix for a weighted approximation problem, and we tailor it by constraining the matrix within the positive semidefinite manifold.</p><p>We summarize our learning algorithm in Al- gorithm 1. Here "•" is the entry-wise prod- uct. We suppose the eigenvalues λ returned by Eigen Decomposition(X) are in descending or- der. Q [1:N ] extracts the 1 to N rows from Q .</p><p>One key issue is how to initialize X. <ref type="bibr" target="#b29">Srebro et al. (2003)</ref> suggest to set X (0) =G * , and point out that X (0) = 0 is far from a local optimum, thus requires more iterations. However we find G * is also far from a local optimum, and this setting con- verges slowly too. Setting X (0) = G * /2 usually yields a satisfactory solution in a few iterations.</p><p>The subroutine PSD Approximate() computes the unweighted nearest rank-N PSD approxima- tion, measured in F-norm <ref type="bibr" target="#b12">(Higham, 1988)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Online Blockwise Regression of V</head><p>In Algorithm 1, the essential subroutine PSD Approximate() does eigendecomposi- tion on G t , which is dense due to the logarithm transformation. Eigendecomposition on a W × W dense matrix requires O(W 2 ) space and O(W 3 ) time, difficult to scale up to a large vocabulary. In addition, the majority of words in the vocabulary are infrequent, and Tikhonov regularization is necessary for them.</p><p>It is observed that, as words become less fre- quent, fewer and fewer words appear around them to form bigrams. Remind that the vocabulary S = {s 1 , · · · , s W } are sorted in decending or- der of the frequency, hence the lower-right blocks of H and f (H) are very sparse, and cause these blocks in <ref type="formula" target="#formula_6">(14)</ref> to contribute much less penalty rela- tive to other regions. Therefore these blocks could be ignored when doing regression, without sacri- ficing too much accuracy. This intuition leads to the following online blockwise regression.</p><p>The basic idea is to select a small set (e.g. 30,000) of the most frequent words as the core words, and partition the remaining noncore words into sets of moderate sizes. Bigrams consist- ing of two core words are referred to as core bi- grams, which correspond to the top-left blocks of G and f (H). The embeddings of core words are learned approximately using Algorithm 1, on the top-left blocks of G and f (H). Then we fix the embeddings of core words, and find the em- beddings of each set of noncore words in turn. After ignoring the lower-right regions of G and f (H) which correspond to bigrams of two non- core words, the quadratic terms of noncore em- beddings are ignored. Consequently, finding these embeddings becomes a weighted ridge regression problem, which can be solved efficiently in closed- form. Finally we combine all embeddings to get the embeddings of the whole vocabulary. The de- tails are as follows:</p><formula xml:id="formula_28">1. Partition S into K consecutive groups S 1 , · · · , S k . Take K = 3 as an example.</formula><p>The first group is core words;</p><p>2. Accordingly partition G into K × K blocks, in this example as</p><formula xml:id="formula_29">  G 11 G 12 G 13 G 21 G 22 G 23 G 31 G 32 G 33   .</formula><p>Partition f (H),A in the same way. G 11 , f (H) 11 , A 11 correspond to core bi-</p><formula xml:id="formula_30">grams. Partition V into S 1 V 1 S 2 V 2 S 3 V 3 ;</formula><p>3. Solve V 1 V 1 + A 11 = G 11 using Algorithm 1, and obtain core embeddings V * 1 ; 4. Set V 1 = V * 1 , and find V * 2 that minimizes the total penalty of the 12-th and 21-th blocks of residuals (the 22-th block is ignored due to its high sparsity):</p><p>• (Levy and Goldberg, 2014b): the PPMI ma- trix without dimension reduction, and SVD of PPMI matrix, both yielded by hyperwords;</p><p>• (Pennington et al., 2014): GloVe 3 ;</p><p>• (Stratos et al., 2015): Singular 4 , which does SVD-based CCA on the weighted bigram fre- quency matrix;</p><p>• (Faruqui et al., 2015): Sparse 5 , which learns new sparse embeddings in a higher dimen- sional space from pretrained embeddings.</p><p>All models were trained on the English Wikipedia snapshot in March 2015. After removing non- textual elements and non-English words, 2.04 bil- lion words were left. We used the default hyperpa- rameters in Hyperwords when training PPMI and SVD. Word2vec, GloVe and Singular were trained with their own default hyperparameters. The embedding sets PSD-Reg-180K and PSD- Unreg-180K were trained using our online block- wise regression. Both sets contain the embed- dings of the most frequent 180,000 words, based on 25,000 core words. PSD-Unreg-180K was traind with all µ i = 0, i.e. disabling Tikhonov regularization. PSD-Reg-180K was trained with <ref type="bibr">[130001,</ref><ref type="bibr">180000]</ref> , i.e. increased regularization as the sparsity increases. To con- trast with the batch learning performance, the per- formance of PSD-25K is listed, which contains the core embeddings only. PSD-25K took advantages that it contains much less false candidate words, and some test tuples (generally harder ones) were not evaluated due to missing words, thus its scores are not comparable to others.</p><formula xml:id="formula_31">µ i =      2 i ∈ [25001, 80000] 4 i ∈ [80001, 130000] 8 i ∈</formula><p>Sparse was trained with PSD-180K-reg as the input embeddings, with default hyperparameters.</p><p>The benchmark sets are almost identical to those in ( <ref type="bibr" target="#b19">Levy et al., 2015)</ref>, except that ( <ref type="bibr" target="#b21">Luong et al., 2013</ref>)'s Rare Words is not included, as many rare words are cut off at the frequency 100, mak- ing more than 1/3 of test pairs invalid.</p><p>Word Similarity There are 5 datasets: Word- Sim Similarity (WS Sim) and WordSim Related- ness (WS Rel) ( <ref type="bibr" target="#b38">Zesch et al., 2008;</ref><ref type="bibr" target="#b0">Agirre et al., 2009</ref>   words that appear less than 100 times in the cor- pus, 7054 instances in MSR and 19364 instances in Google were left. The analogy questions were answered using 3CosAdd as well as 3CosMul pro- posed by <ref type="bibr" target="#b17">Levy and Goldberg (2014a)</ref>. <ref type="table" target="#tab_4">Table 2</ref> shows the results on all tasks. Word2vec significantly outperformed other methods on anal- ogy tasks. PPMI and SVD performed much worse on analogy tasks than reported in ( <ref type="bibr" target="#b19">Levy et al., 2015)</ref>, probably due to sub-optimal hyperparam- eters. This suggests their performance is unstable. The new embeddings yielded by Sparse systemat- ically degraded compared to the old embeddings, contradicting the claim in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Our method PSD-Reg-180K performed well consistently, and is best in 4 similarity tasks. It performed worse than word2vec on analogy tasks, but still better than other MF-based meth- ods. By comparing to PSD-Unreg-180K, we see Tikhonov regularization brings 1-4% performance boost across tasks. In addition, on similarity tasks, online blockwise regression only degrades slightly compared to batch factorization. Their perfor- mance gaps on analogy tasks were wider, but this might be explained by the fact that some hard cases were not counted in PSD-25K's evaluation, due to its limited vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, inspired by the link functions in previous works, with the support from Informa- tion Theory, we propose a new link function of a text window, parameterized by the embeddings of words and the residuals of bigrams. Based on the link function, we establish a generative model of documents. The learning objective is to find a set of embeddings maximizing their posterior likeli- hood given the corpus. This objective is reduced to weighted low-rank positive-semidefinite approxi- mation, subject to Tikhonov regularization. Then we adopt a Block Coordinate Descent algorithm, jointly with an online blockwise regression algo- rithm to find an approximate solution. On seven benchmark sets, the learned embeddings show competitive and stable performance.</p><p>In the future work, we will incorporate global latent factors into this generative model, such as topics, sentiments, or writing styles, and develop more elaborate models of documents. Through learning such latent factors, important summary information of documents would be acquired, which are useful in various applications.</p><formula xml:id="formula_32">M (1) = 1.4 0.8 0 0.8 2.6 0 0 0 2 , M (2) = 0.2 −1.6 0 −1.6 −2.2 0 0 0 2 .</formula><p>They have identical left singular matrix and sin- gular values (3, 2, 1), but their eigenvalues are (3, 2, 1) and (−3, 2, 1), respectively.</p><p>In a rank-2 approximation, the largest two singular values/vectors are kept, and M (1) and M (2) yield identical SVD embeddings V = ( 0.45 0.89 0 0 0 1 ) (the rows may be scaled depending on the algorithm, without affecting the validity of the following conclusion). The embeddings of s 1 and s 2 (columns 1 and 2 of V ) point at the same di- rection, suggesting they are positively correlated. However as M <ref type="formula" target="#formula_4">(2)</ref> 1,2 = M (2) 2,1 = −1.6 &lt; 0, they are actually negatively correlated in the second cor- pus. This inconsistency is because the principal eigenvalue of M (2) is negative, and yet the corre- sponding singular value/vector is kept.</p><p>When using eigendecomposition, the largest two positive eigenvalues/eigenvectors are kept. , which correctly preserves the negative correlation between s 1 , s 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Information Theory</head><p>Redundant information refers to the reduced un- certainty by knowing the value of any one of the conditioning variables (hence redundant). Syner- gistic information is the reduced uncertainty as- cribed to knowing all the values of conditioning variables, that cannot be reduced by knowing the value of any variable alone (hence synergistic). The mutual information I(y; x i ) and the redun- dant information Rdn(y; x 1 , x 2 ) are defined as: I(y; x i ) = E P (x i ,y) [log P (y|x i ) P (y) ]</p><p>Rdn(y; x 1 , x 2 ) = E P (y) min</p><formula xml:id="formula_33">x 1 ,x 2 E P (x i |y) [log P (y|x i ) P (y) ]</formula><p>The synergistic information Syn(y; x 1 , x 2 ) is defined as the PI-function in <ref type="bibr" target="#b35">(Williams and Beer, 2010)</ref>, skipped here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ( y ; x 1 )</head><p>I ( y ; x 2 ) S y n ( y ; x 1 , x 2 ) I ( y ; x 1 , x 2 ) R d n ( y ; x 1 , x 2 ) <ref type="figure">Figure 2</ref>: Different types of information among 3 random variables y, x 1 , x 2 . I(y; x 1 , x 2 ) is the mutual information between y and (x 1 , x 2 ). Rdn(y; x 1 , x 2 ) and Syn(y; x 1 , x 2 ) are the redun- dant information and synergistic information be- tween x 1 , x 2 , conditioning y, respectively.</p><p>The interaction information Int(x 1 , x 2 , y) mea- sures the relative strength of Rdn(y; x 1 , x 2 ) and Syn(y; x 1 , x 2 ) ( <ref type="bibr" target="#b33">Timme et al., 2014</ref>):</p><p>Int(x 1 , x 2 , y) =Syn(y; x 1 , x 2 ) − Rdn(y; x 1 , x 2 ) =I(y; x 1 , x 2 ) − I(y; x 1 ) − I(y; x 2 ) =E P (x 1 ,x 2 ,y) [log P (x 1 )P (x 2 )P (y)P (x 1 , x 2 , y) P (x 1 , x 2 )P (x 1 , y)P (x 2 , y) ] <ref type="figure">Figure 2</ref> shows the relationship of different information among 3 random variables y, x 1 , x 2 (based on <ref type="figure" target="#fig_0">Fig.1</ref> in <ref type="bibr" target="#b35">(Williams and Beer, 2010)</ref>).</p><p>PMI is the pointwise counterpart of mutual information I. Similarly, all the above concepts have their pointwise counterparts, obtained by dropping the expectation operator. Specifically, the pointwise interaction information is defined as PInt(x 1 , x 2 , y) = PMI(y; x 1 , x 2 ) − PMI(y; x 1 ) − PMI(y; x 2 ) = log P (x 1 )P (x 2 )P (y)P (x 1 ,x 2 ,y) P (x 1 ,x 2 )P (x 1 ,y)P (x 2 ,y) . If we know PInt(x 1 , x 2 , y), we can recover PMI(y; x 1 , x 2 ) from the mutual information over the variable subsets, and then recover the joint distribution P (x 1 , x 2 , y).</p><p>As the pointwise redundant information PRdn(y; x 1 , x 2 ) and the pointwise synergistic information PSyn(y; x 1 , x 2 ) are both higher- order interaction terms, their magnitudes are usually much smaller than the PMI terms. We assume they are approximately equal, and thus cancel each other when computing PInt. Given this, PInt is always 0. In the case of three words w 0 , w 1 , w 2 , PInt(w 0 , w 1 , w 2 ) = 0 leads to PMI(w 2 ; w 0 , w 1 ) = PMI(w 2 ; w 0 )+PMI(w 2 ; w 1 ).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 .</head><label>1</label><figDesc>For each word s i , draw the embedding v s i from N (0, 1 2µ i I); 2. For each bigram s i , s j , draw the residual</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Radinsky et al.</head><label></label><figDesc>(2011)'s Mechanical Turk dataset; and (Hill et al., 2014)'s SimLex-999 dataset. The embeddings were evaluated by the Spearman's rank correlation with the human ratings. Word Analogy The two datasets are MSR's analogy dataset (Mikolov et al., 2013c), with 8000 questions, and Google's analogy dataset (Mikolov et al., 2013a), with 19544 questions. After filtering questions involving out-of-vocabulary words, i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Notation Table 

tion of M documents D = {d 1 , · · · , d M }. In the 
vocabulary, each word s i is mapped to a vector v s i 
in N -dimensional Euclidean space. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Performance of each method across different tasks.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> It is referred to as an Expectation-Maximization algorithm by the original authors, but we think this is a misnomer.</note>

			<note place="foot" n="2"> https://code.google.com/p/word2vec/</note>

			<note place="foot" n="3"> http://nlp.stanford.edu/projects/glove/ 4 https://github.com/karlstratos/singular 5 https://github.com/mfaruqui/sparse-coding</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Omer Levy, Thomas Mach, Peilin Zhao, Mingkui Tan, Zhiqiang Xu and Chunlin Wu for their helpful discussions and insights. This re-search is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IDM Futures Funding Initiative and ad-ministered by the Interactive and Digital Media Programme Office.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>We trained our model along with a few state-of- the-art competitors on Wikipedia, and evaluated the embeddings on 7 common benchmark sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Our own method is referred to as PSD. The com- petitors include:</p><p>• (Mikolov et al., 2013b): word2vec 2 , or SGNS in some literature;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Possible Trap in SVD</head><p>Suppose M is the bigram matrix of interest. SVD embeddings are derived from the low rank approx- imation of M M , by keeping the largest singular values/vectors. When some of these singular val- ues correspond to negative eigenvalues, undesir- able correlations might be captured. The follow- ing is an example of approximating a PMI matrix. A vocabulary consists of 3 words s 1 , s 2 , s 3 . Two corpora derive two PMI matrices:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on similarity and relatedness using distributional and wordnet-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Kravalova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies</title>
		<meeting>Human Language Technologies</meeting>
		<imprint>
			<publisher>The</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Marius Pas¸caPas¸ca, and Aitor Soroa</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03520</idno>
		<title level="m">Random walks on discourse spaces: a new generative language model with applications to semantic word embeddings. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributional semantics in technicolor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elia</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namkhanh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Soc. Inf. Sci</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view learning of word embeddings via cca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramveer</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eigenwords: Spectral word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paramveer S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="131" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Euclidean embedding of cooccurrence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="2265" to="2295" />
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computing a nearest symmetric positive semidefinite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="103" to="118" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Simlex-999: Evaluating semantic models with (genuine) similarity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno>abs/1408.3456</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weighted Ridge Regression: Combining Ridge and Robust Regression Methods. NBER Working Papers 0011</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">National Bureau of Economic Research</title>
		<imprint>
			<date type="published" when="1973-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1460" to="1480" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linguistic regularities in sparse and explicit word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2014</title>
		<meeting>CoNLL-2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural word embeddings as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint sentiment/topic model for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and Knowledge Management</title>
		<meeting>the 18th ACM conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="375" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Eigenvalue Algorithms for Symmetric Hierarchical Matrices. Dissertation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Chemnitz University of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2013</title>
		<meeting>NIPS 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL 2013</title>
		<meeting>HLTNAACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine learning</title>
		<meeting>the 24th International Conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A word at a time: Computing word relatedness using temporal semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on World Wide Web, WWW &apos;11</title>
		<meeting>the 20th International Conference on World Wide Web, WWW &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weighted low-rank approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2003</title>
		<meeting>ICML 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="720" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning class-based n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Do-Kyum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Uncertainty in Artificial Intelligence</title>
		<meeting>the Association for Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model-based word embeddings from decompositions of count matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2015</title>
		<meeting>ACL 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Riemannian pursuit for big matrix recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2014</title>
		<meeting>ICML 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1539" to="1547" />
		</imprint>
	</monogr>
	<note>Bart Vandereycken, and Sinno Jialin Pan</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synergy, redundancy, and multivariate information measures: an experimentalist&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Timme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Alford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Flecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John M</forename><surname>Beggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="140" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Topic modeling: beyond bag-of-words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hanna M Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="977" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Nonnegative decomposition of multivariate information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1004.2515</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable maximum margin matrix factorization by active riemannian subspace search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning word representations with hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Using wiktionary for computing semantic relatedness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Zesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2008</title>
		<meeting>AAAI 2008</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="861" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Double updating online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1587" to="1615" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparing twitter and traditional media using topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfei</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval (Proceedings of the 33rd Annual European Conference on Information Retrieval Research)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="338" to="349" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
