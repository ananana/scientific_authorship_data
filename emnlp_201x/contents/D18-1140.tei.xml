<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representing Social Media Users for Sarcasm Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Alex</forename><surname>Kolchinski</surname></persName>
							<email>kolchinski@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
							<email>cgpotts@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Representing Social Media Users for Sarcasm Detection</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1115" to="1121"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>1115</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We explore two methods for representing authors in the context of textual sarcasm detection: a Bayesian approach that directly represents authors&apos; propensities to be sarcastic, and a dense embedding approach that can learn interactions between the author and the text. Using the SARC dataset of Reddit comments, we show that augmenting a bidirectional RNN with these representations improves performance ; the Bayesian approach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Irony and sarcasm 1 are extreme examples of context-dependence in language. Given only the text Great idea! or What a hardship!, we cannot resolve the speaker's intentions unless we have in- sight into the circumstances of utterance -who is speaking, and to whom, and how the content relates to the preceding discourse <ref type="bibr" target="#b3">(Clark, 1996)</ref>. While certain texts are biased in favor of sarcas- tic uses <ref type="bibr" target="#b14">(Kreuz and Caucci, 2007;</ref><ref type="bibr" target="#b23">Wallace et al., 2014</ref>), the non-literal nature of this phenomenon ensures that there is an important role for prag- matic inference <ref type="bibr" target="#b4">(Clark and Gerrig, 1984)</ref>.</p><p>The current paper is an in-depth study of one important aspect of the context dependence of sar- casm: the author. Our guiding hypotheses are that authors vary in their propensity for using sarcasm, that this propensity is influenced by more general facts about the context, and that authors have their own particular ways of indicating sarcasm. These hypotheses are well supported by psycholinguis- tic research ( <ref type="bibr" target="#b5">Colston and Lee, 2004;</ref><ref type="bibr" target="#b10">Gibbs, 2000;</ref><ref type="bibr" target="#b6">Dress et al., 2008</ref>), but our ability to test them <ref type="bibr">1</ref> We use "sarcasm" to include both sarcasm and irony, as the two are generally conflated in the literature we review. at scale has until recently been limited by avail- able annotated corpora. With the release of the Self-Annotated Reddit Corpus (SARC), <ref type="bibr" target="#b13">Khodak et al. (2017)</ref> have helped to address this limitation. SARC is large and diverse, and its distribution of users across comments and forums makes it par- ticularly well suited to modeling authors and their relationship to sarcasm. Our core model of comment texts is a bidirec- tional RNN with GRU cells. To model authors, we propose two strategies for augmenting these RNN representations: a simple Bayesian method that captures only an author's raw propensity for sarcasm, and a dense embedding method that al- lows for complex interactions between author and text ( <ref type="figure" target="#fig_0">Figure 1)</ref>. We find that, on SARC, the simple Bayesian approach does remarkably well, espe- cially in smaller, more focused forums. On the full SARC dataset, author embeddings are able to en- code more kinds of variation and interaction with the text, and thus they achieve the highest predic- tive accuracy. These findings extend and reinforce the prior work on user-level modeling for sarcasm (Section 2), and they indicate that simple represen- tation methods are effective here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>A substantial literature exists around sarcasm de- tection. Many of the prior studies focus on the analysis of Twitter posts, which lend themselves well to sarcasm detection with NLP methods be- cause they are available in large quantities, they tend to correspond roughly to a single utterance, and users' hashtags in tweets (e.g., #sarcasm, #not) can provide imperfect but useful labels. A central theme of this literature is that bringing in contextual features helps performance.</p><p>González-Ibánez et al. (2011) trained classifiers using a combination of lexical and pragmatic fea- tures, including emoticons and whether the user was responding to another tweet (see also <ref type="bibr" target="#b7">Felbo et al. 2017</ref>). <ref type="bibr" target="#b1">Bamman and Smith (2015)</ref> extend this kind of analysis with additional information about the context. Of special interest here are their contextual features: the author's historical senti- ment, topics, and terms; the addressee; and fea- tures drawn from historical interactions between the author and addressee. The study finds most features to be useful, but a model trained on the tweet and author features alone achieved essen- tially the same performance (84.9% accuracy) as a model trained on all features (85.1%).</p><p>In a similar vein, Rajadesingan et al. (2015) used a complex combination of features from users' Twitter histories, including sentiment, grammar, and word choice, as inputs into their model, and report a ≈7% gain in classification ac- curacy upon adding these features to a baseline n- gram classifier.</p><p>Recent papers have also applied deep learning methods to detecting sarcastic tweets. <ref type="bibr" target="#b17">Poria et al. (2016)</ref> use a combination convolutional-SVM ar- chitecture with auxiliary sentiment input features. The architecture of <ref type="bibr" target="#b25">Zhang et al. (2016)</ref> includes an RNN, and uses contextual features as well as tweet text for inputs. <ref type="bibr" target="#b0">Amir et al. (2016)</ref> extend the work of Bamman and Smith by generating author embeddings to re- flect users' word-usage patterns (but not sarcasm history) in a manner similar to the paragraph vec- tors introduced by <ref type="bibr" target="#b15">Le and Mikolov (2014)</ref>. With the inclusion of these embeddings, their convolu- tional neural network (CNN) achieves a 2% gain in accuracy over that of Bamman and Smith.</p><p>Ghosh and Veale (2017) present a combination CNN/LSTM (long short-term memory RNN) ar- chitecture that takes as inputs user affect inferred from recent tweets as well as the text of the tweet and that of the parent tweet. When a tweet was addressed to someone by name, the name of the addressee was included in the text representation of the tweet, providing a loose link between in- terlocutors ( <ref type="bibr" target="#b24">West et al., 2014</ref>) and a ≈1% gain in performance for some data sets.</p><p>There has also been a small amount of previ- ous work on Reddit data for sarcasm ( <ref type="bibr" target="#b21">Tay et al., 2018;</ref><ref type="bibr" target="#b9">Ghosh and Muresan, 2018)</ref>. <ref type="bibr" target="#b23">Wallace et al. (2014)</ref> explore a hand-labeled dataset of ≈3K Reddit comments from six subreddits. They report that, when human graders attempted to mark com- ments as sarcastic or not sarcastic, they needed additional context like subreddit norms and au- thor history roughly 30% of the time, and that the comments which graders found ambiguous were largely the same as those on which a baseline bag- of-words classifier tended to make mistakes. In a follow-up study, <ref type="bibr" target="#b22">Wallace et al. (2015)</ref> find that se- mantic cues for sarcasm differ by subreddit, and they show classifier accuracy gains when model- ing subreddit-specific variation.</p><p>The work that is closest to our own is that of <ref type="bibr" target="#b12">Hazarika et al. (2018)</ref>, who also experiment on the SARC dataset. Their model learns author, forum, and text embeddings, and they show that all three kinds of representation contribute positively to the overall performance. We take a much simpler ap- proach to author embeddings and do not include forum embeddings, and we report comparable per- formance (Section 6). We take this as further indi- cation of the value of author features for modeling sarcasm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The SARC Dataset</head><p>The Self-Annotated Reddit Corpus (SARC) was created by <ref type="bibr" target="#b13">Khodak et al. (2017)</ref>. <ref type="bibr">2</ref> It includes an unprecedented 533M comments. The corpus is self-annotated in the sense that a comment is considered sarcastic if its author marked it with the "/s" tag. As a result, the positive examples are essentially those which the authors considered ambiguous enough to explicitly tag as sarcastic, meaning that the prediction problem is actually to identify which comments are not only sarcastic but both sarcastic and not obviously so.</p><p>The dataset is filtered in numerous ways, and has good precision (only ≈1% false positive rate) but poor recall (2% false negatives relative to 0.25% true positives, or ≈11% recall). To alle- viate the issues caused by low recall, the dataset also includes a balanced sample, where comments are supplied in pairs, both responding to the same parent comment and with exactly one of the two tagged as sarcastic. All comments are accompa- nied with ancestor comments from the original conversation, author information, and a score as voted on by Reddit users.</p><p>This dataset presents numerous advantages for sarcasm detection. For one, it is vastly larger than past sarcasm datasets, which enables the training of more sophisticated models. In addition, most work in sarcasm detection has focused on tweets, which are very short and tend to use abbreviated and atypical language. Reddit comments are not constrained by length and are therefore more rep- resentative of how people typically write. Finally, Reddit is organized into topically-defined commu- nities known as subreddits, each of which has its own community norms and linguistic patterns. By making available large amounts of data from a number of subreddits, SARC facilitates the com- parative analysis of subreddits, and more gener- ally provides a view into the differences between communities. <ref type="table">Table 1</ref> provides basic statistics on the entire corpus as well as the subreddits that we focus on in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>Our baseline model is a bidirectional RNN with GRU cells (BiGRU; <ref type="bibr" target="#b2">Cho et al. 2014</ref>). We tried variants with LSTM cells and did not observe a significant difference in performance. We there- fore chose to use GRU cells as the model with fewer parameters. <ref type="bibr">3</ref> The inputs to the BiGRU model are users' com- ments, which are split into words (and in the case of conjunctions, subwords) and punctuation marks and are converted to word vectors. The final states of the two directions of the BiGRU are concate- nated with each other and run through either a single fully-connected linear layer or two fully- connected linear layers with a rectified linear unit in between. The output of the final linear layer is fed through a sigmoid function which outputs the estimated probability of sarcasm. This base- line does not take author information into account: for each comment, only the words of the comment are considered as inputs.</p><p>The Bayesian prior model extends the Bi- GRU with the sarcastic and non-sarcastic com- ment counts for authors seen in the training data, which serves as a prior for sarcasm frequency. This version of the model takes as inputs both a representation of the comment and the author rep- resentation x author ∈ Z 2 ≥0 to estimate the proba- bility of sarcasm. The model can be interpreted as computing a posterior probability of sarcasm given both the comment and the prior of previous sarcastic and non-sarcastic comment counts -au- thor modeling reduced to a Bernoulli prior. For previously unseen authors, x author is set to (0, 0).</p><p>The author embedding approach extends the baseline BiGRU in a more sophisticated way. Here, each author seen in the training data is asso- ciated with a randomly initialized embedding vec- tor x author ∈ R 15 , which is then provided as an input to the model along with a representation of the words of the comment. A special randomly initialized vector x UNK is used for previously un- seen authors. The author embeddings are updated during training, with the goal of learning more so- phisticated individualized patterns of sarcasm than the Bayesian prior allows. We experimented with training the x UNK vector on infrequently-seen au- thors (fewer than 5 comments in the training set) instead of using a random vector, and found some suggestions of improved performance. However, as the differences in performance were not sub- stantial enough to change the relative performance of the different models, we report the results for the simpler random-x UNK model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We conducted three sets of experiments, one for each model, to evaluate the effectiveness of the different approaches to author modeling. Each set of experiments was conducted on five datasets: the balanced version of the entire corpus as well as the balanced and unbalanced versions of the r/politics and r/AskReddit subcorpora <ref type="table">(Table 1)</ref>.</p><p>In all cases, the raw comment data was tok- enized into words and punctuation marks, with components of contractions treated as individual words. We mapped tokens to FastText embed- ding vectors which had been trained, using sub- word infomation, on Wikipedia 2017, the UMBC webbase corpus, and the statmt.org news dataset ( <ref type="bibr" target="#b16">Mikolov et al., 2018)</ref>. While vectors existed for nearly 100% of tokens generated, exceptions were mapped to a randomly initialized UNK vector.</p><p>All models were trained with early stopping on a randomly partitioned holdout set of either 5% of the data for balanced subreddit corpora or 1% for the others. The performance of the model, as used for hyperparameter tuning, was evaluated against a second holdout set, generated in the same manner as the first holdout set but disjoint from both it and the portion of the data used for training.</p><p>Hyperparameters were tuned to maximize model performance as evaluated in this manner, starting with a randomized search process and fine-tuned manually. The final evaluation was con- ducted against the test set, with a single randomly partitioned holdout set from the training data again used for early stopping. We applied dropout <ref type="bibr" target="#b20">(Srivastava et al., 2014</ref>) during training before and be- tween all linear layers. For additional regulariza- tion, we also applied an l2-norm penalty to the lin- ear weights but not to the GRU weights.</p><p>We attempted other model variations, including multiple GRU layers and an attention mechanism for GRU outputs, but did not observe any gains in performance from the larger models. <ref type="table" target="#tab_2">Table 2</ref> reports the means of 10 runs to control for variation deriving from randomness in the op- timization process <ref type="bibr" target="#b19">(Reimers and Gurevych, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Quantitive assessment</head><p>Where there is overlap between our experiments and those of <ref type="bibr">Hazarika et al. (2018) (CASCADE)</ref>, our model is highly competitive. We slightly under-perform on the full balanced dataset but come out ahead on r/politics. This is striking be- cause our model makes use of much less informa- tion. First, unlike CASCADE, we do not have fo- rum embeddings. Second, CASCADE author em- beddings involve extensive feature engineering in- cluding "stylometric" and "personality" features. Our author embeddings, on the other hand, are either simple empirical estimates (Bayesian pri- ors) or learned embeddings with random initializa- tions, in both cases allowing simpler model spec- ification and training, and more flexibility on the task for which they are used.</p><p>There is also evidence that the BiGRU yields better representations of texts than does Hazarika et al.'s CNN-based model. Our 'No embed' model is akin to their CASCADE with no contextual fea- tures, which achieves only 0.66 on the full bal- anced corpus and 0.70 on the r/politics balanced dataset. Both numbers are well behind our 'No embed'. Unfortunately, we do not have space for a fuller study of the similarities and differences be- tween our model and CASCADE.</p><p>Both of our methods for representing authors perform well. This is perhaps especially strik- ing for the unbalanced experiments, where the percentage of sarcastic comments is tiny (Ta- ble 1). The two methods perform differently on individual forums than on the full dataset. For the r/politics and r/AskReddit communities, the Bayesian priors give the best results. The situa- tion is reversed for the full dataset, where the high- dimensional embeddings outperform the Bayesian priors. This likely reflects two interacting fac- tors. First, with smaller, more focused forums, it is harder to learn good author embeddings, so the simple prior is more reliable. Second, on the full dataset, there are more examples, and also more complex interactions between authors and their texts, so the added representational power of the embeddings proves justified. <ref type="table" target="#tab_3">Table 3</ref> provides example predictions from the dif- ferent models. Each example is taken from the holdout set of a run in which all three models were trained on the same training set and evaluation was conducted on the same holdout set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Qualitative comparisons</head><p>For both sarcastic and non-sarcastic comments, author modeling can be helpful for disambigua- tion. For instance, in examples 1 and 2, omitting   author modeling led to incorrect predictions, but including the frequency of the author's sarcasm use alone was enough to change the prediction from incorrect to correct.</p><p>In cases like examples 3 and 4, where the Bayesian prior was insufficient, including a model of the author's individualized patterns of sarcasm was much more powerful. That said, the more complex embedding model can misfire, as in ex- ample 5, where the simpler models make a correct prediction but it does not. This appeared to happen more for non-sarcastic examples, where the em- bedding model would occasionally strongly influ- ence the predicted probability of sarcasm upward. Evidently, authors have more individualized pat- terns of sarcasm than of non-sarcasm.</p><p>Judging by the relative performance of the Bayesian and multidimensional-embedding mod- els <ref type="table" target="#tab_2">(Table 2</ref>), the multidimensional model wins more disagreements than it loses with the Bayesian model when there is more training data available. However, when there is not, it overfits to such a degree that its predictions of authors' sarcasm patterns are less useful than the Bayesian approach. This suggests a future direction of ex- ploration: the most useful model of all may be one that expands in complexity for authors with more examples available, and shrinks for those who have fewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper evaluated two data-driven methods for modeling the role of the author in sarcasm detec- tion. Both prove effective. As shown by <ref type="bibr" target="#b12">Hazarika et al. (2018)</ref>, similar techniques can be extended to other aspects of the context. While our ex- periments did not support adding these represen- tations, we think listeners rely on them as well, so additional computational modeling work here is likely to prove fruitful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The model architecture. Look-ups are indicated by arrows, dense connections by diamonds. The author embedding can be null (a text-only baseline), a prior reflecting the author's propensity for sarcasm, or a learned embedding. There are potentially multiple layers between the initial example embedding and the output sigmoid layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean macro-averaged F1 scores with bootstrapped 95% confidence intervals, based on 10 runs. CAS-
CADE is the best system of Hazarika et al. (2018), and we report the strongest baseline numbers established by 
Khodak et al. (2017). 

Model Predictions of p(sarcastic) 
Reddit comment 
Sarcastic? No user rep. Bayesian Multidimensional 

1. Good thing Trump is going to bring 
back all those low education high pay-
ing jobs. 

Yes 
.45 
.68 
.84 

2. lol woops! 
No 
.78 
.36 
.25 

3. The most ubiquitous undergarments I 
see these days. 

Yes 
.15 
.17 
.79 

4. Such a deep confession, and it doesn't 
sound like the guy who wrote it is an 
asshole at all. 

Yes 
.33 
.45 
.86 

5. It's not entirely impossible that there 
are recipe's that have yet to be discov-
ered. 

No 
.23 
.23 
.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Examples selected to highlight differences between the models.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> http://nlp.cs.princeton.edu/SARC/2.0/</note>

			<note place="foot" n="3"> Our models and associated experiment code are available at https://github.com/kolchinski/ reddit-sarc</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Modelling context with user embeddings for sarcasm detection in social media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paula Carvalho Mário J</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00976</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextualized sarcasm detection on twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="574" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using Language. &apos;Using&apos; Linguistic Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the pretense theory of irony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gerrig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="126" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gender differences in verbal irony use. Metaphor and Symbol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><forename type="middle">Y</forename><surname>Colston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="289" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regional variation in the use of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megan</forename><forename type="middle">L</forename><surname>Dress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">J</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><forename type="middle">E</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina</forename><forename type="middle">M</forename><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="85" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sune</forename><surname>Iyad Rahwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00524</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Magnets for sarcasm: Making sarcasm detection timely, contextual and very personal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Veale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="482" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">with 1 follower i must be awesome: P&quot;. exploring the role of irony markers in irony recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debanjan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.05253</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibbs</surname></persName>
		</author>
		<title level="m">Irony in talk among friends. Metaphor and Symbol</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="5" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying sarcasm in twitter: a closer look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>González-Ibánez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Wacholder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">CASCADE: Contextual sarcasm detection in online discussion forums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sruthi</forename><surname>Gorantla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno>ArXiv:1805.06413</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A large self-annotated corpus for sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Vodrahalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05579</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lexical influences on the perception of sarcasm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gina</forename><forename type="middle">M</forename><surname>Kreuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on computational approaches to Figurative Language</title>
		<meeting>the Workshop on computational approaches to Figurative Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation (LREC</title>
		<meeting>the International Conference on Language Resources and Evaluation (LREC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Devamanyu Hazarika, and Prateek Vij</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.08815</idno>
	</analytic>
	<monogr>
		<title level="m">A deeper look into sarcastic tweets using deep convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sarcasm detection on twitter: A behavioral modeling approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Rajadesingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eighth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Reporting score distributions makes a difference: Performance study of LSTM-networks for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>ArXiv:1707.09861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu</forename><forename type="middle">Cheung</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02856</idno>
		<title level="m">Reasoning with sarcasm by reading inbetween</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse, contextually informed models for irony detection: Exploiting user communities, entities and sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Byron C Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Humans require context to infer ironic intent (so computers probably do, too)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Byron C Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Kertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="512" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting social network structure for person-to-person sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Paskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tweet sarcasm detection using deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohong</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2449" to="2460" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
