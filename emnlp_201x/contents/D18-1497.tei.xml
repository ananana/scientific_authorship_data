<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Disentangled Representations of Texts with Application to Biomedical Abstracts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
							<email>jain.sar@husky.neu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Banner</surname></persName>
							<email>ebanner@ccs.neu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Willem</forename><surname>Van De Meent</surname></persName>
							<email>j.vandemeent@northeastern.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Marshall</surname></persName>
							<email>iain.marshall@kcl.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
							<email>b.wallace@northeastern.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Northeastern University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<address>
									<settlement>College London</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Disentangled Representations of Texts with Application to Biomedical Abstracts</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="4683" to="4693"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>4683</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and in-terpretability. To induce disentangled embed-dings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations , interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A classic problem that arises in (distributed) rep- resentation learning is that it is difficult to deter- mine what information individual dimensions in an embedding encode. When training a classifier to distinguish between images of people and land- scapes, we do not know a priori whether the model is sensitive to differences in color, contrast, shapes or textures. Analogously, in the case of natural language, when we calculate similarities between document embeddings of user reviews, we cannot know if this similarity primarily reflects user senti- ment, the product discussed, or syntactic patterns. This lack of interpretability makes it difficult to assess whether a learned representations is likely to generalize to a new task or domain, hinder- ing model transferability. Disentangled represen- tations with known semantics could allow more ef- ficient training in settings in which supervision is expensive to obtain (e.g., biomedical NLP).</p><p>Thus far in NLP, learned distributed represen- tations have, with few exceptions ( <ref type="bibr" target="#b27">Ruder et al., 2016;</ref><ref type="bibr" target="#b10">He et al., 2017;</ref><ref type="bibr" target="#b42">Zhang et al., 2017)</ref>, been en- tangled: they indiscriminately encode all aspects of texts. Rather than representing text via a mono- lithic vector, we propose to estimate multiple em- beddings that capture complementary aspects of texts, drawing inspiration from the ML in vision community <ref type="bibr" target="#b40">(Whitney, 2016;</ref><ref type="bibr" target="#b35">Veit et al., 2017a)</ref>.</p><p>As a motivating example we consider docu- ments that describe clinical trials. Such publica- tions constitute the evidence drawn upon to sup- port evidence-based medicine (EBM), in which one formulates precise clinical questions with re- spect to the Populations, Interventions, Compara- tors and Outcomes (PICO elements) of interest ( <ref type="bibr" target="#b28">Sackett et al., 1996)</ref>. <ref type="bibr">1</ref> Ideally, learned represen- tations of such articles would factorize into em- beddings for the respective PICO elements. This would enable aspect-specific similarity measures, in turn facilitating retrieval of evidence concern- ing a given condition of interest (i.e., in a spe- cific patient population), regardless of the inter- ventions and outcomes considered. Better repre- sentations may reduce the amount of supervision needed, which is expensive in this domain.</p><p>Our work is one of the first efforts to induce dis- entangled representations of texts, 2 which we be- lieve may be broadly useful in NLP. Concretely, our contributions in this paper are as follows:</p><p>• We formalize the problem of learning disentan- gled representations of texts, and develop a rela- tively general approach for learning these from aspect-specific similarity judgments expressed as triplets (s, d, o) a , which indicate that docu- ment d is more similar to document s than to document o, with respect to aspect a.</p><p>• We perform extensive experiments that provide evidence that our approach yields disentangled representations of texts, both for our motivating task of learning PICO-specific embeddings of biomedical abstracts, and, more generally, for multi-aspect sentiment corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Framework and Models</head><p>Recent approaches in computer vision have emphasized unsupervised learning of disentan- gled representations by incorporating information- theoretic regularizers into the objective <ref type="bibr" target="#b3">(Chen et al., 2016;</ref><ref type="bibr" target="#b11">Higgins et al., 2017</ref>). These ap- proaches do not require explicit manual annota- tions, but consequently they require post-hoc man- ual assignment of meaningful interpretations to learned representations. We believe it is more nat- ural to use weak supervision to induce meaningful aspect embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning from Aspect Triplets</head><p>As a general strategy for learning disentangled representations, we propose exploiting aspect- specific document triplets (s, d, o) a : this signals that s and d are more similar than are d and o, with respect to aspect a ( <ref type="bibr" target="#b13">Karaletsos et al., 2015;</ref><ref type="bibr" target="#b36">Veit et al., 2017b</ref>), i.e., sim a (d, s) &gt; sim a <ref type="bibr">(d, o)</ref>, where sim a quantifies similarity w.r.t. aspect a.</p><p>We associate with each aspect an encoder enc a (encoders share low-level layer parameters; see Section 2.2 for architecture details). This is used to obtain text embeddings (e a s , e a d , e a o ). To estimate the parameters of these encoders we adopt a sim- ple objective that seeks to maximize the similarity between (e a d , e a s ) and minimize similarity between (e a d , e a o ), via the following maximum margin loss</p><formula xml:id="formula_0">L(e a s , e a d , e a o ) = max{0, 1 − sim(e a d , e a s ) + sim(e a d , e a o )}<label>(1)</label></formula><p>Where similarity between documents i and j with respect to a particular aspect a, sim a (i, j), is sim- ply the cosine similarity between the aspect em- beddings e a i and e a j . This allows for the same doc- uments to be similar with respect to some aspects while dissimilar in terms of others.</p><p>The above setup depends on the correlation be- tween aspects in the training data. At one ex- treme, when triplets enforce identical similarities for all aspects, the model cannot distinguish be- tween aspects at all. At the other extreme, triplets are present for only one aspect a, and absent for  <ref type="table" target="#tab_1">134   135   136   137   138   139   140   141   142   143   144   145   146   147   148   149</ref> where sim a quantifies similarity between two texts with respect to aspect a.</p><p>We associate with each aspect an independent encoder enc a (for details regarding the encoder ar- chitecture used in this work, see Section 2.2). This is used to obtain text embeddings (e a o , e a s , e a d ). To estimate the parameters of these encoders we adopt a simple objective that seeks to maximize the similarity between (e a o , e a s ) and minimize sim- ilarity between (e a o , e a d ) via following maximum margin loss for a given triplet</p><formula xml:id="formula_1">L(e a o , e a s , e a d ) = max{0, 1 sim(e a o , e a s ) + sim(e a d , e a s )}<label>(2)</label></formula><p>Where the similarity between documents i and j with respect to a particular aspect a is simply the <ref type="table" target="#tab_1">2   139   140   141   142   143   144   145   146   147   148   149</ref> is used to obtain text embeddings (e a o , e a s , e a d ). To estimate the parameters of these encoders we adopt a simple objective that seeks to maximize the similarity between (e a o , e a s ) and minimize sim- ilarity between (e a o , e a d ) via following maximum margin loss for a given triplet</p><formula xml:id="formula_2">L(e a o , e a s , e a d ) = max{0, 1 sim(e a o , e a s ) + sim(e a d , e a s )}<label>(2)</label></formula><p>Where the similarity between documents i and j with respect to a particular aspect a is simply the <ref type="table" target="#tab_1">c  o  m  e  io  a  m  p   e  d  m   2   136   137   138   139   140   141   142   143   144   145   146   147   148   149</ref> We associate with each aspect an independent encoder enc a (for details regarding the encoder ar- chitecture used in this work, see Section 2.2). This is used to obtain text embeddings (e a o , e a s , e a d ). To estimate the parameters of these encoders we adopt a simple objective that seeks to maximize the similarity between (e a o , e a s ) and minimize sim- ilarity between (e a o , e a d ) via following maximum margin loss for a given triplet</p><formula xml:id="formula_3">L(e a o , e a s , e a d ) = max{0, 1 sim(e a o , e a s ) + sim(e a d , e a s )}<label>(2)</label></formula><p>Where the similarity between documents i and j with respect to a particular aspect a is simply the <ref type="table" target="#tab_1">At on  of th  infor  certa  objec  mati  eral,  ios, w  aspe  mod  perta   To  enco  distr  mod   139   140   141   142   143   144   145   146   147   148   149</ref> encoder enc a (for details regarding the enc chitecture used in this work, see Section 2.2 is used to obtain text embeddings (e a o , To estimate the parameters of these encod adopt a simple objective that seeks to ma the similarity between (e a o , e a s ) and minimi ilarity between (e a o , e a d ) via following ma margin loss for a given triplet</p><formula xml:id="formula_4">L(e a s , e a d , e a o ) = max{0, 1 sim(e a o , e a s ) + sim(e a d , e a s )</formula><p>Figure 1: We propose associating aspects with en- coders (low-level parameters are shared across as- pects; this is not shown) and training these with triplets codifying aspect-wise relative similarities.</p><p>all other aspects a : In this case the model will use only the embeddings for aspect a to represent similarities. In general, we expect a compromise between these extremes, and propose using nega- tive sampling to enable the model to learn targeted aspect-specific encodings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Encoder Architecture</head><p>Designing an aspect-based model requires speci- fying an encoder architecture. One consideration here is interpretability: a desirable property for aspect encoders is the ability to identify salient words for a given aspect. With this in mind, we propose using gated CNNs, which afford intro- spection via the token-wise gate activations. <ref type="figure">Figure 2</ref> schematizes our encoder architecture. The input is a sequence of word indices d = (w 1 , ..., w N ) which are mapped to m-dimensional word embeddings and stacked into a matrix E = [e 1 , ..., e N ]. These are passed through sequential convolutional layers C 1 , ..., C L , which induce rep- resentations H l ∈ R N ×k :</p><formula xml:id="formula_5">H l = f e (X * K l + b l )<label>(2)</label></formula><p>where X ∈ R N ×k is the input to layer C l (either a set of n-gram embeddings or H l−1 ) and k is the number of feature maps. Kernel K l ∈ R F ×k×k and b l ∈ R k are parameters to be estimated, where F is the size of kernel window. <ref type="bibr">3</ref> An activation function f e is applied element-wise to the output of the convolution operations. We fix the size of H l−1 ∈ R N ×k by zero-padding where necessary. Keeping the size of feature maps constant across layers allows us to introduce residual connections; the output of layer l is summed with the outputs of preceding layers before being passed forward. We multiply the output of the last convolutional layer H L ∈ R N ×k with gates g ∈ R N ×1 to yield  <ref type="figure">Figure 2</ref>: Schematic of our encoder architecture.</p><formula xml:id="formula_6">our final embedding e d ∈ R 1×k : g = σ(H L · w g + b g ) e d = g T H L (3)</formula><p>where w g ∈ R k×1 and b g ∈ R are learned param- eters and σ is the sigmoid activation function. We impose a sparsity-inducing constraint on g via the 1 norm; this allows the gates to effectively serve as an attention mechanism over the input. Addi- tionally, to capture potential cross-aspect correla- tion, weights in the embedding and first convolu- tional layers are shared between aspect encoders. Alternative encoders. To assess the relative im- portance of the specific encoder model architec- ture used, we conduct experiments in which we fine-tune standard document representation mod- els via triplet-based training. Specifically, we con- sider a single-layer MLP with BoW inputs, and a Neural Variational Document Model (NVDM) ( <ref type="bibr" target="#b22">Miao et al., 2016</ref>). For the NVDM we take a weighted sum of the original loss function and the triplet-loss over the learned embeddings, where the weight is a model hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Varieties of Supervision</head><p>Our approach entails learning from triplets that codify relative similarity judgments with respect to specific aspects. We consider two approaches to acquiring such triplets: the first exploits aspect- specific summaries written for texts, and the sec- ond assumes a more general scenario in which we solicit aspect-wise triplet judgments directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deriving Triplets from Aspect Summaries</head><p>In the case of our motivating example -disentan- gled representations for articles describing clini- cal trials -we have obtained aspect-specific sum- maries from the Cochrane Database of System- atic Reviews (CDSR). Cochrane is an international organization that creates and curates biomedical systematic reviews. Briefly, such reviews seek to formally synthesize all relevant articles to answer precise clinical questions, i.e., questions that spec- ify a particular PICO frame. The CDSR consists of a set of reviews {R i }. Reviews include mul- tiple articles (studies) {S ij }. Each study S con- sists of an abstract A and a set of free text sum- maries (s P , s I , s O ) written by reviewers describ- ing the respective P, I and O elements in S.</p><p>Reviews implicitly specify PICO frames, and thus two studies in any given review may be viewed as equivalent with respect to their PICO aspects. We use this observation to derive docu- ment triplets. Recall that triplets for a given as- pect include two comparatively similar texts (s, d) and one relatively dissimilar (o). Suppose the as- pect of interest is the trial population. Here we match a given abstract (d) with its matched popu- lation summary from the CDSR (s); this encour- ages the encoder to yield similar embeddings for the abstract and the population description. The dissimilar o is constructed to distinguish the given abstract from (1) other aspect encodings (of inter- ventions, outcomes), and, (2) abstracts for trials with different populations.</p><p>Concretely, to construct a triplet (s, d, o) for the PICO data, we draw two reviews R 1 and R 2 from the CDSR at random, and sample two studies from the first (s 1 , s 1 ) and one from the second (s 2 ). In- tuitively, s 2 will (very likely) comprise entirely different PICO elements than (s 1 , s 1 ), by virtue of belonging to a different review. To formal- ize the preceding description, our triplet is then:</p><formula xml:id="formula_7">(s = [s P 1 ], d = [s abstract 1 ], o = [s P 2 |s I 1 |s O 1 ]), where s abstract 1</formula><p>is the abstract for study s 1 , and aspect sum- maries for studies are denoted by superscripts. We include a concrete example of triplet construction in the Appendix, Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Directly from Aspect-Wise Similarity Judgments</head><p>The preceding setup assumes a somewhat unique case in which we have access to aspect-specific summaries written for texts. As a more general setting, we also consider learning directly from triplet-wise supervision concerning relative simi- larity with respect to particular aspects ( <ref type="bibr">Amid and Ukkonen, 2015;</ref><ref type="bibr" target="#b35">Veit et al., 2017a;</ref><ref type="bibr" target="#b41">Wilber et al., 2014</ref>). The assumption is that such judgments can be solicited directly from annotators, and thus the approach may be applied to arbitrary domains, so long as meaningful aspects can be defined implic- itly via pairwise similarities regarding them. We do not currently have corpora with such judgments in NLP, so we constructed two datasets using aspect-specific sentiment ratings. Note that this highlights the flexibility of exploiting aspect- wise triplet supervision as a means of learning disentangled representations: existing annotations can often be repurposed into such triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Datasets and Experiments</head><p>We present a series of experiments on three cor- pora to assess the degree to which the learned rep- resentations are disentangled, and to evaluate the utility of these embeddings in simple downstream retrieval tasks. We are particularly interested in the ability to identify documents similar w.r.t. a target aspect. All parameter settings for baselines are re- ported in the Appendix (along with additional ex- perimental results). The code is available at https://github.com/successar/neural-nlp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PICO (EBM) Domain</head><p>We first evaluate embeddings quantitatively with respect to retrieval performance. In particular, we assess whether the induced representations afford improved retrieval of abstracts relevant to a partic- ular systematic review ( <ref type="bibr" target="#b5">Cohen et al., 2006;</ref><ref type="bibr" target="#b39">Wallace et al., 2010</ref>). We then perform two evalua- tions that explicitly assess the degree of disentan- glement realized by the learned embeddings.</p><p>The PICO dataset comprises 41K abstracts of articles describing clinical trials extracted from the CDSR. Each abstract is associated with a review and three summaries, one per aspect (P/I/O). We keep all words that occur in ≥ 5 documents, con- verting all others to unk. We truncate documents to a fixed length (set to the 95th percentile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Quantitative Evaluation</head><p>Baselines. We compare the proposed P, I and O embeddings and their concatenation <ref type="bibr">[P|I|O]</ref> to the following. TF-IDF: standard TF-IDF representa- tion of abstracts. RR-TF: concatenated TF-IDF vectors of sentences predicted to describe the re- spective PICO elements, i.e., sentence predictions made using the pre-trained model from ( <ref type="bibr" target="#b37">Wallace et al., 2016)</ref> -this model was trained using dis- tant supervision derived from the CDSR. doc2vec: standard (entangled) distributed representations of abstracts ( <ref type="bibr" target="#b19">Le and Mikolov, 2014)</ref>. LDA: Latent Dirichlet Allocation. NVDM: A generative model of text where the representation is a vector of log- frequencies that encode a topic ( <ref type="bibr" target="#b22">Miao et al., 2016)</ref>. ABAE: An autoencoder model that discovers la- tent aspects in sentences ( <ref type="bibr" target="#b10">He et al., 2017)</ref>. We ob- tain document embeddings by summing over con- stituent sentence embeddings. DSSM: A CNN based encoder trained with triplet loss over ab- stracts <ref type="bibr" target="#b33">(Shen et al., 2014</ref>).</p><p>Hyperparameters and Settings. We use three layers for our CNN-based encoder (with 200 filters in each layer; window size of 5) and the PReLU activation function ( <ref type="bibr" target="#b9">He et al., 2015)</ref> as f e . We use 200d word embeddings, initialized via pretraining over a corpus of PubMed abstracts ( <ref type="bibr" target="#b26">Pyysalo et al., 2013)</ref>. We used the Adam optimization function with default parameters <ref type="bibr" target="#b16">(Kingma and Ba, 2014</ref>). We imposed 2 regularization over all parameters, the value of which was selected from the range (1e-2, 1e-6) as 1e-5. The 1 regularization pa- rameter for gates was chosen from the range (1e-2, 1e-8) as 1e-6. All model hyperparameters for our models and baselines were chosen via line search over a 10% validation set.</p><p>Metric. For this evaluation, we used a held out set of 15 systematic reviews (comprising 2,223 stud- ies) compiled by <ref type="bibr" target="#b5">Cohen et al. (2006)</ref>. The idea is that good representations should map abstracts in the same review (which describe studies with the same PICO frame) relatively near to one another. To compute AUCs over reviews, we first calculate all pairwise study similarities (i.e., over all studies in the Cohen corpus). We can then construct an ROC for a given abstract a from a particular re- view to calculate its AUC: this measures the prob- ability that a study drawn from the same review will be nearer to a than a study from a different review. A summary AUC for a review is taken as the mean of the study AUCs in that review.</p><p>Results.  in a clinical trial mainly involving patients over qqq with coronary heart disease , ramipril reduced mortality while vitamin e had no preventive effect . in a clinical trial mainly involving patients over qqq with coronary heart disease , ramipril reduced mortality while vitamin e had no preventive effect . in a clinical trial mainly involving patients over qqq with coronary heart disease , ramipril reduced mortality while vitamin e had no preventive effect . as our model. We outperform the latter by an aver- age performance gain of 4 points AUC (significant at 95% level using independent 2-sample t-test).</p><p>We now turn to the more important questions: are the learned representations actually disentan- gled, and do they encode the target aspects? Ta- ble 2 shows aspect-wise gate activations for PICO elements over a single abstract; this qualitatively suggests disentanglement, but we next investigate this in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Qualitative Evaluation</head><p>To assess the degree to which our PICO embed- dings are disentangled -i.e., capture complemen- tary information relevant to the targeted aspects - we performed two qualitative studies.</p><p>First, we assembled 87 articles (not seen in training) describing clinical trials from a review on the effectiveness of decision aids ( <ref type="bibr" target="#b34">Stacey et al., 2014</ref>) for: women with, at risk for, and geneti- cally at risk for, breast cancer (BCt, BCs and BCg, respectively); type II diabetes (D); menopausal women (MW); pregnant women generally (PW) and those who have undergone a C-section previ- ously (PWc); people at risk for colon cancer (CC); men with and at risk of prostate cancer (PCt and PCs, respectively) and individuals with atrial fib- rillation (AF). This review is unusual in that it studies a single intervention (decision aids) across different populations. Thus, if the model is suc- cessful in learning disentangled representations, the corresponding P vectors should roughly clus- ter, while the I/C should not. <ref type="figure">Figure 3</ref> shows a TSNE-reduced plot of the P, I/C and O embeddings induced by our model for these studies. Abstracts are color-coded to indi- cate the populations enumerated above. As hy- pothesized, P embeddings realize the clearest sep- aration with respect to the populations, while the I and O embeddings of studies do not co-localize to the same degree. This is reflected quantitatively in the AUC values achieved using each aspect em- bedding (listed on the <ref type="figure">Figure)</ref>. This result implies disentanglement along the desired axes.</p><p>Next we assembled 50 abstracts describing trials involving hip replacement arthroplasty (HipRepl).</p><p>We selected this topic because HipRepl will either describe the trial population (i.e., patients who have received hip replacements) or it will be the intervention, but not both. Thus, we would expect that abstracts describing trials in which HipRepl describes the population clus- ter in the corresponding embedding space, but not in the intervention space (and vice-versa). To test this, we first manually annotated the 50 abstracts, associating HipRepl with either P or I. We used these labels to calculate pairwise AUCs, reported in           on our test set using various representations le <ref type="bibr">[?]</ref>. We can observe that our model con- tly performs better than baseline strategies aspects. We also present the cross AUC tions where the rows correspond to the em- g used and columns correspond to the as- valuated against. We observe that each as- mbedding perform better on its correspond- pect than other aspects. Note that the reduc- performance on other aspect is not as sig- t since the aspect ratings are highly corre- i.e in most examples, if aspect k is positive, spect k' and vice versa).  arious representations e that our model con- an baseline strategies esent the cross AUC correspond to the em- correspond to the as- observe that each as- ter on its correspond- . Note that the reduc- r aspect is not as sig- ings are highly corre-  st set using various representations e can observe that our model con- rms better than baseline strategies We also present the cross AUC ere the rows correspond to the em- and columns correspond to the as- against. We observe that each as- g perform better on its correspond- other aspects. Note that the reduc- ance on other aspect is not as sig- he aspect ratings are highly corre- st examples, if aspect k is positive, and vice versa). Analysis ing various representations bserve that our model con- er than baseline strategies so present the cross AUC ows correspond to the em- mns correspond to the as- We observe that each as- m better on its correspond- pects. Note that the reduc- other aspect is not as sig- ct ratings are highly corre- ples, if aspect k is positive, versa). s using various representations observe that our model con- etter than baseline strategies also present the cross AUC e rows correspond to the em- lumns correspond to the as- st. We observe that each as- orm better on its correspond- aspects. Note that the reduc- on other aspect is not as sig- ect ratings are highly corre- mples, if aspect k is positive, ce versa). various representations ve that our model con- han baseline strategies resent the cross AUC correspond to the em- s correspond to the as- e observe that each as- etter on its correspond- ts. Note that the reduc- er aspect is not as sig- tings are highly corre- , if aspect k is positive, sa). set using various representations an observe that our model con- better than baseline strategies e also present the cross AUC the rows correspond to the em- columns correspond to the as- inst. We observe that each as- erform better on its correspond- er aspects. Note that the reduc- ce on other aspect is not as sig- aspect ratings are highly corre- xamples, if aspect k is positive, vice versa).  est set using various representations e can observe that our model con- rms better than baseline strategies . We also present the cross AUC ere the rows correspond to the em- and columns correspond to the as- against. We observe that each as- g perform better on its correspond- other aspects. Note that the reduc- ance on other aspect is not as sig- the aspect ratings are highly corre- ost examples, if aspect k is positive, and vice versa).  r test set using various representations . We can observe that our model con- rforms better than baseline strategies cts. We also present the cross AUC where the rows correspond to the em- d and columns correspond to the as- ted against. We observe that each as- ding perform better on its correspond- han other aspects. Note that the reduc- ormance on other aspect is not as sig- e the aspect ratings are highly corre- most examples, if aspect k is positive, k' and vice versa). our test set using various representations <ref type="bibr">[?]</ref>. We can observe that our model con- performs better than baseline strategies pects. We also present the cross AUC ns where the rows correspond to the em- used and columns correspond to the as- uated against. We observe that each as- edding perform better on its correspond- t than other aspects. Note that the reduc- erformance on other aspect is not as sig- ct on our test set using various representations ble <ref type="bibr">[?]</ref>. We can observe that our model con- ntly performs better than baseline strategies ll aspects. We also present the cross AUC ations where the rows correspond to the em- ing used and columns correspond to the as- evaluated against. We observe that each as- embedding perform better on its correspond- spect than other aspects. Note that the reduc- in performance on other aspect is not as sig- ant since the aspect ratings are highly corre- (i.e in most examples, if aspect k is positive, aspect k' and vice versa). ualitative Analysis  pect on our test set using various representations <ref type="table">Table [?]</ref>. We can observe that our model con- stently performs better than baseline strategies all aspects. We also present the cross AUC aluations where the rows correspond to the em- dding used and columns correspond to the as- ct evaluated against. We observe that each as- ct embedding perform better on its correspond- g aspect than other aspects. Note that the reduc- n in performance on other aspect is not as sig- ficant since the aspect ratings are highly corre- ted (i.e in most examples, if aspect k is positive, is aspect k' and vice versa).</p><note type="other">OralHypoglycemics 0.829 0.771 0.695 0.876 0.717 ProtonPumpInhibitors 0.817 0.763 0.735 0.820 0.678 SkeletalMuscleRelaxants 0.579 0.642 0.742 0.714 0.565 Statins 0.758 0.692 0.662 0.828 0.676 Triptans 0.931 0.809 0.827 0.908 0.</note><note type="other">Opiods 0.815 0.748 0.799 0.821 0.759 0 OralHypoglycemics 0.829 0.771 0.695 0.876 0.717 0 ProtonPumpInhibitors 0.817 0.763 0.735 0.820 0.678 0 SkeletalMuscleRelaxants 0.579 0.642 0.742 0.714 0.565 0 Statins 0.758 0.692 0.662 0.828 0.676 0 Triptans 0.931 0.809 0.827 0.908 0.</note><note type="other">0.759 0.921 0.841 0.845 0.899 0.717 0.939 0.907 0.939 0.917 0.678 0.923 0.887 0.748 0.897 0.565 0.685 0.582 0.545 0.723 0.676 0.820 0.751 0.760 0.798 0.805 0.977 0.924 0.727 0.</note><p>(AUC = 0.75) (AUC = 0.57) (AUC = 0.59) <ref type="figure">Figure 3</ref>: TSNE-reduced scatter of disentangled PICO embeddings of abstracts involving "decision aid" interventions. Abstracts are colored by known population group (see legend). Population embeddings for studies in the same group co-localize, more so than in the intervention and outcome space.  enrolled patients with HipRepl and other studies. Likewise, studies in which HipRepl was the inter- vention are grouped in the interventions embed- ding space, but not in the populations space. Aspect words. In <ref type="table">Table 4</ref>, we report the most acti- vated unigrams for each aspect embedding on the decision aids corpus. To derive these we use the outputs of the gating mechanism (Eq. 3), which is applied to all words in the input text. For each word, we average the activations across all ab- stracts and find the top ten words for each aspect. The words align nicely with the PICO aspects, providing further evidence that our model learns to focus on aspect-specific information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Aspect Reviews</head><p>We now turn from the specialized domain of biomedical abstracts to more general applications.</p><p>In particular, we consider learning disentangled representations of beer, hotel and restaurant re-   views. Learned embeddings should capture differ- ent aspects, e.g., taste or look in the case of beer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Beer Reviews (BeerAdvocate)</head><p>We conducted experiments on the BeerAdvocate dataset ( <ref type="bibr" target="#b21">McAuley et al., 2012</ref>), which contains 1.5M reviews of beers that address four aspects: appearance, aroma, palate, and taste. Free-text reviews are associated with aspect-specific numer- ical ratings for each of these, ranging from 1 to 5. We consider ratings &lt; 3 as negative, and &gt; 3 as positive, and use these to generate triplets of reviews. For each aspect a, we construct triplets (s, d, o) a by first randomly sampling a review d. We then select s to be a review with the same sen-Look Aroma Palate Taste Look - - 0.42 0.60 0.40 0.63 0.38 0.65 Aroma 0.33 0.69 - - 0.41 0.59 0.41 0.60 Palate 0.32 0.70 0.46 0.54 - - 0.49 0.52 Taste 0.23 0.80 0.35 0.66 0.33 0.67 - - <ref type="table">Table 7</ref>: 'Decorrelated' cross-AUC results on the BeerAdvocate data, which attempt to mitigate confounding due to overall sentiment being cor- related. Each cell reports metrics over subsets of reviews in which the sentiment differs between the row and column aspects. The numbers in each cell are the AUCs w.r.t. sentiment regarding the col- umn aspect achieved using the row and column aspect representations, respectively.</p><p>timent with respect to a as d, and o to be a re- view with the opposite sentiment regarding a. We selected 90K reviews for experiments, such that we had an equal number of positive and negative reviews for each aspect. We only keep words ap- pearing in at least 5 documents, converting all oth- ers to unk. We truncated reviews to 95 percentile length. We split our data into 80/10/10 ratio for training, validation and testing, respectively. Baselines. We used the same baselines as for the PICO domain, save for RR-TF, which was domain-specific. Here we also evaluate the result of replacing the CNN-based encoder with NVDM, BoW and DSSM based encoders, respectively, each trained using triplet loss. Hyperparameters and Settings. For the CNN- based encoder, we used settings and hyperparam- eters as described for the PICO domain. For the BoW encoder, we used 800d output embeddings and a PReLU activation function with 2 regular- ization set to 1e-5. For the NVDM based encoder, we used 200d embeddings.</p><p>Metrics. We again performed an IR-type evalu- ation to assess the utility of representations. For each aspect k, we constructed an affinity matrix A k such that A k ij = sim k (r i , r j ) for beer reviews r i and r j . We consider two reviews similar un- der a given aspect k if they have the same (di- chotomized) sentiment value for said aspect. We compute AUCs for each review and aspect using the affinity matrix A k . The AUC values are aver- aged over reviews in the test set to obtain a final AUC metric for each aspect. We also report cross AUC measures in which we use embeddings for aspect k to distinguish reviews under aspect k . Results We report the AUC measures for each aspect on our test set using different representa- tions in <ref type="table" target="#tab_19">Table 5</ref>. Our model consistently outper- forms baseline strategies over all aspects. Unsur- prisingly, the model outperforms unsupervised ap- proaches. <ref type="bibr">4</ref> We realize consistent though modest improvement over triplet-supervised approaches that use alternative encoders.</p><p>In <ref type="table" target="#tab_20">Table 6</ref> we present cross AUC evalua- tions. Rows correspond to the embedding used and columns to the aspect evaluated against. As expected, aspect-embeddings perform better w.r.t. the aspects for which they code, suggesting some disentanglement. However, the reduction in performance when using one aspect representation to discriminate w.r.t. others is not as pronounced as above. This is because aspect ratings are highly correlated: if taste is positive, aroma is very likely to be as well. Effectively, here sentiment entangles all of these aspects. <ref type="bibr">5</ref> In <ref type="table">Table 7</ref>, we evaluate cross AUC perfor- mance for beer by first 'decorrelating' the aspects. Specifically, for each cell (k, k ) in the table, we first retrieve the subset of reviews in which the sen- timent w.r.t. k differs from the sentiment w.r.t. k . Then we evaluate the AUC similarity of these re- views on the basis of sentiment concerning k us- ing both k and k embeddings, yielding a pair of AUCs (listed respectively). We observe that the using k embeddings to evaluate aspect k similar- ity yields better results than using k embeddings.</p><p>We present the most activated words for each aspect (as per the gating mechanism) in <ref type="table" target="#tab_21">Table 8</ref>. And we present an illustrative review color-coded with aspect-wise gate activations in <ref type="table" target="#tab_22">Table 9</ref>. For completeness, we reproduce the top words for as- pects discovered using <ref type="bibr" target="#b10">He et al. (2017)</ref> in the Ap- pendix; these do not obviously align with the tar- get aspects, which is unsurprising given that this is an unsupervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hotel &amp; Restaurant Reviews</head><p>Finally, we attempt to learn embeddings that dis- entangle domain from sentiment in reviews. For this we use a combination of TripAdvisor and Yelp! ratings data. The former comprises reviews of hotels, the latter of restaurants; both use a scale of 1 to 5. We convert ratings into positive/negative labels as above. Here we consider aspects to be the domain (hotel or restaurant) and the sentiment (positive or negative). We aim to generate em- beddings that capture information about only one of these aspects. We use 50K reviews from each dataset for training and 5K for testing. Baselines. We use the same baselines as for the BeerAdvocate data, and similarly use different en- coder models trained under triplet loss. Evaluation Metrics. We perform AUC and cross- AUC evaluation as in the preceding section. For the domain aspect, we consider two reviews simi- lar if they are from the same domain, irrespective of sentiment. Similarly, reviews are considered similar with respect to the sentiment aspect if they share a sentiment value, regardless of domain.</p><p>Results. In <ref type="table" target="#tab_1">Table 10</ref> we report the AUCs for each aspect on our test set using different representa- tions. Baselines perform reasonably well on the domain aspect because reviews from different do- mains are quite dissimilar. Capturing sentiment in- formation irrespective of domain is more difficult, and most unsupervised models fail in this respect. In <ref type="table" target="#tab_1">Table 11</ref>, we observe that cross AUC results are much more pronounced than for the BeerAdvocate data, as the domain and sentiment are uncorrelated (i.e., sentiment is independent of domain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Work in representation learning for NLP has largely focused on improving word embeddings ( <ref type="bibr" target="#b20">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b8">Faruqui et al., 2015;</ref><ref type="bibr" target="#b12">Huang et al., 2012)</ref>. But efforts have also been made to embed other textual units, e.g. charac- ters ( <ref type="bibr" target="#b15">Kim et al., 2016)</ref>, and lengthier texts includ- ing sentences, paragraphs, and documents ( <ref type="bibr" target="#b19">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b17">Kiros et al., 2015)</ref>.</p><p>Triplet-based judgments have been used in mul- tiple domains, including vision and NLP, to es- timate similarity information implicitly. For ex- ample, triplet-based similarity embeddings may be learned using 'crowdkernels' with applications to multi-view clustering <ref type="bibr">(Amid and Ukkonen, 2015)</ref>. Models combining similarity with neural networks mainly revolve around Siamese networks ( <ref type="bibr" target="#b4">Chopra et al., 2005</ref>) which use pairwise distances to learn embeddings ( <ref type="bibr" target="#b32">Schroff et al., 2015</ref>), a tactic we have followed here. Similarity judgments have also been used to generate document embeddings for IR tasks <ref type="bibr" target="#b33">(Shen et al., 2014;</ref><ref type="bibr" target="#b6">Das et al., 2016)</ref>.</p><p>Recently, <ref type="bibr" target="#b10">He et al. (2017)</ref> introduced a neural model for aspect extraction that relies on an at- tention mechanism to identify aspect words. They proposed an autoencoder variant designed to tease apart aspects. In contrast to the method we pro- pose, their approach is unsupervised; discovered aspects may thus not have a clear interpretation. Experiments reported here support this hypothe- sis, and we provide additional results using their model in the Appendix.</p><p>Other recent work has focused on text gen- eration from factorized representations ( <ref type="bibr" target="#b18">Larsson et al., 2017)</ref>. And <ref type="bibr" target="#b42">Zhang et al. (2017)</ref> proposed a lightly supervised method for domain adaptation using aspect-augmented neural networks. They exploited source document labels to train a clas- sifier for a target aspect. They leveraged sentence- level scores codifying sentence relevance w.r.t. in- dividual aspects, which were derived from terms a priori associated with aspects. This supervi- sion is used to construct a composite loss that cap- tures both classification performance on the source task and a term that enforces invariance between source and target representations.</p><p>There is also a large body of work that uses probabilistic generative models to recover latent structure in texts. Many of these models de- rive from Latent Dirichlet Allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003)</ref>, and some variants have explicitly rep- resented topics and aspects jointly for sentiment tasks <ref type="bibr" target="#b2">(Brody and Elhadad, 2010;</ref><ref type="bibr" target="#b30">Sauper et al., 2010</ref><ref type="bibr" target="#b31">Sauper et al., , 2011</ref><ref type="bibr" target="#b23">Mukherjee and Liu, 2012;</ref><ref type="bibr" target="#b29">Sauper and Barzilay, 2013;</ref><ref type="bibr" target="#b14">Kim et al., 2013)</ref>.</p><p>A bit more generally, aspects have also been in- terpreted as properties spanning entire texts, e.g., a perspective or theme which may then color the discussion of topics ( <ref type="bibr" target="#b25">Paul and Girju, 2010)</ref>. This intuition led to the development of the factorial LDA family of topic models <ref type="bibr" target="#b24">(Paul and Dredze, 2012;</ref><ref type="bibr" target="#b38">Wallace et al., 2014</ref>); these model individ-Look : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard water and bready malt . light body , with little carbonation .</p><p>Aroma : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard water and bready malt . light body , with little carbonation .</p><p>Palate : deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard water and bready malt . light body , with little carbonation .</p><p>Taste :deep amber hue , this brew is topped with a finger of off white head . smell of dog unk , green unk , and slightly fruity . taste of belgian yeast , coriander , hard water and bready malt . light body , with little carbonation .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have proposed an approach for inducing disen- tangled representations of text. To learn such rep- resentations we have relied on supervision codi- fied in aspect-wise similarity judgments expressed as document triplets. This provides a general su- pervision framework and objective. We evaluated this approach on three datasets, each with differ- ent aspects. Our experimental results demonstrate that this approach indeed induces aspect-specific embeddings that are qualitatively interpretable and achieve superior performance on information re- trieval tasks.</p><p>Going forward, disentangled representations may afford additional advantages in NLP, e.g., by facilitating transfer ( <ref type="bibr" target="#b42">Zhang et al., 2017)</ref>, or sup- porting aspect-focused summarization models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Cross AUC results for different representa- for BeerAdvocate Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 reports</head><label>1</label><figDesc></figDesc><table>the mean AUCs over in-
dividual reviews in the Cohen et al. (2006) corpus, 
and grand means over these (bottom row). In brief: 
The proposed PICO embeddings (concatenated) 
obtain an equivalent or higher AUC than base-
line strategies on 12/14 reviews, and strictly higher 
AUCs in 11/14. It is unsurprising that we outper-
form unsupervised approaches, but we also best 
RR-TF, which was trained with the same CDSR 
corpus (Wallace et al., 2016), and DSSM (Shen 
et al., 2014), which exploits the same triplet loss </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>AUCs achieved using different representations on the Cohen et al. corpus. Models to the right 
of the | are supervised; those to the right of || constitute the proposed disentangled embeddings. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Gate activations for each aspect in a PICO 
abstract. Note that because gates are calculated at 
the final convolution layer, activations are not in 
exact 1-1 correspondence with words. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>The results imply that the popula-
tion embeddings discriminate between studies that 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>AUC results for different representations for 
BeerAdvocate Dataset. 

Results We report the AUC measures for each 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cross A 
tions for BeerAdv 

aspect on our te 
in Table [?]. W 
sistently perfor 
on all aspects. 
evaluations whe 
bedding used a 
pect evaluated a 
pect embedding 
ing aspect than 
tion in perform 
nificant since th 
lated (i.e in mos 
so is aspect k' a 
Qualitative A 
Most Activa 

4.3 TripAdvi 

We also use ou 
disentangle dom 
We use the com 
Ratings data. T 
views of hotels 
ilarly, yelp dat 
same rating sca 
itive/negative la 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>AUC results for different representations for 
BeerAdvocate Dataset. 

Results We report the AUC measures for each 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Cross AUC res 
tions for BeerAdvocate D 

aspect on our test set us 
in Table [?]. We can ob 
sistently performs bett 
on all aspects. We al 
evaluations where the r 
bedding used and colu 
pect evaluated against. 
pect embedding perform 
ing aspect than other as 
tion in performance on 
nificant since the aspec 
lated (i.e in most exam 
so is aspect k' and vice 
Qualitative Analysi 
Most Activated Out 

4.3 TripAdvisor/Yelp 

We also use our mode 
disentangle domain an 
We use the combinatio 
Ratings data. The trip 
views of hotels with ra 
ilarly, yelp data prov 
same rating scale. We 
itive/negative labels as 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>AUCs realized over HipRepl studies us-
ing different embeddings. Column: Study label 
(HipRepl as P or I). Row: Aspect embedding used. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>AUC results for different representations 
on the BeerAdvocate data. Models beneath the 
second line are supervised. 

Look 
Aroma 
Palate 
Taste 
Look 
0.92 
0.89 
0.88 
0.87 
Aroma 0.90 
0.93 
0.91 
0.92 
Palate 
0.89 
0.92 
0.94 
0.95 
Taste 
0.90 
0.94 
0.95 
0.96 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Cross AUC results for different represen-
tations on the BeerAdvocate data. Row: Embed-
ding used. Column: Aspect evaluated against. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Most activated words for aspects on the 
beer corpus, as per the gating mechanism. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22" validated="true"><head>Table 9 : Gate activations for each aspect in an example beer review.</head><label>9</label><figDesc></figDesc><table>Baseline 
Domain 
Sentiment 
TF-IDF 
0.59 
0.52 
Doc2Vec 
0.83 
0.56 
LDA 
0.90 
0.62 
NVDM 
0.79 
0.63 
ABAE 
0.50 
0.50 
BoW + Triplet 
0.99 
0.91 
NVDM + Triplet 0.99 
0.91 
DSSM + Triplet 
0.99 
0.90 
CNN + Triplet 
0.99 
0.92 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>AUC results for different representations 
on the Yelp!/TripAdvisor Data. Models beneath 
the second line are supervised. 

Baseline 
Domain 
Sentiment 
Domain 
0.988 
0.512 
Sentiment 0.510 
0.917 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24" validated="false"><head>Table 11 :</head><label>11</label><figDesc></figDesc><table>Cross AUC results for different represen-
tations for Yelp!/TripAdvisor Dataset. 

ual word probability as a product of multiple latent 
factors characterizing a text. This is similar to the 
Sparse Additive Generative (SAGE) model of text 
proposed by Eisenstein et al. (2011). 

</table></figure>

			<note place="foot" n="1"> We collapse I and C because the distinction is arbitrary. 2 We review the few recent related works that do exist in Section 5.</note>

			<note place="foot" n="3"> The input to C1 is E ∈ R N ×m , thus K1 ∈ R F ×m×k .</note>

			<note place="foot" n="4"> We are not sure why ABAE (He et al., 2017) performs so poorly on the review corpora. It may simply fail to prominently encode sentiment, which is important for these tasks. We note that this model performs reasonably well on the PICO data above, and qualitatively seems to recover reasonable aspects (though not specifically sentiment). 5 Another view is that we are in fact inducing representations of &lt;aspect, sentiment&gt; pairs, and only the aspect varies across these; thus representations remain discriminative (w.r.t. sentiment) across aspects.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This work was supported in part by National Li-brary of Medicine (NLM) of the National In-stitutes of Health (NIH) award R01LM012086, by the Army Research Office (ARO) award W911NF1810328, and research funds from Northeastern University. The content is solely the responsibility of the authors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">received, retention, sheets, sipper, well-balanced Aroma beer, cardboard, cheap, down, follows, mediumlight, rice, settled, skunked, skunky Palate bother, crafted, luscious, mellow, mint, range, recommended, roasted, tasting, weight Taste amazingly, down, highly, product, recommended, tasted, thoroughly, to, truly, wow References Ehsan Amid and Antti Ukkonen</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Multiview triplet embedding: Learning attributes in multiple maps</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An unsupervised aspect-sentiment model for online reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>John Schulman, Ilya Sutskever, and Pieter Abbeel</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005. IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing workload in systematic review preparation using automated citation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Aaron M Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poyin</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Together we stand: Siamese networks for similar question retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpita</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harish</forename><surname>Yenala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Chinnakotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse Additive Generative Models of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Retrofitting word vectors to semantic lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sujay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Jauhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Unsupervised Neural Attention Model for Aspect Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Wee Sun Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahlmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BetaVae: Learning Basic Visual Concepts with a Constrained Variational Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving word representations via global context and multiple word prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Rätsch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05011</idno>
		<title level="m">Bayesian representation learning with oracle constraints</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Hierarchical AspectSentiment Model for Online Reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Disentangled Representations for Manipulation of Sentiment in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kågebäck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.10066</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DependencyBased Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association of Computational Linguistics (ACL)</title>
		<meeting>the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning attitudes and attributes from multiaspect reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 12th International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aspect extraction through semi-supervised modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Factorial LDA: Sparse multi-dimensional text models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A twodimensional topic-aspect model for discovering multi-faceted topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributional semantics resources for biomedical text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Moen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Languages in Biology and Medicine</title>
		<meeting>the 5th International Symposium on Languages in Biology and Medicine</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Hierarchical Model of Reviews for Aspectbased Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsa</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John G</forename><surname>Breslin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evidence based medicine: what it is and what it isn&apos;t</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sackett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J A Muir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bmj</title>
		<imprint>
			<biblScope unit="issue">7023</biblScope>
			<biblScope unit="page">312</biblScope>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic aggregation by joint modeling of aspects and values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Incorporating content structure into text analysis applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Content models with attitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Sauper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A latent semantic model with convolutional-pooling structure for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grégoire</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Decision aids for people facing health treatment or screening decisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N F</forename><surname>Légaré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C L</forename><surname>Col</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K B</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lyddiatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J H C</forename><surname>Trevena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cochrane Database of Systematic Reviews</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conditional Similarity Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional Similarity Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extracting PICO sentences from clinical trial reports using supervised distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joël</forename><surname>Byron C Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakash</forename><surname>Kuiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxi</forename><forename type="middle">Brian</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">132</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A large-scale quantitative analysis of latent factors and sentiment in online doctor reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urmimala</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Thomas A Trikalinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-automated screening of biomedical citations for systematic reviews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Byron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Trikalinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher H</forename><surname>Brodley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Whitney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02383</idno>
		<title level="m">Disentangled Representations in Neural Models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cost-effective hits for relative similarity comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Aspect-augmented Adversarial Networks for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
