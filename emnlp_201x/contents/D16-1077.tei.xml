<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transferring User Interests Across Websites with Unstructured Text for Cold-Start Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yang</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shou-De</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Transferring User Interests Across Websites with Unstructured Text for Cold-Start Recommendation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="805" to="814"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this work, we investigate the possibility of cross-website transfer learning for tackling the cold-start problem. To address the cold-start issues commonly present in a collabora-tive ltering (CF) system, most existing cross-domain CF models require auxiliary rating data from another domain; nevertheless, under the cross-website scenario, such data is often unobtainable. Therefore, we propose the nearest-neighbor transfer matrix factorization (NT-MF) model, where a topic model is applied to the unstructured user-generated content in the source domain, and the similarity between users in the latent topic space is utilized to guide the target-domain CF model. Specically, the latent factors of the nearest-neighbors are regarded as a set of pseudo observations , which can be used to estimate the unknown parameters in the model. Improvement over previous methods, especially for the cold-start users, is demonstrated with experiments on a real-world cross-website dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While collaborative ltering (CF) approaches are one of the most successful methods for building rec- ommender systems, their performance deteriorates dramatically under cold-start situations. That is, low prediction accuracy is observed for users/items with very few ratings. Content-based recommender sys- tems may also suffer from the cold-start problem. For instance, content-based nearest-neighbor mod- els ( <ref type="bibr" target="#b12">Pazzani and Billsus, 2007)</ref> might not be as ef- fective if some users contain too few information to generate a meaningful set of neighbors.</p><p>Two types of solutions have been proposed to ad- dress the cold-start problem. The rst is to cre- ate hybrid recommendation models that impose a content-based model on a CF model to enrich the information for users/items with sparse rating pro- les <ref type="bibr" target="#b2">(Burke, 2002;</ref><ref type="bibr">Burke, 2007)</ref>. The second is to transfer the information from auxiliary domains as a remedy to the cold-start individuals <ref type="bibr" target="#b3">(Deng et al., 2015)</ref>. This paper aims at bringing a marriage be- tween these two types of strategies.</p><p>Although transfer learning gradually gains pop- ularity in handling the cold-start issue ( <ref type="bibr" target="#b13">Roy et al., 2012</ref>), most of them assume a homogeneous model where observations in both domains are of the same type. That is, to transfer knowledge to a rating- based/text-based recommender system, the source system must also be rating-based/text-based. Some earlier works even require the ratings from both do- mains to be in the same format ( <ref type="bibr" target="#b9">Li et al., 2009)</ref>, or assume specic structured text, such as user- provided tags <ref type="bibr" target="#b15">(Shi et al., 2011;</ref><ref type="bibr" target="#b3">Deng et al., 2015)</ref>. In this work, by contrast, no source-domain ratings are available and unstructured user-generated content is treated as the auxiliary data. We propose a hetero- geneous transfer learning framework to utilize un- structured auxiliary text for a better target-domain CF model.</p><p>As there is no single service satisfying all so- cial needs, users nowadays hold multiple accounts across many websites. Furthermore, the account linking mechanism is often available on these web- sites. This allows a precise mapping between the accounts of the same user to be built. One major ap- plication of our approach is to improve the recom-mendation quality in the target service using auxil- iary data obtained from another seemingly irrelevant service.</p><p>For instance, consider a new user on YouTube. The initial recommended videos for this user is likely to be irrelevant as there is very few infor- mation available. However, with the account link- ing mechanism, YouTube accounts can be linked to Twitter accounts with a simple click. Our goal is to utilize the content generated by this user on Twitter, despite the possibility that the content is irrelevant to their preference on video browsing, to produce a better video recommendation list on YouTube.</p><p>Seemingly intuitive, there exist some difculties in such cross-website transfer learning approach. The biggest challenge lies in the fact that most users do not use multiple services (e.g. social media sites) for the same purpose. Usually a user registers for multiple services because each of them serves its own purpose. As a result, we cannot assume the ex- istence of direct mentions about target-domain items in the source-domain text data. For example, a regu- lar YouTube viewer does not necessarily tweet about the videos he/she has viewed. Thus simple meth- ods such as keyword matching are likely to fail. The same reasoning also implies that, when transfer- ring knowledge across websites or services, the as- sumption of a shared rating format or structured text is overly optimistic. Even websites aiming for the same purpose often violate this assumption, let alone websites of different types. Therefore, we expect that the source and target services contain heteroge- neous information (e.g. content vs. rating). In our model, we make a less strong assumption: regard- less of the type of information available in each do- main, the users that are similar in one domain should have similar taste in the other domain. Thus, instead of directly transfer the content material from source to target domain, we transfer the similarity between users, and use it as a guide to improve the CF model in the target domain. <ref type="bibr">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDA-MF Model</head><p>We rst introduce an intuitive model to realize the above-mentioned idea, and point out several in- trinsic weaknesses making it unsuitable for cross- website transfer learning.</p><p>Here we rely on the probabilistic matrix factoriza- tion (PMF) model as our target-domain CF model. In the PMF model, each user latent factor is mod- eled (a priori) by a zero-mean Gaussian. To incor- porate source-domain information into the target- domain PMF model, for each user i, a topic vector θ i is extracted from source-domain text content and assigned as the prior mean of this user's PMF latent factor, that is,</p><formula xml:id="formula_0">u i ∼ N (θ i , λ −1 U I),<label>(1)</label></formula><p>where λ U is the precision parameter and I is the identity matrix. Different from the original PMF model, prior distributions of different user latent fac- tors are no longer identical. For users having simi- lar source-domain topic vectors, their latent factors are expected to be close in the target-domain latent space. Such property allows the similarity between users to be transferred from source domain to the target domain.</p><p>With the latent Dirichlet allocation (LDA) ( <ref type="bibr" target="#b1">Blei et al., 2003</ref>) topic model being used, the graphical model is depicted in <ref type="figure">Figure 1</ref>. This model is sim- ilar in structure to the recently proposed collabora- tive topic regression (CTR) ( <ref type="bibr" target="#b16">Wang and Blei, 2011)</ref> model. The main difference is that, instead of mod- eling description about items, now user-generated content from the source domain is modeled in our problem. We call this model the LDA-MF model.</p><formula xml:id="formula_1">r ij u i v j λ U λ V z w θ i α ψ β N M K Figure 1:</formula><p>The LDA-MF model.</p><p>Although LDA-MF indeed incorporates knowl- edge from the source domain, it has certain weak- nesses which need to be addressed. The most sig- nicant drawback is that the dimensionalities of the LDA topic vector θ i and the PMF user latent fac- tor u i are required to be equal. These two variables are of very different nature. One is extracted from text data in the source domain to model the topics of the user-generated content, and the other is gen- erated from the rating data in the target domain to model the latent interests of users. It is an overly strong assumption to assume the optimal dimension- alities for LDA and PMF are equal. In practice, if we choose the dimensionality to optimize the pre- dictive power of PMF (e.g. by cross-validation on the rating data), the LDA model is likely to yield sub-optimal results and vice versa. The experiments that will be shown later conrm this concern. Fur- thermore, since the two variables are modeling dif- ferent types of observations coming from different websites, the underlying meanings of the latent di- mensions are unlikely to be identical. By treating the LDA topic vector as the prior mean of the PMF user latent factor, the latent dimensions are forced to be one-to-one aligned, which is again a strong as- sumption. Finally, the topic vectors are drawn from the Dirichlet distribution which has a bounded (and positive) support S, while the latent factors in PMF are unbounded Gaussian random vectors. If the op- timal solution of u i is far from S, the performance of the model could be affected, particularly in the cold-start situation where data is sparse and the prior plays an important role. To alleviate the drawbacks of the LDA-MF model, here we propose nearest-neighbor transfer matrix factorization (NT-MF) model to transfer user inter- ests across websites. The entire framework is de- picted in <ref type="figure" target="#fig_1">Figure 2</ref>. We begin by describing the scenario in which our model operates. First, there is a rating-based rec- ommender system (i.e. PMF) in the target domain, which suffers from the cold-start problem. The tar- get domain might or might not contain content in- formation. For example, in the video recommenda- tion task, we can use the titles of all rated videos of a user to generate content information in the target domain. Such information is not available for the cold-start users since they have not rated any videos. However, in the source domain there are some con- tent information available for these users. This can be, say, the content of a user's tweets. As previously mentioned, this type of auxiliary text data is imme- diately available when a user connects the accounts from two domains. Therefore, we assume this aux- iliary text data is available for all users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Outline</head><p>Next, we describe the high level concept of our model. As described previously, we have observed that the hypotheses encoded by the LDA-MF model is too strong as the PMF latent factor is enforced to inherit certain mathematical properties from the LDA topic vector. Here we loosen the constraint to only enforce that users should have similar distri- butions over the target-domain PMF latent factors if there is a high similarity between their source- domain topic vectors.</p><p>It is a reasonable hypothesis since our objective is to make the target-domain rating matrix factorize in a way that is consistent with the knowledge ex- tracted from source-domain text. After all, the fac- torization of incomplete matrix is not unique, and there is no reason that the latent factor should match the topic factor of the user. In fact, our hypothesis implies a different distribution over the PMF latent factor for each user, i.e. u i ∼ N (µ i , Σ i ), where (µ i , Σ i ) are unknown parameters, and are (possibly) different for each user.</p><p>To estimate the unknown parameters in a dis- tribution, normally we need a set of observations,</p><formula xml:id="formula_2">(u (1) i , u (2)</formula><p>i , . . . ). However, the parameters now be- long to a distribution over a latent variable, which is non-trivial to estimate since we have no observations about the user latent factor. An exhaustive search over the parameter space is obviously intractable. Even if we treat the entire model as a hierarchical model and learn the parameters indirectly from rat- ing data, the cold-start problem immediately comes in and forbids us from learning a representative dis- tribution for users.</p><p>We propose the idea of using the latent factors of the nearest-neighbors to estimate the unknown pa- rameters in the distribution for a user. That is, the latent factors of the nearest-neighbors, {u l } l∈kNN(i) , are regarded as a set of pseudo observations to re- place the unavailable data, (u <ref type="formula" target="#formula_0">(1)</ref> i , u <ref type="formula" target="#formula_3">(2)</ref> i , . . . ). How- ever, the denition of closeness is not based on target-domain rating data, but computed by the topic vectors obtained from the content in the source do- main (and the target domain, if available). Our model is thus not hampered by the cold-start prob- lem.</p><p>Note that, in addition to the set of k-nearest- neighbors kNN(i), we also have the corresponding similarity scores sim(i, l) between each neighbor l and user i. The similarity scores along with the list of nearest-neighbors are transferred to the tar- get domain to form a set of weighted samples, D, which can be used to estimate the unknown parame-</p><formula xml:id="formula_3">ters (µ i , Σ i ), i.e., D ≡ { {u l } l∈kNN(i) w l = sim(i, l).<label>(2)</label></formula><p>The main purpose of assigning a sample weight w l to each of the pseudo observations u l is that by doing so, users with a higher source-domain simi- larity to user i will have a larger impact on the esti- mation of the target-domain parameters (µ i , Σ i ). In other words, with this model specication, the simi- larity between users is transferred across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference in NT-MF Model</head><p>To perform inference in our model, we adopt the maximum a posteriori (MAP) strategy and alter- nately update the user and item latent factors (i.e. by block coordinate ascent), similar to some previ- ous solutions <ref type="bibr" target="#b14">(Salakhutdinov and Mnih, 2007;</ref><ref type="bibr" target="#b16">Wang and Blei, 2011)</ref>.</p><p>To solve for the optimal user latent factor u i , we need to rst estimate the unknown parameters (µ i , Σ i ). Therefore, in our coordinate ascent algo- rithm, different from the original PMF model, we update the user latent factors one by one. That is, all user latent factors are regarded as xed constants except for the one, u i , to be updated. By doing so, for each user i, a set of pseudo observations about u i (Eq. 2) is available. Using these pseudo obser- vations, the unknown parameters (µ i , Σ i ) can then be estimated with standard techniques such as maxi- mum likelihood estimation (MLE). After an estima- tor of (µ i , Σ i ) is obtained, we can analytically solve for the MAP solution of the user latent factor u i . Then, we move on to the next user, and the coor- dinate ascent procedure continues. These two steps, namely the estimation of unknown parameters and the updating of the latent factors, are repeated until convergence.</p><p>One advantage of this procedure is that the list of nearest-neighbors and the similarities in Eq. 2 need not be recomputed during inference, avoiding expensive recomputation of pairwise similarities. It is also noticeable that, different from other transfer- based approaches, rating information and structured text from the source domain are not required in this procedure of model optimization. This further adds a level of exibility to our framework for transfer- ring user interests across websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Case Study: Inferring Unknown Mean</head><p>To clarify the previous discussions, we present a simple but detailed case-study on how an NT-MF model and its optimization procedure can be de- rived. The latent factor u i for each user is assumed to be generated from a multivariate normal distribu- tion with unknown mean µ i and a known precision parameter λ U , which is shared among the users.</p><p>The generative process proceeds as follows:</p><p>1. For each user i, draw user latent factor u i ∼ N (µ i , λ −1 U I).</p><p>2. For each item j, draw item latent factor v j ∼ N (0, λ −1 V I).</p><p>3. For each observed user-to-item pair (i, j), draw the rating</p><formula xml:id="formula_4">r ij ∼ N (u T i v j , λ −1 0 ),</formula><p>where λ 0 is the precision parameter of the rat- ings, and λ U , λ V are the precision parameter of the users and items, respectively. We use the notation N (x|µ, Σ) to denote the Gaussian pdf with mean µ and covariance Σ. The model is optimized by maximizing the pos- terior likelihood of the latent variables (an additive term is omitted),</p><formula xml:id="formula_5">L = − λ 0 2 M ∑ i=1 N ∑ j=1 γ ij ( r ij − u T i v j ) 2 − λ U 2 M ∑ i=1 (u i − µ i ) T (u i − µ i ) − λ V 2 N ∑ j=1 v T j v j ,<label>(3)</label></formula><p>where γ ij is an indicator variable which is equal to 1 if item j is rated by user i, and 0 otherwise. To solve the MAP problem, we need to rst es- timate the unknown parameters in the distribution, which in this case is the mean vector µ i . The likelihood function over the pseudo observations, {u l } l∈kNN(i) , is dened as,</p><formula xml:id="formula_6">p(D|µ i , λ U ) = ∏ l∈kNN(i) N (u l |µ i , λ −1 U I).<label>(4)</label></formula><p>By taking derivative of Eq. 4 with respect to µ i and set it to zero, we obtain,</p><formula xml:id="formula_7">∑ l∈kNN(i) (u l − µ i ) = 0,<label>(5)</label></formula><p>which implies that the MLE of µ i is the sample mean. However, since we are dealing with a set of weighted samples, the sample mean is replaced by the weighted average (the weights w l are assumed to add up to one):</p><formula xml:id="formula_8">µ i = ∑ l∈kNN(i) w l u l .<label>(6)</label></formula><p>Our model yields an intuitive result: to estimate the mean vector µ i of u i , we can simply take the weighted average of the latent factors u l from the nearest-neighbors as an estimator, where the weights are the similarity scores between the textual proles of user i and its neighbors.</p><p>Given µ i , we can now maximize Eq. 3 with re- spect to u i and v j . By taking derivative of Eq. 3 with respect to u i and v j and set it to zero, we obtain the update equations,</p><formula xml:id="formula_9">  N ∑ j=1 γ ij v j v T j + λ U λ 0 I   u i = N ∑ j=1 γ ij r ij v j + λ U λ 0 µ i (7) ( M ∑ i=1 γ ij u i u T i + λ V λ 0 I ) v j = M ∑ i=1 γ ij r ij u i .<label>(8)</label></formula><p>Now with Eq. 6 to Eq. 8 at hand, we can itera- tively solve for µ i , u i and v j for all users and items until the model converges.</p><p>It can be seen from this case-study that NT-MF eliminates the three major drawbacks of the previ- ously mentioned LDA-MF model. First, the topic vectors and the user latent factors are not required to have equal dimensionalities, which allows for the optimal dimensionality to be chosen in both models. Second, the mean vector, that is, the kNN weighted average in Eq. 6, is a linear combination of a set of user latent factors; as a result, the latent dimensions of u i and µ i are naturally aligned. Third, the mean vector µ i has the same support as the user latent fac- tor u i , avoiding the risk of prior misspecication in cold-start situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>We use YouTube video recommendation to test the usefulness of NT-MF under the cold-start scenario. The NT-MF model used in this section follows the optimization procedure derived in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Statistics</head><p>To construct a dataset containing both the users' rat- ing history and textual information, we begin with the user prole pages on Google+. A large propor- tion of Google+ users provide links to their prole pages from other social network services (e.g. Twit- ter). More importantly, if a user owns a YouTube account, a link to the user's YouTube channel will be automatically added to his Google+ prole. This makes a fully aligned dataset available. Users' Twit- ter accounts are obtained via their Google+ prole page, and the concatenation of tweets is regarded as the auxiliary text data. It has been shown that by concatenating the tweets, more representative user topic vectors can be obtained <ref type="bibr" target="#b8">(Hong and Davison, 2010)</ref>. We refer to this text data as the Twitter cor- pus.</p><p>Videos in a user's liked or favorite playlists are considered to have a rating r ij = 1. Other videos are assigned r ij = 0. In other words, we are dealing with a one-class collaborative ltering (OCCF) problem <ref type="bibr" target="#b11">(Pan et al., 2008)</ref>. We adopt the same strategy as in ( <ref type="bibr" target="#b16">Wang and Blei, 2011</ref>) to deal with OCCF. First, all ratings are assumed to be ob- served, i.e. γ ij = 1 for all user-item pairs. Next, a condence parameter c ij is introduced to reduce the inuence of the huge number of zeroes during model optimization. The condence parameter takes place of the original rating precision parameter λ 0 and is dened in ( <ref type="bibr" target="#b16">Wang and Blei, 2011</ref>) as c ij = a if r ij = 1 and c ij = b otherwise (a &gt; b &gt; 0). All the derivations in the previous sections follow intu- itively.</p><p>The titles of the liked videos are concatenated and treated as the text data in the target domain (which we refer to as the YouTube corpus). As for the vo- cabulary, stopwords are rst removed, and then 5000 words are selected from the YouTube corpus based on their TF-IDF scores <ref type="bibr" target="#b0">(Blei and Lafferty, 2009</ref>). On average, each user's Twitter text data contains 5149 words and 1193 distinct terms, and each user's YouTube text data contains 158 words and 116 dis- tinct terms. These statistics are in accordance with our assumption that text data in the source domain is abundant comparing to that in the target domain.</p><p>To validate the prediction result, each user has at least 10 liked videos. Videos with less than 5 likes are removed from the dataset. After data cleansing, there are 7328 users and 18691 videos in the dataset. The maximum number of likes received by a video is 98, and the average is 19.1. Among all videos, 92% of them are liked by less than 40 users. The max- imum number of likes given by a user is 908, and the average is 48.8. Among all users, 89% of them have liked less than 100 videos. The sparsity (ratio of zeroes to the total number of entries) of the rating matrix is 99.74%, which illustrates the difculty of this recommendation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation and Scenario</head><p>We choose the area under ROC curve (AUC) as the evaluation metric. AUC is often used to compare models when there is severe class imbalance, which is the case in our OCCF problem since we regard all zeroes as observed. All reported results are the average of 5 random data splits.</p><p>Similar to the experiments performed in ( <ref type="bibr" target="#b16">Wang and Blei, 2011)</ref>, we test the performance of each model under two different scenarios. The rst one is the task of in-matrix prediction. In this task, the likes received by each video are partitioned into three sets, namely the training, validation and testing sets. The ratio of data partition is 3:1:1. There are no cold- start users for the in-matrix prediction.</p><p>The second task is the out-of-matrix prediction, where the users are partitioned into three sets with the same 3:1:1 ratio. To make the two tasks compa- rable, we randomly split the data until the number of observations in each of the three sets is closed to that of the in-matrix task. Users in the testing set are all cold-start users. The only data we have when mak- ing prediction on the cold-start users is the auxiliary text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Methods</head><p>• LDA: We run linear regression on the LDA fea- tures to predict the ratings. This model serves as a content-based baseline.</p><p>• UKNN: The user-kNN algorithm <ref type="bibr" target="#b7">(Herlocker et al., 1999</ref>) based on LDA features is imple- mented. This model serves as a neighborhood- based baseline.</p><p>• PMF: PMF ( <ref type="bibr" target="#b14">Salakhutdinov and Mnih, 2007</ref>) is a classic and widely-used CF model. It uses only the rating information, and thus is not ca- pable of performing the out-of-matrix task.</p><p>•  <ref type="figure">Figure 3</ref>: In-matrix AUC using different corpus. For methods signicantly worse than others, we cut off the plot and put the AUC values on top of the bars. NT-MF is signicantly better than the baselines in all plots, according to a paired t-test (p &lt; 0.05).</p><p>( <ref type="bibr" target="#b10">Muja and Lowe, 2014</ref>). The symmetric Kullback- Leibler divergence is chosen to be the distance met- ric between topic vectors. For all baseline methods, we use K to denote the dimensionality of the latent variables. However, when discussing about NT-MF, since the number of topics can be different from the number of user latent factors, we use T to denote the former and K to denote the latter to avoid confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">In-Matrix Prediction</head><p>In this section, the in-matrix prediction is discussed. First, we test the model's general performance on different corpora. Normally, the optimal number of topics will not be the same for different cor- pora. Since the LDA model performs the best with K = 50 on the YouTube corpus and K = 200 on the Twitter corpus, we report the results when K is set to these two numbers. <ref type="figure">Figure 3(a)</ref> shows the results when no source- domain information is available and thus no trans- fer learning is performed. That is, all models are provided only with the YouTube ratings and the YouTube corpus. Because the YouTube corpus is scarce, the LDA model results in lower AUC when more topics are used, signifying overtting. The same reason also leads to limited improvement of LDA-MF over PMF. Using neighborhood informa- tion alone, UKNN performs poorly. On the other hand, as a model bringing neighborhood information into PMF, NT-MF outperforms all baselines signi- cantly. The above analysis shows that, although us- ing either content (LDA) or neighborhood (UKNN) information alone is insufcient to generate good predictions, they can effectively improve the factor- ization of the rating matrix if used correctly.</p><p>To demonstrate the advantage of transfer learning, we study the scenario where only source-domain text and target-domain ratings are available. That is, the YouTube corpus in the previous analysis is re- placed with the Twitter corpus. The result is shown in <ref type="figure">Figure 3</ref>(b). Comparing to <ref type="figure">Figure 3</ref>(a), we can see that although the Twitter corpus is larger than the YouTube corpus, it leads to a worse performance for LDA and UKNN. Content information from the noisy Twitter corpus alone is not sufcient to capture the rating behavior of users. However, by integrat- ing the content information and rating history, both LDA-MF and NT-MF benet from a larger corpus.</p><p>In the following analyses, we use data from both websites. For LDA, PMF and LDA-MF, we merge the two corpora by summing up the word counts. For UKNN and NT-MF, however, there is a more elegant way to combine the knowledge from differ- ent websites. First, we compute user similarity sep- arately from the two corpora. Then, the two sets of similarity scores are weighted and averaged. Finally, the nearest-neighbors are computed based on this set of newly generated similarity scores. By applying this strategy to NT-MF, not only can θ i and u i dif- fer in dimensionality, but also the optimal number of topics can be used for different corpora. Regardless of K, we use T = 50 for YouTube and T = 200 for Twitter in our NT-MF model. The result is shown in <ref type="figure">Figure 3(c)</ref>. By comparing it with <ref type="figure">Figure 3(b)</ref>, we can see that the AUC of NT-MF increases while that of LDA-MF remains unchanged. UKNN also bene- ts from this strategy. These facts show that, instead of merging the two corpora directly, our strategy of averaging the similarities is more advantageous. Next, as a preliminary investigation of the perfor- mance on cold-start users, in <ref type="figure" target="#fig_2">Figure 4</ref>(a), we plot the cumulative AUC with respect to the total num- ber of observed ratings. NT-MF outperforms other methods in terms of cumulative AUC regardless of the number of observed ratings. The advantage of NT-MF over the baseline methods is even greater as the number of observed ratings decreases (except for LDA). To make it clear, we plot the difference in AUC between NT-MF and the baseline methods in <ref type="figure" target="#fig_2">Figure 4(b)</ref>. This phenomenon sheds light on the advantage of NT-MF under cold-start scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Out-of-Matrix Prediction</head><p>In this section, we discuss the out-of-matrix predic- tion. Users in the testing set are all completely cold- start users. That is, we are only provided the Twit- ter corpus when making prediction for these users. Therefore, our previous strategy of averaging the similarities only applies to users in the training set. For this study we adopt the strategy of merging the two corpus instead of averaging the similarities. The number of topics T = 150 is chosen for NT-MF with respect to the validation AUC.</p><p>The result is presented in <ref type="figure" target="#fig_3">Figure 5</ref>. We plot the AUC against the dimensionality of the latent vari- ables K. It can be observed that NT-MF beats all baseline methods regardless of K. Comparing to <ref type="figure">Figure 3</ref>, the out-of-matrix AUC is much lower, sig- nifying the difculty of cold-start recommendation.</p><p>Under the cold-start scenario, the latent factor used in the prediction phase is taken to be the prior mean for the MF-based models. For LDA-MF the prior mean is the topic vector θ i , while for NT-MF it is the weighted average µ i given by Eq. 6. Since θ i is used in place of u i in the LDA-MF model when generating predictions, the curves of LDA and LDA-MF look very similar. A paired t-test (p &lt; 0.05) shows no statistically signicant difference between these two methods when K = 10 (p = 0.48) and K = 20 (p = 0.09). Despite the fact that u i = θ i is xed for the cold-start users in the LDA-MF model, as K becomes larger, the item latent factors can carry more information in the rat- ing data, which results in a higher AUC than LDA. However, since the dimensionalities of the LDA part and PMF part must match, the inference procedure of LDA-MF becomes very slow when K is large. To make a better use of the available data, the compu- tational efciency must be sacriced.</p><p>On the other hand, note that NT-MF achieves the highest AUC when K = 50. In fact, not only does NT-MF beat all baseline methods under different K values, it also outperforms the best LDA-MF model <ref type="bibr">(K = 200)</ref> with fewer latent factors <ref type="bibr">(K = 20)</ref>. Un- like LDA-MF, the latent factors of the cold-start users are not xed in NT-MF. Therefore, NT-MF can represent the information in a more concise way. In this case, NT-MF is better than LDA-MF in terms of both execution speed and predictive power. In <ref type="figure" target="#fig_4">Figure 6</ref> we investigate the effect of different values of K and T . For each curve, we can see that the performance is about the same for K ≥ 50. This is in accordance with the observation that NT-MF does not need as many latent factors as LDA-MF to achieve the same level of performance. Also, while increasing the number of topics T improves the per- formance in general, increasing T from 150 to 200 gives no signicant improvement. The most impor- tant observation is that the highest AUC is achieved when K = 50 and T = 150. In other words, the op- timal number of topics is different from that of user latent factors. This further justies the advantage of NT-MF against previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Although not directly aiming to solve the problem we have proposed, there exists some models of sim- ilar structure or adopt similar ideas.</p><p>As previously mentioned, LDA-MF is similar in structure to CTR. Collaborative topic Poisson fac- torization (CTPF) ( <ref type="bibr" target="#b6">Gopalan et al., 2014</ref>) combines the ideas of CTR and Poisson factorization <ref type="bibr" target="#b5">(Gopalan et al., 2013</ref>) for a better performance. We have also tried CTPF on our dataset; nevertheless, there is no signicant improvement over LDA-MF.</p><p>Recently, the neighborhood-aware probabilistic matrix factorization (NHPMF) model is proposed ( <ref type="bibr" target="#b17">Wu et al., 2012</ref>) as a method to combine kNN and PMF. It is originally proposed to leverage tagging data for improving PMF. This model can also be applied to our problem if we use the Twitter cor- pus in place of the unavailable tagging data. How- ever, in the NHPMF model, the mean parameters are not treated as constants when the user latent factors are updated. As a result, an extra term appears in the gradient formula, which leads to an O(k 2 ) time complexity, with k being the number of nearest- neighbors considered. On the other hand, the com- putation of the weighted average (i.e. Eq. 6) takes O(k) time complexity. We have implemented NH- PMF for comparison. As we increase k, NHPMF becomes signicantly slower than NT-MF, while its performance is no better than NT-MF on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose NT-MF, a cross-website transfer learning model which integrates content, neighborhood and rating information to alleviate the cold-start problem. A signicant improvement over previous methods is demonstrated on a real-world cross-website dataset. The improvement is even more signicant under the cold-start scenario.</p><p>So far we use the LDA topic vector to represent a user. As future work, different aspects of text can be taken into account to generate a more comprehen- sive user model. For example, writing styles or opin- ion mining may provide different insights on user behavior. Another possible extension is to apply our idea to more realistic settings such as large-scale and online recommender systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The entire system.</figDesc><graphic url="image-1.png" coords="3,321.40,57.38,210.43,191.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Cumulative in-matrix AUC. Each point (x, y) in the gure means that the model gives an averaged AUC of y among all users who have less than or equal to x observed ratings. (b) Difference in cumulative in-matrix AUC between NT-MF and baseline methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Out-of-matrix AUC. NT-MF is signicantly better than the baselines, according to a paired t-test (p &lt; 0.05).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance of NT-MF based on out-of-matrix AUC for different values of K and T .</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by Mi-crosoft Research Asia (MSRA) under award number FY16-RES-THEME-013 and by Taiwan Ministry of Science and Technology (MOST) under grant num-ber 103-2221-E-002-104-MY2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Mining: Theory and Applications</title>
		<imprint>
			<publisher>Taylor and Francis</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hybrid recommender systems: Survey and experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">331370, November. Robin Burke. 2007. The adaptive web. chapter Hybrid Web Recommender Systems</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="377" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Twitter is faster: Personalized timeaware video recommendation from twitter to youtube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitao</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<idno>31:131:23</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scalable recommendation with poisson factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><forename type="middle">M</forename><surname>Hofman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno>abs/1311.1704</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Content-based recommendations with poisson factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3176" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An algorithmic framework for performing collaborative ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;99</title>
		<meeting>the 22Nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Empirical study of topic modeling in twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Social Media Analytics, SOMA &apos;10</title>
		<meeting>the First Workshop on Social Media Analytics, SOMA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can movies and books collaborate?: Cross-domain collaborative ltering for sparsity reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Joint Conference on Artical Intelligence, IJCAI&apos;09</title>
		<meeting>the 21st International Joint Conference on Artical Intelligence, IJCAI&apos;09<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2052" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbor algorithms for high dimensional data. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-class collaborative ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajan</forename><forename type="middle">M</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th IEEE International Conference on Data Mining (ICDM 2008)</title>
		<meeting>the 8th IEEE International Conference on Data Mining (ICDM 2008)<address><addrLine>Pisa, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12-15" />
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The adaptive web. chapter Content-based Recommendation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Billsus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="325" to="341" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Socialtransfer: Cross-domain transfer learning from social streams for media applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Suman Deb Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimedia, MM &apos;12</title>
		<meeting>the 20th ACM International Conference on Multimedia, MM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="649" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-12-03" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tags as bridges between domains: Improving recommendation with tag-induced cross-domain collaborative ltering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on User Modeling, Adaption, and Personalization, UMAP&apos;11</title>
		<meeting>the 19th International Conference on User Modeling, Adaption, and Personalization, UMAP&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="305" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collaborative topic modeling for recommending scientic articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging tagging for neighborhood-aware probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12</title>
		<meeting>the 21st ACM International Conference on Information and Knowledge Management, CIKM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1854" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
