<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T10:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Language Analysis with Recurrent Multistage Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Machine Learning Department</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Language Technologies Institute Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Language Analysis with Recurrent Multistage Fusion</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="150" to="161"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>150</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Computational modeling of human multi-modal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper , we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this mul-tistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of mul-timodal signals, learning increasingly discrim-inative multimodal representations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Computational modeling of human multimodal language is an upcoming research area in natu- ral language processing. This research area fo- cuses on modeling tasks such as multimodal sen- timent analysis <ref type="bibr" target="#b36">(Morency et al., 2011</ref>), emotion recognition ( <ref type="bibr" target="#b6">Busso et al., 2008)</ref>, and personality traits recognition <ref type="bibr" target="#b42">(Park et al., 2014</ref>). The multi- modal temporal signals include the language (spo- ken words), visual (facial expressions, gestures) and acoustic modalities (prosody, vocal expres- sions). At its core, these multimodal signals are At each recursive stage, a subset of multimodal signals is highlighted and then fused with previous fusion representations. The first fu- sion stage selects the neutral word and frowning behaviors which create an intermediate represen- tation reflecting negative emotion when fused to- gether. The second stage selects the loud voice behavior which is locally interpreted as empha- sis before being fused with previous stages into a strongly negative representation. Finally, the third stage selects the shrugging and speech elongation behaviors that reflect ambivalence and when fused with previous stages is interpreted as a representa- tion for the disappointed emotion.</p><p>highly structured with two prime forms of in- teractions: intra-modal and cross-modal interac- tions ( <ref type="bibr" target="#b49">Rajagopalan et al., 2016)</ref>. Intra-modal inter- actions refer to information within a specific modal- ity, independent of other modalities. For example, the arrangement of words in a sentence according to the generative grammar of a language <ref type="bibr" target="#b11">(Chomsky, 1957)</ref> or the sequence of facial muscle activations for the presentation of a frown. Cross-modal in- teractions refer to interactions between modalities. For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed oc- currence of a laughter after the end of a sentence. Modeling these interactions lie at the heart of hu- man multimodal language analysis and has recently become a centric research direction in multimodal natural language processing ( <ref type="bibr" target="#b33">Liu et al., 2018;</ref><ref type="bibr" target="#b44">Pham et al., 2018;</ref>, multimodal speech recognition ( <ref type="bibr" target="#b17">Gupta et al., 2017;</ref><ref type="bibr" target="#b19">Harwath and Glass, 2017;</ref><ref type="bibr" target="#b27">Kamper et al., 2017)</ref>, as well as multimodal machine learning <ref type="bibr">(Tsai et al., 2018;</ref><ref type="bibr" target="#b56">Srivastava and Salakhutdinov, 2012;</ref><ref type="bibr" target="#b38">Ngiam et al., 2011</ref>).</p><p>Recent advances in cognitive neuroscience have demonstrated the existence of multistage aggre- gation across human cortical networks and func- tions ( <ref type="bibr" target="#b59">Taylor et al., 2015)</ref>, particularly during the in- tegration of multisensory information ( <ref type="bibr" target="#b41">Parisi et al., 2017)</ref>. At later stages of cognitive processing, higher level semantic meaning is extracted from phrases, facial expressions, and tone of voice, even- tually leading to the formation of higher level cross- modal concepts ( <ref type="bibr" target="#b41">Parisi et al., 2017;</ref><ref type="bibr" target="#b59">Taylor et al., 2015)</ref>. Inspired by these discoveries, we hypoth- esize that the computational modeling of cross- modal interactions also requires a multistage fusion process. In this process, cross-modal representa- tions can build upon the representations learned during earlier stages. This decreases the burden on each stage of multimodal fusion and allows each stage of fusion to be performed in a more special- ized and effective manner.</p><p>In this paper, we propose the Recurrent Multi- stage Fusion Network (RMFN) which automati- cally decomposes the multimodal fusion problem into multiple recursive stages. At each stage, a sub- set of multimodal signals is highlighted and fused with previous fusion representations (see <ref type="figure" target="#fig_0">Figure 1</ref>). This divide-and-conquer approach decreases the burden on each fusion stage, allowing each stage to be performed in a more specialized and effective way. This is in contrast with conventional fusion approaches which usually model interactions over multimodal signals altogether in one iteration (e.g., early fusion ( ). In RMFN, temporal and intra-modal interactions are modeled by integrating our new multistage fusion process with a system of recurrent neural networks. Overall, RMFN jointly models intra-modal and cross-modal interactions for multimodal language analysis and is differentiable end-to-end. We evaluate RMFN on three different tasks re- lated to human multimodal language: sentiment analysis, emotion recognition, and speaker traits recognition across three public multimodal datasets. RMFN achieves state-of-the-art performance in all three tasks. Through a comprehensive set of ab- lation experiments and visualizations, we demon- strate the advantages of explicitly defining multiple recursive stages for multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous approaches in human multimodal lan- guage modeling can be categorized as follows: Non-temporal Models: These models simplify the problem by using feature-summarizing tempo- ral observations ( . Each modality is represented by averaging temporal information through time, as shown for language-based senti- ment analysis <ref type="bibr" target="#b25">(Iyyer et al., 2015;</ref><ref type="bibr" target="#b9">Chen et al., 2016)</ref> and multimodal sentiment analysis ( <ref type="bibr" target="#b0">Abburi et al., 2016;</ref><ref type="bibr" target="#b39">Nojavanasghari et al., 2016;</ref><ref type="bibr" target="#b67">Zadeh et al., 2016;</ref><ref type="bibr" target="#b36">Morency et al., 2011</ref>). Conventional su- pervised learning methods are utilized to discover intra-modal and cross-modal interactions without specific model design ( <ref type="bibr" target="#b61">Wang et al., 2016;</ref><ref type="bibr" target="#b47">Poria et al., 2016)</ref>. These approaches have trouble mod- eling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics ( <ref type="bibr" target="#b62">Xu et al., 2013)</ref>. Multimodal Temporal Graphical Models: The application of graphical models in sequence mod- eling has been an important research problem. Hid- den Markov Models (HMMs) ( <ref type="bibr" target="#b3">Baum and Petrie, 1966)</ref>, Conditional Random Fields (CRFs) <ref type="bibr" target="#b29">(Lafferty et al., 2001</ref>), and Hidden Conditional Random Fields (HCRFs) (  were shown to work well on modeling sequential data from the language ( <ref type="bibr" target="#b35">Misawa et al., 2017;</ref><ref type="bibr" target="#b34">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b23">Huang et al., 2015</ref>) and acoustic (Yuan and Liber- man, 2008) modalities. These temporal graphical models have also been extended for modeling mul- timodal data. Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views ( <ref type="bibr" target="#b54">Song et al., 2012</ref>), multi-layered CRFs with latent variables to learn hidden spatio- temporal dynamics from multi-view data <ref type="bibr" target="#b54">(Song et al., 2012)</ref>, and multi-view Hierarchical Sequence Summarization models that recursively build up hi- erarchical representations <ref type="bibr" target="#b55">(Song et al., 2013)</ref>. Multimodal Temporal Neural Networks: More recently, with the advent of deep learning, Re- current Neural Networks <ref type="bibr" target="#b14">(Elman, 1990;</ref><ref type="bibr" target="#b26">Jain and Medsker, 1999</ref>) have been used extensively for lan- guage and speech based sequence modeling <ref type="bibr" target="#b68">(Zilly et al., 2016;</ref><ref type="bibr" target="#b53">Soltau et al., 2016)</ref>, sentiment analy- sis ( <ref type="bibr" target="#b52">Socher et al., 2013;</ref><ref type="bibr" target="#b50">dos Santos and Gatti, 2014;</ref><ref type="bibr" target="#b15">Glorot et al., 2011;</ref><ref type="bibr" target="#b7">Cambria, 2016)</ref>, and emotion recognition ( <ref type="bibr" target="#b4">Bertero et al., 2016;</ref><ref type="bibr" target="#b30">Lakomkin et al., 2018)</ref>. Long-short Term Memory (LSTM) networks <ref type="bibr" target="#b21">(Hochreiter and Schmidhuber, 1997a</ref>) have also been extended for multimodal set- tings ( <ref type="bibr" target="#b49">Rajagopalan et al., 2016)</ref> and by learning binary gating mechanisms to remove noisy modali- ties ( . Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions. These use Bayesian ranking algorithms <ref type="bibr" target="#b20">(Herbrich et al., 2007</ref>) to model both person-independent and person-dependent fea- tures ( , generative-discriminative objectives to learn either joint ( <ref type="bibr" target="#b44">Pham et al., 2018)</ref> or factorized multimodal representations <ref type="bibr">(Tsai et al., 2018)</ref>, external memory mechanisms to synchro- nize multimodal data ( <ref type="bibr">Zadeh et al., 2018a</ref>), or low- rank tensors to approximate expensive tensor prod- ucts ( <ref type="bibr" target="#b33">Liu et al., 2018)</ref>. All these methods assume that cross-modal interactions should be discovered all at once rather than across multiple stages, where each stage solves a simpler fusion problem. Our empirical evaluations show the advantages of the multistage fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Recurrent Multistage Fusion Network</head><p>In this section we describe the Recurrent Multi- stage Fusion Network (RMFN) for multimodal lan- guage analysis <ref type="figure" target="#fig_1">(Figure 2</ref>). Given a set of modalities {l(anguage), v(isual), a(coustic)}, the signal from each modality m ∈ {l, v, a} is represented as a temporal sequence</p><formula xml:id="formula_0">X m = {x m 1 , x m 2 , x m 3 , ï¿¿, x m T }, where x m</formula><p>t is the input at time t. Each sequence X m is modeled with an intra-modal recurrent neural network (see subsection 3.3 for details). At time t, each intra-modal recurrent network will output a unimodal representation h m t . The Multistage Fu- sion Process uses a recursive approach to fuse all unimodal representations h m t into a cross-modal representation z t which is then fed back into each intra-modal recurrent network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multistage Fusion Process</head><p>The Multistage Fusion Process (MFP) is a modular neural approach that performs multistage fusion to model cross-modal interactions. Multistage fusion is a divide-and-conquer approach which decreases the burden on each stage of multimodal fusion, allowing each stage to be performed in a more spe- cialized and effective way. The MFP has three main modules: HIGHLIGHT, FUSE and SUMMARIZE.</p><p>Two modules are repeated at each stage: HIGHLIGHT and FUSE. The HIGHLIGHT mod- ule identifies a subset of multimodal signals from [h l t , h v t , h a t ] that will be used for that stage of fu- sion. The FUSE module then performs two sub- tasks simultaneously: a local fusion of the high- lighted features and integration with representa- tions from previous stages. Both HIGHLIGHT and FUSE modules are realized using memory- based neural networks which enable coherence between stages and storage of previously mod- eled cross-modal interactions. As a final step, the SUMMARIZE module takes the multimodal repre- sentation of the final stage and translates it into a cross-modal representation z t . <ref type="figure" target="#fig_0">Figure 1</ref> shows an illustrative example for mul- tistage fusion. The HIGHLIGHT module selects "neutral words" and "frowning" expression for the first stage. The local and integrated fusion at this stage creates a representation reflecting negative emotion. For stage 2, the HIGHLIGHT module identifies the acoustic feature "loud voice". The local fusion at this stage interprets it as an expres- sion of emphasis and is fused with the previous fusion results to represent a strong negative emo- tion. Finally, the highlighted features of "shrug" and "speech elongation" are selected and are lo- cally interpreted as "ambivalence". The integration with previous stages then gives a representation closer to "disappointed".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Module Descriptions</head><p>In this section, we present the details of the three multistage fusion modules: HIGHLIGHT, FUSE and SUMMARIZE. Multistage fusion begins with the concatenation of intra-modal network outputs h t = ï¿¿ m∈M h m t . We use superscript <ref type="bibr">[k]</ref> to denote the indices of each stage k = 1, ï¿¿, K during K total stages of multistage fusion. Let ⇥ denote the neural network parameters across all modules.</p><p>HIGHLIGHT: At each stage k, a subset of the multimodal signals represented in h t will be au-  </p><formula xml:id="formula_1">l t , h v t , h a t .</formula><p>At each stage, the HIGHLIGHT module identifies a subset of multimodal signals and the FUSE module performs local fusion before integration with previous fusion representations. The SUMMARIZE module translates the representation at the final stage into a cross-modal representation z t to be fed back into the intra-modal recurrent networks.</p><p>tomatically highlighted for fusion. Formally, this module is defined by the process function f H :</p><formula xml:id="formula_2">a [k] t = f H (h t ; a [1∶k−1] t , ⇥)<label>(1)</label></formula><p>where at stage k, a <ref type="bibr">[k]</ref> t is a set of attention weights which are inferred based on the previously as- signed attention weights a . As a result, the highlights at a specific stage k will be depen- dent on previous highlights. To fully encapsu- late these dependencies, the attention assignment process is performed in a recurrent manner using a LSTM which we call the HIGHLIGHT LSTM. The initial HIGHLIGHT LSTM memory at stage 0, c</p><formula xml:id="formula_3">HIGHLIGHT[0] t</formula><p>, is initialized using a network M that maps h t into LSTM memory space:</p><formula xml:id="formula_4">c HIGHLIGHT[0] t = M(h t ; ⇥)<label>(2)</label></formula><p>This allows the memory mechanism of the HIGHLIGHT LSTM to dynamically adjust to the intra-modal representations h t . The output of the</p><formula xml:id="formula_5">HIGHLIGHT LSTM h HIGHLIGHT[k] t</formula><p>is softmax ac- tivated to produce attention weights a <ref type="bibr">[k]</ref> t at every stage k of the multistage fusion process:</p><formula xml:id="formula_6">a [k] t j = exp (h HIGHLIGHT[k] t j ) ∑ ï¿¿h HIGHLIGHT[k] t ï¿¿ d=1 exp (h HIGHLIGHT[k] t d )<label>(3)</label></formula><p>and a <ref type="bibr">[k]</ref> t is fed as input into the HIGHLIGHT LSTM at stage k + 1. Therefore, the HIGHLIGHT LSTM functions as a decoder LSTM <ref type="bibr" target="#b58">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b10">Cho et al., 2014</ref>) in order to capture the dependencies on previous attention assignments. Highlighting is performed by element-wise multi- plying the attention weights a <ref type="bibr">[k]</ref> t with the concate- nated intra-modal representations h t :</p><formula xml:id="formula_7">˜ h [k] t = h t ⊙ a [k] t (4)</formula><p>where ⊙ denotes the Hadamard product and˜hand˜ and˜h <ref type="bibr">[k]</ref> t are the attended multimodal signals that will be used for the fusion at stage k.</p><p>FUSE: The highlighted multimodal signals are simultaneously fused in a local fusion and then in- tegrated with fusion representations from previous stages. Formally, this module is defined by the process function f F :</p><formula xml:id="formula_8">s [k] t = f F ( ˜ h [k] t ; s [1∶k−1] t , ⇥)<label>(5)</label></formula><p>where s <ref type="bibr">[k]</ref> t denotes the integrated fusion represen- tations at stage k. We employ a FUSE LSTM to simultaneously perform the local fusion and the integration with previous fusion representations. The FUSE LSTM input gate enables a local fusion while the FUSE LSTM forget and output gates en- able integration with previous fusion results. The initial FUSE LSTM memory at stage 0, c</p><formula xml:id="formula_9">FUSE[0] t</formula><p>, is initialized using random orthogonal matrices <ref type="bibr" target="#b1">(Arjovsky et al., 2015;</ref><ref type="bibr" target="#b31">Le et al., 2015)</ref>. SUMMARIZE: After completing K recur- sive stages of HIGHLIGHT and FUSE, the SUMMARIZE operation generates a cross-modal representation using all final fusion representations s <ref type="bibr">[1∶K]</ref> t . Formally, this operation is defined as:</p><formula xml:id="formula_10">z t = S(s [1∶K] t ; ⇥)<label>(6)</label></formula><p>where z t is the final output of the multistage fusion process and represents all cross-modal interactions discovered at time t. The summarized cross-modal representation is then fed into the intra-modal re- current networks as described in the subsection 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System of Long Short-term Hybrid Memories</head><p>To integrate the cross-modal representations z t with the temporal intra-modal representations, we employ a system of Long Short-term Hybrid Mem- ories (LSTHMs) ( <ref type="bibr" target="#b66">Zadeh et al., 2018b</ref>). The LSTHM extends the LSTM formulation to include the cross-modal representation z t in a hybrid mem- ory component:</p><formula xml:id="formula_11">i m t+1 = (W m i x m t+1 + U m i h m t + V m i zt + b m i )<label>(7)</label></formula><formula xml:id="formula_12">f m t+1 = (W m f x m t+1 + U m f h m t + V m f zt + b m f )<label>(8)</label></formula><formula xml:id="formula_13">o m t+1 = (W m o x m t+1 + U m o h m t + V m o zt + b m o )<label>(9)</label></formula><formula xml:id="formula_14">¯ c m t+1 = W m ¯ c x m t+1 + U m ¯ c h m t + V m ¯ c zt + b m ¯ c<label>(10)</label></formula><formula xml:id="formula_15">c m t+1 = f m t ⊙ c m t + i m t ⊙ tanh(¯ c m t+1 )<label>(11)</label></formula><formula xml:id="formula_16">h m t+1 = o m t+1 ⊙ tanh(c m t+1 )<label>(12)</label></formula><p>where is the (hard-)sigmoid activation function, tanh is the tangent hyperbolic activation function, ⊙ denotes the Hadamard product. i, f and o are the input, forget and output gates respectively. ¯ c m t+1 is the proposed update to the hybrid memory c m t at time t + 1 and h m t is the time distributed output of each modality. The cross-modal representation z t is modeled by the Multistage Fusion Process as discussed in subsection 3.2. The hybrid memory c m t contains both intra-modal interactions from in- dividual modalities x m t as well as the cross-modal interactions captured in z t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>The multimodal prediction task is performed using a final representation E which integrate (1) the last outputs from the LSTHMs and (2) the last cross- modal representation z T . Formally, E is defined as:</p><formula xml:id="formula_17">E = ( ï¿¿ m∈M h m T ) ï¿¿ z T<label>(13)</label></formula><p>where ï¿¿ denotes vector concatenation. E can then be used as a multimodal representation for super- vised or unsupervised analysis of multimodal lan- guage. It summarizes all modeled intra-modal and cross-modal representations from the multi- modal sequences. RMFN is differentiable end-to- end which allows the network parameters ⇥ to be learned using gradient descent approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>To evaluate the performance and generalization of RMFN, three domains of human multimodal lan- guage were selected: multimodal sentiment analy- sis, emotion recognition, and speaker traits recog- nition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multimodal Features and Alignment</head><p>GloVe word embeddings ( <ref type="bibr" target="#b43">Pennington et al., 2014</ref>), Facet (iMotions, 2017) and COVAREP <ref type="bibr" target="#b13">(Degottex et al., 2014</ref>) are extracted for the language, visual and acoustic modalities respectively <ref type="bibr">1</ref> . Forced alignment is performed using P2FA <ref type="bibr" target="#b63">(Yuan and Liberman, 2008)</ref> to obtain the exact utterance times   of each word. We obtain the aligned video and au- dio features by computing the expectation of their modality feature values over each word utterance time interval <ref type="bibr">(Tsai et al., 2018</ref>).</p><formula xml:id="formula_18">Dataset CMU-MOSI Task Sentiment Metric A2 ↑ F1 ↑ A7 ↑ MAE ↓ Corr ↑<label>SOTA3</label></formula><formula xml:id="formula_19">SOT A ↑ 0.8 ↑ 1.6 ↑ 0.3 ↑ 0.1 - ↑ 0.1 - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline Models</head><p>We compare to the following models for mul- timodal machine learning: BC-LSTM ( ) performs context- dependent sentiment analysis and emotion recog- nition, currently state of the art on IEMOCAP. EF- LSTM concatenates the multimodal inputs and uses that as input to a single LSTM <ref type="bibr" target="#b22">(Hochreiter and Schmidhuber, 1997b</ref>). We also implement the Stacked, (EF-SLSTM) ( <ref type="bibr" target="#b16">Graves et al., 2013</ref>) Bidirectional (EF-BLSTM) <ref type="bibr" target="#b51">(Schuster and Paliwal, 1997)</ref> and Stacked Bidirectional (EF-SBLSTM) LSTMs. For descriptions of the remaining base- lines, we refer the reader to EF-HCRF ( , EF/MV-LDHCRF ( , MV-HCRF <ref type="figure" target="#fig_0">(Song et al., 2012)</ref>, EF/MV- HSSHCRF ( <ref type="bibr" target="#b55">Song et al., 2013)</ref>, MV-LSTM (Ra- jagopalan et al., 2016), DF (Nojavanasghari et al., 2016), SAL-CNN ( <ref type="bibr" target="#b61">Wang et al., 2016)</ref>, C-MKL (Poria et al., 2015), THMM (Morency et al., 2011), SVM ( <ref type="bibr" target="#b12">Cortes and Vapnik, 1995;</ref><ref type="bibr" target="#b42">Park et al., 2014)</ref> and RF <ref type="bibr" target="#b5">(Breiman, 2001</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Metrics</head><p>For classification, we report accuracy Ac where c denotes the number of classes and F1 score. For re- gression, we report Mean Absolute Error MAE and Pearson's correlation r. For MAE lower values in- dicate stronger performance. For all remaining met- rics, higher values indicate stronger performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance on Multimodal Language</head><p>Results on CMU-MOSI, IEMOCAP and POM are presented in <ref type="table" target="#tab_2">Tables 1, 2 and 3 respectively 2</ref> . We achieve state-of-the-art or competitive results for all domains, highlighting RMFN's capability in hu- man multimodal language analysis. We observe that RMFN does not improve results on IEMO- CAP neutral emotion and the model outperforming RMFN is a memory-based fusion baseline (Zadeh et al., 2018a). We believe that this is because neu- tral expressions are quite idiosyncratic. Some peo- ple may always look angry given their facial config- uration (e.g., natural eyebrow raises of actor Jack Nicholson). In these situations, it becomes useful to compare the current image with a memorized or aggregated representation of the speaker's face. Our proposed multistage fusion approach can eas- ily be extended to memory-based fusion methods. <ref type="table">Task  Con  Pas  Voi  Cre  Viv  Exp  Res  Rel  Tho  Ner  Per  Hum  Metric</ref>     <ref type="table">Table 5</ref>: Comparison studies of RMFN on CMU- MOSI. Modeling cross-modal interactions using multistage fusion and attention weights are crucial in multimodal language analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POM Speaker Personality Traits</head><formula xml:id="formula_20">A7 ↑ A7 ↑ A7 ↑ A7 ↑ A7 ↑ A7 ↑ A5 ↑ A5 ↑ A5 ↑ A5 ↑ A7 ↑ A6 ↑<label>SOTA3</label></formula><formula xml:id="formula_21">SOT A ↑ 2.9 ↑ 2.9 0.0 ↑ 2.9 ↑ 2.0 ↑ 3.9 ↑ 1.0 ↑ 0.5 ↑ 1.0 ↑ 0.5 ↑ 1.0 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Multistage Fusion</head><p>To achieve a deeper understanding of the multi- stage fusion process, we study five research ques- tions. (Q1): whether modeling cross-modal inter- actions across multiple stages is beneficial. (Q2): the effect of the number of stages K during multi- stage fusion on performance. (Q3): the comparison between multistage and independent modeling of cross-modal interactions. (Q4): whether modeling cross-modal interactions are helpful. (Q5): whether attention weights from the HIGHLIGHT module are required for modeling cross-modal interactions. Q1: To study the effectiveness of the multistage fu- sion process, we test the baseline RMFN-R1 which performs fusion in only one stage instead of across multiple stages. This model makes the strong as- sumption that all cross-modal interactions can be modeled during only one stage. From <ref type="table" target="#tab_7">Table 4</ref>, RMFN-R1 underperforms as compared to RMFN which performs multistage fusion. Q2: We test baselines RMFN-RK which perform K stages of fusion. From <ref type="table" target="#tab_7">Table 4</ref>, we observe that increasing the number of stages K increases the model's capability to model cross-modal in- teractions up to a certain point (K = 3) in our experiments. Further increases led to decreases in performance and we hypothesize this is due to overfitting on the dataset. Q3: To compare multistage against independent modeling of cross-modal interactions, we pay close attention to the performance comparison with re- spect to MARN which models multiple cross- modal interactions all at once (see <ref type="table">Table 5</ref>). RMFN shows improved performance, indicating that mul- tistage fusion is both effective and efficient for hu- man multimodal language modeling. Q4: RMFN (no MFP) represents a system of LSTHMs without the integration of z t from the MFP to model cross-modal interactions. From Ta- ble 5, RMFN (no MFP) is outperformed by RMFN, confirming that modeling cross-modal interactions is crucial in analyzing human multimodal language. Q5: RMFN (no HIGHLIGHT) removes the HIGHLIGHT module from MFP during multistage fusion. From <ref type="table">Table 5</ref>, RMFN (no HIGHLIGHT) underperforms, indicating that highlighting multi- modal representations using attention weights are important for modeling cross-modal interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualizations</head><p>Using an attention assignment mechanism during the HIGHLIGHT process gives more interpretabil- ity to the model since it allows us to visualize the attended multimodal signals at each stage and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>'() stages stages</head><p>It doesn't give any insight or help stages stages time step (see <ref type="figure" target="#fig_5">Figure 3</ref>). Using RMFN trained on the CMU-MOSI dataset, we plot the attention weights across the multistage fusion process for three videos in CMU-MOSI. Based on these vi- sualizations we first draw the following general observations on multistage fusion: Across stages: Attention weights change their be- haviors across the multiple stages of fusion. Some features are highlighted by earlier stages while other features are used in later stages. This supports our hypothesis that RMFN learns to specialize in different stages of the fusion process. Across time: Attention weights vary over time and adapt to the multimodal inputs. We observe that the attention weights are similar if the input contains no new information. As soon as new multimodal information comes in, the highlighting mechanism in RMFN adapts to these new inputs.</p><formula xml:id="formula_22">ℎ " # ℎ " % ℎ " &amp; Language Visual Acoustic (soft) (emphasis) (disappointed) (a) (b) (c) (smile) (smile) '() '() '() '</formula><p>Priors: Based on the distribution of attention weights, we observe that the language and acoustic modalities seem the most commonly highlighted. This represents a prior over the expression of senti- ment in human multimodal language and is closely related to the strong connections between language and speech in human communication <ref type="bibr" target="#b28">(Kuhl, 2000)</ref>. Inactivity: Some attention coefficients are not ac- tive (always orange) throughout time. We hypoth- esize that these corresponding dimensions carry only intra-modal dynamics and are not involved in the formation of cross-modal interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>In addition to the general observations above, <ref type="figure" target="#fig_5">Fig- ure 3</ref> shows three examples where multistage fusion learns cross-modal representations across three different scenarios. Synchronized Interactions: In <ref type="figure" target="#fig_5">Figure 3</ref>(a), the language features are highlighted corresponding to the utterance of the word "fun" that is highly indicative of sentiment (t = 5). This sudden change is also accompanied by a synchronized highlight- ing of the acoustic features. We also notice that the highlighting of the acoustic features lasts longer across the 3 stages since it may take multiple stages to interpret all the new acoustic behaviors (elon- gated tone of voice and phonological emphasis). Asynchronous Trimodal Interactions: In <ref type="figure" target="#fig_5">Fig- ure 3(b)</ref>, the language modality displays ambigu- ous sentiment: "delivers a lot of intensity" can be inferred as both positive or negative. We observe that the circled attention units in the visual and acoustic features correspond to the asynchronous presence of a smile (t = 2 ∶ 5) and phonological emphasis (t = 3) respectively. These nonverbal be- haviors resolve ambiguity in language and result in an overall display of positive sentiment. We further note the coupling of attention weights that highlight the language, visual and acoustic features across stages (t = 3 ∶ 5), further emphasizing the coordina- tion of all three modalities during multistage fusion despite their asynchronous occurrences. Bimodal Interactions: In <ref type="figure" target="#fig_5">Figure 3(c)</ref>, the lan- guage modality is better interpreted in the context of acoustic behaviors. The disappointed tone and soft voice provide the nonverbal information useful for sentiment inference. This example highlights the bimodal interactions (t = 4 ∶ 7) in alternating stages: the acoustic features are highlighted more in earlier stages while the language features are highlighted increasingly in later stages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustrative example for Recurrent Multistage Fusion. At each recursive stage, a subset of multimodal signals is highlighted and then fused with previous fusion representations. The first fusion stage selects the neutral word and frowning behaviors which create an intermediate representation reflecting negative emotion when fused together. The second stage selects the loud voice behavior which is locally interpreted as emphasis before being fused with previous stages into a strongly negative representation. Finally, the third stage selects the shrugging and speech elongation behaviors that reflect ambivalence and when fused with previous stages is interpreted as a representation for the disappointed emotion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Recurrent Multistage Fusion Network for multimodal language analysis. The Multistage Fusion Process has three modules: HIGHLIGHT, FUSE and SUMMARIZE. Multistage fusion begins with the concatenated intra-modal network outputs h l t , h v t , h a t. At each stage, the HIGHLIGHT module identifies a subset of multimodal signals and the FUSE module performs local fusion before integration with previous fusion representations. The SUMMARIZE module translates the representation at the final stage into a cross-modal representation z t to be fed back into the intra-modal recurrent networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>All datasets consist of monologue videos. The speaker's intentions are conveyed through three modalities: language, visual and acoustic. Multimodal Sentiment Analysis involves analyz- ing speaker sentiment based on video content. Mul- timodal sentiment analysis extends conventional language-based sentiment analysis to a multimodal setup where both verbal and non-verbal signals contribute to the expression of sentiment. We use CMU-MOSI (Zadeh et al., 2016) which consists of 2199 opinion segments from online videos each annotated with sentiment in the range [-3,3]. Multimodal Emotion Recognition involves iden- tifying speaker emotions based on both verbal and nonverbal behaviors. We perform experiments on the IEMOCAP dataset (Busso et al., 2008) which consists of 7318 segments of recorded dyadic dia- logues annotated for the presence of human emo- tions happiness, sadness, anger and neutral. Multimodal Speaker Traits Recognition in- volves recognizing speaker traits based on multi- modal communicative behaviors. POM (Park et al., 2014) contains 903 movie review videos each an- notated for 12 speaker traits: confident (con), pas- sionate (pas), voice pleasant (voi), credible (cre), vivid (viv), expertise (exp), reserved (res), trusting (tru), relaxed (rel), thorough (tho), nervous (ner), persuasive (per) and humorous (hum).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of learned attention weights across stages 1,2 and 3 of the multistage fusion process and across time of the multimodal sequence. We observe that the attention weights are diverse and evolve across stages and time. In these three examples, the red boxes emphasize specific moments of interest. (a) Synchronized interactions: the positive word "fun" and the acoustic behaviors of emphasis and elongation (t = 5) are synchronized in both attention weights for language and acoustic features. (b) Asynchronous trimodal interactions: the asynchronous presence of a smile (t = 2 ∶ 5) and emphasis (t = 3) help to disambiguate the language modality. (c) Bimodal interactions: the interactions between the language and acoustic modalities are highlighted by alternating stages of fusion (t = 4 ∶ 7).</figDesc><graphic url="image-15.png" coords="8,290.85,45.28,216.04,162.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Sentiment prediction results on CMU-
MOSI. Best results are highlighted in bold and 
SOT A shows improvement over previous state of 
the art (SOTA). Symbols denote baseline model 
which achieves the reported performance: MFN: 
ï¿¿, MARN:  §, GME-LSTM(A): ◇, TFN:  †, MV-

LSTM: #, EF-LSTM: ♭. The RMFN outperforms 
the current SOTA across all evaluation metrics. Im-
provements are highlighted in green. 

Dataset 
IEMOCAP Emotions 
Task 
Happy 
Sad 
Angry 
Neutral 
Metric 
A2 ↑ F1 ↑ A2 ↑ F1 ↑ A2 ↑ F1 ↑ A2 ↑ F1 ↑ 
SOTA3 86.1 × 83.6  § 83.2 • 81.7 • 85.0 ï¿¿ 84.2  § 68.2 ♭ 66.7 # 
SOTA2 86.5 ï¿¿ 84.0 ï¿¿ 83.4  † 82.1 ï¿¿ 85.1 # 84.3 # 68.8 ♭ 68.5 ♭ 
SOTA1 86.7  § 84.2 ♭ 83.5 ï¿¿ 82.8  † 85.2 ♭ 84.5 ♭ 69.6 ï¿¿ 69.2 ï¿¿ 
RMFN 
87.5 85.8 83.8 82.9 85.1 84.6 69.5 69.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Emotion recognition results on IEMOCAP. 
Best results are highlighted in bold and SOT A 
shows improvement over previous SOTA. Symbols 
denote baseline model which achieves the reported 
performance: MFN: ï¿¿, MARN:  §, BC-LSTM: •, 
TFN:  †, MV-LSTM: #, EF-LSTM: ♭, SVM: ×. 
The RMFN outperforms the current SOTA across 
evaluation metrics except SOT A entries in gray. 
Improvements are highlighted in green. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results for personality trait recognition on POM. Best results are highlighted in bold and SOT A 
shows improvement over previous SOTA. Symbols denote baseline model which achieves the reported 
performance: MFN: ï¿¿, MARN:  §, BC-LSTM: •, TFN:  †, MV-LSTM: #, EF-LSTM: ♭, RF: ♡, SVM: 
×. The MFP outperforms the current SOTA across all evaluation metrics except the SOT A entries 

highlighted in gray. Improvements are highlighted in green. 

Dataset 
CMU-MOSI 
Task 
Sentiment 
Metric 
A2 ↑ 
F1 ↑ 
A7 ↑ 
MAE ↓ 
Corr ↑ 
RMFN-R1 
75.5 
75.5 
35.1 
0.997 
0.653 
RMFN-R2 
76.4 
76.4 
34.5 
0.967 
0.642 
RMFN-R3 
78.4 
78.0 
38.3 
0.922 
0.681 
RMFN-R4 
76.0 
76.0 
36.0 
0.999 
0.640 
RMFN-R5 
75.5 
75.5 
30.9 
1.009 
0.617 
RMFN-R6 
70.4 
70.5 
30.8 
1.109 
0.560 
RMFN 
78.4 
78.0 
38.3 
0.922 
0.681 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Effect of varying the number of stages on 
CMU-MOSI sentiment analysis performance. Mul-
tistage fusion improves performance as compared 
to single stage fusion. 

Dataset 
CMU-MOSI 
Task 
Sentiment 
Metric 
A2 ↑ 
F1 ↑ 
A7 ↑ MAE ↓ Corr ↑ 
MARN 
77.1 
77.0 
34.7 
0.968 0.625 
RMFN (no MFP) 
76.5 
76.5 
30.8 
0.998 0.582 
RMFN (no HIGHLIGHT) 77.9 
77.9 
35.9 
0.952 0.666 
RMFN 
78.4 
78.0 
38.3 
0.922 0.681 

</table></figure>

			<note place="foot" n="1"> Details on feature extraction are in supplementary.</note>

			<note place="foot" n="2"> Results for all individual baseline models are in supplementary. State-of-the-art (SOTA)1/2/3 represent the three best performing baseline models on each dataset.</note>

			<note place="foot" n="6"> Conclusion This paper proposed the Recurrent Multistage Fusion Network (RMFN) which decomposes the multimodal fusion problem into multiple stages, each focused on a subset of multimodal signals. Extensive experiments across three publicly-available datasets reveal that RMFN is highly effective in modeling human multimodal language. In addition to achieving state-of-the-art performance on all datasets, our comparisons and visualizations reveal that the multiple stages coordinate to capture both synchronous and asynchronous multimodal interactions. In future work, we are interested in merging our model with memory-based fusion methods since they have complementary strengths as discussed in subsection 5.1.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work partially sup-ported by the National Science Foundation (Award #1833355) and Samsung. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Samsung, and no official endorse-ment should be inferred. The authors thank Yao Chong Lim, Venkata Ramana Murthy Oruganti, Zhun Liu, Ying Shen, Volkan Cirik, and the anony-mous reviewers for their constructive comments on this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harika</forename><surname>Abburi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajendra</forename><surname>Prasath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suryakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gangashetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mining Intelligence and Knowledge Exploration</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1511.06464</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09406</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Real-time speech emotion and sentiment recognition for interactive dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Farhad Bin Siddique</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricky</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Affective computing and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis with wordlevel fusion and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 19th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial deep averaging networks for cross-lingual sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cardie</surname></persName>
		</author>
		<idno>abs/1606.01614</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Syntactic Structures. Mouton and Co</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
			<pubPlace>The Hague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Supportvector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Covarepa collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
	<note>Cognitive science</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML&apos;11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML&apos;11<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visual features for context-aware speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<idno>abs/1712.00489</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Tashev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning word-like units from joint audio-visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glass</surname></persName>
		</author>
		<idno>abs/1701.07481</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Trueskill™: A bayesian skill rating system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>B. Schölkopf, J. C. Platt, and T. Hoffman</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Facial expression analysis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Medsker</surname></persName>
		</author>
		<title level="m">Recurrent Neural Networks: Design and Applications</title>
		<meeting><address><addrLine>Boca Raton, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CRC Press, Inc</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Visually grounded learning of keyword prediction from untranscribed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Settle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno>abs/1703.08136</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A new view of language acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="11850" to="11857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Reusing neural speech representations for auditory emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Egor</forename><surname>Lakomkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Magg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<idno>abs/1803.11508</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal local-global ranking fusion for emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICMI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient lowrank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Character-based bidirectional lstm-crf with words and characters for japanese named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoki</forename><surname>Taniguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoko</forename><surname>Ohkuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Subword and Character Level Models in NLP</title>
		<meeting>the First Workshop on Subword and Character Level Models in NLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent-dynamic discriminative models for continuous gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnaz</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanth</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th</title>
		<meeting>the 18th</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">ACM International Conference on Multimodal Interaction, ICMI 2016</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emergence of multimodal action representations from neural network selforganization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><forename type="middle">I</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelius</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Systems Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="208" to="221" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><forename type="middle">Suk</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Multimodal Interaction, ICMI &apos;14</title>
		<meeting>the 16th International Conference on Multimodal Interaction, ICMI &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Seq2seq2sentiment: Multimodal sequence to sequence models for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</title>
		<meeting>Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="53" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterancelevel multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional mkl based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM), 2016 IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariadna</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sybor</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1848" to="1852" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Shyam Sundar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goecke</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cícero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maira</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Neural speech recognizer: Acoustic-to-word LSTM model for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<idno>abs/1610.09975</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-view latent variable discriminative models for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2120" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical sequence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3562" to="3569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Look, listen, and decode: Multimodal speech recognition with images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="573" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno>abs/1409.3215</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The global landscape of cognition: hierarchical aggregation as an organizational principle of human cortical networks and functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Hobbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Burroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Siegelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">18112</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<idno type="arXiv">arXiv:1806.06176</idno>
		<title level="m">Learning factorized multimodal representations</title>
		<editor>Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Select-additive learning: Improving cross-individual generalization in multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaksha</forename><surname>Meghawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05244</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<title level="m">A survey on multi-view learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Speaker identification on the scotus corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a. Memory fusion network for multiview sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ThirtySecond AAAI Conference on Artificial Intelligence</title>
		<meeting>the ThirtySecond AAAI Conference on Artificial Intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Prateek Vij, Erik Cambria, and Louis-Philippe Morency</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louisphilippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<title level="m">Recurrent Highway Networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
