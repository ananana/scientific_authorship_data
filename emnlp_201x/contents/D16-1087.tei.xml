<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Named Entity Recognition for Novel Types by Transfer Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Ferraro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Melbourne</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data61</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Australia</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Named Entity Recognition for Novel Types by Transfer Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="899" to="905"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In named entity recognition, we often don&apos;t have a large in-domain training corpus or a knowledge base with adequate coverage to train a model directly. In this paper, we propose a method where, given training data in a related domain with similar (but not identical) named entity (NE) types and a small amount of in-domain training data, we use transfer learning to learn a domain-specific NE model. That is, the novelty in the task setup is that we assume not just domain mismatch, but also label mismatch.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are two main approaches to named entity recog- nition (NER): (i) build sequence labelling models such as conditional random fields (CRFs) ( <ref type="bibr" target="#b11">Lafferty et al., 2001</ref>) on a large manually-labelled training cor- pus ( <ref type="bibr" target="#b7">Finkel et al., 2005</ref>); and (ii) exploit knowledge bases to recognise mentions of entities in text ( <ref type="bibr" target="#b17">Rizzo and Troncy, 2012;</ref><ref type="bibr" target="#b15">Mendes et al., 2011</ref>). For many social media-based or security-related applications, however, we cannot assume that we will have ac- cess to either of these. An alternative is to have a small amount of in-domain training data and access to large-scale annotated data in a second domain, and perform transfer learning over both the features and label set. This is the problem setting in this paper.</p><p>NER of novel named entity (NE) types poses two key challenges. First is the issue of sourcing labelled training data. Handcrafted features play a key role in supervised NER models <ref type="bibr" target="#b21">(Turian et al., 2010</ref>), but if we have only limited training amounts of training data, we will be hampered in our ability to reliably learn feature weights. Second, the absence of target NE types in the source domain makes transfer diffi- cult, as we cannot directly apply a model trained over the source domain to the target domain. <ref type="bibr" target="#b1">Alvarado et al. (2015)</ref> show that even if the NE label set is identical across domains, large discrepancies in the label distribution can lead to poor performance.</p><p>Despite these difficulties, it is possible to transfer knowledge between domains, as related NE types often share lexical and context features. For example, the expressions give lectures and attend tutorials of- ten occur near mentions of NE types PROFESSOR and STUDENT. If only PROFESSOR is observed in the source domain but we can infer that the two classes are similar, we can leverage the training data to learn an NER model for STUDENT. In practice, differences between NE classes are often more sub- tle than this, but if we can infer, for example, that the novel NE type STUDENT aligns with NE types PERSON and UNIVERSITY, we can compose the context features of PERSON and UNIVERSITY to induce a model for STUDENT.</p><p>In this paper, we propose a transfer learning-based approach to NER in novel domains with label mis- match over a source domain. We first train an NER model on a large source domain training corpus, and then learn the correlation between the source and tar- get NE types. In the last step, we reuse the model parameters of the second step to initialise a linear- chain CRF and fine tune it to learn domain-specific patterns. We show that our methods achieve up to 160% improvement in F-score over a strong baseline, based on only 125 target-domain training sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The main scenario where transfer learning has been applied to NER is domain adaptation <ref type="bibr" target="#b2">(Arnold et al., 2008;</ref><ref type="bibr" target="#b14">Maynard et al., 2001;</ref><ref type="bibr" target="#b4">Chiticariu et al., 2010)</ref>, where it is assumed that the label set Y is the same for both the source and target corpora, and only the domain varies. In our case, however, both the domain and the label set differ across datasets.</p><p>Similar to our work, <ref type="bibr" target="#b10">Kim et al. (2015)</ref> use transfer learning to deal with NER data sets with different label distributions. They use canonical correlation analysis (CCA) to induce label representations, and reduce the problem to one of domain adaptation. This supports two different label mappings: (i) to a coarse label set by clustering vector representations of the NE types, which are combined with mention-level predictions over the target domain to train a target domain model; and (ii) between labels based on the k nearest neighbours of each label type, and from this transferring a pre-trained model from the source to the target domain. They showed their automatic label mapping strategies attain better results than a manual mapping, with the pre-training approach achieving the best results. Similar conclusions were reached by <ref type="bibr" target="#b25">Yosinski et al. (2014)</ref>, who investigated the transferability of features from a deep neural net- work trained over the ImageNet data set. <ref type="bibr" target="#b19">Sutton and McCallum (2005)</ref> investigated how the target task affects the source task, and demonstrated that decod- ing for transfer is better than no transfer, and joint decoding is better than cascading decoding.</p><p>Another way of dealing with a lack of annotated NER data is to use distant supervision by exploiting knowledge bases to recognise mentions of entities ( <ref type="bibr" target="#b13">Ling and Weld, 2012;</ref><ref type="bibr" target="#b6">Dong et al., 2015;</ref><ref type="bibr" target="#b24">Yosef et al., 2013;</ref><ref type="bibr" target="#b0">Althobaiti et al., 2015;</ref><ref type="bibr" target="#b23">Yaghoobzadeh and Schütze, 2015)</ref>. Having a fine-grained entity typol- ogy has been shown to improve other tasks such as re- lation extraction <ref type="bibr" target="#b13">(Ling and Weld, 2012</ref>) and question answering ( <ref type="bibr" target="#b12">Lee et al., 2007)</ref>. Nevertheless, for many social media-based or security-related applications, we don't have access to a high-coverage knowledge base, meaning distant supervision is not appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Transfer Learning for NER</head><p>Our proposed approach TransInit consists of three steps: (1) we train a linear-chain CRF on a large source-domain corpus; (2) we learn the correlation between source NE types and target NE types using a two-layer neural network; and (3) we leverage the neural network to train a CRF for target NE types.</p><p>Given a word sequence x of length L, an NER system assigns each word x i a label y i ∈ Y, where the label space Y includes all observed NE types and a special category O for words without any NE type. Let (x, y) be a sequence of words and their labels. A linear-chain CRF takes the form:</p><formula xml:id="formula_0">1 Z L l=1 exp W f f (y l , x) + W g g(y l−1 , y l ) ,<label>(1)</label></formula><p>where f (y l , x) is a feature function depending only on x, and the feature function g(y l−1 , y l ) captures co-occurrence between adjunct labels. The feature functions are weighted by model parameters W, and Z serves as the partition function for normalisation. The source domain model is a linear-chain CRF trained on a labelled source corpus. The co- occurrence of target domain labels is easy to learn due to the small number of parameters (|Y | 2 ). Mostly such information is domain specific so that it is un- likely that the co-occurrence of two source types can be matched to the co-occurrence of the two tar- get types. However the feature functions f (y l , x) capture valuable information about the textual pat- terns associated with each source NE type. Without g(y l−1 , y l ), the linear-chain CRF is reduced to a lo- gistic regression (LR) model:</p><formula xml:id="formula_1">σ(y * , x i ; W f ) = exp(W f .y * f (y * i , x i )) y∈Y exp(W f .y f (y, x i ))</formula><p>. <ref type="formula">(2)</ref> In order to learn the correlation between source and target types, we formulate it as a predictive task by using the unnormalised probability of source types to predict the target types. Due to the simplifica- tion discussed above, we are able to extract a linear layer from the source domain, which takes the form a i = W s x i , where W s denotes the parameters of f (y l , x) in the source domain model, and each a i is the unnormalised probability for each source NE type. Taking a i as input, we employ a multi-class LR classifier to predict target types, which is essentially p(y |a) = σ(y , a i ; W t ), where y is the observed type. From another point of view, the whole architec- ture is a neural network with two linear layers.</p><p>We do not add any non-linear layers between these two linear layers because we otherwise end up with saturated activation functions. An activa- tion function is saturated if its input values are its max/min values <ref type="bibr" target="#b8">(Glorot and Bengio, 2010)</ref>. Taking</p><formula xml:id="formula_2">tanh(x) as an example, ∂tanh(z) ∂z = 1 − tanh 2 (z).</formula><p>If z is, for example, larger than 2, the correspond- ing derivative is smaller than 0.08. Assume that we have a three-layer neural network where z i de- notes the input of layer i, tanh(z) is the middle layer, and L(z i−2 ) is the loss function. We then</p><formula xml:id="formula_3">have ∂L(z i−2 ) ∂z i−2 = ∂L ∂z i+1 ∂ tanh(z i−1 ) ∂z i−1 ∂z i−1 ∂z i−2 .</formula><p>If the tanh layer is saturated, the gradient propagated to the lay- ers below will be small, and no learning based on back propagation will occur.</p><p>If no parameter update is required for the bottom linear layer, we will also not run into the issue of saturated activation functions. However, in our ex- periments, we find that parameter update is neces- sary for the bottom linear layer because of covariate shift ( <ref type="bibr" target="#b18">Sugiyama et al., 2007</ref>), which is caused by dis- crepancy in the distribution between the source and target domains. If the feature distribution differs be- tween domains, updating parameters is a straightfor- ward approach to adapt the model for new domains.</p><p>Although the two-layer neural network is capable of recognising target NE types, it has still two draw- backs. First, unlike a CRF, it doesn't include a label transition matrix. Second, the two-layer neural net- work has limited capacity if the domain discrepancy is large. If we rewrite the two-layer architecture in a compact way, we obtain:</p><formula xml:id="formula_4">p(y |x) = σ(y , x i ; W t W s ).<label>(3)</label></formula><p>As the equation suggests, if we minimize the negative log likelihood, the loss function is not convex. Thus, we could land in a non-optimal local minimum using online learning. The pre-trained parameter matrix W s imposes a special constraint that the computed scores for each target type are a weighted combina- tion of updated source type scores. If a target type shares nothing in common with source types, the pre-trained W s does more harm than good.</p><p>In the last step, we initialise the model parameters of a linear-chain CRF for f (y l , x) using the model parameters from the previous step. Based on the architecture of the NN model, we can collapse the two linear transformations into one by:</p><formula xml:id="formula_5">W f = W t W s ,<label>(4)</label></formula><p>while initialising the other parameters of the CRF to zero. After this transformation, each initialised parameter vector W f .y is a weighted linear combina- tion of the updated parameter vectors of the source types. Compared to the second step, the loss func- tion we have now is convex because it is exactly a linear-chain CRF. Our previous steps have provided guided initialization of the parameters by incorpo- rating source domain knowledge. The model also has significantly more freedom to adapt itself to the target types. In other words, collapsing the two ma- trices simplifies the learning task and removes the constraints imposed by the pre-trained W s .</p><p>Because the tokens of the class O are generally several orders of magnitude more frequent than the tokens of the NE types, and also because of covariate shift, we found that the predictions of the NN mod- els are biased towards the class O (i.e. a non-NE). As a result, the parameters of each NE type will al- ways include or be dominated by the parameters of O after initialisation. To ameliorate this effect, we renormalise W t before applying the transformation, as in Equation (4). We do not include the parameters of the source class O when we initialise parameters of the NE types, while copying the parameters of the source class O to the target class O. In particular, let o be the index of source domain class O. For each parameter vector W t i * of NE type, we set W t io = 0. For the parameter vector for the target class O, we set only the element corresponding to the weight be- tween source type O and target class O to 1, and other elements to 0.</p><p>Finally, we fine-tune the model over the target do- main by maximising log likelihood. The training objective is convex, and thus the local optimum is also the global optimum. If we fully train the model, we will achieve the same model as if we trained from scratch over only the target domain. As the knowl- edge of the source domain is hidden in the initial weights, we want to keep the initial weights as long as they contribute to the predictive task. Therefore, we apply AdaGrad ( <ref type="bibr" target="#b17">Rizzo and Troncy, 2012</ref>) with early stopping based on development data, so that the knowledge of the source domain is preserved as much as possible.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Datasets</head><p>We use CADEC ( <ref type="bibr" target="#b9">Karimi et al., 2015</ref>) and I2B2 (Ben Abacha and Zweigenbaum, 2011) as target cor- pora with the standard training and test splits. From each training set, we hold out 10% as the devel- opment set. As source corpora, we adopt CoNLL (Tjong Kim <ref type="bibr" target="#b20">Sang and De Meulder, 2003)</ref> and <ref type="bibr">BBN (Weischedel and Brunstein, 2005)</ref>.</p><p>In order to test the impact of the target domain training data size on results, we split the training set of CADEC and I2B2 into 10 partitions based on a log scale, and created 10 successively larger training sets by merging these partitions from smallest to largest (with the final merge resulting in the full training set). For all methods, we report the macro-averaged F1 over only the NE classes that are novel to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our methods with the following two in-domain baselines, one cross-domain data-based method, and three cross-domain transfer-based benchmark methods.</p><p>BOW: an in-domain linear-chain CRF with hand- crafted features, from <ref type="bibr" target="#b16">Qu et al. (2015)</ref>.</p><p>Embed: an in-domain linear-chain CRF with hand- crafted features and pre-trained word embeddings, from <ref type="bibr" target="#b16">Qu et al. (2015)</ref>.</p><p>LabelEmbed: take the labels in the source and tar- get domains, and determine the alignment based on the similarity between the pre-trained embeddings for each label.</p><p>CCA: the method of <ref type="bibr" target="#b10">Kim et al. (2015)</ref>, where a one-to-one mapping is generated between source and target NE classes using CCA and k-NN (see Sec- tion 2). We compare our method with one variation, which is to freeze the parameters of the bottom linear layer and update only the parameters of the LR classifier while learning the correlation between the source and target types. <ref type="figure" target="#fig_2">Figure 1</ref> shows the macro-averaged F1 of novel types between our method TransInit and the three baselines on all target corpora. The evaluation re- sults on CADEC with BBN as the source corpus are not reported here because BBN contains all types of CADEC. From the figure we can see that TransInit outperforms all other methods with a wide margin on I2B2. When CoNLL is taken as the source cor- pus, despite not sharing any NE types with I2B2, several target types are subclasses of source types: DOCTOR and PATIENT w.r.t. PERSON, and HOS-PITAL w.r.t. ORGANIZATION.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>In order to verify if TransInit is able to capture semantic relatedness between source and target NE types, we inspected the parameter matrix W t of the LR classifier in the step of learning type correlations. The corresponding elements in W t indeed receive much higher values than the semantically-unrelated NE type pairs. When less than 300 target training sentences are used, these automatically discovered positive correlations directly lead to 10 times higher F1 scores for these types than the baseline Embed, which does not have a transfer learning step. Since TransInit is able to transfer the knowledge of multi- ple source types to related target types, this advantage leads to more than 10% improvement in terms of F1 score on these types compared with LabelEmbed, given merely 268 training sentences in I2B2. We also observe that, in case of few target training exam- ples, LabelEmbed is more robust than CCA if the correlation of types can be inferred from their names.</p><p>We study the effects of transferring a large num- ber of source types to target types by using BBN, which has 64 types. Here, the novel types of I2B2 w.r.t. BBN are DOCTOR, PATIENT, HOSPITAL, PHONE, and ID. For these types, TransInit success- fully recognises PERSON as the most related type to DOCTOR, as well as CARDINAL as the most related type to ID. In contrast, CCA often fails to identify meaningful type alignments, especially for small training data sizes.</p><p>CADEC is definitely the most challenging task when trained on CoNLL, because there is no se- mantic connection between two of the target NE types (DRUG and DISEASE) and any of the source NE types. In this case, the baseline LabelEmbed achieves competitive results with TransInit. This suggests that the class names reflect semantic corre- lations between source and target types, and there are not many shared textual patterns between any pair of source and target NE types in the respective datasets.</p><p>Even with a complex model such as a neural net- work, the transfer of knowledge from the source types to the target types is not an easy task. <ref type="figure" target="#fig_5">Figure 2</ref> shows that with a three-layer neural network, the whole model performs poorly. This is due to the fact that the hard tanh layer suffers from saturated function values. We inspected the values of the output hidden  units computed by W s x on a random sample of tar- get training examples before training on the target corpora. Most values are either highly positive or negative, which is challenging for online learning algorithms. This is due to the fact that these hid- den units are unnormalised probabilities produced by the source domain classifier. Therefore, remov- ing the hidden non-linear-layer layer leads to a dra- matic performance improvement. Moreover, <ref type="figure" target="#fig_5">Figure 2</ref> also shows that further performance improvement is achieved by reducing the two-layer architecture into a linear chain CRF. And updating the hidden layers leads to up to 27% higher F1 scores than not updating them in the second step of TransInit, which indicates that the neural networks need to update lower-level features to overcome the covariate shift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed TransInit, a transfer learning- based method that supports the training of NER mod- els across datasets where there are mismatches in domain and also possibly the label set. Our method was shown to achieve up to 160% improvement in F1 over competitive baselines, based on a handful of in-domain training instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F1</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Macro-averaged F1 results across all novel classes on different source/target domain combinations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>TransDeepCRF: A three-layer deep CRF. The bot- tom layer is a linear layer initialised with W s from the source domain-trained CRF. The middle layer is a hard tanh function (Collobert et al., 2011). The top layer is a linear-chain CRF with all parameters initialised to zero. TwoLayerCRF: A two-layer CRF. The bottom layer is a linear layer initialised with W s from the source domain-trained CRF. The top layer is a linear-chain CRF with all parameters initialised to zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difficulty of Transfer. The source model is trained on BBN.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NICTA, funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Pro-gram.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining minimally-supervised methods for arabic named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maha</forename><surname>Althobaiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udo</forename><surname>Kruschwitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="243" to="255" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaption of named entity recognition to support credit risk assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio Cesar Salinas</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Language Technology Association Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting feature hierarchy for transfer learning in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="245" to="253" />
		</imprint>
	</monogr>
	<note>Cohen</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Medical entity recognition: A comparison of semantic and statistical methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Asma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zweigenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BioNLP 2011 Workshop</title>
		<meeting>BioNLP 2011 Workshop</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation of rule-based annotators for namedentity recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Chiticariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajasekar</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivakumar</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid neural model for type classification of entity mentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1243" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trond</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cadec: A corpus of adverse drug event annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Metke-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madonna</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="73" to="81" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">New transfer learning techniques for disparate label sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young-Bum</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruhi</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwoo</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grained named entity recognition and relation extraction for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Gyu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Gil</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="799" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 26th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition from diverse text types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Maynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Tablan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Ursu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamish</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DBpedia spotlight: shedding light on the web of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Pablo N Mendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>García-Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Semantic Systems</title>
		<meeting>the 7th International Conference on Semantic Systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Big data small data, in domain out-of domain, known word unknown word: The impact of word representations on sequence labelling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Computational Natural Language Learning</title>
		<meeting>the 19th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="83" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">NERD: a framework for unifying named entity recognition and disambiguation extraction tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphaël</forename><surname>Troncy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Composition of conditional random fields for transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</title>
		<meeting>the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT &apos;05</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="748" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Word representations: a simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">BBN pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Corpus-level fine-grained entity typing using contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadollah</forename><surname>Yaghoobzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="715" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HYENAlive: Fine-grained online entity type classification from natural-language text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Amir Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio, and Hod Lipson</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
