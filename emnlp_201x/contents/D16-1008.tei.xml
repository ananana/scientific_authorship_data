<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Recognize Discontiguous Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>November 1-5, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><forename type="middle">Obaja</forename><surname>Muis</surname></persName>
							<email>{aldrian muis,luwei}@sutd.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Recognize Discontiguous Entities</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="75" to="84"/>
							<date type="published">November 1-5, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper focuses on the study of recognizing discontiguous entities. Motivated by a previous work, we propose to use a novel hyper-graph representation to jointly encode discon-tiguous entities of unbounded length, which can overlap with one another. To compare with existing approaches, we first formally introduce the notion of model ambiguity, which defines the difficulty level of interpreting the outputs of a model, and then formally analyze the theoretical advantages of our model over previous existing approaches based on linear-chain CRFs. Our empirical results also show that our model is able to achieve significantly better results when evaluated on standard data with many discontiguous entities.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Building effective automatic named entity recogni- tion (NER) systems that is capable of extracting useful semantic shallow information from texts has been one of the most important tasks in the field of natural language processing. An effective NER sys- tem can typically play an important role in certain downstream NLP tasks such as relation extraction, event extraction, and knowledge base construction ( <ref type="bibr" target="#b7">Hasegawa et al., 2004</ref>; Al-Rfou and Skiena, 2012).</p><p>Most traditional NER systems are capable of ex- tracting entities 1 as short spans of texts. Two ba- sic assumptions are typically made when extract-  ing entities: 1) entities do not overlap with one an- other, and 2) each entity consists of a contiguous se- quence of words. These assumptions allow the task to be modeled as a sequence labeling task, for which many existing models are readily available, such as linear-chain CRFs <ref type="bibr" target="#b10">(McCallum and Li, 2003)</ref>.</p><p>While the above two assumptions are valid for most cases, they are not always true. For example, in the entity University of New Hampshire of type ORG there exists another entity New Hampshire of type LOC. This violates the first assumption above, yet it is crucial to extract both entities for subsequent tasks such as relation extraction and knowledge base construction. Researchers therefore have proposed to tackle the above issues in NER using more so- phisticated models <ref type="bibr" target="#b5">(Finkel and Manning, 2009;</ref><ref type="bibr" target="#b9">Lu and Roth, 2015)</ref>. Such efforts still largely rely on the second assumption.</p><p>Unfortunately, the second assumption is also not always true in practice. There are also cases where the entities are composed of multiple discontiguous sequences of words, such as in disorder mention recognition in clinical texts ( <ref type="bibr" target="#b14">Pradhan et al., 2014b)</ref>, where the entities (disorder mentions in this case) may be discontiguous. Consider the example shown in <ref type="figure" target="#fig_1">Figure 1</ref>. In this example there are four enti-ties, the first one, hiatal hernia, is a conventional contiguous entity. The second one, laceration ... esophagus, is a discontiguous entity, consisting of two parts. The third and fourth ones, blood in stom- ach and stomach ... lac (for stomach laceration), are overlapping with each other, with the fourth be- ing discontiguous at the same time.</p><p>For such discontiguous entities which can poten- tially overlap with other entities in complex man- ners, existing approaches such as those based on simple sequence tagging models have difficulties handling them accurately. This stems from the fact that there is a very large number of possible entity combinations in a sentence when the entities can be discontiguous and overlapping.</p><p>Motivated by this, in this paper we propose a novel model that can better represent both contigu- ous and discontiguous entities which can overlap with one another. Our major contributions can be summarized as follows:</p><p>• We propose a novel model that is able to repre- sent both contiguous and discontiguous entities.</p><p>• Theoretically, we introduce the notion of model ambiguity for quantifying the ambiguity of dif- ferent NER models that can handle discontigu- ous entities. We present a study and make com- parisons about different models' ambiguity un- der this theoretical framework.</p><p>• Empirically, we demonstrate that our model can significantly outperform conventional ap- proaches designed for handling discontiguous entities on data which contains many discontigu- ous entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning to recognize named entities is a popular task in the field of natural language processing. A survey by <ref type="bibr" target="#b12">Nadeau (2007)</ref>   <ref type="bibr" target="#b1">and Matsumoto, 2003)</ref>, and also semi-supervised and unsupervised approaches. Ratinov (2009) uti- lizes averaged perceptron to solve this problem and also focused on four key design decisions, achiev- ing state-of-the-art in MUC-7 dataset. These ap- proaches work on standard texts, such as news ar- ticles, and the entities to be recognized are defined to be contiguous and non-overlapping. Noticing that many named entities contain other named entities inside them, <ref type="bibr" target="#b5">Finkel and Manning (2009)</ref> proposed a model that is capable of extract- ing nested named entities by representing the sen- tence as a constituency parse tree, with named enti- ties as phrases. As a parsing-based model, the ap- proach has a time complexity that is cubic in the number of words in the sentence.</p><p>Recently, <ref type="bibr" target="#b9">Lu and Roth (2015)</ref> proposed a model that can represent overlapping entities. In addition to supporting nested entities, theoretically this model can also represent overlapping entities where nei- ther is nested in another. The model represents each sentence as a hypergraph with nodes indicating en- tity types and boundaries. Compared to the previ- ous model, this model has a lower time complexity, which is linear in the number of words in the sen- tence.</p><p>All the above models focus on NER in conven- tional texts, where the assumption of contiguous en- tities is valid. In the past few years, there is a grow- ing body of works on recognizing disorder mentions in clinical text. These disorder mentions may be discontiguous and also overlapping. To tackle such an issue, a research group from University of Texas Health Science Center at Houston ( <ref type="bibr" target="#b19">Tang et al., 2013;</ref><ref type="bibr" target="#b22">Zhang et al., 2014;</ref><ref type="bibr" target="#b21">Xu et al., 2015</ref>) first utilized a conventional linear-chain CRF to recognize disorder mention parts by extending the standard BIO (Begin, Inside, Outside) format, and next did some postpro- cessing to combine different components. Though effective, as we will see later, such a model comes with some drawbacks. Nevertheless, their work mo- tivated us to perform further analysis on this issue and propose a novel model specifically designed for discontiguous entity extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear-chain CRF Model</head><p>Before we present our approach, we would like to spend some time to discuss a simple approach based on linear-chain CRFs ( <ref type="bibr" target="#b8">Lafferty et al., 2001</ref>). This approach is primarily based on the system by <ref type="bibr" target="#b19">Tang et al. (2013)</ref>, and this will be the baseline system EGD showed hiatal <ref type="bibr">[B]</ref> hernia <ref type="bibr">[I]</ref> and vertical laceration <ref type="bibr">[BD]</ref> in distal esophagus <ref type="bibr">[BD]</ref> with blood <ref type="bibr">[B]</ref> in <ref type="bibr">[I]</ref> stomach <ref type="bibr">[BH]</ref> and overlying lac <ref type="bibr">[BD]</ref> .</p><p>Infarctions <ref type="bibr">[BH]</ref> either water <ref type="bibr">[BD]</ref> shed <ref type="bibr">[ID]</ref> or embolic <ref type="bibr">[BD]</ref>  that we will make comparison with in later sections.</p><p>The problem is regarded as a sequence prediction task, where each word is assigned a label similar to BIO format often used for NER. We used the en- coding used by <ref type="bibr" target="#b19">Tang et al. (2013)</ref>, which uses 7 tags to handle entities that can be discontiguous and overlapping. Specifically, we used B, I, O, BD, ID, BH, and IH to denote Beginning of entity, Inside en- tity, Outside of entity, Beginning of Discontiguous entity, Inside of Discontiguous entity, Beginning of Head, and Inside of Head. To encode a sentence in this format, first we identify the contiguous word sequences which are parts of multiple entities. We call these head components and we label each word inside each component with BH (for the first word in each component) or IH. Then we find contiguous word sequences which are parts of a discontiguous entity, which we call the body components. Words inside those components which have not been la- beled are labeled with BD (for the first word in each component) or ID. Finally, words that are parts of a contiguous entity are called contiguous component, and, if they have not been labeled, are labeled as B (for the first word in each component) or I.</p><p>This encoding is lossy, since the information on which parts constitute the same entity is lost. The top example in <ref type="figure" target="#fig_2">Figure 2</ref> is the encoding of the example shown in <ref type="figure" target="#fig_1">Figure 1</ref>. During decod- ing, based on the labels only it is not entirely clear whether "laceration" should be combined with "esophagus" or with "stomach" to form a single mention. For the bottom example, we cannot deduce that "Infarctions" alone is a mention, since there is no difference in the encoding of a sentence with only two mentions {"Infarctions . . . water shed", "Infarctions . . . embolic"} or having three mentions with "Infarctions" as another mention, since in both cases, the word "Infarctions" is labeled with BH.</p><p>Also, it should be noted that some of the label se- quences are not valid. For example, a sentence in which there is only one word labeled as BD is in- valid, since a discontiguous entity requires at least two words to be labeled as BD or BH. This is, how- ever, a possible output from the linear CRF model, due to the Markov assumption inherent in linear CRF models. Later we see that our models do not have this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Model</head><p>Linear-chain CRF models are limited in their repre- sentational power when handling complex entities, especially when they can be discontiguous and can overlap with one another. While recent models have been proposed to effectively handle overlapping en- tities, how to effectively handle discontiguous en- tities remains a research question to be answered. Motivated by previous efforts on handling overlap- ping entities ( <ref type="bibr" target="#b9">Lu and Roth, 2015)</ref>, in this work we propose a model based on hypergraphs that can bet- ter represent entities that can be discontiguous and at the same time be overlapping with each other.</p><p>Unlike the previous work ( <ref type="bibr" target="#b9">Lu and Roth, 2015)</ref>, we establish a novel theoretical framework to formally quantify the ambiguity of our hypergraph-based models and justify their effectiveness by making comparisons with the linear-chain CRF approach. Now let us introduce our novel hypergraph rep- resentation. A hypergraph can be used to represent entities of different types and their combinations in a given sentence. Specifically, a hypergraph is con- structed as follows. For the word at position k, we have the following nodes:</p><p>• A k : this node represents all entities that begin with the current or a future word (to the right of the current word).</p><p>• E k : this node represents all entities that begin with the current word.</p><p>• T k t : this node represents entities of certain spe- cific type t that begin with the current word. There is one T k t for each different type. • B k t,i : this node indicates that the current word is part of the i-th component of an entity of type t.</p><p>• O k t,i : this node indicates that the current word appears in between (i-1)-th and i-th components of an entity of type t.</p><p>There is also a special leaf node, X-node, which indicates the end (i.e., right boundary) of an entity.</p><p>The nodes are connected by directed hyperedges, which for the purpose of explaining our models are defined as those edges that connect one node, called the parent node, to one or more child nodes. For ease of notation, in the rest of this paper we use edge to refer to directed hyperedge.</p><p>The edges Each A k is a parent to E k and A k+1 , encoding the fact that the set of all entities at position k is the union of the set of entities starting exactly at current position (E k ) with the set of entities starting at or after position k + 1 (A k+1 ).</p><p>Each</p><formula xml:id="formula_0">E k is a parent to T k 1 , . . . , T k T ,</formula><p>where T is the total number of possible types that we consider. Each T k t has two edges where it serves as a parent, within one it is parent to B k t,0 and within another it is to X. These edges encode the fact that at position k, either there is an entity of type t that begins with the current word (to B k t,0 ), or there is no entity of type t that begins with the current word (to X).</p><p>In the full hypergraph, each B k t,i is a parent to B k+1 t,i (encoding the fact that the next word also be- longs to the same component of the same entity), to O k+1 t,i+1 (encoding the fact that this word is part of a discontiguous entity, and the next word is the first word separating current component and the next component), and to X (representing that the entity ends at this word). Also there are edges with all pos- sible combinations of B k+1 t,i , O k+1 t,i+1 , and X as the child nodes, representing overlapping entities. For example, the edge B k t,i → (B k+1 t,i ,X ) denotes that there is an entity which continues to the next word (the edge to B k+1 t,i ), while there is another entity end- ing at k-th word (the edge to X). In total there are 7 edges in which B k t,i is a parent, which are: </p><formula xml:id="formula_1">• B k t,i → (X) • B k t,i → (O k+1 t,i+1 ) • B k t,i → (O k+1 t,i+1 , X) • B k t,i → (B k+1 t,i ) • B k t,i → (B k+1 t,i , X) • B k t,i → (B k+1 t,i , O k+1 t,i+1 ) • B k t,i → (B k+1 t,i , O k+1 t,i+1 , X) Analogously, O k t,i has three edges that connect to A A A A A A E E E E E E</formula><formula xml:id="formula_2">O k+1 t,i , B k+1 t,i+1</formula><p>, and both. Note that O k t,i is not a par- ent to X by definition.</p><p>During testing, the model will predict a subgraph which will result in the predicted entities after de- coding. We call this subgraph representing certain entity combination entity-encoded hypergraph.</p><p>For example, <ref type="figure" target="#fig_3">Figure 3</ref> shows the entity-encoded hypergraph of our model encoding the three men- tions in the second example in <ref type="figure" target="#fig_5">Figure 4</ref>. The edge from the T-node for the first word to the B-node for the first word shows that there is at least one entity starting with this word. The three places where an X-node is connected to a B-node show the end of the three entities. Note that this hypergraph clearly shows the presence of the three mentions without ambiguity, unlike a linear-chain encoding of this ex- ample where it cannot be inferred that "Infarctions" alone is a mention, as discussed previously. In this paper, we set the maximum number of components to be 3 since the dataset does not contain any men- tion with more than 3 components.</p><p>Also note that this model supports discontiguous and overlapping mentions of different types since each type has its own set of O-nodes and B-nodes, unlike the linear-chain model, which supports only overlapping mentions of the same type.</p><p>We also experimented with a variant of this model, where we split the T-nodes, B-nodes, and O-nodes further according to the number of com- ponents. We split B k t,i into B k t,i,j , i = 1 . . . j, j = 1 . . . 3 which represents that the word is part of the i-th component of a mention with total j compo- nents. Similarly we split O k t,i into O k t,i,j and T k t into T k t,j . We call the original version SHARED model, and this variant SPLIT model. The motivation for this variant is that the majority of overlaps in the data are between discontiguous and contiguous enti- ties, and so splitting the two cases -one component (contiguous) and more (discontiguous) -will reduce ambiguity for those cases.</p><p>These models are still ambiguous to some degree, for example when an O-node has two child nodes and two parents, we cannot decide which of the par- ent node is paired with which child node. However, in this paper we argue that:</p><p>• This model is less ambiguous compared to the linear-chain model, as we will show later theo- retically and empirically.</p><p>• Every output of our model is a valid prediction, unlike the linear-chain model since this model will always produce a valid path from T-nodes to the X-nodes representing some entities.</p><p>We will also show through experiments that our models can encode the entities more accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Interpreting Output Structures</head><p>Both the linear-chain CRF model and our models are still ambiguous to some degree, so we need to handle the ambiguity in interpreting the output structures into entities. For all models, we define two gen- eral heuristics: ENOUGH and ALL. The ENOUGH heuristic handles ambiguity by trying to produce a minimal set of entities which encodes to the one pro- duced by the model, while ALL heuristic handles ambiguity by producing the union of all possible en- tity combinations that encode to the one produced by the model. For more details on how these heuris- tics are implemented for each model, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>For both models, the training follows a log-linear formulation, by maximizing the loglikelihood of the training data D:</p><formula xml:id="formula_3">L(D) = (x,y)∈D   e∈E(x,y) w T f (e) − log Z w (x)   −λ||w|| 2</formula><p>Here (x, y) is a training instance consisting of the sentence x and the entity-encoded hypergraph y ∈ Y where Y is the set of all possible mention- encoded hypergraphs. The vector w consists of fea- ture weights, which are the model parameters to be learned. The set E(x, y) consists of all edges present in the entity-encoded hypergraph y for input x. The function f (e) returns the features defined over the edge e, Z w (x) is the normalization term which gives the sum of scores over all possible entity-encoded hypergraphs in Y that is relevant to the input x, and finally λ is the 2 -regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Ambiguity</head><p>The main aim of this paper is to assess how well each model can represent the discontiguous entities, even in the presence of overlapping entities.</p><p>In this section, we will theoretically compare the models' ambiguity, which is defined as the aver- age number of mention combinations that map to the same encoding in a model. Now, to compare two models, instead of calculating the ambiguity di- rectly, we can calculate the relative ambiguity be- tween the two models directly by comparing the number of canonical encodings in the two models.</p><p>A canonical encoding is a fixed, selected repre- sentation of a particular set of mentions in a sen- tence, among (possibly) several alternative represen- tations. Several alternatives may be present due to the ambiguity of the encoding-decoding process and also since the output of the model is not restricted to a specific rule. For example, for the text "John Smith", a model trained in BIO format might output "B-PER I-PER" or "I-PER I-PER", and both will still mean that "John Smith" is a person, although the "correct" encoding would of course be "B-PER I-PER", which is selected as the canonical encoding. Intuitively, a canonical encoding is a formal way to say that we only consider the "correct" encodings.</p><p>A model with larger number of canonical encod- ings will, on average, have less ambiguity compared to the one with smaller number of canonical encod- ings. Subsequently, a model with less ambiguity will be more precise in predicting entities.</p><p>Let M LI (n), M SH (n), M SP (n) denote the num- ber of canonical encodings of the linear-chain, SHARED, and SPLIT model, respectively, for a sen-tence with n words. Then we formally define the relative ambiguity of model M 1 over model M 2 , A r (M 1 , M 2 ), as follows:</p><formula xml:id="formula_4">A r (M 1 , M 2 ) = lim n→∞ log n i=1 M M 2 (i) log n i=1 M M 1 (i)<label>(1)</label></formula><p>A r (M 1 , M 2 ) &gt; 1 means model M 1 is more am- biguous than M 2 . Now, we claim the following:</p><formula xml:id="formula_5">Theorem 4.1. A r (LI, SH) &gt; 1</formula><p>We provide a proof sketch below. Due to space limitation, we cannot provide the full dynamic pro- gramming calculation. We refer the reader to the supplementary material for the details.</p><p>Proof Sketch The number of canonical encodings in the linear-chain model is less than 7 n since there are 7 possible tags for each of the n words and not all of the 7 n tag sequences are canonical encodings. So we have M LI (n) &lt; 7 n and thus we can derive log n i=1 M LI (i) &lt; 3n log 2. For our models, by employing some dynamic pro- gramming adapted from the inside algorithm <ref type="bibr" target="#b2">(Baker, 1979)</ref>, we can calculate the growth order of the num- ber of canonical encodings for SHARED model to ar- rive at a conclusion that ∀n &gt; n 0 , n i=1 M SH (i) &gt; C · 2 10n for some constants n 0 , C. Then we have:</p><p>A r (LI, SH) ≥ lim n→∞ log C +10n log 2 3n log 2 = 10 3 &gt; 1 (2) Theorem 4.1 says that the linear-chain model is more ambiguous compared to our SHARED model. Similarly, we can also establish A r (SH, SP) &gt; 1. Later we also see this empirically from experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data</head><p>To allow us to conduct experiments to empirically assess different models' capability in handling en- tities that can be discontiguous and can potentially overlap with one another, we need a text corpus an- notated with entities which can be discontiguous and overlapping with other entities. We found the largest of such corpus to be the dataset from the task to recognize disorder mentions in clinical text, initially organized by ShARe/CLEF eHealth Evaluation Lab (SHEL) in <ref type="bibr">(Suominen et al., 2013</ref> and contin- ued in <ref type="bibr">SemEval-2014</ref><ref type="bibr" target="#b13">(Pradhan et al., 2014a</ref>). The definition of the task is to recognize men- tions of concepts that belong to the Unified Medi- cal Language System (UMLS) semantic group dis- orders from a set of clinical texts. Each text has been annotated with a list of disorder mentions by two professional coders trained for this task, followed by an open adjudication step <ref type="bibr">(Suominen et al., 2013</ref>).</p><p>Unfortunately, even in this dataset, only 8.95% of the mentions are discontiguous. Working directly on such data would prevent us from understanding the true effectiveness of different models when han- dling entities which can be discontiguous and over- lapping. In order to truly understand how different models behave on data with discontiguous entities, we consider a subset of the data where we consider those sentences which contain at least one discon- tiguous entity. We call the resulting subset the "Dis- contiguous" subset of the "Original" dataset. Later we will also still use the training data of the "Origi- nal" dataset in the experiments.</p><p>Note that this "Discontiguous" subset still con- tains contiguous entities since a sentence usually contains more than one entity. The subset is a bal- anced dataset with 53.61% of the entities being dis- contiguous and the rest contiguous. We then split this dataset into training, development, and test set, according to the split given in SemEval 2014 setting (henceforth LARGE dataset). To see the impact of dataset size, we also experiment on a subset of the LARGE dataset, following the SHEL 2013 setting, with the development set in the LARGE dataset used as test set (henceforth SMALL dataset). The training and development set of the SMALL dataset comes from a random 80% (Tr80) and 20% (Tr20) split of the training set in LARGE dataset.</p><p>The statistics of the datasets, including the num- ber of overlaps between the entities in the "All" col- umn, are shown in <ref type="table">Table 1</ref>.</p><p>We note that this dataset only contains one type of entity. In later experiments, in order to evaluate the models on multiple types, we create another dataset where we split the entities based on the entity-level semantic category. This information is available for some entities through the Concept Unique Identifier (CUI) annotation in the data. In total we have three types: two types (type A and B) based on the seman- tic category, and one type (type N) for those entities Split #Sentences <ref type="table">Train  534 544  607  44 1,195 205  58  -Tr80  416 448  476  33 957 164  48  -Tr20  118  96  131  11 238 41  10  Dev  303 357  421  18 796 240  28  Test  430 584  610  16 1,210 327  61   Table 1</ref>: The statistics of the data. Tr80 and Tr20 refers to the 80% and 20% partitions of the full training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of mentions #Overlaps 1 part 2 parts 3 parts Total All Diff</head><p>having no semantic category information 2 . See the supplementary material for more details. The num- ber of overlaps between different types is shown in the "Diff" column in <ref type="table">Table 1</ref>. Except for a handful overlaps in development set, all overlaps involve at least one discontiguous entity. Our main result will still be based on the dataset with one type of entity.   <ref type="figure" target="#fig_5">Figure 4</ref> shows some examples of the mentions. The first example shows two discontiguous men- tions that do not overlap. The second example shows a typical discontiguous and overlapping case. The last example shows a very hard case of overlapping 2 It is tempting to just ignore these entities since the N type does not convey any specific information about the entities in it. However, due to the dataset size, excluding this type will lead to very small number of interactions between types. So we decided to keep this type and discontiguous mentions, as each of the compo- nents in {blood, dark, black material} is paired with each of the word in {vomit, bowel movement}, re- sulting in six mentions in total, with one having three components (dark . . . material . . . vomit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Features</head><p>Motivated by the features used by <ref type="bibr" target="#b22">Zhang et al. (2014)</ref>, for both the linear-chain CRF model and our models we use the following features: neigh- bouring words with relative position information (we consider previous and next k words, where k=1, 2, 3), neighbouring words with relative posi- tion information paired with the current word, word n-grams containing the current word (n=2,3), POS tag for the current word, POS tag n-grams con- taining the current word (n=2,3), orthographic fea- tures <ref type="bibr">(prefix, suffix, capitalization, lemma)</ref>, note type (discharge summary, echo report, radiology, and ECG report), section name (e.g. Medications, Past Medical History) 3 , Brown cluster, and word- level semantic category information <ref type="bibr">4</ref> . We used Stan- ford POS tagger ( <ref type="bibr" target="#b20">Toutanova et al., 2003</ref>) for POS tagging, and NLP4J package 5 for lemmatization. For Brown cluster features, following <ref type="bibr" target="#b19">Tang et al. (2013)</ref>, we used 1,000 clusters from the combina- tion of training, development, and test set, and used all the subpaths of the cluster IDs as features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Setup</head><p>We evaluated the three models on the SMALL dataset and the LARGE dataset.</p><p>Note that in both the SMALL and LARGE dataset, about half of all mentions are discontiguous, both in training and test set. We also want to see whether training on a set where the majority of the mentions are contiguous will affect the performance on rec- ognizing discontiguous mentions. So we also per- formed another experiment where we trained each model on the original training set where the major- ity of the entities are contiguous. We refer to this original dataset as "Train-Orig" (it contains 10,405 sentences, including those with no entities) and the SMALL LARGE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train-Disc</head><p>Train-Orig Train- <ref type="table">Disc  Train-Orig  P  R  F1  P  R  F1  P  R  F1  P  R  F1</ref> LI-ENH 59.7 39.8 47.8 71.0 45.8 55.7 54.7 41.2 47.0 64.1 46.5 53.9 LI-ALL 16.6 43.5 24.1 55.5 49.2 52.2 15.2 44.9 22.7 52.8 49.4 51.1 SH-ENH 85.9 39.7 54.3 82.2 48.0 60.6 76.9 40.1 52.7 73.9 49.1 59.0 SH-ALL 85.9 39.7 54.3 82.2 48.0 60.6 76.0 40.5 52.8 73.4 49.5 59.1 SP-ENH 86.7 37.8 52.7 82.5 48.0 60.7 79.4 38.6 52.0 75.3 48.8 59.2 SP-ALL 86.7 37.8 52.7 82.5 48.0 60.7 79.4 38.6 52.0 75.3 48.8 59.2 <ref type="table">Table 2</ref>: Results on the two datasets and two different training data after optimizing regularization hyperparameter λ in development set. The -ENH and -ALL suffixes refer to the ENOUGH and ALL heuristics. The best result in each column is put in boldface.</p><p>earlier one as "Train-Disc".</p><p>First we trained each model on the training set, varying the regularization hyperparameter λ, 6 then the λ with best result in the development set using the respective ENOUGH heuristic for each model is chosen for final result in the test set.</p><p>For each experiment setting, we show precision (P), recall (R) and F1 measure. Precision is the percentage of the mentions predicted by the model which are correct, recall is the percentage of men- tions in the dataset correctly discovered by the model, and F1 measure is the harmonic mean of pre- cision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results and Discussions</head><p>The full results are recorded in <ref type="table">Table 2</ref>.</p><p>We see that in general our models have higher pre- cision compared to the linear-chain baseline. This is expected, since our models have less ambiguity, which means that from a given output structure it is easier in our model to get the correct interpretation. We will explore this more in Section 5.5.</p><p>The ALL heuristic, as expected, results in higher recall, and this is more pronounced in the linear- chain model, with up to 4% increase from the ENOUGH heuristic, achieving the highest recall in three out of four settings. The high recall of the ALL heuristic in the linear-chain model can be explained by the high level of ambiguity the model has. Since it has more ambiguity compared to our models, one label sequence predicted by the model produces a lot of entities, and so it is more likely to overlap with the gold entities. But this has the drawback of very low precision as we can see in the result.</p><p>We see switching from one heuristic to the other <ref type="bibr">6</ref> Taken from the set {0.125, 0.25, 0.5, 1.0, 2.0} does not affect the results of our models much. Looking at the output of our models, they tend to produce output structures with less ambiguity, which causes little difference in the two heuristics. One example where the baseline made a mis- take is the sentence: "Ethanol Intoxication and withdrawal". The gold mentions are "Ethanol Intoxication" and "Ethanol withdrawal". But the linear-chain model labeled it as " <ref type="bibr">[Ethanol]</ref>  <ref type="bibr">[B]</ref> [Intoxication] <ref type="bibr">[I]</ref> and [withdrawal] <ref type="bibr">[BD]</ref> ", which is in- consistent since there is only one discontiguous component. Our models do not have this issue be- cause in our models every subgraph that may be pre- dicted translates to valid mention combinations, as discussed in Section 3.2.</p><p>In the "Train-Orig" column, we see that all mod- els can recognize discontiguous entities better when given more data, even though the majority of the en- tities in "Train-Orig" are contiguous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Experiments on Ambiguity</head><p>To see the ambiguity of each model empirically, we run the decoding process for each model given the gold output structure, which is the true label sequence for the linear-chain model and the true mention-encoded hypergraph for our models.</p><p>We used the entities from the training and devel- opment sets for this experiment, and we compare the "Original" datasets with the "Discontiguous" subset to see that the ambiguity is more pronounced when there are more discontiguous entities. Then we show the precision and recall errors (defined as 1 − P and 1 − R, respectively) in <ref type="table">Table 3</ref>.</p><p>Since the ALL heuristics generates all possible mentions from the given encoding, theoretically it should give perfect recall. However, due to errors in the training data, there are mentions which can-Discontiguous Original Prec Err Rec Err Prec Err Rec Err LI-ALL 63.66% 0.30% 23.81% 0.17% SH-ALL 1.73% 0.30% 0.35% 0.17% SP-ALL 1.05% 0.30% 0.22% 0.17% LI-ENH 2.74% 3.82% 0.52% 0.90% SH-ENH 1.21% 1.46% 0.25% 0.38% SP-ENH 0.75% 0.90% 0.17% 0.28% <ref type="table">Table 3</ref>: Precision and recall errors (%) of each model in the "Discontiguous" and "Original" datasets when given the gold output structure (label sequence in linear-chain model, hyper- graph in our models  <ref type="table">Table 4</ref>: Results on the LARGE dataset when entities are split into three types: A, B, and N. #Ent is the number of entities not be properly encoded in the models <ref type="bibr">7</ref> . Removing these errors results in perfect recall (0% recall er- ror). This means that all models are complete: they can encode any mention combinations.</p><p>We see however, a very huge difference on the precision error between the linear-chain model and our models, even more when most of the entities are discontiguous. For the discontiguous subset with the ALL heuristic, the linear-chain model produced 5,463 entities, while the SHARED and SPLIT model produced 2,020 and 2,006 entities, respectively. The total number of gold entities is 1,991. This means one encoding in the linear-chain model produces much more distinct mention combinations compared to our model, which again shows that the linear- chain model has more ambiguity. Similarly, we can deduce that the SHARED model has slightly more ambiguity compared to the SPLIT model. This con- firms our theoretical result presented previously.</p><p>It is also worth noting that in the ENOUGH heuris- tic our models have smaller errors compared to the linear-chain model, showing that when both mod- els can predict the true output structure (the correct <ref type="bibr">7</ref> There are 19 errors in the original dataset, and 6 in the dis- contiguous subset, which include duplicate mentions and men- tions with incorrect boundaries label sequence for the baseline model and mention- encoded hypergraph for our models), it is easier in our models to get the desired mention combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Experiments on Multiple Entity Types</head><p>We used the LARGE dataset with the multiple-type entities for this experiment. We ran our two models and the linear-chain CRF model with the ENOUGH heuristic on this multi-type dataset, in the same set- ting as Train-Orig in previous experiments, and the result is shown in <ref type="table">Table 4</ref>. We used the best lambda from the main experiment for this experiment.</p><p>There is a performance drop compared to the LARGE-Train-Orig results in <ref type="table">Table 2</ref>, which is ex- pected since the presence of multiple types make the task harder. But in general we still see that our mod- els are still better than the baseline, especially the SPLIT model, which shows that in the presence of multiple types, our models can still work better than the baseline model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper we proposed new models that can bet- ter represent discontiguous entities that can be over- lapping at the same time. We validated our claims through theoretical analysis and empirical analysis on the models' ambiguity, as well as their perfor- mances on the task of recognizing disorder men- tions on datasets with a substantial number of dis- contiguous entities. When the true output structure is given, which is still ambiguous in all models, our models show that it is easier to produce the desired mention combinations compared to the linear-chain CRF model with reasonable heuristics. We note that an extension similar to semi-Markov or weak semi- Markov (Muis and Lu, 2016) is possible for our models. We leave this for future investigations.</p><p>The supplementary material and our implementa- tions for the models are available at:</p><p>http://statnlp.org/research/ie</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>EGD showed [hiatal hernia]1 and vertical [laceration]2 in distal [esophagus]2 with [blood in [stomach]4]3 and overlying [lac]4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Discontiguous entities in a medical domain. Words annotated with the same index are part of the same entity. Note that entity 3 and entity 4 overlap with one another.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Entity encoding in the linear-chain model. Top: for the example in Fig 1. Bottom: for the second example in Fig 4. The O labels are not shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The hypergraph for SHARED model for the second example in Figure 4. The type information in T, B, and Onodes is not shown. The X-node is drawn multiple times for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The patient had blood in his mouth and on his tongue, pupils were pinpoint and reactive. -blood in his mouth -blood . . . on his tongue -pupils . . . pinpoint Infarctions either water shed or embolic -Infarctions -Infarctions . . . water shed -Infarctions . . . embolic You see blood or dark/black material when you vomit or have a bowel movement. -blood . . . vomit -blood . . . bowel movement -dark . . . material . . . vomit -dark . . . bowel movement -black material . . . vomit -black material . . . bowel movement</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of discontiguous and overlapping mentions, taken from the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). Lower numbers are better.</figDesc><table>Type #Ent 
Linear-chain 
SHARED 
SPLIT 
P 
R 
F 
P 
R 
F 
P 
R 
F 

A 
289 69.8 59.9 64.4 79.4 56.1 65.7 81.0 56.1 66.3 

B 
418 50.0 34.0 40.5 56.8 29.0 38.4 58.2 28.0 37.8 

N 
503 62.1 37.8 47.0 84.8 43.3 57.4 84.9 42.4 56.5 
Total 1210 60.3 41.7 49.3 74.3 41.4 53.2 75.5 40.7 52.9 

</table></figure>

			<note place="foot" n="1"> Or sometimes mentions are considered, which can be named, nominal or pronominal references to entities (Florian et al., 2004). In this paper we use &quot;mentions&quot; and &quot;entities&quot; interchangeably.</note>

			<note place="foot">T T T T T T B0 O1 O1 O1 O1 B1 B1 B1 X X X X X X X X [[[Infarctions] 1 ] 2 ] 3 either [water shed] 2 or [embolic] 3</note>

			<note place="foot" n="3"> Section names were determined by some heuristics, refer to the supplementary material for more information 4 This is standard information that can be extracted from UMLS. See (Zhang et al., 2014) for more details. 5 http://www.github.com/emorynlp/nlp4j/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their helpful feedback, and also the ShARe/CLEF eHealth Evaluation Lab for providing us the dataset. This work is supported by MOE Tier 1 grant SUTDT12015008.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SpeedRead: A Fast Named Entity Recognition Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Rfou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2012</title>
		<meeting>COLING 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Japanese Named Entity Extraction with Redundant Morphological Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL &apos;03</title>
		<meeting>HLT-NAACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trainable Grammars for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nymble: a highperformance learning name-finder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Bikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weischedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth conference on Applied Natural Language Processing (ANLP &apos;97)</title>
		<meeting>the fifth conference on Applied Natural Language Processing (ANLP &apos;97)</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NYU: Description of the MENE named entity system as used in MUC-7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Borthwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sterling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Message Understanding Conference</title>
		<meeting>the 7th Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nested named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;09)</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="141" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A statistical model for multilingual entity detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hany</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Ittycheriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanda</forename><surname>Kambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nicolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL &apos;04</title>
		<meeting>HLT-NAACL &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering Relations Among Named Entities from Large Corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="415" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML &apos;01)</title>
		<meeting>International Conference on Machine Learning (ICML &apos;01)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint Mention Extraction and Classification with Mention Hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;15)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="857" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL &apos;03</title>
		<meeting>HLT-NAACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="188" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weak SemiMarkov CRFs for Noun Phrase Chunking in Informal Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldrian</forename><surname>Obaja Muis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL &apos;16</title>
		<meeting>HLT-NAACL &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A survey of named entity recognition and classification. Lingvisticae Investigationes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemEval-2014 Task 7: Analysis of Clinical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the state of the art in disorder recognition and normalization of the clinical narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noémie</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">R</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association : JAMIA</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="54" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design Challenges and Misconceptions in Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL &apos;09)</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning (CoNLL &apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NYU: Description of the Japanese NE system used for MET-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Sekine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Message Understanding Conference</title>
		<meeting>the 7th Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanna</forename><surname>Salanterä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumithra</forename><surname>Velupillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">W</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guergana</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brett</forename><forename type="middle">R</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Overview of the ShARe/CLEF eHealth Evaluation Lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><forename type="middle">J F</forename><surname>Mowery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liadh</forename><surname>Leveling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorraine</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Goeuriot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuccon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="212" to="231" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Recognizing clinical entities in hospital discharge summaries using Structural Support Vector Machines with word representation features. BMC medical informatics and decision making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>Suppl. Suppl</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLTNAACL &apos;03</title>
		<meeting>HLTNAACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UTH-CCB : The Participation of the SemEval 2015 Challenge Task 14</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="311" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">UTH CCB: A report for SemEval 2014-Task 7 Analysis of Clinical Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buzhou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation</title>
		<meeting>the 8th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="802" to="806" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
