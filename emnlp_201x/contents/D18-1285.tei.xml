<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T12:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grounding language acquisition by training semantic parsers using captioned videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>October 31-November 4, 2018. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candace</forename><surname>Ross</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Battushig Myanganbayar CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Battushig Myanganbayar CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MIT</roleName><forename type="first">Yevgeni</forename><forename type="middle">Berzak</forename><surname>Bcs</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Battushig Myanganbayar CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MIT</roleName><forename type="first">Boris</forename><forename type="middle">Katz</forename><surname>Csail</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Battushig Myanganbayar CSAIL</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grounding language acquisition by training semantic parsers using captioned videos</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels, Belgium</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="2647" to="2656"/>
							<date type="published">October 31-November 4, 2018. 2018</date>
						</imprint>
					</monogr>
					<note>2647</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We develop a semantic parser that is trained in a grounded setting using pairs of videos cap-tioned with sentences. This setting is both data-efficient, requiring little annotation, and similar to the experience of children where they observe their environment and listen to speakers. The semantic parser recovers the meaning of English sentences despite not having access to any annotated sentences. It does so despite the ambiguity inherent in vision where a sentence may refer to any combination of objects, object properties, relations or actions taken by any agent in a video. For this task, we collected a new dataset for grounded language acquisition. Learning a grounded semantic parser-turning sentences into logical forms using captioned videos-can significantly expand the range of data that parsers can be trained on, lower the effort of training a semantic parser, and ultimately lead to a better understanding of child language acquisition.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Children learn language from observations that are very different in nature from what parsers are trained on today. Most of the time, rather than re- ceiving direct feedback such as annotated sentences or answers to direct questions, children observe and occasionally interact with their environment. They must use these observations to learn the structure of the speaker's language despite never seeing that structure overtly. This weak and indirect super- vision where most of the information is obtained through passive observation poses a difficult disam- biguation problem for learners: how do you know what the speaker is referring to in the environment, i.e., what does the speaker mean? Speakers can refer to actions, objects, the properties of actions and objects, relations between those actions and objects, as well as other features in the environment and generally do so by combining multiple features</p><p>The woman walks by the table with a yellow cup.</p><p>λxyz.woman x, walk x, near x y, table y, hold x z, yellow z, cup z <ref type="figure">Figure 1</ref>: We develop a semantic parser trained on video- sentence pairs, without parses. At inference time a sentence, without a video, is presented and a logical form is produced.</p><p>into complex sentences. Moreover, speakers need not refer to the most visually salient parts of a vi- sual scene. Here, we induce a semantic parser by simultaneously resolving visual ambiguities and grounding the semantics of language using a cor- pus of sentences paired with videos without other annotations.</p><p>The goal of semantic parsing is to convert a natural-language sentence into a representation that encodes its meaning. The parser takes sen- tences as input and produces these representations -a lambda-calculus expression in our case -that can be used for a variety of tasks such as query- ing databases, understanding references in images and videos, and answering questions. To train the parser presented here we collected a video dataset, balanced such that the raw statistics of the co- occurrences of objects and events are not infor- mative, and asked annotators on Mechanical Turk to produce sentences that are true of those videos. The parser is presented with pairs of short clips and sentences. It hypothesizes potential meanings for those sentences as lambda-calculus expressions. Each hypothesized expression serves as input for a modular vision system that constructs a specific detector for that lambda-calculus expression and determines the likelihood of the parse being true of the video. The likelihood of the parse with respect to the video is used as supervision for the parser. To test the parser, we annotated each sentence with its ground-truth semantic parse, but this information is not available at training time.</p><p>This process introduces ambiguity. For example, <ref type="figure">Figure 1</ref> shows a frame from a video annotated with the sentence "The woman walks by the table with a yellow cup.", yet the parse, λx. object(x), corresponding to a sentence like "There exists an object.", is also true of that video. For a single video there exists an infinite number of true parses that have high likelihood with respect to the vision system because they are indeed indicative of some- thing that is occurring in the video. We demonstrate how to construct a semantic parser that resolves this ambiguity and acquires language from captioned videos by learning to tune the amount of polysemy in the induced lexicon.</p><p>This work makes several contributions: We show how to construct a semantic parser that learns lan- guage in a setting closer to that of children. We demonstrate how to jointly resolve linguistic and visual ambiguities at training time in a way that can be adapted to other semantic parsing approaches. We demonstrate how such an approach can be used to augment data where a small number of directly annotated sentences can be combined with a large number of videos paired with sentences in order to improve performance. We release a dataset system- atically constructed and annotated on Mechanical Turk for joint visual and linguistic learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior work</head><p>Learning to understand language in a multimodal environment is a well-developed task. For example, visual question answering (VQA) datasets have led to a number of systems capable of answering com- plex questions about scenes ( <ref type="bibr" target="#b1">Antol et al., 2015)</ref>. The goal of our work is not to produce answers for any one set of questions, although it is possible to do so from our results; it is instead to learn to pre- dict the structure of the sentences and their mean- ing. This is a more general and difficult problem, in particular because at test time we do not receive any visual input, only the sentence. The resulting approach is reusable, generic and more similar to the kind of general-purpose linguistic knowledge that humans have. For example, one could use it to guide robotic actions. <ref type="bibr" target="#b0">Al-Omari et al. (2017)</ref> acquire a grammar for a fragment of English and Arabic from videos paired with sentences. They learn a small number of grammar rules for a lan- guage restricted to robotic commands. Learning occurs mostly in simulation and with little visual ambiguity, and the resulting model is not a parser but a means of associating n-grams with visual concepts. <ref type="bibr" target="#b18">Siddharth et al. (2014)</ref> and <ref type="bibr" target="#b23">Yu et al. (2015)</ref> ac- quire the meaning of a lexicon from videos paired with sentences but assume a fully-trained parser. <ref type="bibr" target="#b16">Matuszek et al. (2012)</ref> similarly present a model to learn the meanings and referents of words re- stricted to attributes and static scenes. <ref type="bibr" target="#b13">Hermann et al. (2017)</ref> extend these notions to train agents that learn to carry out instructions in simulated en- vironments without the need for a parser, but do so using simple adjective-noun-relation utterances. <ref type="bibr" target="#b14">Kollar et al. (2013)</ref> learn to parse similar utterances in an interactive setting. <ref type="bibr" target="#b22">Wang et al. (2016)</ref> cre- ate a language game to learn a parser but do not incorporate visual ambiguity or fallible perception. <ref type="bibr" target="#b4">Berant et al. (2013)</ref> describe semantic parsing with execution by annotating answers to database queries. This learning mechanism provides the same results as the one described here: a parser produces the meanings of sentences at inference time without requiring the database, or in our case a video. Databases have far less ambiguity than videos; there is not a temporal aspect to their con- tents and there is not a notion of unreliable per- ception. <ref type="bibr" target="#b5">Berant and Liang (2014)</ref> learn to parse sentences from paraphrases; one might consider the work here as concerned with visual and not just linguistic paraphrases. <ref type="bibr" target="#b3">Artzi and Zettlemoyer (2013)</ref> consider a setting where a validation func- tion involves the dynamic actions of a simulated robot while sentences describe its actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>Given a dataset of captioned videos, D, we train the parameters and lexicon, θ and Λ, of a semantic parser. At training time, we perform gradient de- scent over the parameters θ and employ GENLEX ( <ref type="bibr" target="#b24">Zettlemoyer and Collins, 2005</ref>) to augment the lexicon Λ. The objective function of the seman- tic parser is written in terms of a visual-linguistic compatibility between a hypothesized parse p and video v. This compatibility computes the likeli- hood of the parse being true of the video, P (v|p). At test time, we take as input a sentence without an associated video and produce a semantic parse. We could in principle also take as input the video and produce a targeted parse for that visual sce-nario. This is a problem similar to that considered by <ref type="bibr" target="#b6">Berzak et al. (2015)</ref>, but we do not do so here.</p><p>We create a CCG-based (Combinatory Categor- ical Grammar; Steedman (1996)) semantic parser capable of being trained in this setting. To do so, we adapt the objective function, training proce- dure, and feature set to this new scenario. The visual-linguistic compatibility function is similar to the Sentence Tracker developed in <ref type="bibr" target="#b18">Siddharth et al. (2014)</ref> and <ref type="bibr" target="#b23">Yu et al. (2015)</ref>. Given a parse, the Sentence Tracker produces a targeted detector that determines if the parse is true of a video, which provides a weak supervision signal for the parser.</p><p>Parses are represented as lambda-calculus ex- pressions consisting of a set of binders and a con- junction of literal expressions referring to those binders. The domain of the variables are the poten- tial object locations, or object tracks, in the videos. For example, in the parse presented in <ref type="figure">Figure 1</ref>, three potential object track slots are available, rep- resented by the binders x, y, and z. Because of perceptual ambiguities and the large number of possible referents in any one video, we do not ex- plicitly enumerate the space of object tracks. In- stead, we rely on a joint-inference process between the parser and the Sentence Tracker. Intuitively, each literal expression of the parse asserts a con- straint; for example, if an expression conveys that one object is approaching another, the Sentence Tracker will search the space of object tracks and attempt to satisfy these constraints. In <ref type="figure">Figure 1</ref>, for instance, there is a constraint that for whichever objects are bound to x and z, x must be near y, x must be walking, x must be a person, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>We develop an approach that combines a semantic parser with a vision system at training time, but does not require the vision system at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Parsing</head><p>We adopt a semantic parsing framework similar to that of <ref type="bibr" target="#b3">Artzi and Zettlemoyer (2013)</ref>, although the general approach of using vision as weak su- pervision for semantic parsing generalizes to other parsers. CCG-based parsing employs a small num- ber of fixed unary and binary derivation rules <ref type="bibr" target="#b20">(Steedman, 2000</ref>) while learning a lexicon. In CCG-based parsing, a parser takes as input a se- quence of tokens and a lexicon that maps tokens to potential syntactic types and derives parse trees by She takes the cup</p><formula xml:id="formula_0">NP (S\NP)/NP NP/N N λx. person x λf gxy. f x, take xy, gy λf x. f x λx. cup x &gt; NP λx.cup x &gt; S\NP λf xy. f x, take xy, cup y &lt; S</formula><p>λxy. person x, take xy, cup y <ref type="figure">Figure 2</ref>: A simple sentence parsed into a lambda-calculus expression using a CCG-based grammar. The parse is deter- mined by the lexicon that associates tokens with syntactic and semantic types as well as the order of function applications. Here, we acquire this lexicon and a means to score derivations.</p><p>creating and ranking multiple hypotheses that com- bine those types together. The syntactic types are richer than other approaches and include forward and backward function application (the forward and backward slash) in addition to the standard syntactic categories. Each derivation has a current syntactic type that is the result of the application of a sequence of rules. To create a derivation, at each step the parser applies each rule to either an indi- vidual subderivation or to a pair of subderivations. This process produces multiple hypotheses. Pars- ing rules are generic, polymorphic, and language- neutral and include concepts like function applica- tion and type raising <ref type="bibr" target="#b9">(Carpenter, 1997)</ref>. The parser accepts a derivation when the tree reaches a single node. We refer to the single node of the parse tree as the logical form. <ref type="figure">Figure 2</ref> shows a parse starting with tokens and their syntactic types along with each rule being applied.</p><p>Semantic parsing with CCGs extends this frame- work to simultaneously derive a logical form while performing syntactic parsing. Each syntactic rule includes a simple semantic component that ma- nipulates the logical form of its arguments. For example, the forward application rule reduces the syntactic type by applying the syntactic type of the right argument to that of the left, while at the same time performing a lambda-calculus reduction of the semantic types of those same arguments. Concretely, consider a case from <ref type="figure">Figure 2</ref> where a determiner is attached to a noun, the cup. The tokens the and cup are hypothesized to have syn- tactic types N P/N and N (a function returning N P given an argument on the right side and a noun) and semantic type λf x.f x and λx.cup(x) (the identity function and a function that adds a cup constraint). These two derivations can be reduced by forward application, denoted by &gt;. Both the syntactic and semantic types are applied and re- duced, which means the semantics helps guide the syntax. Derivations that produce illegal operations, such as applying an argument to a constant, are forbidden.</p><p>Following <ref type="bibr" target="#b24">Zettlemoyer and Collins (2005)</ref> and <ref type="bibr" target="#b11">Curran et al. (2007)</ref>, we adopt a weighted linear semantic parser. For each sentence paired with its hypothesized derivation, this approach computes a feature vector φ and a parameter vector θ. Given a sentence s, a parse p, a lexicon Λ, the set of all possible parses for that sentence with that lexi- con, P (s, Λ), and an n-dimensional feature vector computed for that sentence and parse, φ(s, p), the parser optimizes</p><formula xml:id="formula_1">argmax p∈P θ · φ(s, p)<label>(1)</label></formula><p>to find the best parse p * . Using a fixed-width beam search, the parser enumerates derivations by choos- ing a potential syntactic and semantic type for each token from the lexicon and choosing a set of deriva- tion rules to apply. For the i-th training sample d i , consisting of a sentence d s i and a video d v i in dataset D and the feature function, the parser finds margin-violating positive, E + , and negative, E − , parses, and then uses</p><formula xml:id="formula_2">θ + 1 |E + i | e∈E + i φ i (e, d v i ) − 1 |E − i | e∈E − i φ i (e, d v i )</formula><p>(2) to update the parameter θ. After each sweep through the dataset, the lexicon Λ is augmented using the modified GENLEX from Artzi and Zettle- moyer (2013), which does not require the ground- truth logical form. At no point is the logical form needed for updating the lexicon or parameters; we rely instead on a visual validation function to com- pute the margin-violating examples.</p><p>Rather than attempting to learn a fixed lexicon that directly maps tokens to semantic and syntac- tic parses, we use a factored lexicon like that of <ref type="bibr" target="#b15">Kwiatkowski et al. (2011)</ref>. This represents tokens and any associated constants separately from po- tential syntactic and semantic types. For example, the token chair is associated with a single con- stant chair; chair <ref type="bibr">[chair]</ref>. In addition to the token-constants pairs, there exists a list of pairs of syntactic and semantic types along with placehold- ers for constants; in the case for chair, a useful type might be λv.[N : λx.placeholder(x)]. When parsing, each token is applied to a potential syntac- tic and semantic type and the derivation proceeds from there. The factored lexical entries allow for far greater reuse; the model learns a small number of constants that a word can imply separately from a small number of syntactic and semantic types for any word. The weighted linear CCG-based parser searches over potential lexical entries, applying the token to different syntactic and semantic types and over multiple hypotheses for which rule should be applied. At training time, in order to learn a reasonable lexicon and set of parameters, a super- vision signal is required to validate candidates. We provide that supervision using the vision system described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentence Tracking</head><p>To score a video-parse pair, we employ a frame- work similar to that of <ref type="bibr" target="#b23">Yu et al. (2015)</ref>. This ap- proach constructs a parse-specific model by ex- tracting the number of participants in the scene described by a caption as well as the relationships and properties of those participants. It builds a graphical model where each participant is local- ized by an object tracker and each relationship is encoded by temporal models that express the prop- erties of the trackers that those models refer to. The parser's output representation is chosen to make building the vision system possible. Each target logical form is a lambda expression with a set of binders, whose domain are objects, and a conjunc- tion of constraints that refer to those binders. In essence, this notes which objects should be present in a scene and what static and changing properties and relationships those objects should have with respect to one another.</p><p>The Sentence Tracker creates one Viterbi-based tracker for each participant and, given a map- ping from constraints to Hidden Markov models (HMMs), connects each tracker and each constraint together. Given a video v and a parse p, first a large number of object detections are computed for the video by using a low confidence threshold of an object detector. Trackers weave these bounding- box detections into high-scoring object tracks and use constraints to verify if the tracks have the de- sired properties and relations. Inference proceeds jointly between vision and the parse to allow the parse to focus the vision component on events and properties that might otherwise be missed.</p><p>Understanding the relationship between a sen- tence and a video requires finding the objects that the sentence refers to and determining if those ob- jects follow the behavior implied by the sentence. We carry out a joint optimization that finds objects whose behavior follows certain rules. For clarity, the two steps are presented separately, while we find the global optimum for a linear combination of Equation <ref type="formula" target="#formula_3">(3)</ref> and Equation (4). Object trackers are a maximum-entropy Markov model with a per-frame score f , the likelihood that any one object detec- tion is true, as well as a motion-coherence score g, the likelihood that the bounding boxes selected between frames refer to the same object instance. Given a parse p with L participants and a video v of length T , Equation <ref type="formula" target="#formula_3">(3)</ref> shows the optimization where J is a set of L candidate tracks ranging over every hypothesis from the object detector and b is a candidate object detection.</p><formula xml:id="formula_3">max J L l=1 T t=1 f (b t j t l ) + T t=1 g(b t−1 j t−1 l , b t j t l )<label>(3)</label></formula><p>Determining if an object track follows a set of behaviors implied by a sentence is done using a collection of HMMs. Each has a per-frame score h that observes one or more objects tracks, depend- ing on the number of participants in the behavior being modeled, and a transition function a that determines the temporal sequence of the behav- ior. Given a parse p with C behaviors, also termed constraints, along with a video v of length T , Equa- tion (4) shows the optimization where K is a set of states, one for each constraint, and γ is a linking function. </p><p>The linking function is an indicator variable that encodes the structure of the logical form thereby filling in the correct trackers as arguments for the corresponding constraints. The exposition above presents a variant using binary constraints that is trivially generalized to n-ary constraints by extend- ing γ and adding arguments to the appropriate con- straint observation functions h c . The domain of the optimization problem is the combination of all ob- jects at all timesteps that the logical form can refer to as well as every state of each constraint. The Viterbi algorithm carries out this optimization in time linear in the length of the video and quadratic in the number of detections per frame. The result is a likelihood of the parse being true of a video. This is used to create the joint model that supervises the parser with vision. The tracker can also produce a time series of bounding boxes that make explicit the groundings of the sentences, though we do not use these directly here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Joint Model</head><p>At training time, we jointly learn using both the semantic parser and the language-vision compo- nent. At test time, only the parser is used. Two parameters are learned, a set of weights θ and the lexicon Λ. For both the parser and the associated language-vision component, Λ is used to structure inference. To induce new lexical entries, we em- ploy a variant of GENLEX ( <ref type="bibr" target="#b3">Artzi and Zettlemoyer, 2013</ref>) that takes as input a validation function - the compatibility between a parse and the video. This GENLEX uses an ontology of predicates, a validation function, and templates from the current lexicon to construct new syntactic and semantic forms. A ground-truth logical form is not required or used.</p><p>The joint model must learn these parameters despite three sources of noise. First, the vision- language component may simply fail to produce the correct likelihood because machine vision is far from perfect. Overcoming this requires large beam widths to avoid falling into local minima due to these errors.</p><p>Second, an infinite number of possibly- erroneous parses are true of a video. When children learn language, they face this same challenge as they do not have access to bounding boxes or to logical forms. The parse λx.person(x) as well as many other seemingly reasonable parses are true and cannot be distinguished from the ground-truth parse -which is not available -by the vision component. This is a far less constrained environ- ment than other approaches to semantic parsing. It is easy to be misguided by a loss function that is often true when it should not be and thus cre- ate many special-purpose definitions of words that happen to fit the peculiarities of any video. This results in two different problems: assigning empty semantics to many words since the likelihood of a subset of a parse is always the same or higher than the whole parse and excessive polysemy where the meaning of a word is highly specific to some irrele- vant feature in a video. We introduce two features to the parser that bias it against empty semantics and against excessive polysemy. Models of com- munication such as the Rational Speech Acts model <ref type="bibr" target="#b12">(Frank and Goodman, 2012)</ref> predict that speakers will avoid inserting meaningless words. One fea- ture counts the number of predicates mapped onto semantic forms which are empty that occur in each parse. The other feature attempts to prevent exces-sive polysemy by counting how many new seman- tic forms are introduced for existing tokens by the generated entries from each parse. As the parser becomes more capable of handling sentences in the training set, these features begin to bias it against adding empty semantics and new semantic forms.</p><p>Third, models in computer vision are computa- tionally expensive while many evaluations of parse- video pairs are required to train a parser. To over- come this, we construct a provably-correct cache that keeps track of failing subexpressions. This is possible because of a feature of this particu- lar vision-language scoring function: the score decreases monotonically with the number of con- straints. With these improvements, the modified semantic parser employing vision-language-based validation learns to map sentences into semantic parses despite facing a challenging setting with few examples and much ambiguity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Dataset</head><p>We collected and annotated a dataset of captioned videos with fully annotated semantic parses of the captions. The videos contain people carrying out one of 15 actions, such as picking things up and putting things down, with one of 20 objects span- ning 10 different colors. We control for 11 spatial relations between objects and actors. Many videos depict multiple agents performing actions leading to additional ambiguity. Videos were filmed in mul- tiple locations with multiple agents but care was taken to ensure that the background and agents are not informative of the events depicted.</p><p>On Mechanical Turk we asked participants to provide sentences that describe something about the video. We did not specify what participants should describe to avoid biasing them and to add richness to the dataset. This sometimes led to sen- tences that referred to properties of the video that are well beyond the capacities of the vision sys- tem, e.g., descriptions of an agent being lazy or references to the camera's movement. We removed such sentences. At training time, the parser re- ceives captioned videos but no annotations about which objects those captions refer to. Each sen- tence was annotated with a ground-truth semantic form by two trained annotators using a set of 34 predicates. Each sentence was then reviewed and corrected by one other annotator.</p><p>To detect the objects in the videos, we used two off-the-shelf detectors, OpenPose <ref type="bibr" target="#b8">(Cao et al., 2017)</ref> for person detection <ref type="bibr">and YOLO version 3 (Redmon and Farhadi, 2018)</ref> for the remaining objects. In each case we significantly lowered the confidence threshold to avoid false negatives. Many objects in this dataset are small and are handled by humans, which leads to regular object detector failures that are only partially compensated for by lowering the detection threshold at the cost of a large number of false positives. We rely on the inference mecha- nism of the grounded parser to automatically elimi- nate these numerous false positives as candidates when grounding sentences due to their low likeli- hoods. False negatives are much more misleading and difficult to overcome than false positives. It is harder to read in where an unseen object might be than to eliminate a low-confidence detection.</p><p>In total, the dataset contains 1200 captions from 401 videos, which selected out of a larger body of sentences collected and pruned as described above. This is comparable to the size of other datasets used for semantic parsing such as two datasets from Tang and Mooney (2001) with 880 and 640 ex- amples respectively and the navigation instruction dataset (Chen and Mooney, 2011) with 706 exam- ples (containing 3236 single sentences). The sen- tences comprising our dataset contain 169 unique tokens with an average of 7.93 tokens per caption. There are an average of 2.31 objects per caption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We adapted the Cornell SFP (Semantic Parsing Framework) developed by Artzi (2016) to jointly reason about sentences and videos. We selected 720 examples for training and used 120 examples for the validation set to fine-tune the model param- eters. We used the remaining 360 examples for the test set. This split was fixed and used in all experi- ments below. No sentences or videos occurred in both the training and test sets. During training, each hypothesized parse for each sentence is marked as either correct or incorrect, using either direct super- vision with the target parse or compatibility with the video, depending on the experiment.</p><p>We use beams of 80 for the CKY-parser and GENLEX. CCG-based semantic parsers are seeded with a small number of generic combinations of syntactic and semantic types. For example, Artzi (2016) seed with 141 lexical entries; we provide 98. GENLEX uses these entries along with an ontology to form new syntactic and semantic types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision</head><p>Recall F1 Direct supervision 0.851 0.946 0.84 0.933 0. show exact match results and on the right, in italics, results for the near miss metric. In the case of direct supervision, we train with the target parses. In the case of noisy supervision, a per- centage of the time (60% here) the parser randomly accepts or rejects a parse. In the case of shuffled labels, the target logical forms are assigned to random sentences. For shuffled videos the sentences are assigned to random videos. The likelihood of any sentence being true of a random video is low. In the case of object-only vision, the vision system consists solely of an object detector discarding any other predicates. The full vision-language approach learns to parse a significant fraction of sentences, far outperforming the object-only approach, and usually being within one predicate of the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Figures 3 and 4 summarize the experiments and ablation studies performed. The metrics we use when reporting results are exact matches, where the predicted parses must perfectly match the target parses, and near misses, where a single predicate in the semantic parse is allowed to differ from the target. Experiments were averaged across 5 runs.</p><p>To establish chance-level performance, we trained the directly supervised approach on shuffled labels, assigning random correct parses to random sentences. This is more powerful than a simple chance-level performance calculation as the parser can still take advantage of any dataset biases. Even with the ability to exploit potential biases, perfor- mance is very low with F1 scores of 0.136 and 0.349 for the exact and near miss metrics. Both metrics pose a challenging learning problem.</p><p>As a baseline, we directly supervised the parser with the target logical forms. When doing so, it achieved high performance with F1 scores of 0.841 and 0.911 for the exact match and near miss cases. <ref type="figure" target="#fig_2">Figure 4</ref> shows performance of direct supervision as a function of training set size.</p><p>We then added noise to the directly supervised parser. Doing so simulates the unreliable nature of vision and, to an extent, the ambiguities inher- ent in vision. Noise was introduced by modifying the compatibility function which determines if a parse is correct. A certain percentage of the time, that function returned true or false randomly when given a hypothesized logical form. With around 60% noise, performance was 0.22 and 0.39 F1 for the noisy and near miss cases. <ref type="figure" target="#fig_2">Figure 4</ref> shows per- formance of the noisy baseline as a function of how much noise was introduced.</p><p>The fully grounded parser produced 0.2 and 0.6 F1 scores for the exact and near miss metrics. This is far beyond chance performance and corresponds to direct supervision with around 55% noise. There are a number of reasons for why performance is not perfect. First, the evaluation metrics cannot consider equivalences in meaning, just form. A hypothesized parse may carry the same meaning as the target logical form yet it will be considered incorrect. This is less of a problem with direct supervision where the preferences that annotators have for a particular way of encoding the mean- ing of a sentence can be learned. In the grounded case, this cannot be learned; visually equivalent parses are equally likely. Second, computer vision is unreliable, i.e., object detectors fail. We find that in many of our videos while person detection is fairly reliable, object detection is unreliable. Third, vision in the real world is very ambiguous. Predi- cates like hold are true in almost every interaction. This makes learning the meanings of words much more difficult resulting in the grounded parser of- ten adding useless entries into the predicted logical forms or substituted one predicate for a similar one. The near miss metric shows that overall the parser learned reasonable logical forms. <ref type="figure">Figure 5</ref> shows six examples from our dataset along with expected and predicted parses, both correct and incorrect.</p><p>To understand how much of the performance of the grounded parser comes from visual correla- tions, like the presence or absence of particular ob- jects, as opposed to more complex and cognitively relevant spatio-temporal relations like actions, we ablated the parser. We removed all features other than objects. The resulting grounded parser accepts any hypothesized parse as long as the objects men- tioned in that parse are present in the video. This led to a significant performance drop, near-chance level performance on the exact metric, F1 0.05, and nearly half the F1 score on the near miss metric, 0.37. Having a sophisticated vision system to infer about agents and interactions is crucial for learning. In dashed blue, noisy supervision uses the whole training set but accepts and rejects parses at random for a given fraction of the time. The red cross is the full vision system while the green o is the object detector ablation. The orange triangle represents shuffled videos and shows chance performance. While direct supervision outperforms vision-only supervision, the grounded parser closes the gap and operates like noisy direct supervision with roughly 55% noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We present a semantic parser that learns the struc- ture of language using weak supervision from vi- sion. At test time, the model parses sentences with- out the need for visual input. Learning by passive observation in this way extends the capabilities of semantic parsers and points the way to a more cog- nitively plausible model of language acquisition. Several limits remain. Evaluating parses as correct or incorrect depending on a match to a human- annotated logical form is an overly strict criterion and is a problem that also plagues fully-supervised syntactic parsing <ref type="bibr" target="#b7">(Berzak et al., 2016)</ref>. Since two logical forms may express the same meaning, it is not yet clear what an effective evaluation metric is for these grounded scenarios. In addition, learning in such a passive scenario is hard as correlations between events, e.g., every pick up event involves a touch event, are very difficult to disentangle. An interesting source of error in the experimen- tal results comes from visual ambiguities. At the level of relative motions of labeled bounding boxes, the analysis performed by the language-vision sys- tem we employed here has difficulty distinguishing certain parts of actions. For example, carrying a shirt and wearing a shirt appear very similar to one another as they are actions that mostly involve mov- ing alongside a person detection. Moreover, since every agent is wearing a shirt it becomes more dif- ficult to learn to distinguish the two actions using positive evidence alone, i.e., a maximum likelihood approach. A more robust vision system, perhaps including object segmentations, person pose, and weak negative evidence for the occurrence of ac- tions, would likely significantly improve the results presented.</p><p>In the future, we intend to add a generative model along with a physical simulation allowing the learner to imagine scenarios where a predicate might not hold. This would help mitigate sys- tematic correlations between sentences and videos. The sentences selected here were all chosen such that they are true of the video being shown, yet much of what people discuss is ungrounded, or at least not grounded in the current visual scene. We intend to combine the weakly supervised parser with an unsupervised parser and learn to determine whether a sentence should be grounded visually during training. We hope this work will find ap- plications in robotics where learning to adapt to the specific language of a user while engaging with them is of utmost importance when deploying robots in users' homes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pairs of results for each condition. On the left, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results from training the grounded semantic parser. In blue, direct supervision as a function of the amount of training data. In dashed blue, noisy supervision uses the whole training set but accepts and rejects parses at random for a given fraction of the time. The red cross is the full vision system while the green o is the object detector ablation. The orange triangle represents shuffled videos and shows chance performance. While direct supervision outperforms vision-only supervision, the grounded parser closes the gap and operates like noisy direct supervision with roughly 55% noise.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural language acquisition and grounding for embodied robotic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhannad</forename><surname>Al-Omari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony G</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4349" to="4356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cornell SPF: Cornell semantic parsing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods for Natural Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic parsing via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Do You See What I Mean ? Visual Resolution of Linguistic Ambiguities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Harari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<biblScope unit="page" from="1477" to="1487" />
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anchoring and agreement in syntactic annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeni</forename><surname>Berzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Type-logical semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions fro mobservations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linguistically motivated large-scale nlp with c&amp;c and boxer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>James R Curran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting pragmatic reasoning in language games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Noah D Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">336</biblScope>
			<biblScope unit="issue">6084</biblScope>
			<biblScope unit="page" from="998" to="998" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojtek</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06551</idno>
		<title level="m">Grounded language learning in a simulated 3D world</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward interactive grounded language acqusition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lexical Generalization in CCG Grammar Induction for Semantic Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1512" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liefeng</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1671" to="1678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Seeing what you&apos;re told: Sentence-guided activity recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanaswamy</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Surface Structure and Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Syntactic Process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using multiple clause constructors in inductive logic programming for semantic parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lappoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="466" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning language games through interaction. Meeting of the Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Sida I Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A compositional framework for grounding language inference, generation, and acquisition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haonan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">Mark</forename><surname>Siskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05)</title>
		<meeting>the Twenty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-05)<address><addrLine>Arlington, Virginia</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
