<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Models for Political Ideology Detection of News Articles</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 31-November 4, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kulkarni</surname></persName>
							<email>vvkulkarni@cs.ucsb.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
							<email>skiena@cs.stonybrook.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Stony Brook University</orgName>
								<orgName type="institution" key="instit2">Stony Brook University</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Models for Political Ideology Detection of News Articles</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Brussels; Belgium</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="3518" to="3527"/>
							<date type="published">October 31-November 4, 2018</date>
						</imprint>
					</monogr>
					<note>3518</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>A news article&apos;s title, content and link structure often reveal its political ideology. However , most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many issues covered or discussed by the me- dia and politicians today are so subtle that even word-choice may require one to adopt a particular ideological position <ref type="bibr" target="#b11">(Iyyer et al., 2014</ref>). For example, conservatives tend to use the term tax reform, while liberals use tax simplification. Though objectivity and un- biased reporting remains a cornerstone of profes- sional journalism, several scholars argue that the media displays ideological bias <ref type="bibr" target="#b6">(Gentzkow and Shapiro, 2010;</ref><ref type="bibr" target="#b8">Groseclose and Milyo, 2005;</ref><ref type="bibr" target="#b11">Iyyer et al., 2014</ref>). Even if one were to argue that such bias may not be reflective of a lack of objectiv- ity, prior research <ref type="bibr" target="#b5">Dardis et al. (2008)</ref>; <ref type="bibr" target="#b2">Card et al. (2015)</ref> note that framing of topics can significantly influence policy.</p><p>Since manual detection of political ideology is challenging at a large scale, there has been exten- sive work on developing computational models for automatically inferring the political ideology of articles, blogs, statements, and congressional speeches <ref type="bibr" target="#b6">(Gentzkow and Shapiro, 2010;</ref><ref type="bibr" target="#b11">Iyyer et al., 2014;</ref><ref type="bibr" target="#b18">Preot¸iucPreot¸iuc-Pietro et al., 2017;</ref><ref type="bibr" target="#b19">Sim et al., 2013)</ref>. In this paper, we consider the detection of ideo- logical bias at the news article level, in contrast to recent work by <ref type="bibr" target="#b11">Iyyer et al. (2014)</ref> who focus on the sentence level or the work of (Preot¸iuc <ref type="bibr" target="#b18">Preot¸iuc-Pietro et al., 2017</ref>) who focus on inferring ideological bias of social media users. Prior research exists on detecting ideological biases of news articles or documents <ref type="bibr" target="#b6">(Gentzkow and Shapiro, 2010;</ref><ref type="bibr" target="#b7">Gerrish and Blei, 2011;</ref><ref type="bibr" target="#b11">Iyyer et al., 2014</ref>). However, all of these works generally only model the text of the news article. However, in the online world, news articles do not just contain text but have a rich structure to them. Such an online setting in- fluences the article in subtle ways: (a) choice of the title since this is what is seen in snippet views online (b) links to other news media and sources in the article and (c) the actual textual content itself. Except for the textual content, prior models ignore the rest of these cues. <ref type="figure" target="#fig_1">Figure 1</ref> shows an example from The New York Times. Note the pres- ence of hyperlinks in the text, which link to other sources like The Intercept( <ref type="figure" target="#fig_1">Figure 1a</ref>). We hypothesize that such a link structure is reflective of homophily between news sources sharing sim- ilar political ideology -homophily which can be exploited to build improved predictive models (see <ref type="figure" target="#fig_1">Figure 1b</ref>). Building on this insight, we propose a new model MVDAM: Multi-view document atten- tion model to detect the ideological bias of news articles by leveraging cues from multiple views: the title, the link structure, and the article content. Specifically, our contributions are:</p><p>1. We propose a generic framework MVDAM to incorporate multiple views of the news article and show that our model outperforms state of the art by 10 percentage points on the F1 score. 2. We propose a method to estimate the ideo- logical proportions of sources and rank them by the degree to which they lean towards a particular ideology. 3. Finally, differing from most works, which typ- ically focus on congressional speeches, we conduct ideology detection of news articles by assembling a large-scale diverse dataset spanning more than 50 sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several works study the detection of political ide- ology through the lens of computational linguistics and natural language processing ( <ref type="bibr" target="#b13">Laver et al., 2003;</ref><ref type="bibr" target="#b17">Monroe and Maeda, 2004;</ref><ref type="bibr" target="#b20">Thomas et al., 2006;</ref><ref type="bibr" target="#b14">Lin et al., 2008;</ref><ref type="bibr" target="#b3">Carroll et al., 2009;</ref><ref type="bibr" target="#b0">Ahmed and Xing, 2010;</ref><ref type="bibr" target="#b6">Gentzkow and Shapiro, 2010;</ref><ref type="bibr" target="#b7">Gerrish and Blei, 2011;</ref><ref type="bibr" target="#b19">Sim et al., 2013</ref>). Gentzkow and Shapiro (2010) first attempt to rate the ideological leaning of news sources by proposing a measure called "slant index" which captures the degree to which a particular newspaper uses partisan terms or co-allocations. Gerrish and Blei <ref type="formula" target="#formula_1">(2011)</ref>   <ref type="bibr" target="#b0">and Xing, 2010;</ref><ref type="bibr" target="#b14">Lin et al., 2008)</ref>. <ref type="bibr" target="#b19">Sim et al. (2013)</ref> propose a novel HMM-based model to infer the ideological propor- tions of the rhetoric used by political candidates in their campaign speeches which relies on a fixed lexicon of bigrams associated with ideologies. The work that is most closely related to our work is that of <ref type="bibr" target="#b11">Iyyer et al. (2014);</ref><ref type="bibr" target="#b18">Preot¸iucPreot¸iuc-Pietro et al. (2017)</ref>. <ref type="bibr" target="#b11">Iyyer et al. (2014)</ref> use recurrent neural net- works to predict political ideology of congressional debates and articles in the ideological book corpus (IBC) and demonstrate the importance of compo- sitionality in predicting ideology where modifier phrases and punctuality affect the political ideolog- ical position. Preot¸iucPreot¸iuc- <ref type="bibr" target="#b18">Pietro et al. (2017)</ref> propose models to infer political ideology of Twitter users based on their everyday language. Most crucially, they also show how to effectively use the relation- ship between user groups to improve prediction accuracy. Our work draws inspiration from both of these works but differentiates itself from these in the following aspects: We leverage the structure of a news article by noting that an article is just not free-form text, but has a rich structure to it. In par- ticular, we model cues from the title, the inferred network, and the content in a joint generic neural variational inference framework to yield improved models for this task. Furthermore, differing from <ref type="bibr" target="#b11">Iyyer et al. (2014)</ref>, we also incorporate attention mechanisms in our model which enables us to in- spect which sentences (or words) have the most predictive power as captured by our model. Finally, since we work with news articles (which also con- tain hyperlinks), naturally our setting is different from all other previous works in general (which mostly focus on congressional debates) and in par- ticular from <ref type="bibr" target="#b11">Iyyer et al. (2014)</ref> where only textual content is modeled or Preot¸iucPreot¸iuc- <ref type="bibr" target="#b18">Pietro et al. (2017)</ref> which focuses on social media users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset Construction</head><p>News Sources We rely on the data released by ALLSIDES.COM 1 to obtain a list of 59 US-based news sources along with their political ideology ratings: LEFT, CENTER or RIGHT which specify our target label space. While we acknowledge that there is no "perfect" measure of political ideology, ALLSIDES.COM is an apt choice for two main rea- sons. First, and most importantly the ratings are based on a blind survey, where readers are asked to rate news content without knowing the identity of the news source or the author being rated. This is also precisely the setting in which our proposed computational models operate (where the models have access to the content but are agnostic of the source itself) thus seeking to mirror human judg- ment closely. Second, these are normalized by ALLSIDES to ensure they closely reflect popular opinion and political diversity present in the United States. These ratings also correlate with indepen- dent measurements made by the PEW RESEARCH CENTRE. All these observations suggest that these ratings are fairly robust and generally "reflective of the average judgment of the American People" 2 .</p><p>Content Extraction Given the set of news sources selected above, we extract the article con- tent for these news sources. We control for time by obtaining article content over a fixed time-period for all sources. Specifically, we spider several news sources and perform data cleaning. In particular, the spidering component collates the raw HTML of news sources into a storage engine (MongoDB). We track thousands of US based news outlets in- cluding country wide popular news sources as well as many local/state news based outlets like the  Boston Herald 3 . However, in this paper, we con- sider only the 59 US news sources for which we can derive ground truth labels for political ideol- ogy. For each of the news sources considered, we extract the title, the cleaned pre-processed content, and the hyperlinks within the article that reveal the network structure. The label for each article is the label assigned to its source as obtained from ALL- SIDES. We choose a random sample of 120, 000 articles and create 3 independent splits for training (100, 000), validation (10, 000) and test (10, 000) with a roughly balanced label distribution. 4</p><p>Data Pre-processing and Cleaning Since the la- bels were derived from the source, we are care- ful to remove any systematic features in each ar- ticle which are trivially reflective of the source, since that would result in over-fitting. In particu- lar we perform the following operations: (a) Re- move source link mentions When modeling the link structure of an article, we explicitly remove any link to the source itself. Second, we also explicitly remove any systematic link structures in articles that are source specific. In particular, some sources may always have links to other do- mains (like their own franchisees or social me- dia sites). These links are removed explicitly by noting their high frequency. (b) Remove head- ers, footers, advertisements News sources sys- tematically introduce footers, and advertisements which we remove explicitly. For example, every article of the The Daily Beast has the fol- lowing footer You can subscribe to the Daily Beast here which we filter out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem</head><p>Formulation Given X = {X title , X net , X content } which represents a set of multi-modal features of news articles and a label set Y = {LEFT, CENTER, RIGHT}, we would like to model Pr(Y |X).</p><p>Overview of MVDAM We consider a Bayesian approach with stochastic attention units to effec- tively model textual cues. Bayesian approaches with stochastic attention have been noted to be quite effective at modeling ambiguity as well as avoiding over-fitting scenarios especially in the case of small training data sets <ref type="bibr" target="#b15">(Miao et al., 2016</ref>). In particular, we assume a latent representation h learned from the multiple modalities in X which is then mapped to the label space Y . In the most general setting, instead of learning a deterministic encoding h given X, we posit a latent distribu- tion over the hidden representation h, Pr(h|X) to model the overall document where Pr(h|X) is parameterized by a diagonal Gaussian distribution N (h|µ(X), σ 2 (X)).</p><p>Specifically, consider the distribution Pr(Y |X) which can be written as follows:</p><formula xml:id="formula_0">Pr(Y |X) = h Pr(Y |h) Pr(h|X) (1)</formula><p>As noted by <ref type="bibr" target="#b15">Miao et al. (2016)</ref>, computing the exact posterior is in general intractable. Therefore, we posit a variational distribution q φ (h) and maximize  the evidence lower bound L ≤ Pr(y|X) namely,</p><formula xml:id="formula_1">L = E q φ (h) [p(Y |h)] − D KL (q φ (h)||p(h|X)),<label>(2)</label></formula><p>where p(Y |h) denotes a probability distribution over Y given the latent representation h, and p(h|X) denotes the probability distribution over h conditioned on X.</p><p>Equation 2 can be interpreted as consisting of three components, each of which can modeled sep- arately: (a) Discriminator p(Y |h) can be viewed as a discriminator given the hidden representation h. Maximizing the first term is thus equivalent to minimizing the cross-entropy loss between the model's prediction and true labels. (b) The second term, the KL Divergence term consists of two com- ponents: (1) Approximate Posterior The term q φ (h) also known as the approximate posterior pa- rameterizes the latent distribution which encodes the multi-modal features X of a document. (2) Prior The term p(h|X) can be viewed as a prior which can be uninformative (a standard Gaussian prior in the most general case, or any other prior model based on other features). We now discuss how we model each of these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Discriminator</head><p>We use a simple feed-forward network with a lin- ear layer that accepts as input the latent hidden representation of X, followed by a ReLU for non- linearity followed by a linear layer and a final soft- max layer to model this component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Posterior</head><p>Here we model the approximate posterior q φ (h) by an inference network shown succinctly in <ref type="figure" target="#fig_3">Fig- ure 2b</ref>. The inference network takes as input the features X and learns a corresponding hidden rep- resentation h. More specifically, it outputs two components: (µ, ς) corresponding to the mean and log-variance of the gaussian parametrizing the hidden representation h. We model this using a "multi-view" network which incorporates hidden representations learned from multiple modalities into a joint representation. Specifically, given d- dimensional hidden representations corresponding to multiple modalities z title , z network , and z content the model first concatenates these representations into a single 3d-dimensional representation z concat which is then input through a 2-layer feed-forward network to output a d-dimensional mean vector µ and a d-dimensional log-variance vector ς that pa- rameterizes the latent distribution governing h. We now discuss the models used for capturing each view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Modeling the Title</head><p>We learn a latent representation of the title of a article by using a convolutional network. Convolu- tional networks have been shown to be very effec-tive for modeling short sentences like titles of news articles. In particular, we use the same architecture proposed by <ref type="bibr" target="#b12">(Kim, 2014</ref>). The input words of the title are mapped to word embeddings and concate- nated and passed through convolutional filters of varying window sizes. This is then followed by a max-over-time pooling <ref type="bibr" target="#b4">(Collobert et al., 2011</ref>). The outputs of this layer are input to a fully con- nected layer of dimension d with drop-out which outputs z title , the latent representation of the title.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Modeling the Network Structure of articles</head><p>Capturing the network structure of article consists of two steps: (a) Learning a network representation of each source based on its social graph G. (b) Using the learned representation of each source to capture the link structure of a particular article.</p><p>We use a state-of-the-art network representa- tion learning algorithm to learn representations of nodes in a social network. In particular, we use Node2Vec ( <ref type="bibr" target="#b9">Grover and Leskovec, 2016)</ref>, which learns a d-dimensional representation of each source given the hyperlink structure graph G. Node2Vec seeks to maximize the log likeli- hood of observing the neighborhood of a node N (u), given the node u. Let F be a matrix of size (V, d) where F (u) represents the embedding of node u. We then maximize the following like- lihood function max F u log Pr(N (u)|u). We model the above likelihood similar to the Skip- gram architecture ( <ref type="bibr" target="#b16">Mikolov et al., 2013</ref>) by as- suming that the likelihood of observing a node v ∈ N (u) is conditionally independent of any other node in the neighborhood given u. That is log Pr(N (u)|u) = v∈N (u) log Pr(v|u). We then model Pr(v|u) = e F (u).F (v) v e F (u).F (v) . Having fully specified the log likelihood function, we can now optimize it using stochastic gradient ascent.</p><p>Having learned the embedding matrix F for each source node, we now model the link structure of any given article A simply by the average of the net- work embedding representations for each link l ref- erenced in A. In particular, we compute z network as:</p><formula xml:id="formula_2">z network = 1 |A| l∈A F (l).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Modeling the Content of articles</head><p>To model the content of an article, we use a hier- archical approach with attention. In particular, we compute attention at both levels: (a) words and (b) sentences. We closely follow the approach by <ref type="bibr" target="#b21">(Yang et al., 2016</ref>) which learns a latent representa- tion of a document d using both word and sentence attention models. We model the article A hierarchically, by first representing each sentence i with a hidden repre- sentation s i . We model the fact that not all words contribute equally in the sentence through a word level attention mechanism. We then learn the rep- resentation of the article A by composing these individual sentence level representations with a sen- tence level attention mechanism.</p><p>Learning sentence representations We first map each word to its embedding matrix through a lookup embedding matrix W . We then learn a hidden representation of the given sentence h it cen- tered around word w i by embedding the sentence through a bi-directional GRU as described by <ref type="bibr" target="#b1">(Bahdanau et al., 2014</ref>). Since not all words contribute equally to the representation of the sentence, we introduce a word level attention mechanism which attempts to extract relevant words that contribute to the meaning of the sentence. Specifically we learn a word level attention matrix W w as follows Composing sentence representations We fol- low a similar method to learn a latent represen- tation of an article. Given the embedding s i of each sentence in the article, we learn a hid- den representation of the given sentence h i cen- tered around s i by embedding the list of sen- tences through a bi-directional GRU as described by ( <ref type="bibr" target="#b1">Bahdanau et al., 2014</ref>). Once again, since not all sentences contribute equally to the rep- resentation of the article, we introduce a sen- tence level attention mechanism which attempts to extract relevant sentences that contribute to the meaning of the article. Specifically we learn the weights of a sentence level attention matrix W s as α s ∝ exp(W s h s + b s ), z content = s α s h s , where z content is the latent representation of the article. In this case we let the hidden representa- tion of the sentence be a stochastic representation similar to the work by <ref type="bibr" target="#b15">(Miao et al., 2016)</ref> and use the Gaussian re-parameterization trick to enable training via end-to-end gradient based methods 5 . Such techniques have been shown to be useful in modeling ambiguity and also generalize well to small training datasets ( <ref type="bibr" target="#b15">Miao et al., 2016</ref>).</p><formula xml:id="formula_3">α i ∝ exp(W w h it + b w ), s i = t α i h it</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prior</head><p>The prior models p(h|X) in Equation 2. Note that our proposed framework is general and can be used to incorporate a variety of priors. Here, we assume the prior is drawn from a Gaussian distribution with diagonal co-variances. The KL Divergence term in Equation 2 can thus be analytically computed. In particular, the KL Divergence between two K di- mensional Gaussian distributions A, B with means µ A , µ B and diagonal co-variances κ A , κ B is:</p><formula xml:id="formula_4">D KL (A, B) = − 1 2 j=K j=1 (1 + log κ Aj κ Bj − κ Aj κ Bj − (µ Aj − µ Bj ) 2 /κ Bj ) (3)</formula><p>Parameter Estimation Having described pre- cisely, the models for each of the components in Equation 2, we can reformulate the maximization of the variational lower bound to the following loss function on the set of all learn-able model parame- ters θ: J (θ) as follows:</p><formula xml:id="formula_5">J (θ) = NLL(y|X) + λD KL (q(h)||p(h|X)),<label>(4)</label></formula><p>where NLL is the negative log likelihood loss com- puted between the predicted label and the true la- bel, and λ is a hyper-parameter that controls the amount of regularization offered by the KL Diver- gence term. We use ADADELTA to minimize this loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate our model against several competitive baselines which model only a single view to place our model in context:</p><p>1. Chance Baseline We consider a simple base- line that returns a draw from the label distri- bution as the prediction. 2. Logistic Regression LR (Title) We consider a bag of words classifier using Logistic Re- gression that can capture linear relationships in the feature space and use the words of the title as the feature set. 3. CNN (Title) We consider a convolutional net classifier based on exactly the same architec- ture as <ref type="bibr" target="#b12">(Kim, 2014</ref>) which uses the title of the news article. Convolutional Nets have been shown to be extremely effective at classify- ing short pieces of text and can capture non- linearities in the feature space <ref type="bibr" target="#b12">(Kim, 2014</ref>). 4. FNN (Network) We also consider a simple fully-connected feed forward neural network using only the network features to characterize the predictive power of the network alone. 5. HDAM Model (Content) We use the state of the art hierarchical document attention model proposed by <ref type="bibr" target="#b21">(Yang et al., 2016</ref>) that models the content of the article using both word and sentence level attention mechanisms.</p><p>We consider three different flavors of our proposed model which differ in the subset of modalities used (a) Title and Network (b) Title and Content, and (c) Full model: Title, Network, and Content. We train all of our models and the baselines on the training data set choosing all hyper-parameter using the validation set. We report the performance of all models on the held-out test set.</p><p>Experimental Settings We set the embedding latent dimension captured by each view to be 128 including the final latent representation obtained by fusing multiple modalities. In case of the CNN's, we consider three convolutional filters of window sizes 3, 4, 5 each yielding a 100 dimensional fea- ture map followed by max-over time pooling which is then passed through a fully connected layer to yield the output. In all the neural models, we used AdaDelta with an initial learning rate of 1.0 to learn the parameters of the model via back-propagation.  <ref type="table">Table 1</ref>: Precision, Recall, and F1 scores of our model MV- DAM on the test set compared with several baselines. All flavors of our model significantly outperform baselines and yield state of the art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results and Analysis</head><p>Quantitative Results <ref type="table">Table 1</ref> shows the results of the evaluation. First note that the logistic regres- sion classifier and the CNN model using the Title outperforms the CHANCE classifier significantly <ref type="bibr">(F1: 59.12, 59.24 vs 34.53)</ref>. Second, only mod- eling the network structure yields a F1 of 55.10 but still significantly better than the chance base- line. This confirms our intuition that modeling the network structure can be useful in prediction of ideology. Third, note that modeling the con- tent (HDAM) significantly outperforms all previ- ous baselines (F1:68.92). This suggests that con- tent cues can be very strong indicators of ideology. Finally, all flavors of our model outperform the baselines. Specifically, observe that incorporating the network cues outperforms all uni-modal mod- els that only model either the title, the network, or the content. It is also worth noting that without the network, only the title and the content show only a small improvement over the best perform- ing baseline (69.54 vs 68.92) suggesting that the network yields distinctive cues from both the title, and the content. Finally, the best performing model effectively uses all three modalities to yield a F1 score of 79.67 outperforming the state of the art baseline by 10 percentage points. Altogether our results suggest the superiority of our model over competitive baselines. In order to obtain deeper in- sights into our model, we also perform a qualitative analysis of our model's predictions. <ref type="figure" target="#fig_6">Figure 3</ref> shows a visualization of sentences based on their attention scores. Note that for a left leaning article (see <ref type="figure" target="#fig_6">Figure 3a)</ref>, the model focuses on sentences in- volving gun-control, feminists, and transgender. In contrast, a visualization of sentence attention scores for an article which the model predicted as "right-leaning" ((see <ref type="figure" target="#fig_6">Fig- ure 3b)</ref>) reveals a focus on words like god, religion etc. These observations qualitatively suggest that the model is able to effectively pick up on content cues present in the article. By ex- amining the distribution over the sentence indices corresponding to the maximum attention scores, we noted that only in about half the instances, the model focuses its greatest attention on the begin- ning of the article suggesting that the ability to selectively focus on sentences in the news article contributes to the superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing Attention Scores</head><p>Challenging Cases In <ref type="table" target="#tab_3">Table 2</ref>, we highlight some of the challenges of our model. In particular, our model finds it quite challenging to identify the political ideology of the source for articles that are non-political and related to global events, or entertainment. Examples include instances like Tourist dies hiking in Australia Outback heat or Juan Williams makes the 'case for Oprah'. We also note that articles with "click-baity" titles like We are all Just Overclocked Chimpanzees are not necessarily discriminative of the underlying ideology. In summary, while our proposed model significantly advances the state of art, it also suggests scope for further improvement especially in identifying political ideologies of articles in topics like Entertainment or Sports. For example, prior research suggests that engagement in particular sports is correlated with the political leanings <ref type="bibr" target="#b10">(Hoberman, 1977)</ref> which suggest that improved models might need to capture deeper linguistic and contextual cues.</p><p>Ideological Proportions of News Sources Fi- nally, we compute the expected proportion of an ideology in a given source based on the probabil- ity estimates output by our model for the various articles. While one might expect that the expected degree of "left-ness" (or "right-ness") for a given source can easily be computed by taking a simple mean of the prediction probability for the given ide- ology over all articles belonging to the source, such an approach can be in-accurate because the proba- bility estimates output by the model are not neces- sarily calibrated and therefore cannot be interpreted as a confidence value. We therefore use isotonic regression to calibrate the probability scores output by the model. Having calibrated the probability scores, we now compute the degree to which a par- ticular news source leans toward an ideology by simply computing the mean output score over all articles corresponding to the source. <ref type="table" target="#tab_4">Table 3</ref> shows the top 10 sources ranked according to their pro- portions for each ideology. We note that sources like CNN, Buzz Feed, SF Chronicle are consid- ered more left-leaning than the Washington Post. Similarly, we note that NPR and Reuters are con- sidered to be the most center-aligned while Breit- bart, Infowars and Blaze are considered to be most right-aligned by our model. These observations are moderately aligned with survey results that place news sources on the ideology spectrum based on the political beliefs of their consumers 6 .     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a model to leverage cues from mul- tiple views in the predictive task of detecting po- litical ideology of news articles. We show that incorporating cues from the title, the link struc- ture and the content significantly beats state of the art. Finally, using the predicted probabilities of our model, we draw on methods for probability cali- bration to rank news sources by their ideological proportions which moderately correlates with exist- ing surveys on the ideological placement of news sources. To conclude, our proposed framework effectively leverages cues from multiple views to yield state of the art interpret-able performance and sets the stage for future work which can easily in- corporate other modalities like audio, video and images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A sample news article. Note the presence of hyperlinks to other sources like The Intercept. (b) Homophily in link structure (viewed in color) of var- ious news sources which can be observed by noting the presence of clusters corresponding to political ideologies. The blue, orange and green clusters correspond to left, right and center leaning sources respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed framework MVDAM models multiple views of the news article including the content and the link structure. Figure 1a shows a sample article from the New York Times. The presence of such links can provide informative signals for predictive tasks like ideology detection primarily due to homophily (Figure 1b).</figDesc><graphic url="image-1.png" coords="3,72.00,84.03,163.27,98.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Overview of the inference network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A broad overview of our MVDAM model depicting the three major components:a discriminator, an inference network and a prior and captures cues from multiple views of the news article. As noted by Miao et al. (2016) we use stochastic attention units which are shown to model ambiguity better. We thus train the model end-to-end using neural variational inference.</figDesc><graphic url="image-3.png" coords="4,72.00,62.81,226.77,222.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>where s i is the latent representation of the sentence i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Sample attention on sentences for a Left aligned article. (b) Sample attention on sentences for a Right aligned article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of attention on different sentences on two sample articles from the Left and Right aligned sources respectively. Note the different focus based in the ideology reflected by the highlighted words.</figDesc><graphic url="image-6.png" coords="8,117.35,151.07,362.85,98.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Few failure cases of our model illustrating what our model finds challenging. Articles with "click-baity" titles are 

not necessarily very discriminative of the ideology. Similarly, articles that are non-political and related to global events or 
entertainment are quite challenging. 

Rank Source 
1 
CNN 
2 
BuzzFeed 
3 
SF Chronicle 
4 
CBS News 
5 
BoingBoing 
6 
Mother Jones 
7 
Think Progress 
8 
The Atlantic 
9 
The Washington Post 
10 
Rolling Stone 

(a) Left aligned 

Rank Source 
1 
NPR 
2 
Reuters 
3 
USA Today 
4 
BBC 
5 
CNBC 
6 
Chicago Tribune 
7 
Business Insider 
8 
Forbes 
9 
APR 
10 
The Wall Street Journal 

(b) Centre aligned 

Rank Source 
1 
Breitbart 
2 
Infowars 
3 
Blaze 
4 
Fox News 
5 
KSL 
6 
Townhall 
7 
CBN 
8 
ConservativeHQ 
9 
NewsMax 
10 
DailyWire 

(c) Right aligned 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>A Top 10 ranking of Ideological sources as obtained by our model which correlate moderately with external surveys.</figDesc><table></table></figure>

			<note place="foot" n="1"> https://www.allsides.com/media-bias/media-bias-ratings 2 https://www.allsides.com/media-bias/about-bias</note>

			<note place="foot" n="3"> This is a part of an ongoing project called MediaRank. More details can be found at http://media-rank.com 4 Note that we do not restrict the articles to be strictly political since even articles on other topics like health and sports can be reflective of political ideology (Hoberman, 1977).</note>

			<note place="foot" n="5"> Using deterministic sentence representations is a special case.</note>

			<note place="foot" n="6"> http://www.journalism.org/2014/10/21/politicalpolarization-media-habits/pj 14-10-21 mediapolarization08/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their com-ments. This research was supported in part by DARPA Grant D18AP00044 funded under the DARPA YFA program. This work was also par-tially supported by NSF grants DBI-1355990 and IIS-1546113. The authors are solely responsible for the contents of the paper, and the opinions ex-pressed in this publication do not reflect those of the funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1140" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The media frames corpus: Annotations of frames across issues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="438" to="444" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring bias and uncertainty in dw-nominate ideal point estimates via the parametric bootstrap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royce</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">B</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Keith T Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="275" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Media framing of capital punishment and its impact on individuals&apos; cognitive responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frank E Dardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><forename type="middle">E</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanna</forename><forename type="middle">De</forename><surname>Boydstun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyuan</forename><surname>Boef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mass Communication &amp; Society</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="140" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What drives media slant? evidence from us daily newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gentzkow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="71" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting legislative roll calls from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (icml-11)</title>
		<meeting>the 28th international conference on machine learning (icml-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A measure of media bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Groseclose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Milyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1237" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sport and political ideology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John M Hoberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of sport and social issues</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="80" to="114" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Political ideology detection using recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extracting policy positions from political texts using words as data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Laver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Garry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="331" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A joint topic and perspective model for ideological discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural variational inference for text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1727" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Talks cheap: Text-based estimation of rhetorical ideal-points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond binary labels: political ideology prediction of twitter users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Preot¸iucpreot¸iuc-Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="729" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Measuring ideological proportions in political speeches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchuan</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Brice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">H</forename><surname>Acree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Get out the vote: Determining support or opposition from congressional floor-debate transcripts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 conference on empirical methods in natural language processing</title>
		<meeting>the 2006 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
