<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/ana/installs/grobid/grobid-0.5.1/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.1-SNAPSHOT" ident="GROBID" when="2019-04-18T11:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Controlling Output Length in Neural Encoder-Decoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>November 1-5, 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
							<email>kikuchi@lr.pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
							<email>sasano@pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
							<email>takamura@pi.titech.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tokyo Institute of Technology</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Controlling Output Length in Neural Encoder-Decoders</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
						<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing <address><addrLine>Austin, Texas</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1328" to="1338"/>
							<date type="published">November 1-5, 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. 1 Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Since its first use for machine translation <ref type="bibr" target="#b19">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b4">Cho et al., 2014;</ref><ref type="bibr" target="#b36">Sutskever et al., 2014</ref>), the encoder-decoder ap- proach has demonstrated great success in many other sequence generation tasks including image caption generation ( <ref type="bibr" target="#b39">Vinyals et al., 2015b;</ref><ref type="bibr" target="#b43">Xu et al., 2015</ref>), parsing ( <ref type="bibr" target="#b38">Vinyals et al., 2015a</ref>), dialogue response generation ( <ref type="bibr" target="#b21">Li et al., 2016a;</ref><ref type="bibr" target="#b35">Serban et al., 2016</ref>) and sentence summarization ( <ref type="bibr" target="#b32">Rush et al., 2015;</ref><ref type="bibr" target="#b5">Chopra et al., 2016</ref>). In particular, in this pa- per we focus on sentence summarization, which as its name suggests, consists of generating shorter ver- sions of sentences for applications such as document summarization ( <ref type="bibr" target="#b29">Nenkova and McKeown, 2011</ref>) or headline generation ( <ref type="bibr" target="#b8">Dorr et al., 2003)</ref>. Recently, <ref type="bibr" target="#b32">Rush et al. (2015)</ref> automatically constructed large training data for sentence summarization, and this has led to the rapid development of neural sentence summarization (NSS) or neural headline generation (NHG) models. There are already many studies that address this task ( <ref type="bibr" target="#b0">Ayana et al., 2016;</ref><ref type="bibr" target="#b30">Ranzato et al., 2015;</ref><ref type="bibr" target="#b24">Lopyrev, 2015;</ref><ref type="bibr" target="#b16">Gulcehre et al., 2016;</ref><ref type="bibr" target="#b15">Gu et al., 2016;</ref><ref type="bibr" target="#b5">Chopra et al., 2016</ref>).</p><p>One of the essential properties that text summa- rization systems should have is the ability to gen- erate a summary with the desired length. Desired lengths of summaries strongly depends on the scene of use, such as the granularity of information the user wants to understand, or the monitor size of the device the user has. The length also depends on the amount of information contained in the given source document. Hence, in the traditional setting of text summarization, both the source document and the desired length of the summary will be given as input to a summarization system. However, methods for controlling the output sequence length of encoder- decoder models have not been investigated yet, de- spite their importance in these settings.</p><p>In this paper, we propose and investigate four methods for controlling the output sequence length for neural encoder-decoder models. The former two methods are decoding-based; they receive the de- sired length during the decoding process, and the training process is the same as standard encoder- decoder models.</p><p>The latter two methods are learning-based; we modify the network architecture to receive the desired length as input.</p><p>In experiments, we show that the learning-based methods outperform the decoding-based methods for long (such as 50 or 75 byte) summaries. We also find that despite this additional length-control capability, the proposed methods remain competi- tive to existing methods on standard settings of the DUC2004 shared task-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Text summarization is one of the oldest fields of study in natural language processing, and many summarization methods have focused specifically on sentence compression or headline generation. Traditional approaches to this task focus on word deletion using rule-based ( <ref type="bibr" target="#b8">Dorr et al., 2003;</ref><ref type="bibr" target="#b44">Zajic et al., 2004</ref>) or statistical ( <ref type="bibr" target="#b41">Woodsend et al., 2010;</ref><ref type="bibr" target="#b13">Galanis and Androutsopoulos, 2010;</ref><ref type="bibr" target="#b11">Filippova and Strube, 2008;</ref><ref type="bibr" target="#b10">Filippova and Altun, 2013;</ref><ref type="bibr" target="#b12">Filippova et al., 2015)</ref> methods. There are also several studies of abstractive sentence summarization us- ing syntactic transduction <ref type="bibr" target="#b6">(Cohn and Lapata, 2008;</ref><ref type="bibr" target="#b27">Napoles et al., 2011</ref>) or taking a phrase-based sta- tistical machine translation approach ( <ref type="bibr" target="#b2">Banko et al., 2000;</ref><ref type="bibr">Wubben et al., 2012;</ref><ref type="bibr" target="#b7">Cohn and Lapata, 2013)</ref>.</p><p>Recent work has adopted techniques such as encoder-decoder <ref type="bibr" target="#b19">(Kalchbrenner and Blunsom, 2013;</ref><ref type="bibr" target="#b36">Sutskever et al., 2014;</ref><ref type="bibr" target="#b4">Cho et al., 2014</ref>) and atten- tional ( <ref type="bibr" target="#b1">Bahdanau et al., 2015;</ref><ref type="bibr" target="#b25">Luong et al., 2015</ref>) neural network models from the field of machine translation, and tailored them to the sentence sum- marization task. <ref type="bibr" target="#b32">Rush et al. (2015)</ref> were the first to pose sentence summarization as a new target task for neural sequence-to-sequence learning. Several studies have used this task as one of the bench- marks of their neural sequence transduction meth- ods ( <ref type="bibr" target="#b30">Ranzato et al., 2015;</ref><ref type="bibr" target="#b24">Lopyrev, 2015;</ref><ref type="bibr" target="#b0">Ayana et al., 2016)</ref>. Some studies address the other im- portant phenomena frequently occurred in human- written summaries, such as copying from the source document ( <ref type="bibr" target="#b15">Gu et al., 2016;</ref><ref type="bibr" target="#b16">Gulcehre et al., 2016)</ref>.  investigate a way to solve many important problems capturing keywords, or inputting multiple sentences.</p><p>Neural encoder-decoders can also be viewed as statistical language models conditioned on the tar- get sentence context. <ref type="bibr" target="#b31">Rosenfeld et al. (2001)</ref> have proposed whole-sentence language models that can consider features such as sentence length. However, as described in the introduction, to our knowledge, explicitly controlling length of output sequences in neural language models or encoder-decoders has not been investigated. Finally, there are some studies to modify the out- put sequence according some meta information such as the dialogue act <ref type="bibr" target="#b40">(Wen et al., 2015)</ref>, user person- ality ( <ref type="bibr">Li et al., 2016b</ref>), or politeness ( <ref type="bibr" target="#b34">Sennrich et al., 2016)</ref>. However, these studies have not focused on length, the topic of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Importance of Controlling Output Length</head><p>As we already mentioned in Section 1, the most standard setting in text summarization is to input both the source document and the desired length of the summary to a summarization system. Summa- rization systems thus must be able to generate sum- maries of various lengths. Obviously, this property is also essential for summarization methods based on neural encoder-decoder models.</p><p>Since an encoder-decoder model is a completely data-driven approach, the output sequence length depends on the training data that the model is trained on. For example, we use sentence-summary pairs extracted from the Annotated English Gigaword cor- pus as training data <ref type="bibr" target="#b32">(Rush et al., 2015)</ref>, and the average length of human-written summary is 51.38 bytes. <ref type="figure" target="#fig_0">Figure 1</ref> shows the statistics of the corpus. When we train a standard encoder-decoder model and perform the standard beam search decoding on the corpus, the average length of its output sequence is 38.02 byte.</p><p>However, there are other situations where we want summaries with other lengths. For exam- ple, DUC2004 is a shared task where the maximum length of summaries is set to 75 bytes, and summa- rization systems would benefit from generating sen- tences up to this length limit.</p><p>While recent NSS models themselves cannot con- trol their output length, <ref type="bibr" target="#b32">Rush et al. (2015)</ref> and others following use an ad-hoc method, in which the sys- tem is inhibited from generating the end-of-sentence (EOS) tag by assigning a score of −∞ to the tag and   generating a fixed number of words 2 , and finally the output summaries are truncated to 75 bytes. Ideally, the models should be able to change the output se- quence depending on the given output length, and to output the EOS tag at the appropriate time point in a natural manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network Architecture: Encoder-Decoder with Attention</head><p>In this section, we describe the model architec- ture used for our experiments: an encoder-decoder consisting of bi-directional RNNs and an attention mechanism. <ref type="figure" target="#fig_1">Figure 2</ref> shows the architecture of the model. Suppose that the source sentence is represented as a sequence of words x = (x 1 , x 2 , x 3 , ..., x N ). For 2 According to the published code (https://github.com/facebook/NAMAS), the default num- ber of words is set to 15, which is too long for the DUC2004 setting. The average number of words of human summaries in the evaluation set is 10.43. a given source sentence, the summarizer generates a shortened version of the input (i.e. N &gt; M ), as summary sentence y = (y 1 , y 2 , y 3 , ..., y M ). The model estimates conditional probability p(y|x) us- ing parameters trained on large training data consist- ing of sentence-summary pairs. Typically, this con- ditional probability is factorized as the product of conditional probabilities of the next word in the se- quence:</p><formula xml:id="formula_0">p(y|x) = M ∏ t=1 p(y t |y &lt;t , x),</formula><p>where y &lt;t = (y 1 , y 2 , y 3 , ..., y t−1 ). In the following, we describe how to compute p(y t |y &lt;t , x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>We use the bi-directional RNN (BiRNN) as en- coder which has been shown effective in neural ma- chine translation ( <ref type="bibr" target="#b1">Bahdanau et al., 2015</ref>) and speech recognition <ref type="bibr" target="#b33">(Schuster and Paliwal, 1997;</ref><ref type="bibr" target="#b14">Graves et al., 2013)</ref>.</p><p>A BiRNN processes the source sentence for both forward and backward directions with two separate RNNs. During the encoding process, the BiRNN computes both forward hidden states</p><formula xml:id="formula_1">( − → h 1 , − → h 2 , ..., − → h N ) and backward hidden states ( ← − h 1 , ← − h 2 , ..., ← − h N )</formula><p>as follows:</p><formula xml:id="formula_2">− → h t = g( − → h t−1 , x t ), ← − h t = g( ← − h t+1 , x t ).</formula><p>While g can be any kind of recurrent unit, we use long short-term memory (LSTM) <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997</ref>) networks that have memory cells for both directions ( − → c t and ← − c t ).</p><p>After encoding, we set the initial hidden states s 0 and memory-cell m 0 of the decoder as follows:</p><formula xml:id="formula_3">s 0 = ← − h 1 , m 0 = ← − c 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder and Attender</head><p>Our decoder is based on an RNN with LSTM g:</p><formula xml:id="formula_4">s t = g(s t−1 , x t ).</formula><p>We also use the attention mechanism developed by <ref type="bibr" target="#b25">Luong et al. (2015)</ref>, which uses s t to compute contextual information d t of time step t. We first summarize the forward and backward encoder states by taking their sum ¯ h i = − → h i + ← − h i , and then calcu- late the context vector d t as the weighted sum of these summarized vectors:</p><formula xml:id="formula_5">d t = ∑ i a ti ¯ h i ,</formula><p>where a t is the weight at the t-th step for ¯ h i com- puted by a softmax operation:</p><formula xml:id="formula_6">a ti = exp(s t · ¯ h i ) ∑ ¯ h ′ exp(s t · ¯ h ′ ) .</formula><p>After context vector d t is calculated, the model updates the distribution over the next word as fol- lows:</p><formula xml:id="formula_7">˜ s t = tanh(W hs [s t ; d t ] + b hs ), p(y t |y &lt;t , x) = softmax(W sõ s t + b so ).</formula><p>Note that˜sthat˜ that˜s t is also provided as input to the LSTM with y t for the next step, which is called the input feeding architecture (Luong et al., 2015).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training and Decoding</head><p>The training objective of our models is to maximize log likelihood of the sentence-summary pairs in a given training set D:</p><formula xml:id="formula_8">L t (θ) = ∑ (x,y)∈D log p(y|x; θ), p(y|x; θ) = ∏ t p(y t |y &lt;t , x).</formula><p>Once models are trained, we use beam search to find the output that maximizes the conditional probabil- ity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Controlling Length in Encoder-decoders</head><p>In this section, we propose our four methods that can control the length of the output in the encoder- decoder framework. In the first two methods, the decoding process is used to control the output length without changing the model itself. In the other two methods, the model itself has been changed and is trained to obtain the capability of controlling the length. Following the evaluation dataset used in our experiments, we use bytes as the unit of length, al- though our models can use either words or bytes as necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">f ixLen: Beam Search without EOS Tags</head><p>The first method we examine is a decoding approach similar to the one taken in many recent NSS meth- ods that is slightly less ad-hoc. In this method, we inhibit the decoder from generating the EOS tag by assigning it a score of −∞. Since the model can- not stop the decoding process by itself, we simply stop the decoding process when the length of output sequence reaches the desired length. More specifi- cally, during beam search, when the length of the se- quence generated so far exceeds the desired length, the last word is replaced with the EOS tag and also the score of the last word is replaced with the score of the EOS tag (EOS replacement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">f ixRng: Discarding Out-of-range Sequences</head><p>Our second decoding method is based on discarding out-of-range sequences, and is not inhibited from generating the EOS tag, allowing it to decide when to stop generation. Instead, we define the legitimate range of the sequence by setting minimum and max- imum lengths. Specifically, in addition to the normal beam search procedure, we set two rules:</p><p>• If the model generates the EOS tag when the output sequence is shorter than the minimum length, we discard the sequence from the beam.</p><p>• If the generated sequence exceeds the maxi- mum length, we also discard the sequence from the beam. We then replace its last word with the EOS tag and add this sequence to the beam (EOS replacement in Section 4.1). 3</p><p>In other words, we keep only the sequences that contain the EOS tag and are in the defined length range. This method is a compromise that allows the model some flexibility to plan the generated se- quences, but only within a certain acceptable length range.</p><p>It should be noted that this method needs a larger beam size if the desired length is very different from the average summary length in the training data, as it will need to preserve hypotheses that have the de- sired length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">LenEmb: Length Embedding as Additional Input for the LSTM</head><p>Our third method is a learning-based method specif- ically trained to control the length of the output se- quence. Inspired by previous work that has demon- strated that additional inputs to decoder models can effectively control the characteristics of the output <ref type="bibr" target="#b40">(Wen et al., 2015;</ref><ref type="bibr">Li et al., 2016b</ref>), this model pro- vides information about the length in the form of an additional input to the net. Specifically, the model uses an embedding e 2 (l t ) ∈ R D for each potential desired length, which is parameterized by a length embedding matrix W le ∈ R D×L where L is the number of length types. In the decoding process, we input the embedding of the remaining length l t as additional input to the LSTM <ref type="figure">(Figure 3</ref>). l t is initial- ized after the encoding process and updated during the decoding process as follows:</p><formula xml:id="formula_9">l 1 = length, l t+1 = { 0 (l t − byte(y t ) ≤ 0) l t − byte(y t ) (otherwise),</formula><p>where byte(y t ) is the length of output word y t and length is the desired length. We learn the values of the length embedding matrix W le during train- ing. This method provides additional information about the amount of length remaining in the output sequence, allowing the decoder to "plan" its output based on the remaining number of words it can gen- erate.</p><formula xml:id="formula_10">j t o t i t f t s t mt mt1 st1 e 1 (x t ) e 2 (l t )</formula><p>l t x t <ref type="figure">Figure 3</ref>: LenEmb: remaining length is used as addi- tional input for the LSTM of the decoder.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LenInit: Length-based Memory Cell Initialization</head><p>While LenEmb inputs the remaining length l t to the decoder at each step of the decoding process, the LenInit method inputs the desired length once at the initial state of the decoder. <ref type="figure" target="#fig_3">Figure 4</ref> shows the ar- chitecture of LenInit. Specifically, the model uses the memory cell m t to control the output length by initializing the states of decoder (hidden state s 0 and memory cell m 0 ) as follows:</p><formula xml:id="formula_11">s 0 = ← − h 1 , m 0 = b c * length,<label>(1)</label></formula><p>where b c ∈ R H is a trainable parameter and length is the desired length. While the model of LenEmb is guided towards the appropriate output length by inputting the re- maining length at each step, this LenInit attempts to provide the model with the ability to manage the output length on its own using its inner state. Specif- ically, the memory cell of LSTM networks is suit- able for this endeavour, as it is possible for LSTMs to learn functions that, for example, subtract a fixed amount from a particular memory cell every time they output a word. Although other ways for man- aging the length are also possible, <ref type="bibr">4</ref> we found this approach to be both simple and effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset</head><p>We trained our models on a part of the Annotated English Gigaword corpus ( <ref type="bibr" target="#b28">Napoles et al., 2012)</ref>, which Rush et al. <ref type="formula" target="#formula_11">(2015)</ref> constructed for sentence summarization. We perform preprocessing using the standard script for the dataset <ref type="bibr">5</ref> . The dataset con- sists of approximately 3.6 million pairs of the first sentence from each source document and its head- line. <ref type="figure" target="#fig_0">Figure 1</ref> shows the length histograms of the summaries in the training set. The vocabulary size is 116,875 for the source documents and 67,564 for the target summaries including the beginning-of- sentence, end-of-sentence, and unknown word tags. For LenEmb and LenInit, we input the length of each headline during training. Note that we do not train multiple summarization models for each head- line length, but a single model that is capable of con- trolling the length of its output. We evaluate the methods on the evaluation set of DUC2004 task-1 (generating very short single- document summaries). In this task, summarization systems are required to create a very short sum- mary for each given document. Summaries over the length limit (75 bytes) will be truncated and there is no bonus for creating a shorter summary. The evaluation set consists of 500 source documents and 4 human-written (reference) summaries for each source document. <ref type="figure" target="#fig_4">Figure 5</ref> shows the length his- tograms of the summaries in the evaluation set. Note that the human-written summaries are not always as long as 75 bytes. We used three variants of ROUGE <ref type="bibr" target="#b23">(Lin, 2004</ref>) as evaluation metrics: ROUGE-1 (uni- gram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence). The two-sided permutation test <ref type="bibr" target="#b3">(Chinchor, 1992)</ref> was used for statistical signif- icance testing (p ≤ 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation</head><p>We use Adam ( <ref type="bibr" target="#b20">Kingma and Ba, 2015</ref>) (α=0.001, β 1 =0.9, β 2 =0.999, eps=10 −8 ) to optimize param- eters with a mini-batch of size 80. Before every 10,000 updates, we first sampled 800,000 training examples and made groups of 80 examples with the same source sentence length, and shuffled the 10,000 groups.</p><p>We set the dimension of word embeddings to 100 and that of the hidden state to 200. For LSTMs, we initialize the bias of the forget gate to 1.0 and use 0.0 for the other gate biases <ref type="bibr">(Józefowicz et al., 2015)</ref>. We use Chainer ( <ref type="bibr" target="#b37">Tokui et al., 2015</ref>) to im- plement our models. For LenEmb, we set L to 300, which is larger than the longest summary lengths in our dataset (see <ref type="figure" target="#fig_0">Figure 1</ref>-(b) and <ref type="figure" target="#fig_4">Figure 5-(b)</ref>).</p><p>For all methods except f ixRng, we found a beam size of 10 to be sufficient, but for f ixRng we used a beam size of 30 because it more aggressively dis- cards candidate sequences from its beams during de- coding. <ref type="table">Table 1</ref> shows the ROUGE scores of each method with various length limits (30, 50 and 75 byte). Re- gardless of the length limit set for the summariza-   <ref type="table">Table 1</ref>: ROUGE scores with various length limits. The scores with * are significantly worse than the best score in the column (bolded).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">ROUGE Evaluation</head><note type="other">byte 50 byte 75 byte model R</note><formula xml:id="formula_12">-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L f ixLen</formula><note type="other">13.75 3.30 12.68 20.62 6.22 18.64 26.42 8.26 23.59 LenInit (0,∞) 13.92 3.49 12.90 20.87 6.19 19.09 25.29 * 8.00 22.71 *</note><p>source five-time world champion michelle kwan withdrew from the #### us figure skating championships on wednesday , but will petition us skating officials for the chance to compete at the #### turin olympics . reference injury leaves kwan 's olympic hopes in limbo f ixLen <ref type="formula">(30)</ref> kwan withdraws from us gp <ref type="formula">(50)</ref> kwan withdraws from us skating championships <ref type="formula">(75)</ref> kwan pulls out of us figure skating championships for turin olympics f ixRng <ref type="formula">(30)</ref> kwan withdraws from us gp <ref type="formula">(50)</ref> kwan withdraws from figure skating championships <ref type="formula">(75)</ref> kwan pulls out of us figure skating championships for turin olympics bid LenEmb <ref type="formula">(30)</ref> kwan withdraws from us skating (50) kwan withdraws from us figure skating championships <ref type="formula">(75)</ref> world champion kwan withdraws from #### olympic figure skating championships LenInit <ref type="formula">(30)</ref> kwan quits us figure skating (50) kwan withdraws from #### us figure skating worlds <ref type="formula">(75)</ref> kwan withdraws from #### us figure skating championships for #### olympics tion methods, we use the same reference summaries. Note that, f ixLen and f ixRng generate the sum- maries with a hard constraint due to their decod- ing process, which allows them to follow the hard constraint on length. Hence, when we calculate the scores of LenEmb and LenInit, we impose a hard constraint on length to make the comparison fair (i.e. LenEmb (0,L) and LenInit (0,L) in the table). Specifically, we use the same beam search as that for f ixRng with minimum length of 0. For the purpose of showing the length control capability of LenEmb and LenInit, we show at the bottom two lines the results of the standard beam search without the hard constraints on the length 6 . We will use the results of LenEmb (0,∞) and LenInit (0,∞) in the discussions in Sections 6.2 and 6.3.</p><p>The results show that the learning-based meth- 6 f ixRng is equivalence to the standard beam search when we set the range as (0, ∞).</p><p>ods (LenEmb and LenInit) tend to outperform decoding-based methods (f ixLen and f ixRng) for the longer summaries of 50 and 75 bytes. How- ever, in the 30-byte setting, there is no significant difference between these two types of methods. We hypothesize that this is because average compres- sion rate in the training data is 30% <ref type="figure" target="#fig_0">(Figure 1-(c)</ref>) while the 30-byte setting forces the model to gen- erate summaries with 15.38% in average compres- sion rate, and thus the learning-based models did not have enough training data to learn compression at such a steep rate. <ref type="table" target="#tab_3">Tables 2 and 3</ref> show examples from the validation set of the Annotated Gigaword Corpus. The ta- bles show that all models, including both learning- based methods and decoding-based methods, can of- ten generate well-formed sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Examples of Generated Summaries</head><p>We can see various paraphrases of "#### us <ref type="figure">figure source</ref> at least two people have tested positive for the bird flu virus in eastern turkey , health minister recep akdag told a news conference wednesday . reference two test positive for bird flu virus in turkey f ixLen <ref type="formula">(30)</ref> two infected with bird flu (50) two infected with bird flu in eastern turkey <ref type="formula">(75)</ref> two people tested positive for bird flu in eastern turkey says minister f ixRng <ref type="formula">(30)</ref> two infected with bird flu (50) two more infected with bird flu in eastern turkey <ref type="formula">(75)</ref> two people tested positive for bird flu in eastern turkey says minister LenEmb <ref type="formula">(30)</ref> two bird flu cases in turkey <ref type="formula">(50)</ref> two confirmed positive for bird flu in eastern turkey <ref type="formula">(75)</ref> at least two bird flu patients test positive for bird flu in eastern turkey LenInit <ref type="formula">(30)</ref> two cases of bird flu in turkey <ref type="formula">(50)</ref> two people tested positive for bird flu in turkey <ref type="formula">(75)</ref> two people tested positive for bird flu in eastern turkey health conference <ref type="table">Table 3</ref>: More examples of the output of each method.</p><p>championships" 7 and "withdrew". Some examples are generated as a single noun phrase ( <ref type="bibr">LenEmb(30)</ref> and LenInit <ref type="formula">(30)</ref>) which may be suitable for the short length setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Length Control Capability of</head><p>Learning-based Models <ref type="figure" target="#fig_6">Figure 6</ref> shows histograms of output length from the standard encoder-decoder, LenEmb, and LenInit. While the output lengths from the standard model disperse widely, the lengths from our learning-based models are concentrated to the desired length. These histograms clearly show the length controlling capa- bility of our learning-based models. <ref type="table">Table 4</ref>-(a) shows the final state of the beam when LenInit generates the sentence with a length of 30 bytes for the example with standard beam search in <ref type="table">Table 3</ref>. We can see all the sentences in the beam are generated with length close to the desired length. This shows that our method has obtained the ability to control the output length as expected. For com- parison, <ref type="table">Table 4</ref>-(b) shows the final state of the beam if we perform standard beam search in the stan- dard encoder-decoder model (used in f ixLen and f ixRng). Although each sentence is well-formed, the lengths of them are much more varied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison with Existing Methods</head><p>Finally, we compare our methods to existing meth- ods on standard settings of the DUC2004 shared <ref type="bibr">7</ref> Note that "#" is a normalized number and "us" is "US" (United States). task-1. Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is of interest whether our length-controllable models are competitive on this task. <ref type="table" target="#tab_5">Table 5</ref> shows that the scores of our methods, which are copied from <ref type="table">Table  1</ref>, in addition to the scores of some existing methods. ABS ( <ref type="bibr" target="#b32">Rush et al., 2015)</ref> is the most standard model of neural sentence summarization and is the most similar method to our baseline setting (f ixLen). This table shows that the score of f ixLen is com- parable to those of the existing methods. The table also shows the LenEmb and the LenInit have the capability of controlling the length without decreas- ing the ROUGE score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we presented the first examination of the problem of controlling length in neural encoder- decoder models, from the point of view of sum- marization. We examined methods for controlling length of output sequences: two decoding-based methods (f ixLen and f ixRng) and two learning- based methods ( <ref type="bibr">LenEmb and LenInit)</ref>. The re- sults showed that learning-based methods generally outperform the decoding-based methods, and the learning-based methods obtained the capability of controlling the output length without losing ROUGE score compared to existing summarization methods. <ref type="bibr">logp(y|x)</ref> byte candidate summary -4. <ref type="bibr">27</ref> 31 two cases of bird flu in turkey -4. <ref type="bibr">41</ref> 28 two bird flu cases in turkey -4. <ref type="bibr">65</ref> 30 two people tested for bird flu -5. <ref type="bibr">25</ref> 30 two people tested in e. turkey -5. <ref type="bibr">27</ref> 31 two bird flu cases in e. turkey -5. <ref type="bibr">51</ref> 29 two bird flu cases in eastern -5.55 32 two people tested in east turkey -5. <ref type="bibr">72</ref> 30 two bird flu cases in turkey : -6.04 30 two people fail bird flu virus (a) the beam of LenInit logp(y|x) byte candidate summary -5.05 57 two people tested positive for bird flu in eastern turkey -5.13 50 two tested positive for bird flu in eastern turkey - <ref type="table" target="#tab_3">5.30  39  two people tested positive for bird flu  -5.49  51  two people infected with bird flu in eastern turkey  -5.52  32  two tested positive for bird flu  -5.55  44  two infected with bird flu in eastern turkey  -6.00</ref> 49 two more infected with bird flu in eastern turkey -6.04 54 two more confirmed cases of bird flu in eastern turkey -6.50 49 two people tested positive for bird flu in turkey (b) the beam of the standard encoder-decoder <ref type="table">Table 4</ref>: Final state of the beam when the learning-based model is instructed to output a 30 byte summary for the source document in <ref type="table">Table 3</ref>.</p><p>(a) encoder-decoder LenInit. For LenEmb and LenInit, the bracketed numbers in each region are the desired lengths we set.  <ref type="bibr" target="#b32">(Rush et al., 2015)</ref> 26.55 7.06 22.05 ABS+ <ref type="bibr" target="#b32">(Rush et al., 2015)</ref> 28.18 8.49 23.81 RAS-Elman <ref type="bibr" target="#b5">(Chopra et al., 2016)</ref> 28.97 8.26 24.06 RAS-LSTM <ref type="bibr" target="#b5">(Chopra et al., 2016)</ref> 27.41 7.69 23.06 opportunity to use the Kurisu server of Dwango Co., Ltd. for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>model</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Histograms of first sentence length, headline length, and their ratio in Annotated Gigaword English Giga- word corpus. Bracketed values in each subcaption are averages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The encoder-decoder architecture we used as a base model in this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: LenInit: initial state of the decoder's memory cell m 0 manages output length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Histograms of first sentence length, summary length, and their ratio in DUC2004.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Histograms of output lengths generated by (a) the standard encoder-decoder , (b) LenEmb, and (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Examples of the output of each method with various specified lengths.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison with existing studies for 
DUC2004. 
Note that top four rows are 
reproduced from Table 1. 

</table></figure>

			<note place="foot" n="3"> This is a workaround to prevent the situation in which all sequences are discarded from a beam.</note>

			<note place="foot" n="4"> For example, we can also add another memory cell for managing the length. 5 https://github.com/facebook/NAMAS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by JSPS KAKENHI Grant Number JP26280080. We are grateful to have the</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural Headline Generation with Minimum Risk Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno>abs/1604.01904</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR15</title>
		<meeting>ICLR15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Headline generation based on statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vibhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Witbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL00</title>
		<meeting>ACL00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="318" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The statistical significance of the muc-4 results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nancy</forename><surname>Chinchor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings MUC4 &apos;92</title>
		<meeting>MUC4 &apos;92</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="30" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP14</title>
		<meeting>the EMNLP14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Abstractive sentence summarization with attentive recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT16</title>
		<meeting>NAACL-HLT16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sentence compression beyond word deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING08</title>
		<meeting>COLING08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An abstractive approach to sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>41:1-41:35</idno>
	</analytic>
	<monogr>
		<title level="j">ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hedge trimmer: A parse-and-trim approach to headline generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the HLT-NAACL 03</title>
		<meeting>the HLT-NAACL 03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overcoming the lack of parallel data in sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP13</title>
		<meeting>EMNLP13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1481" to="1491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dependency tree based sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INLG08</title>
		<meeting>INLG08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sentence compression by deletion with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Alfonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">A</forename><surname>Colmenares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP15</title>
		<meeting>EMNLP15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An extractive supervised two-stage method for sentence compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT10</title>
		<meeting>NAACL-HLT10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="885" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Workshop on ASRU13</title>
		<meeting>IEEE Workshop on ASRU13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incorporating copying mechanism in sequenceto-sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">O K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL16</title>
		<meeting>ACL16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1631" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointing the unknown words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL16</title>
		<meeting>ACL16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="140" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML15</title>
		<meeting>ICML15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics</title>
		<meeting><address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP13</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR15</title>
		<meeting>ICLR15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT16</title>
		<meeting>NAACL-HLT16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Georgios Spithourakis, Jianfeng Gao, and Bill Dolan. 2016b. A persona-based neural conversation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL16</title>
		<meeting>ACL16</meeting>
		<imprint>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL04 Workshop</title>
		<meeting>the ACL04 Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generating news headlines with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<idno>abs/1512.01712</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP15</title>
		<meeting>EMNLP15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence rnns for text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1602.06023</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Paraphrastic sentence compression with a character-based metric: Tightening without deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Monolingual Text-To-Text Generation</title>
		<meeting>the Workshop on Monolingual Text-To-Text Generation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Annotated gigaword</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</title>
		<meeting>the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="95" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends R ⃝ in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="103" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Marc&amp;apos;aurelio Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zaremba</surname></persName>
		</author>
		<idno>abs/1511.06732</idno>
		<title level="m">Sequence level training with recurrent neural networks. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Whole-sentence exponential language models: a vehicle for linguistic-statistical integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="73" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP15</title>
		<meeting>EMNLP15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Controlling politeness in neural machine translation via side constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACLHLT16</title>
		<meeting>NAACLHLT16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Iulian Vlad Serban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI16</title>
		<meeting>AAAI16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3776" to="3784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS14</title>
		<meeting>NIPS14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chainer: a next-generation open source framework for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiya</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS15 Workshop on LearningSys</title>
		<meeting>NIPS15 Workshop on LearningSys</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Shohei Hido, and Justin Clayton</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS15</title>
		<meeting>NIPS15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milica</forename><surname>Tsung-Hsien Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Mrkši´mrkši´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Portugal, September. Association for Computational Linguistics</title>
		<meeting><address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP15</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Title generation with quasi-synchronous grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP10</title>
		<meeting>the EMNLP10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL12</title>
		<meeting>ACL12</meeting>
		<imprint>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<editor>David Blei and Francis Bach</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
	<note>Proceedings of ICML15</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bbn/umd at duc-2004: Topiary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonnie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT04 Document Understanding Workshop</title>
		<meeting>NAACL-HLT04 Document Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
